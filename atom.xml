<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PLM&#39;s Notes</title>
  
  <subtitle>好好学习，天天笔记</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://plmsmile.github.io/"/>
  <updated>2018-05-16T03:34:28.776Z</updated>
  <id>http://plmsmile.github.io/</id>
  
  <author>
    <name>PLM</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RNet Gated Self-Matching Networks</title>
    <link href="http://plmsmile.github.io/2018/05/15/46-rnet-selfmatch/"/>
    <id>http://plmsmile.github.io/2018/05/15/46-rnet-selfmatch/</id>
    <published>2018-05-15T02:46:49.000Z</published>
    <updated>2018-05-16T03:34:28.776Z</updated>
    
    <content type="html"><![CDATA[<blockquote><ol style="list-style-type: decimal"><li>Gated Attention-based RNN 来获得question-aware passage representation，即编码P</li><li>Self-matching Attention来修正编码P，即P与自己做match，有效从全文中编码信息</li><li>Pointer Network预测开始和结束位置</li></ol></blockquote><p>论文地址：</p><ul><li><a href="http://www.aclweb.org/anthology/P17-1018" target="_blank" rel="noopener">Gated Self-Matching Networks for Reading Comprehension and Question Answering</a></li><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf" target="_blank" rel="noopener">R-NET: MACHINE READING COMPREHENSION WITH SELF-MATCHING NETWORKS</a></li></ul><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="introduction">Introduction</h1><h2 id="经典模型">经典模型</h2><p><strong>1. Match-LSTM</strong></p><p><a href="https://plmsmile.github.io/2018/05/09/45-match-lstm/">Match-LSTM and Answer Pointer笔记</a></p><p><strong>2. Dynamic Coatteion Network</strong></p><p><a href="https://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/">DCN笔记</a>。<a href="https://plmsmile.github.io/2018/03/14/31-co-attention-vqa/">Coattention</a>同时处理P和Q，动态迭代预测答案的位置。</p><p><strong>3. Bi-Directional Attention Flow Network</strong></p><h2 id="本文模型概要">本文模型概要</h2><p><strong>1. BiRNN 分别编码P和Q</strong></p><p>分别编码Question和Passage</p><p><strong>2. gated matching layer 编码Q-Aware的Passage</strong></p><p><code>Gated Attention-based RNN</code>。在<a href="https://plmsmile.github.io/2018/05/09/45-match-lstm/#match-lstm">Match-LSTM</a>上添加了<strong>门机制</strong>。</p><ul><li>段落有多个部分，根据与Q的相关程度，分配重要性权值</li><li>忽略不重要的，强调重要的部分</li></ul><p><strong>3. self-matching layer</strong></p><p>再次从整个Passage中提取信息。它的<code>缺点</code>：</p><ul><li>RNN只能存储少部分上下文内容</li><li>一个候选答案不知道其他部分的线索</li></ul><p>解决方法：<strong>对P做self-match</strong>。使用Gated Attention-based RNN对P和P自己做match。</p><p><strong>4. pointer-network</strong></p><h1 id="模型">模型</h1><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/papers/rnet.png" style="display:block; margin:auto" width="60%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/papers/gan-selfmatch.png" style="display:block; margin:auto" width="60%"></p><p>BiRNN，GARNN（P+Q），GARNN-Selfmatch（P+P），Pointer Network</p><h2 id="birnn编码q和p">BiRNN编码Q和P</h2><p><span class="math inline">\(Q=\{w_t^Q\}_{t=1}^m\)</span>，<span class="math inline">\(P=\{w_t^P\}_{t=1}^n\)</span>。 P是n个单词，Q是m个单词。</p><p><strong>词向量和字符向量</strong></p><p><code>词向量</code>：<span class="math inline">\(\{e_t^Q\}_{t=1}^m\)</span>、<span class="math inline">\(\{e_t^P\}_{t=1}^n\)</span></p><p><code>字符向量</code>：<span class="math inline">\(\{c_t^Q\}_{t=1}^m\)</span>、<span class="math inline">\(\{c_t^P\}_{t=1}^n\)</span></p><p>字符向量，使用RNN，用每个单词的最后时刻的隐状态，作为字符向量。有助于处理OOV词汇。</p><p><strong>编码</strong> <span class="math display">\[\mathbf u_t^Q = \rm{BiRNN }(u_{t-1}^Q, [e_t^Q, c_t^Q])\]</span></p><p><span class="math display">\[\mathbf u_t^P = \rm{BiRNN }(u_{t-1}^P, [e_t^P, c_t^P])\]</span></p><h2 id="gated-attention-based-rnn">Gated Attention-based RNN</h2><p>要基于U（<span class="math inline">\(U^Q\)</span>）去编码P（<span class="math inline">\(U^P\)</span>） ，得到<strong>Question-Aware的Passage编码</strong>，<span class="math inline">\(V^P\)</span>。</p><p><strong>1. Attention RNN</strong></p><p><span class="math inline">\(p_t\)</span>与<span class="math inline">\(q_j\)</span>两个单词的<strong>相关性函数</strong>（能量函数） <span class="math display">\[s_j^t = v^T \tanh (W_u^Q\mathbf u_j^Q + W_u^P \mathbf u_t^P + W_v^P \mathbf v_{t-1}^P), \quad j = 1, \cdots, m\]</span> <span class="math inline">\(p_t\)</span>与所有Q单词的<strong>注意力权值</strong><span class="math inline">\(\mathbf \alpha^t\)</span> <code>doc 2 query attention</code> <span class="math display">\[\alpha_{j}^t = \rm{softmax}(s_j^t)\]</span> <span class="math inline">\(p_t\)</span>基于<span class="math inline">\(\mathbf \alpha^t\)</span><strong>对<span class="math inline">\(Q\)</span>的信息汇总</strong>（注意力）<code>attention pooling vector of the whole question</code> <span class="math display">\[\mathbf c_t = \sum_{i=1}^m \alpha_i^t \mathbf u_i^Q\]</span> 实际上：<span class="math inline">\(\mathbf c_t = \rm{attn}(U^Q, [\mathbf u_t^P, \mathbf v_{t-1}^P])​\)</span>。</p><p>注意力<strong>$c_t <span class="math inline">\(**和上一时刻隐状态**\)</span>v_{t-1}^P$</strong>，输入RNN，<strong>计算当前的信息</strong> <span class="math display">\[\mathbf v_t^P = \rm{RNN}(\mathbf v_{t-1}^P, \mathbf c_t)\]</span> 每个<span class="math inline">\(\mathbf v_t^P\)</span>动态地合并了来自整个Q的匹配信息。</p><p><strong>2. Match RNN</strong></p><p><a href="https://plmsmile.github.io/2018/05/09/45-match-lstm/#match-lstm">Match-LSTM</a>。在输入RNN计算时，把当前<span class="math inline">\(\mathbf u_t^P\)</span>也输入进去，带上Passage的信息。输入是<strong><span class="math inline">\(\rm{input}=[\mathbf u_t^P, \mathbf c_t]\)</span></strong>。 <span class="math display">\[\mathbf v_t^P = \rm{RNN}(\mathbf v_{t-1}^P, [\mathbf u_t^P, \mathbf c_t])\]</span> <strong>3. Gated Attention-based RNN</strong></p><p>用门机制去控制每个<span class="math inline">\(p_t\)</span>的重要程度。 <span class="math display">\[g_t = \rm{sigmoid}(W_g \cdot [\mathbf u_t^P, \mathbf c_t])\]</span></p><p><span class="math display">\[[\mathbf u_t^P, \mathbf c_t]^* = g_t \odot [\mathbf u_t^P, \mathbf c_t]\]</span></p><p><span class="math display">\[\mathbf v_t^P = \rm{RNN}(\mathbf v_{t-1}^P, [\mathbf u_t^P, \mathbf c_t]^*)\]</span></p><p>GARNN的<strong>门机制</strong></p><ul><li>与GRU和LSTM不同</li><li>门机制是基于<strong>当前<span class="math inline">\(p_t\)</span></strong>和它的对应的Q的<strong>注意力向量<span class="math inline">\(\mathbf c_t\)</span></strong>（包含当前<span class="math inline">\(p_t\)</span>和Q的关系）</li><li><code>模拟</code>了阅读理解中，<strong>只有<span class="math inline">\(P\)</span>的一部分才与问题相关</strong>的特点</li></ul><p>最终得到了<code>question-aware passage representation</code> ：<span class="math inline">\(\{\mathbf v_t^P\}_{t=1}^n\)</span>。它的缺点如下：</p><ul><li>对Passage的上下文感知太少</li><li>候选答案对它窗口之外的线索未知</li><li>Question和Passage在词法、句法上有区别</li></ul><h2 id="self-matching-attention">Self-Matching Attention</h2><p>为了充分利用Passage的上下文信息。<span class="math inline">\(\{\mathbf v_t^P\}_{t=1}^n\)</span></p><p><strong>对P做self-match</strong>。使用Gated Attention-based RNN对P和P自己做match。</p><p>注意力计算 <span class="math display">\[s_j^t = v^T \tanh (W_v^P \mathbf v_j^P + W_v^{\bar P} \mathbf v_t^P), \quad j = 1, \cdots, n\]</span></p><p><span class="math display">\[\alpha_{j}^t = \rm{softmax}(s_j^t)\]</span></p><p><span class="math display">\[\mathbf c_t = \sum_{i=1}^n \alpha_i^t \mathbf v_i^P\]</span></p><p>RNN计算 <span class="math display">\[\mathbf h_t^P = \rm{BiRNN}(\mathbf h_{t-1}^P, [\mathbf v_t^P, \mathbf c_t]^*)\]</span> Self-Matching根据当前p单词、Q，从整个Passage中提取信息。<strong>最终得到Passage的表达<span class="math inline">\(H^P\)</span></strong>。</p><h2 id="output-layer">Output Layer</h2><p>其实就是个<a href="https://plmsmile.github.io/2018/05/09/45-match-lstm/#answer-pointer%E5%B1%82">Pointer Network的边界模型</a>，预测起始和结束位置。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;ol style=&quot;list-style-type: decimal&quot;&gt;
&lt;li&gt;Gated Attention-based RNN 来获得question-aware passage representation，即编码P&lt;/li&gt;
&lt;li&gt;S
      
    
    </summary>
    
      <category term="机器阅读理解" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器阅读" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>Match-LSTM and Answer Pointer</title>
    <link href="http://plmsmile.github.io/2018/05/09/45-match-lstm/"/>
    <id>http://plmsmile.github.io/2018/05/09/45-match-lstm/</id>
    <published>2018-05-09T06:20:56.000Z</published>
    <updated>2018-05-15T07:11:40.345Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1608.07905" target="_blank" rel="noopener">Machine Comprehension Using Match-LSTM and Answer Pointer</a></p><h1 id="背景">背景</h1><h2 id="阅读理解任务">阅读理解任务</h2><p>后面会详细补充。</p><p>传统解决问答的方法：语言分析、特征工程等，具体包括句法分析、命名实体识别、问题分类、语义分析等。</p><h2 id="squad数据集">Squad数据集</h2><ul><li>答案是文章中出现的任意长度片段</li><li>Wiki文章为主</li><li>众包人工标注产生</li><li>每个问题3人标注，降低了人工标注误差</li><li>数量较大：500多篇文章，2万多个段落，10万个问题</li><li>鼓励用自己的语言提问</li></ul><h2 id="match-lstm">Match-LSTM</h2><p><strong>1. 文本蕴含任务</strong></p><p>一个前提集合P，一个假设H。去预测P里是否能蕴含出H。</p><p><strong>2. Match-LSTM</strong></p><p>有K个前提<span class="math inline">\(\{P_1, \cdots, P_K\}\)</span>，1个假设<span class="math inline">\(H\)</span>。假设的长度为m。遍历假设的每一个词汇<span class="math inline">\(h_i\)</span></p><ul><li><p>在<span class="math inline">\(h_i\)</span>处，利用注意力机制，综合K个前提，得到一个向量<span class="math inline">\(p_i\)</span></p></li><li><p>聚合匹配<span class="math inline">\([h_i, p_i]\)</span>一起，给到LSTM</p></li></ul><p>其实类似于Attention-Based NMT的解码过程。</p><h2 id="pointer-net">Pointer-Net</h2><p>从一个输入序列中，选择一个位置作为输出。</p><ul><li>序列模型：选择多个位置，就组成一个序列</li><li>边界模型：选择开始和结束位置，中间的片段是答案</li></ul><h1 id="模型">模型</h1><p>段落<span class="math inline">\(P\)</span>有m个单词，问题<span class="math inline">\(Q\)</span>有n个单词。</p><h2 id="lstm编码层">LSTM编码层</h2><p>单向LSTM编码 <span class="math display">\[H^p = \rm{LSTM}(P), \quad H^q = \rm{LSTM}(Q)\]</span> 取每一时刻的隐状态，得到对文章和问题的编码。<span class="math inline">\(H^p \in \mathbb R^{m \times h}, H^q \in \mathbb R^{n \times h}\)</span>。<span class="math inline">\(h\)</span>是编码的维度。</p><h2 id="match-lstm层">Match-LSTM层</h2><p>这一层实际上是一个LSTM，<strong>输入依次是P中的各个单词<span class="math inline">\(p_i\)</span></strong>。每一时刻，利用注意力机制计算相对应的Q的编码。</p><p><strong>问题--前提，段落--假设，看问题蕴含P的哪些部分</strong>。</p><p>先计算<code>注意力权值</code> <span class="math display">\[\overrightarrow{ G_i} = \tanh (W^qH^q + (W^p\mathbf h_i^p + W^r \overrightarrow{\mathbf h_{i-1}^r} + \mathbf b^p) \otimes \mathbf e_Q)\]</span></p><p><span class="math display">\[\overrightarrow{ \mathbf \alpha_i} = \rm{softmax}(\mathbf w^T  \overrightarrow{ G_i} + b \otimes \mathbf e_Q)\]</span></p><p>利用注意力机制，计算所有Q基于当前<span class="math inline">\(p_i\)</span>的<code>注意力</code>，把<strong>注意力和<span class="math inline">\(\mathbf h_i^p\)</span>拼接起来</strong> <span class="math display">\[\overrightarrow {\mathbf z_i} = [\mathbf h_i^p, \underbrace{H^q \overrightarrow{ \mathbf \alpha_i}}_{\color{blue}{\rm{attention}}}]\]</span> 把match后的结果，输入到LSTM， <span class="math display">\[\overrightarrow {\mathbf h_i^r} = \rm{LSTM}(\overrightarrow {\mathbf z_i}, \overrightarrow {\mathbf h_{i-1}^r})\]</span> 定义从右向左，得到<span class="math inline">\(\overleftarrow {\mathbf h_i^r}\)</span>。最终，拼接两个方向的向量，得到 <span class="math display">\[H^r = [\overrightarrow{H^r}, \overleftarrow{H^r}] \quad \in \mathbb R^{m \times 2h}\]</span></p><h2 id="answer-pointer层">Answer-Pointer层</h2><p>输入Match-LSTM层对Passage的编码结果<span class="math inline">\(H^r\)</span>，输出一个序列。</p><p><strong>序列模型</strong></p><p>不断生成一个序列<span class="math inline">\(\mathbf a = (a_1, a_2, \cdots)\)</span>，表示P中的位置。</p><p>在P的末尾设置一个停止标记，如果选择它，则停止迭代。新的<span class="math inline">\(\bar H^r \in \mathbb R^{(m+1) \times 2h}\)</span></p><p>1、计算<strong>注意力权值</strong><span class="math inline">\(\mathbf \beta_k\)</span>，<span class="math inline">\(\beta_{k,j}\)</span>表示，选<span class="math inline">\(p_j\)</span>作为<span class="math inline">\(a_k\)</span>的概率 <span class="math display">\[F_k = \tanh(V \bar H^r + (W^a \mathbf h_{k-1}^a + \mathbf b^a) \otimes \mathbf e_{(m+1)})\]</span></p><p><span class="math display">\[\mathbf \beta_k = \rm{softmax}(\mathbf v^TF_k + \mathbf c \otimes \mathbf e_{(m+1)})\]</span></p><p>2、使用<strong>注意力机制</strong>得到<strong>当前时刻需要的<span class="math inline">\(H^r\)</span>的信息</strong>，结合<strong>上一时刻的隐状态</strong>，输入到LSTM中 <span class="math display">\[\mathbf h_k^a = \overrightarrow{\rm{LSTM}} ( \underbrace{\bar H^r \mathbf \beta_k^T}_{\color{blue}{\rm{attention}}}, \mathbf h_{k-1}^r)\]</span> 答案的概率计算如下： <span class="math display">\[p(\mathbf a \mid H^r) = \prod_{k} p(a_k \mid a_1, \cdots, a_{k-1}, H^r)\]</span></p><p><span class="math display">\[p(a_k = j \mid a_1, \cdots, a_{k-1}, H^r) = \beta_{k,j}\]</span></p><p>目标函数： <span class="math display">\[- \sum_{n=1}^N \log p(\mathbf a_n \mid P_n, Q_n)\]</span> <strong>边界模型</strong></p><p>不用预测完整的序列，只<strong>预测开始和结束位置</strong>就可以了。 <span class="math display">\[p(\mathbf a \mid H^r) = p(a_s \mid H^r) \cdot p(a_e \mid a_s, H^r)\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.07905&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Machine Comprehension Using Match-LSTM and Answer Pointer&lt;/a&gt;&lt;/
      
    
    </summary>
    
      <category term="机器阅读理解" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="机器阅读" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB/"/>
    
      <category term="Match-LSTM" scheme="http://plmsmile.github.io/tags/Match-LSTM/"/>
    
      <category term="Pointer Net" scheme="http://plmsmile.github.io/tags/Pointer-Net/"/>
    
  </entry>
  
  <entry>
    <title>强化学习在NLP中的应用</title>
    <link href="http://plmsmile.github.io/2018/05/03/44-reinforce-nlp/"/>
    <id>http://plmsmile.github.io/2018/05/03/44-reinforce-nlp/</id>
    <published>2018-05-03T06:00:55.000Z</published>
    <updated>2018-05-03T10:00:19.574Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="阿里小蜜的任务型问答">阿里小蜜的任务型问答</h1><p>小蜜包含<code>QA问答</code>、<code>开放域聊天</code>、<code>任务型对话</code>。</p><h2 id="任务型对话">任务型对话</h2><blockquote><p>1、TaskBot：由任务驱动的多轮对话，每一轮去读取用户的slot信息，直到槽填满，全部ok</p><p>2、Action Policy：强化学习去管理多轮对话，小蜜每一轮给出一个动作，询问用户或者完成订单</p><p>3、Belief Tracker：深度学习去提取slot信息，LSTM-CRF标注</p></blockquote><p><strong>1. TaskBot</strong></p><p>任务型对话是指<strong>由任务驱动的多轮对话</strong>。在对话中帮助用户完成某个任务，比如订机票、订酒店等。</p><ul><li><code>传统</code>：用<a href="https://plmsmile.github.io/2018/05/02/43-intent-detection-slot-filling/">slot filling</a>来做，但需要大量人工模板、规则和训练语料</li><li><code>小蜜</code>：基于<strong>强化学习</strong>和<strong>Neural Belief Tracker</strong>的<strong>端到端</strong>可训练的TaskBot方案</li></ul><p>在每轮对话中，都需要抽取用户当前给的<strong>slot状态</strong>（<strong>任务需要的组件信息</strong>）。不断地去填满所有的slot，最后去下单。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/alime/vslots.png" style="display:block; margin:auto" width="60%"></p><p><strong>2. Action Policy - 强化学习</strong></p><p>系统如何给用户合适的回复：接着询问用户、出订单。使用<a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/">强化学习</a>去<strong>管理这个多轮对话</strong>。各个定义如下：</p><ul><li>智能体：小蜜（系统）</li><li><strong>策略</strong>：小蜜给用户的回答，反问哪个slot、出订单</li><li>环境：用户</li><li><strong>状态</strong>：用户回答中提取出的slot状态（信息）</li><li><strong>反馈</strong>：继续聊天、退出、下单</li></ul><p><strong>3. Belief Tracker - 深度学习</strong></p><p>Belief Tracker用来提取用户的slot状态，实际是一个序列标注问题。使用<code>LSTM-CRF</code>进行标注。传统是<a href="%5Bslot%20filling%5D(https://plmsmile.github.io/2018/05/02/43-intent-detection-slot-filling/)">slot filling</a></p><h2 id="系统结构">系统结构</h2><p>系统分为下面三层。</p><ul><li><code>数据预处理层</code> ： 分词、实体抽取等。</li><li><code>端到端的对话管理层</code> ：强化学习</li><li><code>任务生成层</code></li></ul><p>强化学习包括：</p><ul><li><strong>Intent Network</strong> ：处理用户输入</li><li><strong>Neural Belief Tracker</strong> ：记录读取slot信息</li><li><strong>Policy Network</strong> ：决定小蜜的回答：反问哪个slot 或 出订单。</li></ul><h2 id="intent-network">Intent Network</h2><p><a href="https://plmsmile.github.io/2018/03/31/36-alime-chat/#%E6%84%8F%E5%9B%BE%E5%88%86%E7%B1%BB">阿里小蜜意图分类</a>。使用CNN学一个<code>sentence embedding</code>来表示用户的意图。后面给到Policy Network。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/alime/cnn.png" style="display:block; margin:auto" width="70%"></p><h2 id="belief-tracker">Belief Tracker</h2><p>使用BiLSTM-CRF来进行标记句子，提取出slot信息。</p><table><thead><tr class="header"><th align="center">句子</th><th align="center">first</th><th align="center">class</th><th align="center">fares</th><th align="center">from</th><th align="center">Boston</th><th align="center">to</th><th align="center">Denver</th></tr></thead><tbody><tr class="odd"><td align="center"><strong>Slots</strong></td><td align="center">B-机舱类别</td><td align="center">I-机舱类别</td><td align="center">O</td><td align="center">O</td><td align="center">B-出发地</td><td align="center">O</td><td align="center">B-目的地</td></tr></tbody></table><h2 id="policy-network">Policy Network</h2><p>四个关键：episode、reward、state、action。</p><p><strong>1. 一轮交互的定义</strong></p><ul><li>episode开始：识别出用户意图为<code>购买机票</code></li><li>episode结束：用户<code>成功购买机票</code> 或 <code>退出会话</code></li></ul><p><strong>2. 反馈</strong></p><p>获取用户的反馈非常关键。</p><ul><li>收集线上用户的反馈，如用户下单、退出等行为</li><li>使用预训练环境</li></ul><p>预训练环境的两部分反馈</p><ul><li>Action Policy ：<a href="https://plmsmile.github.io/2018/04/22/41-strategy-learning/#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6">策略梯度</a> 更新模型。正反馈<span class="math inline">\(r=1\)</span>，负反馈<span class="math inline">\(r=-1\)</span></li><li>Belief Tracker：仅使用正反馈作为正例，出现错误由小二标出正确的slots</li></ul><p><strong>3. 状态</strong></p><p>当前slot：Intent Network得到的Sentence Embedding，再过Belief Tracker得到的slot信息。</p><p>使用当前slot+历史slot，过线性层，softmax，到各个Action。</p><p><strong>4. 动作</strong></p><p>订机票，Action是离散的。主要是：<strong>对各个Slot的反问</strong>和<strong>下单</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/alime/pretrained-alime.png" style="display:block; margin:auto" width="70%"></p><h2 id="整体模型">整体模型</h2><p>符号定义</p><ul><li><span class="math inline">\(q_i\)</span> ：当前用户的问题</li><li><span class="math inline">\(a_{i-1}\)</span> ：上一轮问题的答案</li><li><span class="math inline">\(S_i\)</span> ：历史slot信息</li></ul><p><span class="math display">\[\begin{align}&amp; O_i = \rm{IntentNet}(q_i)  \\ &amp; C_i = \rm{BeliefTracker}(q_i, a_{i-1}) \\&amp; X_i = O_i \oplus C_i \oplus S_{i-1} \\&amp; H_i = \rm{Linear} (X_i) \\&amp; P(\cdot) = \rm{Softmax}(H_i)\end{align}\]</span></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/alime/TaskBot.png" style="display:block; margin:auto" width="70%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;阿里小蜜的任务型问答&quot;&gt;阿里小蜜的任务型问答&lt;/h1&gt;
&lt;p&gt;小蜜包含&lt;code&gt;QA问答&lt;/code&gt;、&lt;code&gt;开放域聊
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="强化学习" scheme="http://plmsmile.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TaskBot" scheme="http://plmsmile.github.io/tags/TaskBot/"/>
    
  </entry>
  
  <entry>
    <title>意图识别和槽填充</title>
    <link href="http://plmsmile.github.io/2018/05/02/43-intent-detection-slot-filling/"/>
    <id>http://plmsmile.github.io/2018/05/02/43-intent-detection-slot-filling/</id>
    <published>2018-05-02T06:11:38.000Z</published>
    <updated>2018-05-03T05:59:02.336Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1609.01454" target="_blank" rel="noopener">Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling</a></p><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="背景">背景</h1><h2 id="语义理解的两个方面">语义理解的两个方面</h2><p><strong>1. 语义理解的两个主要方面</strong></p><p>在对话系统中，<code>Spoken language understanding</code>（语言理解）很重要。主要是下面两个关键点：</p><ul><li>理解说话人的意图 -- <strong>意图检测</strong>（Intent Detection）</li><li>从句子中提取语义成分 -- <strong>槽填充</strong>（Slot Filling）</li></ul><p><strong>2. 意图检测</strong></p><p>意图检测是一个语义句子的<code>分类问题</code>。可以用SVM、DNN来进行分类。</p><p><strong>3. 槽填充</strong></p><p>槽填充是要读取句子中的一些语义成分，是一个<code>序列标注问题</code>。可以用MEMMs来做。</p><p><strong>4. 处理</strong></p><p>传统一般是用两个模型去分别处理意图检测和槽填充，现在可以使用一个模型<a href="https://plmsmile.github.io/2017/10/10/attention-model/#encoder-decoder">Encoder-Decoder</a>去同时解决这两个问题。</p><p><strong>5. 对齐和注意力</strong></p><p>序列标注具有明确的对齐信息。</p><p>输入n，输出n，相同长度。输入和输出每一个位置严格<strong>对齐</strong>。Alignment-based RNN。</p><p>输入n，输出m，不同长度，本身不具有对齐信息。需要<a href="https://plmsmile.github.io/2017/10/10/attention-model/#attention-model">注意力机制</a>来进行对齐。Attention-based Encoder-Decoder。</p><h2 id="槽填充">槽填充</h2><p><strong>1. 问题</strong></p><p><strong>槽填充</strong>是一个序列标注问题，具有明确的对齐信息。</p><table><thead><tr class="header"><th align="center">句子</th><th align="center">first</th><th align="center">class</th><th align="center">fares</th><th align="center">from</th><th align="center">Boston</th><th align="center">to</th><th align="center">Denver</th></tr></thead><tbody><tr class="odd"><td align="center"><strong>Slots</strong></td><td align="center">B-机舱类别</td><td align="center">I-机舱类别</td><td align="center">O</td><td align="center">O</td><td align="center">B-出发地</td><td align="center">O</td><td align="center">B-目的地</td></tr></tbody></table><p><strong>意图</strong>：订机票。</p><p>本质上是学得一个<strong>映射函数</strong><span class="math inline">\(\cal {X \to Y}\)</span>。训练样本：<span class="math inline">\(\{ (\mathbf x^{(n)}, \mathbf y^{(n)}), n=1,\cdots, N \}\)</span>。</p><p><strong>2. RNN 槽填充</strong></p><p>符号定义</p><ul><li><span class="math inline">\(\mathbf x\)</span> ：输入序列</li><li><span class="math inline">\(\mathbf y\)</span> ：输出序列</li><li><span class="math inline">\(y_t\)</span> ：第t个单词的slot lable</li></ul><p>预测<span class="math inline">\(y_t\)</span>，需要<span class="math inline">\(\mathbf x\)</span>和<span class="math inline">\(y_{t-1}\)</span>。</p><p><strong>训练</strong>是找到一个最大的使概率似然最大的参数<span class="math inline">\(\theta\)</span> ： <span class="math display">\[\arg \max_{\theta} \prod P(y_t \mid y_{t-1}, \mathbf x; \theta)\]</span> <strong>预测</strong>是找到最大概率的标记序列<span class="math inline">\(\mathbf y\)</span> <span class="math display">\[\mathbf {\hat y} = \arg \max_{\mathbf y} P(\mathbf y \mid \mathbf x)\]</span> <strong>3. RNN Encoder-Decoder 槽填充</strong></p><p>序列标注有明确的对齐信息，所以先没有使用注意力机制。把<span class="math inline">\(\mathbf x\)</span>编码为语义向量<span class="math inline">\(\mathbf c\)</span>： <span class="math display">\[P(\mathbf y) = \prod_{t=1}^T P(y_t \mid y_{t-1}, \mathbf c)\]</span> Seq2Seq可以处理不同长度的映射信息，这时没有明确的对齐信息。但是可以使用注意力机制来进行<code>软对齐Soft Alignment</code>。</p><h1 id="两种方法">两种方法</h1><h2 id="seq2seq方法">Seq2Seq方法</h2><p>Encoder-Decoder with Aligned Inputs</p><p><strong>1. 编码</strong></p><p>使用双向RNN对输入序列进行编码，<span class="math inline">\(\mathbf {h_i} = [fh_i, bh_i]\)</span>。</p><p><strong>2. 意图识别</strong></p><p>最后时刻的隐状态<span class="math inline">\(\mathbf {h_T}\)</span>携带了整个句子的信息，使用它进行意图分类。</p><p><strong>3. 槽填充</strong></p><p>用单向RNN作为Decoder。初始<span class="math inline">\(\mathbf s_0= \mathbf h_T\)</span>。有3种方式：</p><ol style="list-style-type: lower-alpha"><li><p>只有注意力输入</p></li><li><p>只有对齐输入</p></li><li><p>有注意力和对齐两个输入</p></li></ol><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/papers/en-decoder-slot-filling.png" style="display:block; margin:auto" width="60%"></p><p><strong>4. 带注意力和对齐输入的RNN槽填充计算方式</strong> <span class="math display">\[s_0 = h_T\]</span> 计算注意力的上下文<span class="math inline">\(\mathbf c_i\)</span> <span class="math display">\[\alpha_{ij} = \rm{softmax}(e_{ij})\]</span></p><p><span class="math display">\[e_{ij} = g(\mathbf s_{i-1}, \mathbf h_k)\]</span></p><p><span class="math display">\[\mathbf c_i = \sum_{j=1}^T\alpha_{ij} \mathbf h_j\]</span></p><p>计算新的状态 <span class="math display">\[s_i = f(\mathbf s_{i-1}, y_{i-1}, \mathbf h_i, \mathbf c_i)\]</span></p><h2 id="基于注意力的rnn">基于注意力的RNN</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/papers/atten-rnn-slot-filling.png" style="display:block; margin:auto" width="60%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.01454&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention-Based Recurrent Neural Network Models for Joint Inten
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="注意力" scheme="http://plmsmile.github.io/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
      <category term="意图检测" scheme="http://plmsmile.github.io/tags/%E6%84%8F%E5%9B%BE%E6%A3%80%E6%B5%8B/"/>
    
      <category term="槽填充" scheme="http://plmsmile.github.io/tags/%E6%A7%BD%E5%A1%AB%E5%85%85/"/>
    
      <category term="RNN" scheme="http://plmsmile.github.io/tags/RNN/"/>
    
      <category term="对齐" scheme="http://plmsmile.github.io/tags/%E5%AF%B9%E9%BD%90/"/>
    
  </entry>
  
  <entry>
    <title>强化学习算法小结</title>
    <link href="http://plmsmile.github.io/2018/04/24/42-reinforce-conclusion-simple/"/>
    <id>http://plmsmile.github.io/2018/04/24/42-reinforce-conclusion-simple/</id>
    <published>2018-04-24T08:28:41.000Z</published>
    <updated>2018-04-24T09:44:34.384Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/">强化学习基础</a> 、<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/">基于值函数的学习方法</a>、<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/">基于值函数的学习方法</a></p><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="强化学习的目标">强化学习的目标</h1><p>强化学习的目标是<strong>学习到一个策略<span class="math inline">\(\pi_{\theta}(a\mid s)\)</span></strong>，来<strong>最大化这个策略的期望回报</strong>。<strong>希望智能体能够获得更多的回报</strong>。本质上是策略搜索。 <span class="math display">\[J(\theta) = E_{\tau \sim p_{\theta}(\tau)} [\sum_{t=0}^{T-1}\gamma ^tr_{t+1}]\]</span></p><p><span class="math display">\[J(\theta) = \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau\]</span></p><h1 id="基于值函数的学习方法">基于值函数的学习方法</h1><h2 id="策略迭代">策略迭代</h2><p>已知模型。利用<strong>贝尔曼方程</strong>（<code>算均值</code>）迭代计算出<span class="math inline">\(V(s)\)</span>，再算出<span class="math inline">\(Q(s,a)\)</span>。选择最好的动作<span class="math inline">\(a\)</span>去优化策略<span class="math inline">\(\pi(s)\)</span>。 <span class="math display">\[\forall s, \quad V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)] \]</span></p><p><span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[\forall s, \qquad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span></p><h2 id="值迭代">值迭代</h2><p>已知模型。利用<strong>贝尔曼最优方程</strong>迭代算出<span class="math inline">\(V(s)\)</span>，再算出<span class="math inline">\(Q(s,a)\)</span>。选择最好的动作<span class="math inline">\(a\)</span>去优化策略<span class="math inline">\(\pi(s)\)</span>。 <span class="math display">\[\forall s \in S, \quad V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span></p><p><span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[\forall s, \quad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span></p><h2 id="蒙特卡罗">蒙特卡罗</h2><p>未知模型。从<span class="math inline">\((s,a)\)</span><strong>随机游走，采集N个样本</strong>。使用<strong>所有轨迹回报平均值近似估计<span class="math inline">\(Q(s,a)\)</span></strong> ，再去改进策略。重复，直至收敛。 <span class="math display">\[Q^\pi(s, a)  \approx \hat Q^\pi(s, a) = \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\]</span></p><h2 id="时序差分算法">时序差分算法</h2><p>无需知道完整轨迹就能对策略进行评估。</p><p>时序差分学习=动态规划-贝尔曼估计<span class="math inline">\(G(\tau)\)</span> + 蒙特卡罗采样-增量计算<span class="math inline">\(Q(s,a)\)</span></p><p>贝尔曼估计轨迹总回报<span class="math inline">\(G(\tau)\)</span> <span class="math display">\[G(\tau) \leftarrow r(s, a, s^\prime) + \gamma \cdot Q(s^\prime, a^\prime)\]</span> 增量计算<span class="math inline">\(Q(s,a)\)</span> <span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (\underbrace{ r+ \gamma \cdot Q(s^\prime, a^\prime)}_{\color{blue}{实际值}} - \underbrace{Q(s, a)}_{\color{blue}{预期值}})\]</span></p><h2 id="sarsa">SARSA</h2><p>同策略的时序差分算法，是Q学习的改进。</p><p>1、当前状态<span class="math inline">\(s\)</span>，当前动作<span class="math inline">\(a\)</span> （初始时选择<span class="math inline">\(a=\pi^\epsilon(s)\)</span>，后续是更新得到的）</p><p>2、<strong>执行动作</strong><span class="math inline">\(a\)</span>，得到<strong>新状态</strong><span class="math inline">\(s^\prime\)</span>，得到<strong>奖励</strong><span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>4、<strong>依概率选择新动作</strong><span class="math inline">\(a = \pi^\epsilon(s^\prime)\)</span>，<strong>新状态新动作的值函数</strong>：<span class="math inline">\(Q(s^\prime, a^\prime)\)</span></p><p>5、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 6、更新状态和动作：<span class="math inline">\(s = s^\prime, a = a^\prime\)</span></p><h2 id="q学习">Q学习</h2><p>1、当前状态<span class="math inline">\(s\)</span>，选择当前动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>、得到新状态<span class="math inline">\(s^\prime\)</span>和奖励 <span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>3、<strong>不依概率选择新动作</strong>，而是<strong>直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span></strong></p><p>4、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot \max_{a^\prime} Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 5、更新状态：<span class="math inline">\(s = s^\prime\)</span></p><h2 id="q网络">Q网络</h2><p>使用神经网络<span class="math inline">\(Q_{\phi}(\mathbf{s,a})\)</span>去近似值函数<span class="math inline">\(Q(s,a)\)</span>。两个问题：实际目标值不稳定；样本之间具有强相关性。 <span class="math display">\[L(s, a, s^\prime; \phi) = \left(\underbrace{r + \gamma \cdot \max_{a^\prime} Q_\phi(\mathbf s^\prime, \mathbf a^\prime)}_{\color{blue}{实际目标值}} - \underbrace{Q_\phi(\mathbf s, \mathbf a)}_{\color{blue}{\text{网络值}}}\right)^2\]</span></p><h2 id="dqn">DQN</h2><p>深度Q网络：</p><ul><li><strong>目标网络冻结</strong>-<strong>稳定目标值</strong>。<span class="math inline">\(Q_{\phi}(\mathbf{s,a})\)</span>训练网络，<span class="math inline">\(Q_{\hat \phi}(\mathbf{s,a})\)</span>目标值网络。定期更新参数<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></li><li><strong>经验池的经验回放</strong>-<strong>去除样本相关性</strong>- 每次采集一条数据放入经验池，再从经验池取数据进行训练。</li></ul><p><strong>生成新数据加入经验池</strong></p><p>1、状态<span class="math inline">\(s\)</span>， 选择动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>， 得到<span class="math inline">\(r\)</span>和<span class="math inline">\(s^\prime\)</span></p><p>3、<span class="math inline">\((s,a, r, s^\prime)\)</span> 加入经验池<span class="math inline">\(\cal D\)</span></p><p><strong>采经验池中采样一条数据计算</strong></p><p>1、从<span class="math inline">\(\cal D\)</span>中采样一条数据，<span class="math inline">\((ss,aa, rr, ss^\prime)\)</span>。 （<strong>去除样本相关性</strong>）</p><p>2、<strong>计算实际目标值</strong><span class="math inline">\(Q_{\hat \psi}(\mathbf{ss, aa})\)</span>。 （<strong>解决目标值不稳定的问题</strong>） <span class="math display">\[Q_{\hat \psi}(\mathbf{ss, aa}) =\begin{cases}&amp; rr, &amp; ss^\prime 为终态 \\&amp; rr + \gamma \cdot \max_\limits{\mathbf a^\prime}Q_{\hat \phi}(\mathbf {ss^\prime}, \mathbf {a^\prime}), &amp;其它\end{cases}\]</span> 3、<code>损失函数</code>如下，<strong>梯度下降法去训练Q网络</strong> <span class="math display">\[J(\phi)= \left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - y \right)^2=\left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - Q_{\hat \psi}(\mathbf{ss, aa}) \right)^2\]</span> <strong>状态前进</strong></p><p><span class="math inline">\(s \leftarrow s^\prime\)</span></p><p><strong>更新目标Q网络的参数</strong></p><p>每隔C步更新：<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></p><h1 id="基于策略函数的学习方法">基于策略函数的学习方法</h1><p><code>策略搜索</code>本质上是一个<code>优化问题</code>，无需值函数可以直接优化策略。参数化的策略可以处理连续状态和动作。</p><p><strong>策略梯度</strong> ：是一种基于梯度的强化学习方法。</p><p><strong>策略连续可微假设</strong>：假设<span class="math inline">\(\pi_{\theta}(a \mid s)\)</span>是一个关于<span class="math inline">\(\theta\)</span>的连续可微函数。</p><p>最大化策略的期望回报 <span class="math display">\[J(\theta) = \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau\]</span></p><h2 id="策略梯度">策略梯度</h2><p><span class="math display">\[\frac{\partial J(\theta)}{\partial \theta}  \triangleq E_{\tau \sim  p_{\theta}(\tau)} \left[ \color{blue}{\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau)} \cdot G(\tau)\right]\]</span></p><p><span class="math display">\[\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau) =  \sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \color{blue}{ \log\pi_{\theta}(a_t \mid s_t)}\]</span></p><p><span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; =  E_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \cdot \gamma^t G(\tau_{t:T})\right)\right] \end{align}\]</span></p><h2 id="reinforce算法">REINFORCE算法</h2><p>期望用采样的方式来近似，随机采样N个轨迹。 <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; \approx  \frac{1}{N}\sum_{n=1}^   N\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a^{(n)}_t \mid s^{(n)}_t) \cdot \gamma^t G(\tau^{(n)}_{t:T})\right)\right] \end{align}\]</span> 1、根据<span class="math inline">\(\pi_\theta(a\mid s)\)</span><strong>生成一条完整的轨迹</strong> ：<span class="math inline">\(\tau = s_0, a_0, s_1,a_1, \cdots, s_{T-1}, a_{T-1}, s_{T}\)</span></p><p>2、<strong>在每一时刻更新参数</strong> (0~T)</p><p>先计算<strong>当前时刻的回报<span class="math inline">\(G(\tau_{t:T})\)</span></strong>，再更新参数： <span class="math display">\[\theta \leftarrow \theta + \alpha \cdot \gamma^tG(\tau_{t:T}) \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\]</span></p><p>缺点：</p><ul><li>需要完整的轨迹</li><li>不同轨迹之间的策略梯度方差大，导致训练不稳定</li></ul><h2 id="带基准函数的reinforce算法">带基准函数的REINFORCE算法</h2><p>每个时刻<span class="math inline">\(t\)</span>的策略梯度 <span class="math display">\[\frac{\partial J_t(\theta)}{\partial \theta} =  E_{s_t,a_t}\left[\alpha \cdot \gamma^tG(\tau_{t:T}) \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right]\]</span> <strong>基准函数</strong></p><ul><li>为了减小策略梯度的方差</li><li>引入与<span class="math inline">\(a_t\)</span>无关的基准函数<span class="math inline">\(b(s_t) = V(s_t)\)</span></li><li>越相关方差越小，所以选择值函数</li></ul><p>每一时刻的策略梯度为： <span class="math display">\[\frac{\partial \hat J_t(\theta)}{\partial \theta} =  E_{s_t,a_t}\left[\alpha \cdot \gamma^t \left( \color{blue} {G(\tau_{t:T})  - b(s_t)}\right)\cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right]\]</span> 1、根据策略<span class="math inline">\(\pi_\theta(a\mid s)\)</span><strong>生成一条完整轨迹</strong> ：<span class="math inline">\(\tau = s_0, a_0, s_1,a_1, \cdots, s_{T-1}, a_{T-1}, s_{T}\)</span></p><p>2、在每一时刻更新参数</p><p>计算<strong>当前时刻的轨迹回报<span class="math inline">\(G(\tau_{t:T})\)</span></strong> ，再利用<code>基准函数(值函数)</code>进行修正，得到<span class="math inline">\(\delta\)</span> <span class="math display">\[\delta \leftarrow  G(\tau_{t:T}) - V_{\phi} (s_t)\]</span> 更新<strong>值函数<span class="math inline">\(V_\phi(s)\)</span>的参数<span class="math inline">\(\phi\)</span></strong> <span class="math display">\[\phi \leftarrow  \phi + \beta \cdot \delta \cdot \frac{\partial}{ \partial \phi} V_{\phi}(s_t)\]</span> 更新<strong>策略函数<span class="math inline">\(\pi_\theta(a \mid s)\)</span>的参数<span class="math inline">\(\theta\)</span></strong> <span class="math display">\[\theta \leftarrow  \theta + \alpha \cdot \gamma^t\delta \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\]</span> <code>缺点</code>： 需要根据策略采集一条完整的轨迹。</p><h1 id="actor-critic算法">Actor-Critic算法</h1><p><code>演员-评论家</code>算法结合了<code>策略梯度</code>和<code>时序差分</code>算法。不需要一条完整的轨迹，可以单步更新参数，无需等到回合结束才进行更新。</p><p><strong>演员</strong></p><p>根据<span class="math inline">\(s\)</span>和策略<span class="math inline">\(\pi_\theta(a\mid s)\)</span>，执行动作<span class="math inline">\(a\)</span>，环境变为<span class="math inline">\(s^\prime\)</span>，得到奖励<span class="math inline">\(r\)</span></p><p><strong>评论员</strong></p><p>根据<code>真实奖励</code><span class="math inline">\(r\)</span>和<code>之前的标准</code>，<strong>打分（估计回报）</strong>：<span class="math inline">\(r + \gamma V_\phi(s^\prime)\)</span> ，<strong>再调整自己的打分标准<span class="math inline">\(\phi\)</span></strong>。<span class="math inline">\(\min_{\phi} \left(\hat G(\tau_{t:T}) - V_{\phi}(s_{t}) \right)^2\)</span></p><p>使评分更加接近环境的真实回报。</p><p><strong>演员</strong></p><p>演员<strong>根据评论的打分</strong>，<strong>调整自己的策略<span class="math inline">\(\pi_\theta\)</span></strong>，争取下次做得更好。<span class="math inline">\(\theta \leftarrow \theta + \alpha \cdot \gamma^t \left( G(\tau_{t:T}) - V_{\phi} (s_t)\right) \cdot \frac{\partial}{\partial \theta} \log\pi_{\theta}(a_t \mid s_t)\)</span></p><p><strong>1. 执行策略，生成样本</strong> <span class="math display">\[s, a, r, s^\prime\]</span> <strong>2. 估计回报，生成<span class="math inline">\(\delta\)</span></strong> <span class="math display">\[G(s) = r + \gamma V_\phi(s^\prime), \quad \delta  =  G(s) - V_{\phi}(s)\]</span> <strong>3. 更新值函数和策略</strong> <span class="math display">\[\phi  \leftarrow  \phi + \beta \cdot \delta \cdot \frac{\partial V_{\phi}(s)}{\partial \phi}\]</span></p><p><span class="math display">\[\theta  \leftarrow  \theta + \alpha \cdot \lambda \delta \cdot \frac{\partial }{\partial \theta} \log \pi_\theta(a\mid s)\]</span></p><p><strong>4. 更新折扣率和状态</strong> <span class="math display">\[\lambda \leftarrow \lambda \cdot \gamma, \quad s \leftarrow s^\prime\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://plmsmile.github.io/2018/04/01/37-reinforce-learning/&quot;&gt;强化学习基础&lt;/a&gt; 、&lt;a href=&quot;https://plmsmile.github.io/2018/04/21/40-valu
      
    
    </summary>
    
      <category term="强化学习" scheme="http://plmsmile.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="策略梯度" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/"/>
    
      <category term="时序差分" scheme="http://plmsmile.github.io/tags/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/"/>
    
      <category term="SARSA" scheme="http://plmsmile.github.io/tags/SARSA/"/>
    
      <category term="Q学习" scheme="http://plmsmile.github.io/tags/Q%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Q网络" scheme="http://plmsmile.github.io/tags/Q%E7%BD%91%E7%BB%9C/"/>
    
      <category term="DQN" scheme="http://plmsmile.github.io/tags/DQN/"/>
    
      <category term="蒙特卡罗" scheme="http://plmsmile.github.io/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97/"/>
    
      <category term="策略迭代" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3/"/>
    
      <category term="值迭代" scheme="http://plmsmile.github.io/tags/%E5%80%BC%E8%BF%AD%E4%BB%A3/"/>
    
  </entry>
  
  <entry>
    <title>基于策略函数的学习方法</title>
    <link href="http://plmsmile.github.io/2018/04/22/41-strategy-learning/"/>
    <id>http://plmsmile.github.io/2018/04/22/41-strategy-learning/</id>
    <published>2018-04-22T05:44:08.000Z</published>
    <updated>2018-04-24T09:29:42.602Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/">强化学习基础</a> 、<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/">基于值函数的学习方法</a></p><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="基于策略函数的学习方法">基于策略函数的学习方法</h1><h2 id="强化学习目标">强化学习目标</h2><p>强化学习的目标是<strong>学习到一个策略<span class="math inline">\(\pi_{\theta}(a\mid s)\)</span></strong>，来<strong>最大化这个策略的期望回报</strong>。<strong>希望智能体能够获得更多的回报</strong>。本质上是策略搜索。 <span class="math display">\[J(\theta) = E_{\tau \sim p_{\theta}(\tau)} [\sum_{t=0}^{T-1}\gamma ^tr_{t+1}]\]</span></p><p><span class="math display">\[J(\theta) = \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau\]</span></p><p><code>策略搜索</code>本质上是一个<code>优化问题</code>，无需值函数可以直接优化策略。参数化的策略可以处理连续状态和动作。</p><ul><li><strong>基于梯度的优化</strong></li><li>无梯度的优化</li></ul><h2 id="策略梯度">策略梯度</h2><p><strong>1. 思想和假设</strong></p><p><strong>策略梯度</strong> ：是一种基于梯度的强化学习方法。</p><p><strong>策略连续可微假设</strong>：假设<span class="math inline">\(\pi_{\theta}(a \mid s)\)</span>是一个关于<span class="math inline">\(\theta\)</span>的连续可微函数。</p><p><strong>2. 优化目标</strong></p><p>最大化策略的期望回报。</p><p><span class="math display">\[J(\theta) = \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau\]</span></p><p><strong>3. 策略梯度推导</strong></p><p>采用梯度上升法来优化参数<span class="math inline">\(\theta\)</span>来使得<span class="math inline">\(J(\theta)\)</span>最大。</p><p><strong>策略梯度<span class="math inline">\(\frac{\partial J(\theta)}{\partial \theta}\)</span></strong>的推导如下：</p><p>1、参数<span class="math inline">\(\theta\)</span>的优化方向是总回报<span class="math inline">\(G(\tau)\)</span>大的轨迹<span class="math inline">\(\tau\)</span>，其概率<span class="math inline">\(p_\theta(\tau)\)</span>也就越大。 <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; = \frac{\partial} {\partial \theta}  \int p_{\theta}(\tau) \cdot G(\tau) \, {\rm d}\tau = \int \left(\frac{\partial}{\partial \theta}  p_{\theta}(\tau)\right) \cdot G(\tau) \, {\rm d}\tau \\&amp; =  \int \color{blue}{p_{\theta}(\tau)} \cdot \left(\color{blue}{\frac{1} {p_{\theta}(\tau)}}\frac{\partial}{\partial \theta}  p_{\theta}(\tau)\right) \cdot G(\tau) \, {\rm d}\tau \\&amp; = \int p_{\theta}(\tau) \cdot \left( \color{blue}{ \frac{\partial}{\partial \theta} \log  p_{\theta}(\tau) }\right) \cdot G(\tau) \, {\rm d}\tau \\&amp; \triangleq E_{\tau \sim  p_{\theta}(\tau)} \left[ \color{blue}{\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau)} \cdot G(\tau)\right]\end{align}\]</span> 2、梯度只和策略相关，轨迹的梯度 == 各个时刻的梯度的求和 <span class="math display">\[\begin{align}\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau)&amp; = \frac{\partial}{\partial \theta} \log \left( p(s_0) \cdot \prod_{t=0}^{T-1} \underbrace {\pi_{\theta}(a_t \mid s_t) }_{\color{blue}{执行动作}} \underbrace{p(s_{t+1} \mid s_t,a_t)}_{\color{blue}{环境改变}}\right) \\&amp; = \frac{\partial}{\partial \theta} \log  \left( \log p(s_0) + \color{blue}{\sum_{t=0}^{T-1} \log\pi_{\theta}(a_t \mid s_t) }+  \sum_{t=0}^{T-1} \log p(s_{t+1} \mid s_t,a_t)  \right) \\&amp; = \sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \color{blue}{ \log\pi_{\theta}(a_t \mid s_t)}\end{align}\]</span> 3、策略梯度 == 轨迹的梯度*轨迹的回报 的期望 <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; = E_{\tau \sim p_{\theta}(\tau)}\left[ \left(\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \right)\cdot G(\tau)\right] \\&amp; =  E_{\tau \sim p_{\theta}(\tau)}\left[ \left(\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \right)\cdot \left( \color{blue}{G(\tau_{1:k-1}) + \gamma^k \cdot G(\tau_{k:T})}\right)\right] \\&amp; =  E_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \cdot \gamma^t G(\tau_{t:T})\right)\right] \end{align}\]</span> 其中<span class="math inline">\(G(\tau_{t:T})​\)</span>是从时刻<span class="math inline">\(t​\)</span>作为起始时刻收到的总回报 <span class="math display">\[G(\tau_{t:T}) =  \sum_{i=t}^{T-1} \gamma ^{i-t} r_{i+1}\]</span> <strong>4. 总结</strong></p><p><span class="math display">\[\frac{\partial J(\theta)}{\partial \theta}  \triangleq E_{\tau \sim  p_{\theta}(\tau)} \left[ \color{blue}{\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau)} \cdot G(\tau)\right]\]</span></p><p><span class="math display">\[\frac{\partial}{\partial \theta} \log  p_{\theta}(\tau) =  \sum_{t=0}^{T-1} \frac{\partial}{\partial \theta}  \color{blue}{ \log\pi_{\theta}(a_t \mid s_t)}\]</span></p><p><span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; =  E_{\tau \sim p_{\theta}(\tau)}\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t) \cdot \gamma^t G(\tau_{t:T})\right)\right] \end{align}\]</span></p><h2 id="reinforce算法">REINFORCE算法</h2><p><strong>期望</strong>可以通过<strong>采样的方法来近似</strong>。对于当前策略<span class="math inline">\(\pi_\theta\)</span>，可以<strong>随机游走采集N个轨迹</strong>。<span class="math inline">\(\tau^{(1)},\cdots, \tau^{(N)}\)</span> <span class="math display">\[\begin{align}\frac{\partial J(\theta)}{\partial \theta} &amp; \approx  \frac{1}{N}\sum_{n=1}^   N\left[ \sum_{t=0}^{T-1} \left(\frac{\partial}{\partial \theta}  \log\pi_{\theta}(a^{(n)}_t \mid s^{(n)}_t) \cdot \gamma^t G(\tau^{(n)}_{t:T})\right)\right] \end{align}\]</span> 可微分策略函数<span class="math inline">\(\pi_\theta(a\mid s)​\)</span>，折扣率<span class="math inline">\(\gamma​\)</span>，学习率<span class="math inline">\(\alpha​\)</span></p><p>初始化参数<span class="math inline">\(\theta\)</span>， 训练，直到<span class="math inline">\(\theta\)</span>收敛</p><p>1、<strong>根据<span class="math inline">\(\pi_\theta(a\mid s)\)</span>生成一条轨迹</strong> <span class="math display">\[\tau = s_0, a_0, s_1,a_1, \cdots, s_{T-1}, a_{T-1}, s_{T}\]</span> 2、<strong>在每一时刻更新参数</strong> (0~T)</p><ul><li>计算<span class="math inline">\(G(\tau_{t:T})\)</span></li><li><span class="math inline">\(\theta \leftarrow \theta + \alpha \cdot \gamma^tG(\tau_{t:T}) \cdot \frac{\partial}{\partial \theta} \log\pi_{\theta}(a_t \mid s_t)\)</span></li></ul><p>REINFORCE算法<code>缺点</code>：<strong>不同路径之间的方差很大，导致训练不稳定</strong>；需要根据一个策略<strong>采集一条完整的轨迹</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/rl/reinforce.png" style="display:block; margin:auto" width="80%"></p><h2 id="带基准线的reinforce算法">带基准线的REINFORCE算法</h2><blockquote><p>值函数作为基准函数，去减小策略梯度的方差</p></blockquote><p>由于不同轨迹之间的方差很大，导致训练不稳定，使用基准函数去减小策略梯度的方差。</p><p><strong>1. 减小方差的办法</strong></p><p>目标：估计函数<span class="math inline">\(f\)</span>的期望，同时要减小<span class="math inline">\(f\)</span>的方差。</p><p>方法</p><ul><li>引入已知期望的函数<span class="math inline">\(g\)</span></li><li><strong><span class="math inline">\(\hat f = f - \alpha(g - E[g])\)</span></strong></li><li>推导可知： <span class="math inline">\(E[f] = E[\hat f]\)</span></li><li>用<span class="math inline">\(g\)</span>去减小<span class="math inline">\(f\)</span>的方差， <span class="math inline">\(D(f)=Var(f)\)</span></li></ul><p><span class="math display">\[D(\hat f) = D(f) - 2\alpha \cdot \rm{Cov}(f,g) + \alpha^2 \cdot D(g)\]</span></p><p><span class="math display">\[令\frac{\partial D(\hat f)}{\partial \alpha} = 0 \quad \to \quad \frac{D(\hat f)}{D(f)} = 1 - \rho^2(f, g)\]</span></p><p>所以<strong>，<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>的相关性越高，<span class="math inline">\(D(\hat f)\)</span>越小</strong>。</p><p><strong>2. 带基准线的REINFORCE算法核心</strong></p><p>每个时刻<span class="math inline">\(t\)</span>的策略梯度 <span class="math display">\[\frac{\partial J_t(\theta)}{\partial \theta} =  E_{s_t,a_t}\left[\alpha \cdot \gamma^tG(\tau_{t:T}) \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right]\]</span> 为了<strong>减小策略梯度的方差</strong>，引入一个和<span class="math inline">\(a_t\)</span>无关的<strong>基准函数<span class="math inline">\(b(s_t)\)</span></strong>，策略梯度为： <span class="math display">\[\frac{\partial \hat J_t(\theta)}{\partial \theta} =  E_{s_t,a_t}\left[\alpha \cdot \gamma^t \left( \color{blue} {G(\tau_{t:T})  - b(s_t)}\right)\cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right]\]</span> 因为<span class="math inline">\(b(s_t)\)</span>与<span class="math inline">\(a_t\)</span>无关，可以证明得到：（使用积分求平均，<span class="math inline">\(\int_{a_t} \pi_{\theta}(a_t \mid s_t) \,{\rm d} a_t= 1\)</span>） <span class="math display">\[E_{a_t}\left[b(s_t)\cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\right] =  \frac{\partial}{\partial \theta} (b(s_t) \cdot 1) = 0\]</span> 所以得到：<span class="math inline">\(\frac{\partial J_t(\theta)}{\partial \theta} = \frac{\partial \hat J_t(\theta)}{\partial \theta}​\)</span></p><p><strong>4. 基准函数的选择</strong></p><p>为了减小策略梯度的方差，希望<strong><span class="math inline">\(b(s_t)\)</span>与<span class="math inline">\(G(\tau_{t:T})\)</span> 越相关越好</strong>，所以选择<strong><span class="math inline">\(b(s_t) = V(s_t)\)</span></strong>（<a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/#%E5%80%BC%E5%87%BD%E6%95%B0">值函数</a>）。</p><p>1、可学习的函数<span class="math inline">\(V_{\phi}(s_t)\)</span>来近似值函数，类似于<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/#%E6%B7%B1%E5%BA%A6q%E7%BD%91%E7%BB%9C">Q网络</a> <span class="math display">\[\phi^* = \arg \min_{\phi} \left(V(s_t) - V_{\phi}(s_t)\right)^2\]</span> 2、<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/#%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E9%87%87%E6%A0%B7%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95">蒙塔卡罗方法</a>进行估计值函数 也就是<a href="https://plmsmile.github.io/2018/04/21/40-value-learning/#%E6%80%BB%E4%BD%93%E6%80%9D%E6%83%B3-1">增量计算</a>Q(s,a)嘛。</p><p><strong>5. 带基准线的REINFORCE算法步骤</strong></p><p>输入</p><ul><li>状态空间和动作空间，<span class="math inline">\(\cal {S,A}\)</span></li><li>可微分的策略函数<span class="math inline">\(\pi_\theta(a \mid s)\)</span></li><li>可微分的状态值函数<span class="math inline">\(V_{\phi}(s_t)\)</span></li><li>折扣率<span class="math inline">\(\gamma\)</span>，学习率<span class="math inline">\(\alpha, \beta\)</span></li></ul><p>随机初始化参数<span class="math inline">\(\theta, \phi\)</span></p><p>不断训练，直到<span class="math inline">\(\theta\)</span>收敛</p><p>1、根据策略<span class="math inline">\(\pi_\theta(a\mid s)\)</span><strong>生成一条完整轨迹</strong> ：<span class="math inline">\(\tau = s_0, a_0, s_1,a_1, \cdots, s_{T-1}, a_{T-1}, s_{T}\)</span></p><p>2、在每一时刻更新参数</p><p>计算<strong>当前时刻的轨迹回报<span class="math inline">\(G(\tau_{t:T})\)</span></strong> ，再利用<code>基准函数(值函数)</code>进行修正，得到<span class="math inline">\(\delta\)</span> <span class="math display">\[\delta \leftarrow  G(\tau_{t:T}) - V_{\phi} (s_t)\]</span> 更新<strong>值函数<span class="math inline">\(V_\phi(s)\)</span>的参数<span class="math inline">\(\phi\)</span></strong> <span class="math display">\[\phi \leftarrow  \phi + \beta \cdot \delta \cdot \frac{\partial}{ \partial \phi} V_{\phi}(s_t)\]</span> 更新<strong>策略函数<span class="math inline">\(\pi_\theta(a \mid s)\)</span>的参数<span class="math inline">\(\theta\)</span></strong> <span class="math display">\[\theta \leftarrow  \theta + \alpha \cdot \gamma^t\delta \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\]</span> <code>缺点</code>： 需要根据策略采集一条完整的轨迹。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/rl/reinforce-base.png" style="display:block; margin:auto" width="80%"></p><h1 id="actor-critic算法">Actor-Critic算法</h1><h2 id="思想">思想</h2><p><code>演员-评论员算法</code>是一种结合<code>策略梯度</code>和<code>时序差分学习</code>的强化学习方法。</p><p>开始，演员随机表演，评论员随机打分；不断学习，评论员评分越来越准，演员的动作越来越好。</p><ul><li>演员：<strong>策略函数<span class="math inline">\(\pi_\theta(s,a)\)</span></strong>，学习一个策略来得到尽可能高的回报</li><li>评论员：<strong>值函数<span class="math inline">\(V_\phi(s)\)</span></strong>，评估当前策略函数（演员）的好坏</li></ul><p><strong>演员</strong></p><p>根据<span class="math inline">\(s\)</span>和策略<span class="math inline">\(\pi_\theta(a\mid s)\)</span>，执行动作<span class="math inline">\(a\)</span>，环境变为<span class="math inline">\(s^\prime\)</span>，得到奖励<span class="math inline">\(r\)</span></p><p><strong>评论员</strong></p><p>根据<code>真实奖励</code><span class="math inline">\(r\)</span>和<code>之前的标准</code>，<strong>打分（估计回报）</strong>：<span class="math inline">\(r + \gamma V_\phi(s^\prime)\)</span> ，<strong>再调整自己的打分标准<span class="math inline">\(\phi\)</span></strong>。<span class="math inline">\(\min_{\phi} \left(\hat G(\tau_{t:T}) - V_{\phi}(s_{t}) \right)^2\)</span></p><p>使评分更加接近环境的真实回报。</p><p><strong>演员</strong></p><p>演员根据评论的打分，调整自己的策略<span class="math inline">\(\pi_\theta\)</span>，争取下次做得更好。</p><p><code>优点</code>：可以单步更新参数，不需要等到回合结束才进行更新。</p><h2 id="值函数的三个功能">值函数的三个功能</h2><p><strong>1. 估计轨迹真实回报（打分）</strong> <span class="math display">\[\hat G(\tau_{t:T}) = r_{t+1} + \gamma V_{\phi}(s_{t+1})\]</span> <strong>2. 更新值函数参数<span class="math inline">\(\phi\)</span> （调整打分标准）</strong> <span class="math display">\[\min_{\phi} \left(\hat G(\tau_{t:T}) - V_{\phi}(s_{t}) \right)^2\]</span> <strong>3. 更新策略参数<span class="math inline">\(\theta\)</span>时，作为基函数来减少策略梯度的方差（调整策略）</strong> <span class="math display">\[\theta \leftarrow  \theta + \alpha \cdot \gamma^t \left( G(\tau_{t:T}) - V_{\phi} (s_t)\right) \cdot \frac{\partial}{\partial \theta}  \log\pi_{\theta}(a_t \mid s_t)\]</span></p><h2 id="算法实现步骤">算法实现步骤</h2><p><strong>输入</strong></p><ul><li>状态空间和动作空间，<span class="math inline">\(\cal {S,A}\)</span></li><li>可微分的策略函数<span class="math inline">\(\pi_\theta(a \mid s)\)</span></li><li>可微分的状态值函数<span class="math inline">\(V_{\phi}(s_t)\)</span></li><li>折扣率<span class="math inline">\(\gamma\)</span>，学习率<span class="math inline">\(\alpha &gt;0, \beta&gt;0\)</span></li></ul><p><strong>随机初始化参数<span class="math inline">\(\theta, \phi\)</span></strong></p><p><strong>迭代直到<span class="math inline">\(\theta\)</span>收敛，初始状态<span class="math inline">\(s\)</span>, <span class="math inline">\(\lambda=1\)</span></strong></p><p>从s开始，直到<span class="math inline">\(s\)</span>为终止状态</p><p>1、状态s，选择动作<span class="math inline">\(a = \pi_\theta(a\mid s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>，得到即时奖励<span class="math inline">\(r\)</span>和新状态<span class="math inline">\(s^\prime\)</span></p><p>3、利用值函数作为基函数计算<span class="math inline">\(\delta\)</span>，<span class="math inline">\(\delta \leftarrow r + \gamma V_{\phi}(s^\prime) - V_{\phi}(s)\)</span></p><p>4、更新值函数：<span class="math inline">\(\phi \leftarrow \phi + \beta \cdot \delta \cdot \frac{\partial V_{\phi}(s)}{\partial \phi}\)</span></p><p>5、更新策略函数：<span class="math inline">\(\theta \leftarrow \theta + \alpha \cdot \lambda \delta \cdot \frac{\partial }{\partial \theta} \log \pi_\theta(a\mid s)\)</span></p><p>6、更新折扣率和状态：<span class="math inline">\(\lambda \leftarrow \lambda \cdot \gamma, \quad s \leftarrow s^\prime\)</span></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/rl/actor-critic.png" style="display:block; margin:auto" width="80%"></p><h1 id="强化学习算法总结">强化学习算法总结</h1><h2 id="方法总览">方法总览</h2><p><strong>1. 通用步骤</strong></p><ol style="list-style-type: decimal"><li>执行策略，生成样本</li><li>估计回报</li><li>更新策略</li></ol><p><strong>2. 值函数与策略函数的比较</strong></p><p><code>值函数的方法</code></p><p>策略更新，导致值函数的改变比较大，对收敛性有一定的影响</p><p><code>策略函数的方法</code></p><p>策略更新，<strong>更加平稳</strong>。</p><p>缺点：策略函数的解空间很大，难以进行充分采样，导致方差较大，容易陷入局部最优解。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/rl/reinforce-all.png" style="display:block; margin:auto" width="75%"></p><h2 id="四个典型方法">四个典型方法</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/rl/four-compare.png" style="display:block; margin:auto" width="70%"></p><h2 id="与监督学习的区别">与监督学习的区别</h2><table><thead><tr class="header"><th align="center"></th><th align="center">强化学习</th><th align="center">监督学习</th></tr></thead><tbody><tr class="odd"><td align="center">样本</td><td align="center">与环境进行交互产生样本，进行试错学习</td><td align="center">人工收集并标注</td></tr><tr class="even"><td align="center">反馈</td><td align="center">只有奖励，并且是延迟的</td><td align="center">需要明确的指导信息（每个状态对应一个动作）</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://plmsmile.github.io/2018/04/01/37-reinforce-learning/&quot;&gt;强化学习基础&lt;/a&gt; 、&lt;a href=&quot;https://plmsmile.github.io/2018/04/21/40-valu
      
    
    </summary>
    
      <category term="强化学习" scheme="http://plmsmile.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="策略函数" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E5%87%BD%E6%95%B0/"/>
    
      <category term="策略梯度" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/"/>
    
      <category term="REINFORCE" scheme="http://plmsmile.github.io/tags/REINFORCE/"/>
    
      <category term="基准函数" scheme="http://plmsmile.github.io/tags/%E5%9F%BA%E5%87%86%E5%87%BD%E6%95%B0/"/>
    
      <category term="Actot-Critic" scheme="http://plmsmile.github.io/tags/Actot-Critic/"/>
    
      <category term="强化学习算法总结" scheme="http://plmsmile.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>基于值函数的学习</title>
    <link href="http://plmsmile.github.io/2018/04/21/40-value-learning/"/>
    <id>http://plmsmile.github.io/2018/04/21/40-value-learning/</id>
    <published>2018-04-21T05:14:36.000Z</published>
    <updated>2018-04-21T13:27:53.427Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="70%"></p><blockquote><p>强化学习基于值函数的学习方法。最重要要是SARSA、Q学习、DQN。但是这些都依赖于前面的动态规划和蒙特卡罗方法。</p></blockquote><p><a href="https://plmsmile.github.io/2018/04/01/37-reinforce-learning/">强化学习基础笔记</a></p><h1 id="贝尔曼和贝尔曼最优方程">贝尔曼和贝尔曼最优方程</h1><blockquote><ol style="list-style-type: decimal"><li><span class="math inline">\(V(s)\)</span>函数和<span class="math inline">\(Q(s,a)\)</span>函数</li><li>贝尔曼方程（选择所有可能的均值）</li><li>贝尔曼最优方程（直接选择最大值）</li></ol></blockquote><h2 id="v函数与q函数">V函数与Q函数</h2><p>V函数：以<strong>s为初始状态</strong>，执行策略<span class="math inline">\(\pi\)</span>得到的<code>期望回报</code>（所有轨迹回报的均值） <span class="math display">\[V^\pi(s) = E_{\tau \sim p(\tau)} [\sum_{t=0}^{T-1}r_{t+1} \mid \tau_{s_0} = s]\]</span> Q函数：以<strong>s为初始状态，执行动作a</strong>，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> 利用V函数去计算Q函数 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><h2 id="贝尔曼方程">贝尔曼方程</h2><p><span class="math inline">\(V(s)\)</span>的贝尔曼方程，选择<strong>所有a的期望回报</strong>， 也是<strong>Q函数的均值</strong>，<span class="math inline">\(V(s)=E_a[Q(s, a)]\)</span> <span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}[Q^\pi(s, a)]\]</span></p><p><span class="math inline">\(Q(s,a)\)</span>函数的贝尔曼方程 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma E_{a\prime \sim \pi(a\prime \mid s\prime)}[Q^\pi(s\prime, a\prime)]]\]</span></p><h2 id="贝尔曼最优方程">贝尔曼最优方程</h2><p><span class="math inline">\(V(s)\)</span>函数的贝尔曼最优方程，实际上是<strong>直接选择所有a中的最大回报</strong> ： <span class="math display">\[V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span> <span class="math inline">\(Q(s,a)\)</span>函数的贝尔曼最优方程 <span class="math display">\[Q^*(s, a) =  E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma \max_\limits{a\prime}Q^*(s^\prime, a^\prime)]\]</span></p><h1 id="值函数的学习方法">值函数的学习方法</h1><blockquote><ol style="list-style-type: decimal"><li>穷举所有策略选择最好策略（没用）</li><li>迭代优化策略，<strong>根据策略的值函数去优化策略</strong> （重点）</li><li>动态规划（已知状态转移概率<span class="math inline">\(p(s^\prime \mid s, a)\)</span>和奖励函数<span class="math inline">\(r(s, a, s^\prime)\)</span>）</li><li>蒙特卡罗方法（不知模型），先采一些样本，再优化</li><li>时序差分学习算法：SARAS和Q学习</li><li>深度Q网络</li></ol></blockquote><p>值函数（<span class="math inline">\(V^\pi(s)\)</span>、<span class="math inline">\(Q^\pi(s, a)\)</span> ）用来对策略<span class="math inline">\(\pi(a \mid s)\)</span>进行评估。</p><p><strong>1. 穷举策略法</strong></p><p>如果策略有限，可以对所有策略进行评估，选出最优策略 <span class="math display">\[\forall s, \qquad \pi^* = \arg \max_\limits{\pi} V^\pi(s)\]</span> 策略空间<span class="math inline">\(\vert \mathcal A\vert^{\vert \mathcal S\vert}\)</span>非常大，根本无法搜索。</p><p><strong>2. 迭代法优化策略</strong></p><p>步骤如下，直到收敛</p><ol style="list-style-type: decimal"><li>随机初始化一个策略</li><li><strong>计算该策略的值函数</strong>：<code>动态规划</code>， <code>蒙特卡罗</code></li><li><strong>根据值函数来设置新的策略</strong></li></ol><p>比如</p><p>给一个初始策略<span class="math inline">\(\pi(a\mid s)\)</span>， 根据<span class="math inline">\(Q^\pi(s, a)\)</span>去不断迭代去优化，得到新的策略函数<span class="math inline">\(\pi^\prime (a\mid s)\)</span>（<strong>确定性策略</strong>），直到收敛。 <span class="math display">\[\pi^\prime (a\mid s) = \begin {cases}&amp; 1  &amp; a  = \arg \max_\limits{\hat a}Q^\pi(s, \hat a) \\ &amp; 0 &amp;  \text{others}\\\end {cases}\]</span> <strong>新策略的值函数</strong>会不断变大： <span class="math display">\[Q^{\pi^\prime}(s, \hat a) \ge Q^\pi(s, \hat a)\]</span></p><h1 id="动态规划算法">动态规划算法</h1><h2 id="总体思想">总体思想</h2><blockquote><ol style="list-style-type: decimal"><li>思想：已知模型，通过贝尔曼方程来<strong>迭代计算值函数</strong>，通过<strong>值函数去优化策略</strong>为固定策略</li><li>两种方法：策略迭代-贝尔曼方程（所有可能的均值），值迭代-贝尔曼最优方程（直接）</li><li>缺点：要求模型已知，效率太低</li></ol></blockquote><p>基于模型的强化学习，叫做<code>模型相关的强化学习</code>，或有模型的强化学习。</p><p><strong>1. 动态规划思想</strong></p><ul><li><strong>已知模型</strong>：状态转移概率<span class="math inline">\(p(s \prime \mid s, a)\)</span> 和奖励<span class="math inline">\(r(s, a, s\prime)\)</span></li><li>可以通过<code>贝尔曼方程</code>或<code>贝尔曼最优方程</code>来<strong>迭代计算值函数<span class="math inline">\(V(s)\)</span> </strong>，再<strong>通过<span class="math inline">\(V(s)\)</span>去计算<span class="math inline">\(Q(s,a)\)</span></strong></li><li><strong>通过值函数来优化策略</strong>，一般为优化为固定策略<span class="math inline">\(\pi(s)=a\)</span></li></ul><p><strong>2. 两种方法</strong></p><ul><li>策略迭代算法 ： <code>贝尔曼方程</code>更新值函数，算出所有值函数，计算均值</li><li>值迭代算法：<code>贝尔曼最优方程</code>更新值函数，<strong>直接优化计算最大值</strong></li></ul><p><strong>3. 缺点</strong></p><ul><li>要求模型已知：<span class="math inline">\(p(s \prime \mid s, a)\)</span>、 <span class="math inline">\(r(s, a, s\prime)\)</span></li><li>效率太低：可以通过神经网络来近似计算值函数</li></ul><h2 id="策略迭代算法">策略迭代算法</h2><blockquote><ol style="list-style-type: decimal"><li>已知模型：状态转移概率<span class="math inline">\(p(s^\prime \mid s, a)\)</span>和奖励函数<span class="math inline">\(r(s, a, s^\prime)\)</span></li><li>使用<code>贝尔曼方程</code>迭代计算<span class="math inline">\(V^{\pi}(s)\)</span>，<strong>求均值</strong>， <span class="math inline">\(V^{\pi}(s) = E_{a}E_{s^\prime}[r(s,a,s^\prime)+ \gamma V^\pi(s\prime)]\)</span></li><li>利用<span class="math inline">\(V^{\pi}(s)\)</span>去计算<span class="math inline">\(Q^{\pi}(s,a)\)</span>。<strong>求均值</strong>，<span class="math inline">\(Q^\pi(s, a) = E_{s\prime} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\)</span></li><li>根据<span class="math inline">\(Q^\pi(s, a)\)</span>更新策略<span class="math inline">\(\pi(s)=a\)</span>，选择最好的动作a，<strong>更新为固定策略</strong></li></ol></blockquote><p><strong>1. 初始化策略</strong> <span class="math display">\[\forall s, \forall a, \quad \pi(a \mid s) = \frac{1}{\vert \cal A\vert}\]</span> <strong>2. 使用贝尔曼方程迭代计算值函数<span class="math inline">\(V^\pi(s)\)</span></strong> <span class="math display">\[\forall s, \quad V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> <strong>3. 利用值函数<span class="math inline">\(V^\pi(s)\)</span>计算<span class="math inline">\(Q^\pi(s, a)\)</span></strong> <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> <strong>4. 根据<span class="math inline">\(Q^\pi(s, a)\)</span>更新策略，选择最好的动作a，更新为固定策略</strong> <span class="math display">\[\forall s, \qquad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/img/rl/strategy-iter.png" style="display:block; margin:auto" width="70%"></p><h2 id="值迭代算法">值迭代算法</h2><blockquote><ol style="list-style-type: decimal"><li>最优值函数：最优策略对应的值函数</li><li>贝尔曼最优方程：<span class="math inline">\(V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\)</span> ，直接选择最大的a</li><li>值迭代算法：最优方程更新值函数<span class="math inline">\(V(s)\)</span>， 计算<span class="math inline">\(Q(s,a)\)</span>， 更新策略<span class="math inline">\(\pi(s)=a\)</span></li></ol></blockquote><p><strong>1. 最优值函数</strong></p><p><strong>最优策略<span class="math inline">\(\pi^*\)</span>对应的值函数</strong>就是<code>最优值函数</code> <span class="math display">\[V^*(s) = \max_\limits{a} Q^*(s, a)\]</span> <strong>2. 贝尔曼最优方程</strong> <span class="math display">\[V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span></p><p><span class="math display">\[Q^*(s, a) =  E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma \max_\limits{a\prime}Q^*(s^\prime, a^\prime)]\]</span></p><p><strong>3. 值迭代算法</strong></p><p>值迭代算法：使用<strong>贝尔曼最优方程去更新值函数</strong>，收敛时的值函数就是最优值函数，对应的策略也是最优的策略。</p><p>1、 <strong>初始化值函数</strong> <span class="math display">\[\forall s \in S, \quad V(s) = 0\]</span> 2、 <strong>使用贝尔曼最优方程更新<span class="math inline">\(V(s)\)</span>，直到收敛</strong> <span class="math display">\[\forall s \in S, \quad V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span> 3、 <strong>计算<span class="math inline">\(Q(s,a)\)</span></strong> <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> 4、 <strong>更新策略<span class="math inline">\(\pi(s)=a\)</span></strong> <span class="math display">\[\forall s, \quad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span> 5、 <strong>输出策略<span class="math inline">\(\pi\)</span></strong></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/rl/value-iter.png" style="display:block; margin:auto" width="70%"></p><h1 id="蒙特卡罗采样学习方法">蒙特卡罗采样学习方法</h1><blockquote><ol style="list-style-type: decimal"><li>蒙特卡罗方法：随机游走采集样本，估计 <span class="math inline">\(Q^\pi(s, a) \approx \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\)</span>，根据Q去改进策略</li><li><span class="math inline">\(\epsilon 贪心法\)</span>: 依概率选择<span class="math inline">\(\pi^\epsilon (s) = \pi(s)\)</span></li><li>同策略和异策略：采样与改进策略相同为同策略</li><li>需要拿到完整的轨迹才能对策略评估更新模型，效率较低</li></ol></blockquote><p><code>模型无关的强化学习</code>也称为无模型的强化学习。蒙特卡罗方法：</p><blockquote><p>在不知<span class="math inline">\(p(s \prime \mid s, a)\)</span>、 <span class="math inline">\(r(s, a, s\prime)\)</span> 的情况下， 需要智能体和环境进行交互，并且收集一些样本。然后根据这些样本去求解马尔可夫决策过程最优策略</p></blockquote><p>Q函数<span class="math inline">\(Q^\pi(s, a)\)</span>，初始状态为<span class="math inline">\(s\)</span>， 执行动作<span class="math inline">\(a\)</span>后，策略<span class="math inline">\(\pi\)</span>能得到的期望总回报。<br><span class="math display">\[Q^\pi(s, a) = E_{\tau \sim p(\tau)} [G(\tau) \mid \tau_{s_0} = s, \tau_{a_0} = a]\]</span> 模型未知，Q函数可以通过采样来计算。</p><p><strong>1. 蒙特卡罗方法</strong></p><p>1、从状态<span class="math inline">\(s\)</span>、 动作<span class="math inline">\(a\)</span>开始<strong>随机游走</strong>探索环境， <strong>采集N个样本</strong>（N个轨迹）</p><p>2、得到N个轨迹<span class="math inline">\(\tau^{(1)}, \cdots, \tau^{(N)}\)</span>，得到它们的总回报<span class="math inline">\(G(\tau^{(1)}), \cdots, G(\tau^{(N)})\)</span></p><p>3、利用<strong>轨迹的总回报去估计出<span class="math inline">\(Q^\pi(s, a)\)</span></strong> 。 <span class="math inline">\(Q^\pi(s, a) \approx \hat Q^\pi(s, a) = \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\)</span></p><p>4、基于<span class="math inline">\(Q^\pi(s, a)\)</span> 去<strong>改进策略</strong>， <strong><span class="math inline">\(\epsilon\)</span>贪心法</strong></p><p>5、在新的策略下，再去采集样本、去估计Q，再去改进策略，直到收敛</p><p><strong>2. 利用和探索</strong></p><p>如果采用<code>确定性策略</code> :</p><ul><li>则每次试验得到的轨迹是一样的</li><li>只能计算出<span class="math inline">\(Q^\pi(s, \pi(s))\)</span> ，无法计算出<span class="math inline">\(Q^\pi(s, a\prime)\)</span>，即无法计算出其它的<span class="math inline">\(a\prime\)</span>的Q函数</li><li>只对当前环境进行了利用，而没有探索</li><li>而试验的轨迹应该覆盖所有的状态和动作，以找到更好的策略</li></ul><p>采用<strong><span class="math inline">\(\epsilon\)</span>贪心法</strong> <span class="math display">\[\pi^\epsilon(s) =\begin {cases}&amp; \pi(s)  &amp; \text{依概率 } 1-\epsilon \\ &amp; a\prime \quad\text{(随机选择)} &amp;  \text{依概率 } \epsilon\\\end {cases}\]</span> <strong>3. 同策略和异策略</strong></p><ul><li>同策略：采样策略<span class="math inline">\(\pi^\epsilon(s)\)</span>， 改进策略<span class="math inline">\(\pi^\epsilon(s)\)</span>， 相同</li><li>异策略：采样策略<span class="math inline">\(\pi^\epsilon(s)\)</span>， 改进策略<span class="math inline">\(\pi(s)\)</span>， 不同。可以使用重要性采样、重要性权重来优化<span class="math inline">\(\pi\)</span></li></ul><h1 id="时序差分学习算法">时序差分学习算法</h1><h2 id="总体思想-1">总体思想</h2><blockquote><ol style="list-style-type: decimal"><li>无需知道完整轨迹就能对策略进行评估。时序差分学习=蒙特卡罗+动态规划</li><li>贝尔曼<strong>估计轨迹的回报</strong>。<span class="math inline">\(G(\tau_{0:T}^{(N)}) = r(s, a, s^\prime) + \gamma \cdot \hat Q^\pi_{N-1}(s^\prime, a^ \prime)\)</span></li><li><strong>增量计算<span class="math inline">\(\hat Q_N^\pi(s,a)\)</span></strong>。 <span class="math inline">\(\hat Q_N^\pi(s,a) = \hat Q_{N-1}^\pi(s,a) + \alpha \cdot \left(G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a) \right)\)</span></li></ol></blockquote><p>蒙特卡罗方法需要拿到完整的轨迹，才能对策略进行评估。</p><p>时序差分学习（temporal-difference learning）结合了动态规划和蒙特卡罗方法。</p><p><strong>1. 增量计算<span class="math inline">\(\hat Q_N^\pi(s,a)\)</span></strong></p><p>蒙特卡罗方法：从状态<span class="math inline">\(s\)</span>，动作<span class="math inline">\(a\)</span>开始，随机游走，采样N个样本</p><p><span class="math inline">\(G(\tau ^{(N)})\)</span> ：第N次试验的总回报</p><p><span class="math inline">\(\hat Q_N^\pi(s,a)\)</span> ：第N次试验后值函数的平均值，推导如下： <span class="math display">\[\begin{align}\hat Q_N^\pi(s,a) &amp; = \frac{1}{N} \sum_{i=1}^NG(\tau ^{(i)}) \\&amp; = \frac{1}{N} \left(G(\tau ^{(N)}) + \color{blue}{\sum_{i=1}^{N-1}G(\tau ^{(i)})} \right) \\&amp; = \frac{1}{N} \left(G(\tau ^{(N)}) + \color{blue}{(N-1) \hat Q_{N-1}^\pi(s,a)} \right) \\&amp; = \hat Q_{N-1}^\pi(s,a) +  \frac{1}{N} \left(G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a)\right)\end{align}\]</span> <strong>值函数</strong><span class="math inline">\(\hat Q_{N}^\pi (s, a)\)</span> ： <strong>第N次后的平均</strong> = <strong>N-1次后的平均</strong> + <strong>一个增量</strong>， <span class="math inline">\(\alpha\)</span>是一个较小的权值。 <span class="math display">\[\hat Q_N^\pi(s,a)  = \hat Q_{N-1}^\pi(s,a) + \alpha \cdot \left(G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a) \right)\]</span> <strong>增量</strong> ：实际回报与估计回报直接的误差。 <span class="math display">\[\delta = G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a)\]</span> <strong>2. 贝尔曼方程估计<span class="math inline">\(G(\tau ^{(N)})\)</span></strong></p><p>从<span class="math inline">\(s, a\)</span>开始，采样下一步的状态和动作<span class="math inline">\((s^\prime, a^\prime)\)</span> ，得到奖励<span class="math inline">\(r(s, a, s^\prime)\)</span>。</p><p><strong>无需得到完整的轨迹去计算总回报</strong>，使用贝尔曼方程去估计第N次试验后面<span class="math inline">\((s^\prime, a^\prime)\)</span>的总回报。</p><p>使用N-1次实验后的<span class="math inline">\(\hat Q^\pi_{N-1}(s^\prime, a^\prime)\)</span>，去估计第N次试验中后续<span class="math inline">\((s^\prime, a^\prime)\)</span>的总回报 <span class="math inline">\(G(\tau_{1:T}^{(N)} \mid \tau_{s_1} = s^\prime, \tau_{a_1} = a^\prime)\)</span>。 <span class="math display">\[\begin{align}G(\tau_{0:T}^{(N)}) &amp; =r(s, a, s^\prime) + \gamma \cdot \color{blue}{G(\tau_{1:T}^{(N)} \mid \tau_{s_1} = s^\prime, \tau_{a_1} = a^\prime)}\\&amp; \approx r(s, a, s^\prime) + \gamma \cdot \color{blue}{\hat Q^\pi_{N-1}(s^\prime, a^ \prime)} \end{align}\]</span> <strong>3. 两种算法</strong></p><ul><li>SARSA：同策略。采样下一个动作：<span class="math inline">\(a^\prime = \pi^\epsilon (s^\prime)\)</span>，值函数更新<span class="math inline">\(Q(s^\prime, a^\prime)\)</span>，更新的Q是关于策略<span class="math inline">\(\pi^\epsilon\)</span>的</li><li>Q学习算法：直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span>更新，更新的Q是关于策略<span class="math inline">\(\pi\)</span>的。</li></ul><p><strong>4. 蒙特卡罗方法和时序差分方法比较</strong></p><ul><li>蒙特卡罗方法：需要完整路径才能知道总回报，不依赖马尔可夫性质</li><li>时序差分学习：只需要一步就能知道总回报，依赖于马尔可夫性质</li></ul><p><strong>5. 总结</strong></p><p>贝尔曼估计总回报（马尔可夫性，动态规划） <span class="math display">\[G(\tau) \leftarrow r(s, a, s^\prime) + \gamma \cdot Q(s^\prime, a^\prime)\]</span> 增量更新值函数（蒙特卡罗） <span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (G(\tau) - Q(s, a))\]</span></p><p><span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (\underbrace{ r+ \gamma \cdot Q(s^\prime, a^\prime)}_{\color{blue}{实际值}} - \underbrace{Q(s, a)}_{\color{blue}{预期值}})\]</span></p><p><span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (r+ \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a)\]</span></p><h2 id="sarsa算法">SARSA算法</h2><blockquote><ol style="list-style-type: decimal"><li>当前<span class="math inline">\(s, a\)</span>， 奖励<span class="math inline">\(r(s, a, s^\prime)\)</span>， 新的<span class="math inline">\(s^\prime, a^\prime\)</span>， 优化<span class="math inline">\(Q(s,a)\)</span></li><li>贝尔曼<strong>估计实际奖励<span class="math inline">\(G(\tau)\)</span></strong>：<span class="math inline">\(r+ \gamma \cdot Q(s^\prime, a^\prime)\)</span></li><li><strong>增量计算Q</strong>：<span class="math inline">\(Q(s, a) \leftarrow Q(s, a) + \alpha \cdot (r+ \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a))\)</span></li><li><strong>更新策略<span class="math inline">\(\pi(s)\)</span></strong> ：<span class="math inline">\(\pi(s) = \arg \max_\limits{a \in \cal A} Q(s, a)\)</span></li><li>SARAS：优化所有<span class="math inline">\(Q(s,a)\)</span>直到收敛。对每一个<span class="math inline">\(s,a\)</span>，每一步状态转移，计算Q，直到s为终止状态</li></ol></blockquote><p>SARAS<code>State Action Reward State Action</code>算法，是一种同策略的时序差分学习算法。</p><p><strong>1. 思想目的</strong></p><p>要算出<span class="math inline">\(\hat Q_N^\pi(s,a)\)</span>，只需知道下面三项：</p><ul><li>当前状态和动作<span class="math inline">\((s, a)\)</span></li><li>得到的奖励<span class="math inline">\(r(s, a, s^\prime)\)</span></li><li>下一步的状态和动作<span class="math inline">\((s^\prime, a^\prime)\)</span></li></ul><p>不断优化Q函数，减少实际值和预期值的差距。</p><p><strong>2. 核心计算</strong></p><p>结合增量计算<span class="math inline">\(\hat Q_N^\pi(s,a)\)</span> ， 贝尔曼方程估计<span class="math inline">\(G(\tau ^{(N)})\)</span> <span class="math display">\[\hat Q_N^\pi(s,a)  = \hat Q_{N-1}^\pi(s,a) + \alpha \cdot \left(G(\tau ^{(N)}) - \hat Q_{N-1}^\pi(s,a) \right)\]</span></p><p><span class="math display">\[G(\tau_{0:T}^{(N)}) = r(s, a, s^\prime) + \gamma \cdot \hat Q^\pi_{N-1}(s^\prime, a^ \prime)\]</span></p><p>得到： <span class="math display">\[\hat Q_N^\pi(s,a) = (1-\alpha)\cdot \hat Q_{N-1}^\pi(s,a) + \alpha \cdot \left( r(s, a, s^\prime) + \gamma \cdot \hat Q^\pi_{N-1}(s^\prime, a^ \prime)\right)\]</span> <strong>简单点</strong>： <span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (G(\tau) - Q(s, a))\]</span></p><p><span class="math display">\[G(\tau) \leftarrow r(s, a, s^\prime) + \gamma \cdot Q(s^\prime, a^\prime)\]</span></p><p><span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (\underbrace{ r+ \gamma \cdot Q(s^\prime, a^\prime)}_{\color{blue}{实际值}} - \underbrace{Q(s, a)}_{\color{blue}{预期值}})\]</span></p><p><strong>3. SARSA算法步骤</strong></p><p>输入：状态空间<span class="math inline">\(\cal S\)</span>， 动作空间<span class="math inline">\(\cal A\)</span>，折扣率<span class="math inline">\(\gamma\)</span>， 学习率<span class="math inline">\(\alpha\)</span></p><p>输出：策略<span class="math inline">\(\pi(s)\)</span></p><p>1 初始化</p><p>随机初始化<span class="math inline">\(Q(s,a)\)</span>，平均初始化策略<span class="math inline">\(\pi(s)\)</span> <span class="math display">\[\forall s, \forall a, \quad \pi(a \mid s) = \frac{1}{\vert \cal A\vert}\]</span> 2 计算所有<span class="math inline">\(Q(s,a)\)</span>， 直到全部收敛</p><p>选择初始状态<span class="math inline">\(s\)</span>，和动作<span class="math inline">\(a=\pi^\epsilon(s)\)</span>。从<span class="math inline">\((s, a)\)</span>开始向后执行，直到<span class="math inline">\(s\)</span>为终止状态</p><p><strong>a. 执行动作，得到新状态和新动作</strong></p><ul><li>当前状态<span class="math inline">\(s\)</span>，动作<span class="math inline">\(a\)</span></li><li>执行动作<span class="math inline">\(a\)</span>：得到奖励<span class="math inline">\(r\)</span>和新状态<span class="math inline">\(s^\prime\)</span></li><li>选择新动作：<span class="math inline">\(a^\prime=\pi^\epsilon(s^\prime)\)</span></li></ul><p><strong>b. 增量计算 <span class="math inline">\(Q(s, a)\)</span></strong> <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> <strong>c. 更新策略<span class="math inline">\(\pi(s)\)</span></strong> <span class="math display">\[\pi(s) = \arg \max_\limits{a \in \cal A} Q(s, a)\]</span> <strong>d. 状态前进</strong> <span class="math display">\[s \leftarrow s^\prime, \quad a \leftarrow a^\prime\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/img/rl/sarsa.png" style="display:block; margin:auto" width="80%"></p><h2 id="q学习算法">Q学习算法</h2><blockquote><ol style="list-style-type: decimal"><li>SARAS：<span class="math inline">\(s,a \to r, s^\prime\)</span>， 选择新状态<span class="math inline">\(a^\prime = \pi^\epsilon(s^\prime)\)</span>，值函数<span class="math inline">\(Q(s^\prime, a^\prime)\)</span></li><li>Q：<span class="math inline">\(s\)</span>，选择当前动作<span class="math inline">\(a= \pi^\epsilon(s)\)</span>，<span class="math inline">\(\to r, s^\prime\)</span>，直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span></li></ol></blockquote><p>SARAS是Q学习算法的一种改进。</p><p><strong>1. SARAS</strong></p><p>1、当前状态<span class="math inline">\(s\)</span>，当前动作<span class="math inline">\(a\)</span> （初始时选择<span class="math inline">\(a=\pi^\epsilon(s)\)</span>，后续是更新得到的）</p><p>2、<strong>执行动作</strong><span class="math inline">\(a\)</span>，得到<strong>新状态</strong><span class="math inline">\(s^\prime\)</span>，得到<strong>奖励</strong><span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>4、<strong>依概率选择新动作</strong><span class="math inline">\(a = \pi^\epsilon(s^\prime)\)</span>，<strong>新状态新动作的值函数</strong>：<span class="math inline">\(Q(s^\prime, a^\prime)\)</span></p><p>5、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 6、更新状态和动作：<span class="math inline">\(s = s^\prime, a = a^\prime\)</span></p><p><strong>2. Q学习</strong></p><p>1、当前状态<span class="math inline">\(s\)</span>，选择当前动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>、得到新状态<span class="math inline">\(s^\prime\)</span>和奖励 <span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>3、<strong>不依概率选择新动作</strong>，而是<strong>直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span></strong></p><p>4、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot \max_{a^\prime} Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 5、更新状态：<span class="math inline">\(s = s^\prime\)</span></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/rl/q-learning.png" style="display:block; margin:auto" width="80%"></p><h1 id="深度q网络">深度Q网络</h1><h2 id="q网络">Q网络</h2><p><strong>1. Q网络</strong></p><p><span class="math inline">\(\mathbf s, \mathbf a\)</span> 是状态动作<span class="math inline">\(s,a\)</span>的向量表达。 用函数<span class="math inline">\(Q_\phi(\mathbf s, \mathbf a) \approx Q^\pi (s, a)\)</span> 。</p><p>参数为<span class="math inline">\(\phi\)</span>的神经网络<span class="math inline">\(Q_\phi(\mathbf s, \mathbf a)\)</span>，则称为<code>Q网络</code>。输入两个向量，输出为1个实数。 <span class="math display">\[Q_\phi(\mathbf s) = \begin{bmatrix}Q_\phi(\mathbf s, \mathbf a_1) \\\vdots \\Q_\phi(\mathbf s, \mathbf a_m) \\\end{bmatrix}\approx\begin{bmatrix}Q^\pi(s,  a_1) \\\vdots \\Q^\pi(s,  a_1) \\\end{bmatrix}\]</span> <strong>2. 两种逼近</strong></p><p>学习一组参数<span class="math inline">\(\phi\)</span>使得<span class="math inline">\(Q_\phi(\mathbf s, \mathbf a)\)</span>逼近值函数<span class="math inline">\(Q^\pi(s, a)\)</span>。</p><p>蒙特卡罗方法：<span class="math inline">\(\hat Q^\pi(s, a) = \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\)</span>，总回报的平均</p><p>时序差分方法：<span class="math inline">\(E[r + \gamma Q_\phi(\mathbf s^\prime, \mathbf a^\prime)]\)</span></p><p><strong>3. Q学习的目标函数</strong></p><p>以Q学习为例，采用随机梯度下降来优化，目标函数（减小差距）如下： <span class="math display">\[L(s, a, s^\prime; \phi) = \left(\underbrace{r + \gamma \cdot \max_{a^\prime} Q_\phi(\mathbf s^\prime, \mathbf a^\prime)}_{\color{blue}{实际目标值}} - \underbrace{Q_\phi(\mathbf s, \mathbf a)}_{\color{blue}{\text{网络值}}}\right)^2\]</span> 一般，标记是一个标量，不包含参数；不依赖于网络参数，与网络独立。 <span class="math display">\[J(\theta) = \frac{1}{2m} \sum_{i=1}^m\left (\underbrace{y^{(i)}}_{\color{blue}{\text{实际目标值}}}-\underbrace{f_\theta(\mathbf x^{(i)}) }_{\color{blue}{网络值}}\right)^2\]</span> 两个问题：</p><ul><li><strong>实际目标值不稳定</strong>。参数学习的目标依赖于参数本身。<strong>label本身也包含参数</strong></li><li>样本之间有很强的相关性</li></ul><h2 id="deep-q-network">Deep Q Network</h2><p>深度Q网络（deep Q-networks, DQN）</p><ul><li><strong>目标网络冻结</strong>。在一个时间段，固定目标值中的参数</li><li><strong>经验回放</strong>。构建<strong>经验池</strong>来去除数据相关性</li><li><code>经验池</code>。最近的经历组成的数据集</li></ul><h2 id="带经验回放的dqn算法">带经验回放的DQN算法</h2><p>带经验回放的深度Q网络。</p><p><strong>1. 初始化经验池、Q网络参数、目标Q网络参数</strong></p><ul><li>经验池： <span class="math inline">\(\cal D\)</span>，容量为N</li><li>Q网络参数：<span class="math inline">\(\phi\)</span></li><li>目标Q网络的参数：<span class="math inline">\(\hat \phi = \phi\)</span></li></ul><p><strong>2. 要让<span class="math inline">\(\forall s, \forall a, \; Q_\phi(\mathbf s,\mathbf a)\)</span>都收敛</strong></p><p>每一次初始化起始状态为<span class="math inline">\(s\)</span>， 遍历直到<span class="math inline">\(s\)</span>为最终态</p><p><strong>3. 每一时刻</strong></p><p><strong>生成新数据加入经验池</strong></p><p>1、状态<span class="math inline">\(s\)</span>， 选择动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>， 得到<span class="math inline">\(r\)</span>和<span class="math inline">\(s^\prime\)</span></p><p>3、<span class="math inline">\((s,a, r, s^\prime)\)</span> 加入经验池<span class="math inline">\(\cal D\)</span></p><p><strong>采经验池中采样一条数据计算</strong></p><p>1、从<span class="math inline">\(\cal D\)</span>中采样一条数据，<span class="math inline">\((ss,aa, rr, ss^\prime)\)</span>。 （<strong>去除样本相关性</strong>）</p><p>2、<strong>计算实际目标值</strong><span class="math inline">\(Q_{\hat \psi}(\mathbf{ss, aa})\)</span>。 （<strong>解决目标值不稳定的问题</strong>） <span class="math display">\[Q_{\hat \psi}(\mathbf{ss, aa}) =\begin{cases}&amp; rr, &amp; ss^\prime 为终态 \\&amp; rr + \gamma \cdot \max_\limits{\mathbf a^\prime}Q_{\hat \phi}(\mathbf {ss^\prime}, \mathbf {a^\prime}), &amp;其它\end{cases}\]</span> 3、损失函数如下，梯度下降法去训练Q网络 <span class="math display">\[J(\phi)= \left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - y \right)^2=\left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - Q_{\hat \psi}(\mathbf{ss, aa}) \right)^2\]</span> <strong>状态前进</strong></p><p><span class="math inline">\(s \leftarrow s^\prime\)</span></p><p><strong>更新目标Q网络的参数</strong></p><p>每隔C步更新：<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></p><p><strong>4. DQN算法中经验池的优点</strong></p><p>1、去除样本相关性，避免陷入局部最优</p><p>经验池中抽取样本代替当前样本进行训练，打破了与相邻训练样本的相关性，避免陷入局部最优。</p><p>2、经验回放类似于监督学习</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/rl/deep-qnet.png" style="display:block; margin:auto" width="70%">s</p><h1 id="总结">总结</h1><h2 id="策略迭代">策略迭代</h2><p>已知模型。利用<strong>贝尔曼方程</strong>（<code>算均值</code>）迭代计算出<span class="math inline">\(V(s)\)</span>，再算出<span class="math inline">\(Q(s,a)\)</span>。选择最好的动作<span class="math inline">\(a\)</span>去优化策略<span class="math inline">\(\pi(s)\)</span>。 <span class="math display">\[\forall s, \quad V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)] \]</span></p><p><span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[\forall s, \qquad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span></p><h2 id="值迭代">值迭代</h2><p>已知模型。利用<strong>贝尔曼最优方程</strong>迭代算出<span class="math inline">\(V(s)\)</span>，再算出<span class="math inline">\(Q(s,a)\)</span>。选择最好的动作<span class="math inline">\(a\)</span>去优化策略<span class="math inline">\(\pi(s)\)</span>。 <span class="math display">\[\forall s \in S, \quad V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span></p><p><span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[\forall s, \quad \pi(s) = \arg \max_\limits{a} Q(s, a)\]</span></p><h2 id="蒙特卡罗">蒙特卡罗</h2><p>未知模型。从<span class="math inline">\((s,a)\)</span><strong>随机游走，采集N个样本</strong>。使用<strong>所有轨迹回报平均值近似估计<span class="math inline">\(Q(s,a)\)</span></strong> ，再去改进策略。重复，直至收敛。 <span class="math display">\[Q^\pi(s, a)  \approx \hat Q^\pi(s, a) = \frac{1}{N} \sum_{n=1}^NG(\tau^{(n)})\]</span></p><h2 id="时序差分算法">时序差分算法</h2><p>无需知道完整轨迹就能对策略进行评估。</p><p>时序差分学习=动态规划-贝尔曼估计<span class="math inline">\(G(\tau)\)</span> + 蒙特卡罗采样-增量计算<span class="math inline">\(Q(s,a)\)</span></p><p>贝尔曼估计轨迹总回报<span class="math inline">\(G(\tau)\)</span> <span class="math display">\[G(\tau) \leftarrow r(s, a, s^\prime) + \gamma \cdot Q(s^\prime, a^\prime)\]</span> 增量计算<span class="math inline">\(Q(s,a)\)</span> <span class="math display">\[Q(s, a) \leftarrow  Q(s, a) + \alpha \cdot (\underbrace{ r+ \gamma \cdot Q(s^\prime, a^\prime)}_{\color{blue}{实际值}} - \underbrace{Q(s, a)}_{\color{blue}{预期值}})\]</span></p><h2 id="sarsa">SARSA</h2><p>同策略的时序差分算法，是Q学习的改进。</p><p>1、当前状态<span class="math inline">\(s\)</span>，当前动作<span class="math inline">\(a\)</span> （初始时选择<span class="math inline">\(a=\pi^\epsilon(s)\)</span>，后续是更新得到的）</p><p>2、<strong>执行动作</strong><span class="math inline">\(a\)</span>，得到<strong>新状态</strong><span class="math inline">\(s^\prime\)</span>，得到<strong>奖励</strong><span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>4、<strong>依概率选择新动作</strong><span class="math inline">\(a = \pi^\epsilon(s^\prime)\)</span>，<strong>新状态新动作的值函数</strong>：<span class="math inline">\(Q(s^\prime, a^\prime)\)</span></p><p>5、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 6、更新状态和动作：<span class="math inline">\(s = s^\prime, a = a^\prime\)</span></p><h2 id="q学习">Q学习</h2><p>1、当前状态<span class="math inline">\(s\)</span>，选择当前动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>、得到新状态<span class="math inline">\(s^\prime\)</span>和奖励 <span class="math inline">\(r(s,a,s^\prime)\)</span></p><p>3、<strong>不依概率选择新动作</strong>，而是<strong>直接选择最大的值函数<span class="math inline">\(\max_\limits{a^\prime}Q(s^\prime, a^\prime)\)</span></strong></p><p>4、更新Q函数 <span class="math display">\[Q(s, a) \leftarrow Q(s, a) + \alpha\cdot \left( r + \gamma \cdot \max_{a^\prime} Q(s^\prime, a^\prime) - Q(s, a) \right)\]</span> 5、更新状态：<span class="math inline">\(s = s^\prime\)</span></p><h2 id="q网络-1">Q网络</h2><p>使用神经网络<span class="math inline">\(Q_{\phi}(\mathbf{s,a})\)</span>去近似值函数<span class="math inline">\(Q(s,a)\)</span>。两个问题：实际目标值不稳定；样本之间具有强相关性。 <span class="math display">\[L(s, a, s^\prime; \phi) = \left(\underbrace{r + \gamma \cdot \max_{a^\prime} Q_\phi(\mathbf s^\prime, \mathbf a^\prime)}_{\color{blue}{实际目标值}} - \underbrace{Q_\phi(\mathbf s, \mathbf a)}_{\color{blue}{\text{网络值}}}\right)^2\]</span></p><h2 id="dqn">DQN</h2><p>深度Q网络：</p><ul><li><strong>目标网络冻结</strong>-<strong>稳定目标值</strong>。<span class="math inline">\(Q_{\phi}(\mathbf{s,a})\)</span>训练网络，<span class="math inline">\(Q_{\hat \phi}(\mathbf{s,a})\)</span>目标值网络。定期更新参数<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></li><li><strong>经验池的经验回放</strong>-<strong>去除样本相关性</strong>- 每次采集一条数据放入经验池，再从经验池取数据进行训练。</li></ul><p><strong>生成新数据加入经验池</strong></p><p>1、状态<span class="math inline">\(s\)</span>， 选择动作<span class="math inline">\(a = \pi^\epsilon(s)\)</span></p><p>2、执行动作<span class="math inline">\(a\)</span>， 得到<span class="math inline">\(r\)</span>和<span class="math inline">\(s^\prime\)</span></p><p>3、<span class="math inline">\((s,a, r, s^\prime)\)</span> 加入经验池<span class="math inline">\(\cal D\)</span></p><p><strong>采经验池中采样一条数据计算</strong></p><p>1、从<span class="math inline">\(\cal D\)</span>中采样一条数据，<span class="math inline">\((ss,aa, rr, ss^\prime)\)</span>。 （<strong>去除样本相关性</strong>）</p><p>2、<strong>计算实际目标值</strong><span class="math inline">\(Q_{\hat \psi}(\mathbf{ss, aa})\)</span>。 （<strong>解决目标值不稳定的问题</strong>） <span class="math display">\[Q_{\hat \psi}(\mathbf{ss, aa}) =\begin{cases}&amp; rr, &amp; ss^\prime 为终态 \\&amp; rr + \gamma \cdot \max_\limits{\mathbf a^\prime}Q_{\hat \phi}(\mathbf {ss^\prime}, \mathbf {a^\prime}), &amp;其它\end{cases}\]</span> 3、<code>损失函数</code>如下，<strong>梯度下降法去训练Q网络</strong> <span class="math display">\[J(\phi)= \left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - y \right)^2=\left ( Q_{\phi}(\mathbf {ss}, \mathbf {aa}) - Q_{\hat \psi}(\mathbf{ss, aa}) \right)^2\]</span> <strong>状态前进</strong></p><p><span class="math inline">\(s \leftarrow s^\prime\)</span></p><p><strong>更新目标Q网络的参数</strong></p><p>每隔C步更新：<span class="math inline">\(\hat \phi \leftarrow \phi\)</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;强化学习基于值函数的学习方法。最重要要是SARSA、Q学习、DQN。但是这些都依赖于前面的动态规划和蒙特卡罗方
      
    
    </summary>
    
      <category term="强化学习" scheme="http://plmsmile.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="值函数" scheme="http://plmsmile.github.io/tags/%E5%80%BC%E5%87%BD%E6%95%B0/"/>
    
      <category term="时序差分" scheme="http://plmsmile.github.io/tags/%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/"/>
    
      <category term="SARSA" scheme="http://plmsmile.github.io/tags/SARSA/"/>
    
      <category term="Q学习" scheme="http://plmsmile.github.io/tags/Q%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Q网络" scheme="http://plmsmile.github.io/tags/Q%E7%BD%91%E7%BB%9C/"/>
    
      <category term="DQN" scheme="http://plmsmile.github.io/tags/DQN/"/>
    
      <category term="蒙特卡罗" scheme="http://plmsmile.github.io/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97/"/>
    
      <category term="动态规划" scheme="http://plmsmile.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
      <category term="策略迭代" scheme="http://plmsmile.github.io/tags/%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3/"/>
    
      <category term="值迭代" scheme="http://plmsmile.github.io/tags/%E5%80%BC%E8%BF%AD%E4%BB%A3/"/>
    
  </entry>
  
  <entry>
    <title>阅读理解模型总结</title>
    <link href="http://plmsmile.github.io/2018/04/12/39-squard-models/"/>
    <id>http://plmsmile.github.io/2018/04/12/39-squard-models/</id>
    <published>2018-04-12T05:54:12.000Z</published>
    <updated>2018-04-12T07:15:35.145Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="qanet">QANet</h1><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/cnn_self_attention.png" style="display:block; margin:auto" width="100%"></p><h1 id="bidaf">BiDAF</h1><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/papers/bidaf.png" style="display:block; margin:auto" width="100%"></p><h1 id="aoa">AoA</h1><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/papers/aoa.png" style="display:block; margin:auto" width="100%"></p><h1 id="dcn">DCN</h1><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/deep-residual-coattention-encoder.png" style="display:block; margin:auto" width="90%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dcn-decoder.png" style="display:block; margin:auto" width="100%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/hmn.png" style="display:block; margin:auto" width="60%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;qanet&quot;&gt;QANet&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://otafnwsmg.bkt.clouddn.com
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="QANet" scheme="http://plmsmile.github.io/tags/QANet/"/>
    
      <category term="机器阅读理解" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>卷积相关</title>
    <link href="http://plmsmile.github.io/2018/04/11/38-convolution/"/>
    <id>http://plmsmile.github.io/2018/04/11/38-convolution/</id>
    <published>2018-04-11T07:42:36.000Z</published>
    <updated>2018-04-12T03:35:38.783Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="卷积神经网络">卷积神经网络</h1><p>全连接网络的两个问题：</p><ul><li>参数太多：训练效率低、容易过拟合</li><li>局部不变形特征：全连接很难提取出图片的不变性特征</li></ul><h2 id="三个特性">三个特性</h2><p><strong>1. 局部性</strong></p><p>图片特征只在局部。图片特征决定图片类别，这些图片特征在一些局部的区域中。</p><p>局部连接。</p><p><strong>2. 相同性</strong></p><p>用同样的检测模式去检测不同图片的相同特征。只是这些特征出现在图片的不同位置。</p><p>参数共享。</p><p><strong>3. 不变性</strong></p><p>对于一张大图片，进行下采样，图片的性质基本保持不变。</p><p>下采样保持不变性。</p><h2 id="卷积">卷积</h2><blockquote><ol style="list-style-type: decimal"><li>一维卷积：卷积核、步长、首位0填充</li><li>三种卷积：窄卷积、宽卷积、等长卷积</li><li>二维卷积</li></ol></blockquote><p><strong>1. 一维卷积</strong></p><ul><li>卷积核：参数<span class="math inline">\([1, 0, -1]\)</span>就是一个<code>卷积核</code> 或 <code>滤波器</code></li><li>步长：卷积核滑动的间隔</li><li>零填充：在输入向量两端进行补零</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/ondim-conv-2.jpeg" style="display:block; margin:auto" width="70%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/onedim-conv-0.png" style="display:block; margin:auto" width="50%"> <strong>2. 三种卷积</strong></p><p>输入n，卷积大小m，步长s，输入神经元各两端填补p个0</p><ul><li>窄卷积：<code>s=1</code>，不补0，输出长度为<code>n-m+1</code></li><li>宽卷积：<code>s=1</code>，两端补0，<span class="math inline">\(p=m-1\)</span>， 输出长度为<code>n+m-1</code></li><li>等长卷积：<code>s=1</code>，两端补0，<span class="math inline">\(p=\frac{m-1}{2}\)</span>， 输出长度为<code>n</code></li></ul><p>一般卷积默认为窄卷积。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/onedim-conv-1.png" style="display:block; margin:auto" width="50%"></p><p><strong>3. 二维卷积</strong></p><p>输入一张图片（假设深度为1），<span class="math inline">\(X \in \mathbb R^{M \times N}\)</span>， 卷积核<span class="math inline">\(W \in \mathbb R ^{m \times n}\)</span> ，则卷积（互相关代替）结果为： <span class="math display">\[y_{ij} = \sum_{u=1}^m \sum_{v=1}^n w_{uv} x_{i+u-1, j+v-1}\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/twodim-conv-0.png" style="display:block; margin:auto" width="70%"></p><p>一个卷积核提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。</p><p>卷积后的结果称为<code>特征映射（feature map）</code>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/twodim-conv-2.png" style="display:block; margin:auto" width="50%"></p><h2 id="卷积层">卷积层</h2><blockquote><ol style="list-style-type: decimal"><li>一个卷积核</li></ol><p><span class="math inline">\(W_p \in \mathbb R ^{m \times n \times D}\)</span>， 对D个通道做卷积，结果相加求和，过激活函数，得到一个特征图<span class="math inline">\(Y^p \in\mathbb R^{M^\prime \times N^\prime}\)</span></p><ol start="2" style="list-style-type: decimal"><li>多个卷积核：得到P个特征图</li></ol></blockquote><p>输入图片(feature map)是<span class="math inline">\(X \in \mathbb R^{M \times N \times D}\)</span>，深度是D</p><p><strong>1. 一个卷积核</strong></p><ul><li>用1个卷积核<span class="math inline">\(W_p \in \mathbb R ^{m \times n \times D}\)</span>（实际上是D个<span class="math inline">\(\mathbb R^{m\times n}\)</span>）去卷积这张图片（所有深度）</li><li>对各个深度的卷积结果进行<strong>相加求和</strong>，再<strong>加上偏置</strong></li><li><strong>过激活函数</strong>，输出最终的FM，是<span class="math inline">\(Y^p\)</span></li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv.png" style="display:block; margin:auto" width="100%"></p><p><strong>2. 多个卷积核</strong></p><p>多个卷积核可以提取出多种不同的特征。输入图片是<span class="math inline">\(X \in \mathbb R^{M \times N \times D}\)</span>，</p><ul><li>有P个不同的卷积核<span class="math inline">\(W_p \in \mathbb R ^{m \times n \times D}\)</span>， 实际上是四维的[m, n, D, P]，后两维是<code>in_channel</code>、<code>out_channel</code></li><li>输出P个特征图<span class="math inline">\(\mathbb R^{M^\prime \times N^\prime \times P}\)</span></li><li>对每一个卷积核<span class="math inline">\(W \in \mathbb R ^{m \times n \times D}\)</span>，对D个深度<span class="math inline">\(\mathbb R ^{m \times n}\)</span>分别做卷积，对D个卷积结果进行求和相加，经过激活函数，得到一个特征图 <span class="math inline">\(Y^p \in\mathbb R^{M^\prime \times N^\prime}\)</span></li><li>一共需要<span class="math inline">\(P \times D \times (m \times n) + P\)</span>个参数</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv_layer.gif" style="display:block; margin:auto" width="80%"></p><h2 id="卷积代替全连接">卷积代替全连接</h2><blockquote><ol style="list-style-type: decimal"><li>局部连接：卷积核只与输入的一个局部做连接，计算出FM中的一个值，局部性</li><li>权值共享：同一个卷积核与图片的各个位置进行连接，权值是一样的，提取出同样的特征</li></ol></blockquote><p><strong>1. 局部连接</strong></p><ul><li>卷积层的神经元只与输入数据的一个局部区域做连接</li><li>因为图片的局部性，图片的特征在局部</li><li>FM中的每一个值，只与输入的局部相关。而不是与所有的相关</li></ul><p><strong>2. 权值共享</strong></p><ul><li>一个卷积核会分多次对输入数据的各个部分做卷积操作</li><li>对每个部分的连接参数实际上是相同的，因为是同一个卷积核</li><li>因为图片的相同性，同样的卷积核可以检测出相同的特征，只是特征在不同的位置</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/full-conv.png" style="display:block; margin:auto" width="60%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/threedim-conv.png" style="display:block; margin:auto" width="60%"></p><h2 id="汇聚层">汇聚层</h2><blockquote><ol style="list-style-type: decimal"><li>卷积层的不足：FM的维数很高</li><li>汇聚层的作用：选择特征、降低特征数量、减少参数数量、避免过拟合</li><li>两种汇聚方式：最大和平均。</li></ol></blockquote><p><strong>1. 卷积层的不足</strong></p><ul><li>减少网络连接数量</li><li>但是<strong>FM中的神经元个数依然很多</strong></li><li>如果直接接分类器全连接，则维数会很高，<strong>容易过拟合</strong></li></ul><p><strong>2. 汇聚层的作用</strong></p><p><code>汇聚层</code>(pooling layer)，也作子采样层(subsampling layer)。作用是：</p><ul><li>进行特征选择</li><li>降低特征数量</li><li>进而减少参数数量、避免过拟合</li><li>拥有更大感受野，大图片缩小，保持<code>不变性</code></li></ul><p><strong>3. 两种汇聚方式</strong></p><ul><li>最大汇聚：一个区域内所有神经元的最大值</li><li>平均汇聚：一个区域内所有神经元的平均值</li></ul><p>过大采样区会急剧减少神经元的数量，造成过多的信息损失！</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/max-pool.png" style="display:block; margin:auto" width="90%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/pool.jpeg" style="display:block; margin:auto" width="40%"></p><h2 id="典型的卷积网络结构">典型的卷积网络结构</h2><p>由多个捐几块组成，一个卷积块：</p><ul><li>连续2~5个卷积层，ReLU激活函数</li><li>0~1个汇聚层</li></ul><p>目前，趋向于使用更小的卷积核，比如<span class="math inline">\(1\times 1, 3 \times 3\)</span>。汇聚层的比例也逐渐降低，趋向于全卷积网络。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv-nn.png" style="display:block; margin:auto" width="100%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/convnet.jpeg" style="display:block; margin:auto" width="70%"></p><h1 id="常见卷积网络">常见卷积网络</h1><h2 id="lenet">LeNet</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/LeNet.png" style="display:block; margin:auto" width="70%"></p><h2 id="alex-net">Alex Net</h2><p>使用ReLU作为非线性激活函数、Dropout防止过拟合、数据增强提高模型准确率。</p><p><strong>AlexNet分组卷积</strong></p><ul><li>对所有通道进行分组，进行分组卷积，执行<strong>标准卷积操作</strong></li><li>在最后时刻才使用两个全连接融合通道的信息</li><li>降低了模型的泛化能力</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/AlexNet.png" style="display:block; margin:auto" width="80%"></p><h2 id="inception-net">Inception Net</h2><p>如何选择卷积核大小非常关键：</p><ul><li><strong>一个卷积层同时使用多种尺寸的卷积核</strong></li><li>先过<span class="math inline">\(1\times 1\)</span>卷积减少卷积层参数量</li></ul><p>Inception Net由多个Inception模块堆叠而成。一个Inception同时使用<span class="math inline">\(1\times 1\)</span>、<span class="math inline">\(3\times 3\)</span>、<span class="math inline">\(5\times 5\)</span> 的卷积，如下：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/inception.png" style="display:block; margin:auto" width="70%"></p><p><span class="math inline">\(3\times 3\)</span> 、<span class="math inline">\(5\times 5\)</span> 卷积前，先进行<span class="math inline">\(1\times 1\)</span>卷积的作用：</p><ul><li>减少输入数据的深度</li><li>减少各个深度的冗余信息，先进行一次特征抽取</li></ul><p>后续还有各种各样的Inception Net，最终演变成Xception Net。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/depth-wise-conv.jpg" style="display:block; margin:auto" width="60%"></p><p>Inception Net的极限就是，对每个channel做一个单独的卷积。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/depth-wise-conv2.jpg" style="display:block; margin:auto" width="60%"></p><h2 id="res-net">Res Net</h2><p>越深的网络可以用ResNet来训练。<a href="https://zhuanlan.zhihu.com/p/28124810?group_id=883267168542789632" target="_blank" rel="noopener">ResNet可以很深的原因</a></p><p><code>残差连接</code>通过<strong>给非线性的卷积层增加直连边</strong>的方式</p><ul><li>来<strong>提高信息的传播效率</strong></li><li>可以减小梯度消失问题</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/resnet.png" style="display:block; margin:auto" width="70%"></p><h2 id="xception">Xception</h2><p>卷积需要同时考虑所有通道吗？</p><p>输入图片(feature map)是<span class="math inline">\(X \in \mathbb R^{M \times N \times D}\)</span>，深度是D</p><p><strong>1. 传统卷积核会同时考虑所有通道</strong></p><ul><li>传统<strong>1个卷积核</strong>会对所有channel的FM做同样的卷积</li><li>得到D个卷积结果</li><li>再<strong>对D个卷积结果进行相加求过激活函数</strong>得到<strong>一个FM</strong></li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv-stantard.jpg" style="display:block; margin:auto" width="70%"></p><p><strong>2. 深度可分离卷积核</strong></p><p><code>Depth Separable Convolution</code></p><p>输入数据有D个FM，输出P个FM。<code>深度可分离卷积(DepthWise Convolution)</code> 如下：</p><ul><li>对<span class="math inline">\(X\)</span>的每个channel，分别做一个单独的卷积，得到D个新的FM</li><li>对D个新的FM，做<span class="math inline">\(1\times 1\)</span>的传统卷积(<code>PointWise Convolution</code>)，<span class="math inline">\(P \times D \times (1 \times 1)\)</span></li><li>最终输出P个FM （通道数变换）</li></ul><p>卷积操作不一定需要同时考虑通道和区域。<code>可分离卷积</code>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/xception-2.jpg" style="display:block; margin:auto" width="70%"></p><p><strong>3. 可分离卷积参数大大减小</strong></p><p>输入通道<span class="math inline">\(D=3\)</span>，输出通道<span class="math inline">\(P=256\)</span>，卷积核大小为<span class="math inline">\(3 \times 3\)</span></p><ul><li>传统卷积参数：<span class="math inline">\(256 \times 3 \times (3 \times 3) = 6912\)</span></li><li>DepthWise卷积参数：<span class="math inline">\(3 \times 3 \times 3 +256 \times 3 \times (1 \times 1) =795\)</span>， 降低九分之一</li></ul><p>同时，效果更好。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/depthwise-conv-compare.jpg" style="display:block; margin:auto" width="60%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/xception.jpg" style="display:block; margin:auto" width="70%"></p><h2 id="shuffle-net">Shuffle Net</h2><p><strong>1. AlexNet分组卷积</strong></p><ul><li>对所有通道进行分组，进行分组卷积，执行<strong>标准卷积操作</strong></li><li>在最后时刻才使用全连接融合通道的信息</li><li>降低了模型的泛化能力</li></ul><p><strong>2. ShuffleNet 分组卷积</strong></p><p><code>ShuffleNet</code> = <strong>分组卷积</strong>（通道分组）+ <strong>深度可分离卷积</strong>（Depthwise+PointWise）</p><p>对通道进行分组卷积时</p><ul><li>每一个组执行深度可分离卷积，而不是标准传统卷积</li><li>每一次层叠分组卷积时，都进行channel shuffle</li><li>实际上每个组各取一个也能实现shuffle</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/Reading_Note_20170720_ShuffleNet_0.png" style="display:block; margin:auto" width="100%"></p><h2 id="senet">SENet</h2><p>Inception、ShuffleNet等网络中，<strong>对所有通道产生的特征</strong>都是<strong>不分权重直接相加求和</strong>的。</p><p>为什么所有通道的特征对模型的作用是相等的呢？</p><p><img src="" style="display:block; margin:auto" width="70%"></p><h2 id="总结">总结</h2><p>参考自<a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">知乎卷积网络中十大拍案叫绝的操作</a></p><p><strong>1. 卷积核</strong></p><ol style="list-style-type: decimal"><li>大卷积核用多个小卷积核代替</li><li>单一尺寸卷积核用多尺寸卷积核代替</li><li>固定形状卷积核趋于用可变形卷积核</li><li>使用<span class="math inline">\(1\times 1\)</span>卷积核</li></ol><p><strong>2. 卷积层通道</strong></p><ol style="list-style-type: decimal"><li>标准卷积使用深度可分离卷积代替</li><li>使用分组卷积</li><li>分组卷积前使用channel shuffle</li><li>通道加权计算</li></ol><p><strong>3. 卷积层连接</strong></p><ol style="list-style-type: decimal"><li>使用skip connection，让模型更深</li><li>densely connection，使每一层都融合其它层的特征输出</li></ol><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv-compare.jpg" style="display:block; margin:auto" width="70%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;卷积神经网络&quot;&gt;卷积神经网络&lt;/h1&gt;
&lt;p&gt;全连接网络的两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参数太多：训练效率低、容易
      
    
    </summary>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="卷积" scheme="http://plmsmile.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="可分离卷积" scheme="http://plmsmile.github.io/tags/%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>强化学习</title>
    <link href="http://plmsmile.github.io/2018/04/01/37-reinforce-learning/"/>
    <id>http://plmsmile.github.io/2018/04/01/37-reinforce-learning/</id>
    <published>2018-04-01T01:30:47.000Z</published>
    <updated>2018-04-24T09:57:02.791Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="70%"></p><blockquote><p>强化学习的基础知识。基本要素、轨迹、值函数、V函数和Q函数、贝尔曼方程。</p></blockquote><h1 id="强化学习定义">强化学习定义</h1><h2 id="概览">概览</h2><p>强化学习是指一个<strong>智能体</strong>从与<strong>环境</strong>的<strong>交互</strong>中不断学习去完成特定的目标。</p><p>强化学习不需要给出正确策略作为监督信息，只需要给出<code>策略的（延迟）回报</code>，并通过<strong>调整策略</strong>来取得最大化的期望回报。</p><blockquote><ol style="list-style-type: decimal"><li>智能体、环境</li><li>环境状态<span class="math inline">\(s\)</span>，智能体的动作<span class="math inline">\(a\)</span>，智能体的策略<span class="math inline">\(\pi(a\mid s)\)</span>，状态转移概率<span class="math inline">\(p(s_{t+1}\mid s_t, a_t)\)</span>，即使奖励<span class="math inline">\(r(s, a, s \prime)\)</span></li></ol></blockquote><h2 id="智能体和环境">智能体和环境</h2><p><strong>智能体</strong></p><ul><li>感知环境的状态和反馈的奖励，进行学习和决策</li><li><code>决策</code> ：根据 -- 环境状态 -- 做出不同的动作</li><li><code>学习</code>： 根据 -- 反馈奖励 -- 调整策略</li></ul><p><strong>环境</strong></p><ul><li>智能体外部的所有事物</li><li>收到 -- 智能体的动作 -- 改变状态</li><li>给 -- 智能体 -- 反馈奖励</li></ul><h2 id="个基本要素">5个基本要素</h2><p><strong>状态<span class="math inline">\(s\)</span></strong></p><p>环境的状态，状态空间<span class="math inline">\(\mathcal S\)</span>， 离散/连续</p><p><strong>动作<span class="math inline">\(a\)</span></strong></p><p>智能体的行为，动作空间<span class="math inline">\(\mathcal A\)</span>， 离散/连续</p><p><strong>策略<span class="math inline">\(\pi(a\mid s)\)</span></strong></p><p>智能体 根据 -- 环境状态s -- 决定下一步的动作a 的函数</p><p><strong>状态转移概率<span class="math inline">\(p(s \prime \mid s, a)\)</span></strong></p><p>根据 -- 当前状态<span class="math inline">\(s\)</span>和智能体的动作<span class="math inline">\(a\)</span> -- 环境状态变为<span class="math inline">\(s \prime\)</span>的概率</p><p><strong>即时奖励<span class="math inline">\(r(s, a, s\prime)\)</span></strong></p><p>环境给智能体的奖励，标量函数。根据 -- 环境当前状态 、智能体执行的动作、环境新状态</p><h2 id="智能体的策略">智能体的策略</h2><p><span class="math inline">\(\pi(a \mid s)\)</span> 智能体根据环境状态决定下一步的动作。分为确定性策略和<code>随机性策略</code>。 <span class="math display">\[\pi(a\mid s) \triangleq p(a\mid s) , \quad \quad \sum_{a \in \mathcal A} \pi(a\mid s) = 1\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/img/rl/agent-env-interact.png" style="display:block; margin:auto" width="50%"></p><h1 id="马尔科夫决策过程">马尔科夫决策过程</h1><blockquote><ol style="list-style-type: decimal"><li>马尔可夫过程，<span class="math inline">\(p(s_{t+1}\mid s_t)\)</span></li><li>马尔可夫决策过程，<span class="math inline">\(p(s_{t+1} \mid s_t, a_t)\)</span></li><li>轨迹，给初始状态，智能体与环境的一次交互过程</li></ol></blockquote><h2 id="马尔可夫过程">马尔可夫过程</h2><p>状态序列<span class="math inline">\(s_0, s_1, \cdots, s_t\)</span>具有马尔可夫性，<span class="math inline">\(s_{t+1}\)</span>只依赖于<span class="math inline">\(s_t\)</span> <span class="math display">\[p(s_{t+1}\mid s_t, \cdots, s_0) = p(s_{t+1} \mid s_t)\]</span> ## 马尔可夫决策过程</p><p><span class="math inline">\(s_{t+1}\)</span>依赖于<span class="math inline">\(s_t\)</span>和<span class="math inline">\(a_t\)</span>， 即<strong>环境新状态</strong>依赖于<strong>当前状态</strong>和<strong>当前智能体的动作</strong>。 <span class="math display">\[p(s_{t+1} \mid s_t, a_t, \cdots, s_0, a_0) = p(s_{t+1}\mid s_t, a_t)\]</span> <strong>智能体与环境的交互</strong>是一个<code>马尔可夫决策过程</code>。</p><h2 id="轨迹">轨迹</h2><p>给定策略<span class="math inline">\(\pi(a\mid s)\)</span>， <strong>轨迹</strong>是智能体与环境的<strong>一次交互过程</strong>，是一个马尔可夫决策过程，如下： <span class="math display">\[\tau = s_0, a_0, s_1, r_1, \cdots, s_{T-1}, a_{T-1}, s_{T}, r_{T}\]</span> 其中<span class="math inline">\(r_t = r(s_{t-1}, a_{t-1}, s_t)\)</span>是时刻<span class="math inline">\(t\)</span>的即时奖励。</p><p>轨迹的概率</p><ul><li>初始状态</li><li>所有时刻概率的乘积</li><li><strong>智能体执行动作，环境更新状态</strong></li></ul><p><span class="math display">\[p(\tau) = p(s_0) \prod_{t=0}^{T-1}\pi(a_t \mid s_t) p(s_{t+1} \mid s_t, a_t)\]</span></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/rl/markov-decision-process.png" style="display:block; margin:auto" width="70%"></p><h1 id="目标函数">目标函数</h1><blockquote><ol style="list-style-type: decimal"><li>一个轨迹的总回报。<span class="math inline">\(G(\tau) = \sum_{t=0}^{T-1} r_{t+1}\)</span></li><li>一个策略的期望回报。<span class="math inline">\(E_{\tau \sim p(\tau)} [G(\tau)]\)</span>。 所有轨迹的回报的期望</li><li>强化学习的目标。学一个策略<span class="math inline">\(\pi_{\theta}(a \mid s)\)</span>， 最大化这个策略的期望回报</li></ol></blockquote><h2 id="轨迹的总回报">轨迹的总回报</h2><p><strong>1. 某一时刻的奖励</strong></p><p><span class="math inline">\(r_t = r(s_{t-1}, a_{t-1}, s_t)\)</span>是<span class="math inline">\(t\)</span>时刻， 环境给智能体的<strong>奖励</strong>。</p><p>给定策略<span class="math inline">\(\pi(a\mid s)\)</span>， 智能体与环境<code>一次交互过程</code>(回合，试验)为轨迹<span class="math inline">\(\tau\)</span></p><p><strong>2. 一条轨迹的总回报</strong></p><p>总回报是一条轨迹所有时刻的累积奖励和。 <span class="math display">\[G(\tau) = \sum_{t=0}^{T-1} r(s_{t-1}, a_{t-1}, s_t) =  \sum_{t=0}^{T-1} r_{t+1}\]</span> <strong>3. 一条轨迹的折扣回报</strong></p><p>折扣回报引入<code>折扣率</code>，<strong>降低远期回报的权重</strong>（T无限大时）。 <span class="math display">\[G(\tau) = \sum_{t=0}^{T-1} \gamma^t r_{t+1}, \quad \quad \gamma \in [0, 1]\]</span> <code>折扣率</code><span class="math inline">\(\gamma\)</span></p><ul><li><span class="math inline">\(\gamma \sim 0\)</span>， 在意短期回报</li><li><span class="math inline">\(\gamma \sim 1\)</span>， 在意长期回报</li></ul><h2 id="策略的期望回报">策略的期望回报</h2><p>给一个策略<span class="math inline">\(\pi(a\mid s)\)</span>， <strong>有多个轨迹</strong>。</p><p>一个策略的<code>期望回报</code>：该策略下<strong>所有轨迹总回报的期望值</strong>。 <span class="math display">\[E_{\tau \sim p(\tau)} [G(\tau)] =E_{\tau \sim p(\tau)} [\sum_{t=0}^{T-1}r_{t+1}]\]</span></p><h2 id="强化学习的目标">强化学习的目标</h2><p>强化学习的目标是<strong>学习到一个策略<span class="math inline">\(\pi_{\theta}(a\mid s)​\)</span></strong>，来<strong>最大化这个策略的期望回报</strong>。<strong>希望智能体能够获得更多的回报</strong>。 <span class="math display">\[J(\theta) = E_{\tau \sim p_{\theta}(\tau)} [\sum_{t=0}^{T-1}\gamma ^tr_{t+1}]\]</span></p><h1 id="值函数">值函数</h1><blockquote><ol style="list-style-type: decimal"><li>状态值函数。<span class="math inline">\(V^\pi(s)\)</span>, 初始状态为s，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报。</li><li>贝尔曼方程迭代计算值函数</li><li>状态-动作值函数。<span class="math inline">\(Q^\pi(s, a)\)</span>， 初始状态为s，进行动作a，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报</li><li>V函数与Q函数的关系。<span class="math inline">\(V^\pi(s) = E_{a \sim \pi(a \mid s)}[Q^\pi(s, a)]\)</span></li><li>值函数的作用。评估策略<span class="math inline">\(\pi(a \mid s)\)</span>， 对好的动作a（<span class="math inline">\(Q^\pi(s, a)\)</span>大 ），增大其概率<span class="math inline">\(\pi(a \mid s)\)</span></li></ol></blockquote><h2 id="状态值函数">状态值函数</h2><p>状态值函数<span class="math inline">\(V^\pi(s)\)</span>是初始状态为<span class="math inline">\(s\)</span>，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报。（因为有多个轨迹，每个轨迹的初始状态都是<span class="math inline">\(\tau_{s_0} = s\)</span>） <span class="math display">\[V^\pi(s) = E_{\tau \sim p(\tau)} [\sum_{t=0}^{T-1}r_{t+1} \mid \tau_{s_0} = s]\]</span></p><h2 id="贝尔曼方程计算值函数">贝尔曼方程计算值函数</h2><p><strong>当前状态的值函数</strong>，可以<strong>通过下个状态的值函数</strong>进行<strong>递推计算</strong>。</p><p>核心：<strong><span class="math inline">\(V^\pi(s) \sim r(s, a, s \prime) + V^\pi(s\prime)\)</span></strong>。 有动态规划的意思</p><ul><li>关键在于状态转移：<span class="math inline">\(s \sim s\prime\)</span></li><li><strong>选动作</strong>、<strong>选新状态</strong> ： <span class="math inline">\(s \sim a\)</span>， <span class="math inline">\(s, a \sim s\prime\)</span></li><li>策略<span class="math inline">\(\pi(a\mid s)\)</span> 和状态转移概率<span class="math inline">\(p(s\prime \mid s, a)\)</span></li><li>对这两层可能性的所有值函数，求期望即可</li></ul><p>给定<code>策略</code><span class="math inline">\(\pi(a\mid s)\)</span>、<code>状态转移概率</code><span class="math inline">\(p(s\prime \mid s, a)\)</span>、<code>奖励</code><span class="math inline">\(r(s, a, s\prime)\)</span>， <strong>迭代计算值函数</strong>： <span class="math display">\[V^\pi(s) = E[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><strong>V函数的贝尔曼方程</strong> <span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><h2 id="状态-动作值函数">状态-动作值函数</h2><p>状态-动作值函数是 <strong>初始状态为<span class="math inline">\(s\)</span></strong>并<strong>进行动作<span class="math inline">\(a\)</span></strong>， <strong>执行策略<span class="math inline">\(\pi\)</span></strong>得到的<code>期望总回报</code>。 也称为<strong>Q函数</strong>。 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> <strong>Q函数的贝尔曼方程</strong> <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma E_{a\prime \sim \pi(a\prime \mid s\prime)}[Q^\pi(s\prime, a\prime)]]\]</span> ## V函数与Q函数</p><ul><li><p><span class="math inline">\(V(s)\)</span>函数要 先确定动作<span class="math inline">\(s \sim a\)</span>， 再确定新状态<span class="math inline">\(s, a \sim s\prime\)</span></p></li><li><p><span class="math inline">\(Q(s,a)\)</span>函数是确定动作a后的V函数</p></li></ul><p>V函数是所有动作a的Q函数的期望 <span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}[Q^\pi(s, a)]\]</span> ## 值函数的作用</p><p><code>值函数</code>用<strong>来对策略<span class="math inline">\(\pi(a\mid s)\)</span>进行评估</strong>。</p><p>如果在状态s，有一个动作a使得<span class="math inline">\(Q^\pi(s, a) &gt; V^\pi(s)\)</span></p><ul><li>s状态，执行动作a 比 s状态 所有动作的期望，都要好。<strong>状态a高于所有状态的平均值</strong></li><li>说明执行动作a比当前策略<span class="math inline">\(\pi(a \mid s)\)</span>好</li><li><strong>调整参数使<span class="math inline">\(\pi(a \mid s)\)</span>的概率增加</strong></li></ul><h1 id="贝尔曼和贝尔曼最优方程">贝尔曼和贝尔曼最优方程</h1><blockquote><ol style="list-style-type: decimal"><li><span class="math inline">\(V(s)\)</span>函数和<span class="math inline">\(Q(s,a)\)</span>函数</li><li>贝尔曼方程（选择所有可能的均值）</li><li>贝尔曼最优方程（直接选择最大值）</li></ol></blockquote><h2 id="v函数与q函数">V函数与Q函数</h2><p>V函数：以<strong>s为初始状态</strong>，执行策略<span class="math inline">\(\pi\)</span>得到的<code>期望回报</code>（所有轨迹回报的均值） <span class="math display">\[V^\pi(s) = E_{\tau \sim p(\tau)} [\sum_{t=0}^{T-1}r_{t+1} \mid \tau_{s_0} = s]\]</span> Q函数：以<strong>s为初始状态，执行动作a</strong>，执行策略<span class="math inline">\(\pi\)</span>得到的期望回报 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span> 利用V函数去计算Q函数 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><h2 id="贝尔曼方程">贝尔曼方程</h2><p><span class="math inline">\(V(s)\)</span>的贝尔曼方程，选择<strong>所有a的期望回报</strong>， 也是<strong>Q函数的均值</strong>，<span class="math inline">\(V(s)=E_a[Q(s, a)]\)</span> <span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}E_{s\prime \sim p(s\prime \mid s, a)}[ r(s, a, s\prime) + \gamma V^\pi(s\prime)]\]</span></p><p><span class="math display">\[V^\pi(s) = E_{a \sim \pi(a \mid s)}[Q^\pi(s, a)]\]</span></p><p><span class="math inline">\(Q(s,a)\)</span>函数的贝尔曼方程 <span class="math display">\[Q^\pi(s, a) = E_{s\prime \sim p(s\prime \mid s, a)} [r(s, a, s\prime) + \gamma E_{a\prime \sim \pi(a\prime \mid s\prime)}[Q^\pi(s\prime, a\prime)]]\]</span></p><h2 id="贝尔曼最优方程">贝尔曼最优方程</h2><p><span class="math inline">\(V(s)\)</span>函数的贝尔曼最优方程，实际上是<strong>直接选择所有a中的最大回报</strong> ： <span class="math display">\[V^*(s) = \max_\limits{a} E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma V^*(s^\prime)]\]</span> <span class="math inline">\(Q(s,a)\)</span>函数的贝尔曼最优方程 <span class="math display">\[Q^*(s, a) =  E_{s^\prime \sim p(s^\prime \mid s, a)}[r(s, a, s^\prime) + \gamma \max_\limits{a\prime}Q^*(s^\prime, a^\prime)]\]</span></p><h1 id="深度强化学习">深度强化学习</h1><p>有些任务的状态和动作非常多，并且是连续的。普通方法很难去计算。</p><p>可以使用更复杂的函数（深度神经网络）使智能体来感知更复杂的环境状态，建立更复杂的策略。</p><p>深度强化学习</p><ul><li><code>强化学习</code> -- <strong>定义问题和优化目标</strong></li><li><code>深度学习</code> -- 解决<strong>状态表示</strong>、<strong>策略表示</strong>等问题</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;强化学习的基础知识。基本要素、轨迹、值函数、V函数和Q函数、贝尔曼方程。&lt;/p&gt;
&lt;/blockquote&gt;
      
    
    </summary>
    
      <category term="强化学习" scheme="http://plmsmile.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="智能体" scheme="http://plmsmile.github.io/tags/%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    
      <category term="环境" scheme="http://plmsmile.github.io/tags/%E7%8E%AF%E5%A2%83/"/>
    
      <category term="值函数" scheme="http://plmsmile.github.io/tags/%E5%80%BC%E5%87%BD%E6%95%B0/"/>
    
      <category term="贝尔曼方程" scheme="http://plmsmile.github.io/tags/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/"/>
    
      <category term="V函数" scheme="http://plmsmile.github.io/tags/V%E5%87%BD%E6%95%B0/"/>
    
      <category term="Q函数" scheme="http://plmsmile.github.io/tags/Q%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>阿里小蜜论文</title>
    <link href="http://plmsmile.github.io/2018/03/31/36-alime-chat/"/>
    <id>http://plmsmile.github.io/2018/03/31/36-alime-chat/</id>
    <published>2018-03-31T06:23:20.000Z</published>
    <updated>2018-04-04T13:03:41.177Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://www.aclweb.org/anthology/P17-2079" target="_blank" rel="noopener">AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine</a></p><p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="alime-chat">AliMe Chat</h1><h2 id="概览">概览</h2><p><strong>1. IR Model</strong></p><p>Information Retrieval。有一个QA对知识库，给一个问题，选择最相似的问题Pair，得出答案。</p><p>缺点：很难处理那些不在QA知识库里面的<code>Long tail</code>问句</p><p><strong>2. Generation Model</strong></p><p><code>(Seq2Seq)</code> ：基于Question生成一个回答</p><p>缺点：会产生一些不连贯或者没意义的回答</p><p><strong>3. 小蜜的混合模型</strong></p><p>集成了IR和生成式模型。</p><ol style="list-style-type: decimal"><li>收到一个句子</li><li><strong>IR模型</strong>：从QA知识库中选择一些答案作为候选答案</li><li><strong>打分模型</strong>：利用<code>Attentive Seq2Seq</code>对候选答案进行打分</li><li>最高得分大于<code>阈值</code>，直接输出该得分</li><li><strong>生成式模型</strong>：否则，利用生成式模型生成一个回答</li></ol><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/alime/alimechat.png" style="display:block; margin:auto" width="80%"></p><h2 id="qa知识库">QA知识库</h2><p>从用户和员工的对话数据中，提取一些问题和答案。也会把几个问题连在一起。最终共9164834个QA对。</p><h2 id="ir模型">IR模型</h2><p><strong>1. IR步骤</strong></p><ol style="list-style-type: decimal"><li>建立索引：每个单词 -- 多个问题</li><li>收到一个问句计算它的词集：分词 -- 去掉停用词 -- 扩展同义词</li><li>利用词集和索引去找到若干个QA对</li><li>利用BM25算法，去计算问句和所有候选QA对里问题的相似度</li><li>选择最相似的QA对</li></ol><p><strong>2. BM25算法</strong></p><p>BM25通常用来搜索相关性评分。</p><p>一个<code>query</code>和一个<code>d</code> 。把query分割成<span class="math inline">\(K\)</span>个<strong>语素<span class="math inline">\(q_i\)</span></strong>（中文是分词） <span class="math display">\[\rm{score}(q, d) = \sum_{i}^K w_i \cdot r(q_i, d)\]</span> <span class="math inline">\(w_i​\)</span>是判断一个词与一个文档的相关性权重。这里使用<code>IDF</code>来计算。</p><ul><li><span class="math inline">\(N\)</span>是总文档数，<span class="math inline">\(N(q_i)\)</span>为包含词<span class="math inline">\(q_i\)</span>的文档数</li><li><span class="math inline">\(f_i\)</span> 为<span class="math inline">\(q_i\)</span>在d中的出现频率，<span class="math inline">\(g_i\)</span>为<span class="math inline">\(q_i\)</span>在<span class="math inline">\(q\)</span>中的出现频率<br></li><li><span class="math inline">\(d_l\)</span> 为<span class="math inline">\(d\)</span>的长度，<span class="math inline">\(\rm{avg}(d_l)\)</span>是所有文档的长度<br></li><li><span class="math inline">\(k_1, k_2, b\)</span> 为调节因子，一般<span class="math inline">\(k_1=2,b=0.75\)</span></li></ul><p><span class="math display">\[w_i = \rm{IDF}(q_i) = \log \frac{N - N(q_i) + 0.5}{N(q_i) + 0.5}\]</span></p><p><span class="math display">\[r(q_i, d) = \frac{f_i \cdot (k_1 + 1)} {f_i + K} \cdot \frac{g_i \cdot (k_2 +1 )}{g_i + k_2}\]</span></p><p><span class="math display">\[K = k_1 \cdot (1 - b + b \cdot \frac{d_l}{\rm{avg}(d_l)})\]</span></p><p>特别地，一般<span class="math inline">\(q_i\)</span>只在<span class="math inline">\(q\)</span>中出现一次，即<span class="math inline">\(g_i = 1\)</span>， 则 <span class="math display">\[r(q_i, d) = \frac{f_i \cdot (k_1 + 1)} {f_i + K}\]</span> 调节因子</p><ul><li>K ：相同<span class="math inline">\(f_i\)</span>的情况下，文档越长，相似性越低</li><li>b：越大，提高长文档与<span class="math inline">\(q_i\)</span>的相似性</li></ul><h2 id="生成式模型">生成式模型</h2><p><strong>1. Attentive Seq2Seq</strong></p><ol style="list-style-type: decimal"><li>输入问句的语义信息： <span class="math inline">\((h_1, h_2, \cdots, h_m)\)</span></li><li>上一时刻的单词和隐状态：<span class="math inline">\(y_{i-1},s_{i-1}\)</span></li><li>计算注意力分布：<span class="math inline">\(\alpha_{ij} = a(s_{i-1}, h_j)\)</span></li><li>语义信息：<span class="math inline">\(c_i = \sum_{j=1}^m \alpha_{ij}h_j\)</span></li><li>预测当前单词：<span class="math inline">\(p(y_i=w_i \mid \theta_i) = p(y_i=w_i \mid y_1, \cdots, y_{i-1}, c_i) = f(y_{i-1}, s_{i-1}, c_i)\)</span></li></ol><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/alime/attentive-seq2seq.png" style="display:block; margin:auto" width="70%"></p><p><strong>2. 数据padding</strong></p><p>利用Tensorflow的<code>Bucket Mechanism</code>组织。选择(5,5),(5,10),(10,15),(20,30),(45,60)。</p><p><strong>3. Softmax</strong></p><p>训练时，softmax词表使用目标词汇+512个随机词汇。</p><p><strong>4. BeamSearch 解码</strong></p><p>每个时刻选择top-k(k=10)</p><h2 id="打分模型">打分模型</h2><p>打分模型，对所有候选答案计算一个得分，然后选择得分最高的答案。</p><p>生成式模型，在解码时会计算各个单词的概率。打分模型和生成式模型使用同一个模型。</p><p>打分模型，计算候选回答中每个单词在Decoder时出现的概率。再求平均值作为该回答的得分。 <span class="math display">\[s^{\text{avg}(p)} = \frac{1}{n} \sum_{i=1}^n p(y_i = w_i \mid \theta_i)\]</span></p><h2 id="评价方法">评价方法</h2><p>5个评价规则：</p><ul><li>语法正确</li><li>意义相关</li><li>标准的表达</li><li>上下文无关 context independent</li><li>not overly generalized</li></ul><p>答案的三个级别：</p><ul><li>2 ：适合。满足所有规则</li><li>1： 一般。满足前三项，不满足后面两项其中一项</li><li>0：不适合</li></ul><p>top-1概率 <span class="math display">\[P_{\rm{top}_1} = \frac{N_{合适} + N_{一般}}{N_{所有}}\]</span></p><h1 id="阿里小蜜助手">阿里小蜜助手</h1><p><a href="https://arxiv.org/abs/1801.05032" target="_blank" rel="noopener">小蜜助手</a></p><p>小蜜主要包括：助手(Task)服务、客户服务、聊天服务。支持声音、文本输入，支持多轮对话。</p><h2 id="系统概览">系统概览</h2><p><strong>1. 系统概览</strong></p><ol style="list-style-type: decimal"><li>输入层：接收多个终端和多种形式的输入</li><li>意图分类层：<code>Rules Parser</code> 直接解析意图，失败则通过 <code>Intention Classifier</code> 解析意图</li><li>处理问题组件层：语义解析、知识图引擎、信息提取引擎、Slot Filling引擎、聊天引擎</li><li>知识库：QA对，知识图。</li></ol><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/alime/ali-assist.png" style="display:block; margin:auto" width="70%"></p><p><strong>2. 问题的信息流</strong></p><p>收到一个问句后</p><p>1 使用<code>Business Rules Parser</code> （trie-based）去解析q，如果匹配到一个模式</p><ol style="list-style-type: decimal"><li>q是一个任务型的问题（助手服务）：给<code>Slot Filling Engine</code>（槽填充） 直接给答案</li><li>q是一个促销活动：给到<code>Sales Promotion</code> ，回答准备好的答案</li><li>q是请求人工：则先询问客户有什么问题</li></ol><p>2 没有匹配到一个模式，给到<code>意图分类器</code>去识别意图，也就是识别出意图的场景（比如退货、退款、人工等）</p><p>3 如果场景是要转人工，则直接转人工</p><p>4 否则，q给到语义解析器，去识别是否包含语义标签（知识图谱中的实体）</p><ol style="list-style-type: decimal"><li>如果识别出语义标签，则通过知识图谱去找答案，如果找到直接输出</li><li>如果知识图谱没有答案</li><li>如果有上下文，结合上下文和q再去解析语义，再给到语义解析器解析语义标签</li><li>如果没有上下文，则判断是否要询问用户</li><li>如果要询问，则通过模板去询问用户</li><li>如果不用询问，则通过<code>IR</code>去提取信息，如果有答案，则输出；如果没有，则转人工</li></ol><p>5 如果不包含语义标签</p><ol style="list-style-type: decimal"><li>如果要聊天，则通过聊天引擎去产生结果</li><li>否则，通过词模板去输出结果</li></ol><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/alime/alime-assist-flow.png" style="display:block; margin:auto" width="70%"></p><h2 id="意图分类">意图分类</h2><p>对一个问句结合上下文（前面的文件）去识别出它的意图。有3个大范围：</p><ul><li>助手。我要订机票</li><li>信息咨询、解决方案。怎么找回密码</li><li>聊天。我不开心</li></ul><p>每一个大的范围都会进行商业细化。比如助手服务会包含订机票、手机充值。</p><p>意图分类由<strong>商业规则解析器</strong>和<strong>意图分类器</strong>组成。前者解析失败，才会执行后者</p><ul><li>规则解析器：一颗很大的trie树，写了很多的规则</li><li>意图分类器：CNN</li></ul><p>CNN，使用<code>fast-text</code>训练的词向量，fine tuned in CNN</p><ul><li>输入1：问题q</li><li>输入2：问题q和上下文(之前的问题)的语义标签</li></ul><p>CNN的好处</p><ul><li>也可以捕获上下文信息（前一个和后一个），足够好、够用的结果就行</li><li>CNN快啊，QPS=200，Query per Second</li><li>多个卷积池化层或者RNN能实现一个更好的结果，但是扩展性不好？为什么？</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/alime/cnn.png" style="display:block; margin:auto" width="70%"></p><h1 id="taskbot">TaskBot</h1><p>主要是以强化学习为中心的端到端的对话管理，由下面三个部分组成：</p><h2 id="intent-network">Intent network</h2><blockquote><p>处理用户的输入</p></blockquote><p>使用单层CNN对用户的问句进行编码，得到用户的意图语义向量</p><h2 id="neural-belief-tracker">Neural belief tracker</h2><blockquote><p>提取记录用户的slot信息</p></blockquote><ul><li>使用BiLSTM-CRF来提取用户每次输入的slot信息</li><li>根据上一轮系统的回答和当前用户的问句生成当前的Context信息，给到后面的Policy Network</li><li>优点是：BiLSTM可以挖掘出当前词的Context信息，而CRF能有效地对标记序列进行建模</li></ul><h2 id="policy-networker">Policy networker</h2><blockquote><p>决定系统的操作，继续反问用户或者直接产生相应的实际操作</p></blockquote><p>这也是强化学习的核心点，主要包含Episode，Reward，State和Action四个部分。 <strong>Episode</strong> 在某个场景下，识别出用户该场景的意图，则认为一个Episode开始；执行目的操作或者退出，则认为Episode结束 <strong>Reward</strong> 收集线上用户的反馈，并根据正负给出相应的Reward。特别注意要使用预训练的环境 <strong>State</strong> 结合当前新的Slot状态（Context）、历史的Slot信息和用户的当前问句信息，使用线性层+Softmax直接算出各个Actions的概率 <strong>Action</strong> Action就是系统可以给用户的一些反馈操作，比如继续询问用户、执行一个真实的操作等等。</p><p>该Taskbot的瓶颈主要是难以确定用户退出的原因，从而很难给出一些确定的惩罚。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P17-2079&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AliMe Chat: A Sequence to Sequence and Rerank based Cha
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="chatbot" scheme="http://plmsmile.github.io/tags/chatbot/"/>
    
      <category term="qa" scheme="http://plmsmile.github.io/tags/qa/"/>
    
      <category term="IR" scheme="http://plmsmile.github.io/tags/IR/"/>
    
      <category term="BM25" scheme="http://plmsmile.github.io/tags/BM25/"/>
    
      <category term="seq2seq" scheme="http://plmsmile.github.io/tags/seq2seq/"/>
    
      <category term="CNN" scheme="http://plmsmile.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>网络优化</title>
    <link href="http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/"/>
    <id>http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/</id>
    <published>2018-03-30T05:54:34.000Z</published>
    <updated>2018-04-03T14:43:40.128Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法</p></blockquote><p><img src="" style="display:block; margin:auto" width="70%"></p><h1 id="神经网络的问题">神经网络的问题</h1><p>神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过<code>优化</code>和<code>正则化</code>来提升网络。</p><h2 id="优化问题">优化问题</h2><p><strong>优化问题的难点</strong></p><ul><li>网络是一个<strong>非凸函数</strong>，深层网络的<strong>梯度消失</strong>问题，很难优化</li><li>网络<strong>结构多样性</strong>，很难找到通用优化方法</li><li>参数多、数据大，<strong>训练效率低</strong></li><li>参数多，存在<strong>高维变量的非凸优化</strong></li></ul><p>低维空间非凸优化：存在局部最优点，难在初始化参数和逃离局部最优点</p><p>高维空间非凸优化：难在如何逃离<code>鞍点</code>。 鞍点是梯度为0，但一些维度是最高点，另一些维度是最低点。</p><p>梯度下降法<strong>很难逃离鞍点</strong>。</p><p><strong>梯度下降法面临的问题</strong></p><ul><li>如何初始化参数</li><li>预处理数据</li><li>如何选择合适的学习率，避免陷入局部最优</li></ul><h2 id="泛化问题">泛化问题</h2><p>神经网络拟合能力很强，容易过拟合。<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/">解决过拟合的5个方法</a></p><h1 id="参数初始化">参数初始化</h1><p><a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96">我之前的参数初始化笔记</a></p><h2 id="对称权重问题">对称权重问题</h2><p><strong>全0产生的对称权重问题</strong></p><p><strong>参数千万不能全0初始化</strong>。如果全0初始化，会导致隐层神经元激活值都相同，导致深层神经元没有区分性。这就是<code>对称权重</code>现象。</p><p>通俗点：</p><ul><li>每个神经元输出相同 -- BP时梯度也相同 -- 参数更新也相同</li><li>神经元之间就<strong>失去了不对称性的源头</strong></li></ul><p>应该对每个参数<code>随机初始化</code>，打破这个对称权重现象，<strong>使得不同神经元之间区分性更好</strong>。</p><p><strong>参数区间的选择</strong></p><p><code>参数太小</code>时</p><p>使得<code>Sigmoid</code>激活函数<strong>丢失非线性的能力</strong>。在0附近近似线性，多层神经网络的优势也不存在。</p><p><code>参数太大</code>时</p><p><code>Sigmoid</code>的输入会变得很大，<strong>输出接近1</strong>。<strong>梯度直接等于0</strong>。</p><p>选择一个<strong>合适的初始化区间非常重要</strong>。如果，一个神经元输入连接很多，那么每个输入连接上的权值就应该小一些。</p><h2 id="高斯分布初始化">高斯分布初始化</h2><p>高斯分布也就是正态分布。</p><p>初始化一个深度网络，比较好的方案是<strong>保持每个神经元输入的方差</strong>为一个<code>常量</code>。</p><p>如果神经元输入是<span class="math inline">\(n_{in}\)</span>， 输出是<span class="math inline">\(n_{out}\)</span>， 则按照<span class="math inline">\(N(0, \sqrt{\frac {2}{n_{in} + n_{out}}})\)</span> 来初始化参数。</p><h2 id="均匀分布初始化">均匀分布初始化</h2><p>在<span class="math inline">\([-r, r]\)</span>区间内，采用均匀分布来初始化参数</p><h2 id="xavier均匀分布初始化">Xavier均匀分布初始化</h2><p>会自动计算超参数<span class="math inline">\(r\)</span>， 来对参数进行<span class="math inline">\([-r, r]\)</span>均匀分布初始化。</p><p>设<span class="math inline">\(n^{l}\)</span>为第<span class="math inline">\(l\)</span> 层神经元个数， <span class="math inline">\(n^{l-1}\)</span> 是第<span class="math inline">\(l-1\)</span>层神经元个数。</p><ul><li><code>logsitic</code>激活函数 ：<span class="math inline">\(r = \sqrt{\frac{6}{n^{l-1} + n^l}}\)</span></li><li><code>tanh</code>激活函数： <span class="math inline">\(r = 4 \sqrt{\frac{6}{n^{l-1} + n^l}}\)</span></li></ul><p><span class="math inline">\(l\)</span>层的一个神经元<span class="math inline">\(z^l\)</span>，收到<span class="math inline">\(l-1\)</span>层的<span class="math inline">\(n^{l-1}\)</span>个神经元的输出<span class="math inline">\(a_i^{l-1}\)</span>, <span class="math inline">\(i \in [1, n^{(l-1)}]\)</span>。 <span class="math display">\[z^l = \sum_{i=1}^n w_i^l a_i^{l-1}\]</span> 为了避免初始化参数使得激活值变得饱和，尽量使<span class="math inline">\(z^l\)</span>处于线性区间，即<strong>神经元的输出</strong><span class="math inline">\(a^l = f(z^l) \approx z^l\)</span>。</p><p>假设<span class="math inline">\(w_i^l\)</span>和<span class="math inline">\(a_i^{l-1}\)</span>相互独立，均值均为0，则a的均值为 <span class="math display">\[E[a^l] = E[\sum_{i=1}^n w_i^l a_i^{l-1}] = \sum_{i=1}^d E[\mathbf w_i] E[a_i^{l-1}] = 0\]</span> <span class="math inline">\(a^l\)</span>的方差 <span class="math display">\[\mathrm{Var} [a^l] = n^{l-1} \cdot \mathrm{Var} [w_i^l] \cdot \mathrm{Var} [a^{l-1}_i]\]</span> 输入信号经过该神经元后，被放大或缩小了<span class="math inline">\(n^{l-1} \cdot \mathrm{Var} [w_i^l]\)</span>倍。</p><p>为了使输入信号经过多层网络后，不被过分放大或过分缩小，应该使<span class="math inline">\(n^{l-1} \cdot \mathrm{Var} [w_i^l]=1\)</span>。</p><p>综合前向和后向，使<strong>信号在前向和反向传播中都不被放大或缩小</strong>，综合设置方差： <span class="math display">\[\mathrm{Var} [w_i^l] = \frac{2} {n^{l-1} + n^l}\]</span></p><h1 id="数据预处理">数据预处理</h1><h2 id="为什么要归一化">为什么要归一化</h2><p>每一维的特征的来源和度量单位不同，导致特征分布不同。</p><p><strong>未归一化数据的3个坏处</strong></p><ol style="list-style-type: decimal"><li>样本之间的欧式距离度量不准。取值范围大的特征会占主导作用。类似于<a href="https://plmsmile.github.io/2018/03/05/29-desicion-tree/#%E7%86%B5%E5%92%8C%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A">信息增益和信息增益比</a></li><li>降低神经网络的训练效率</li><li>降低梯度下降法的搜索效率</li></ol><p><strong>未归一化对梯度下降的影响</strong></p><ul><li>取值范围不同：大多数位置的梯度方向不是最优的，要多次迭代才能收敛</li><li>取值范围相同：大部分位置的梯度方向近似于最优搜索方向，每一步都指向最小值，训练效率大大提高</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/data_standard.png" style="display:block; margin:auto" width="70%"></p><p><strong>归一化要做的事情</strong></p><ol style="list-style-type: decimal"><li>各个维度特征归一化到同一个取值区间</li><li>消除不同特征的相关性</li></ol><h2 id="标准归一化">标准归一化</h2><p>实际上是由<code>中心化</code>和<code>标准化</code>结合的。 把<strong>数据归一化到标准正态分布</strong>。<span class="math inline">\(X \sim N(0, 1^2)\)</span></p><p>计算均值和方差 <span class="math display">\[\mu = \frac{1}{N} \sum_{i=1}^n x^{(i)} \\\sigma^2 =  \frac{1}{N} \sum_{i=1}^n(x^{(i)} - \mu)^2\]</span> 归一化数据，减均值除以标准差。如果<span class="math inline">\(\sigma = 0\)</span>， 说明特征没有区分性，应该直接删掉。 <span class="math display">\[\hat x^{(i)} = \frac {x^{(i)} - \mu}{ \sigma }\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-process.jpg" style="display:block; margin:auto" width="70%"></p><h2 id="缩放归一化">缩放归一化</h2><p>把数据归一化到<span class="math inline">\([0, 1]\)</span> 或者<span class="math inline">\([-1, 1]\)</span> 直接。 <span class="math display">\[x^{(i)} = \frac {x^{(i)} - \min(x)}{\max(x) - \min (x)}\]</span></p><h2 id="白化">白化</h2><p><code>白化</code>用来降低输入数据特征之间的冗余性。白化主要使用PCA来去掉特征之间的相关性。<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#%E7%99%BD%E5%8C%96">我的白化笔记</a></p><p>处理后的数据</p><ul><li>特征之间相关性较低</li><li>所有特征具有相同的方差</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-pca-process.jpg" style="display:block; margin:auto" width="70%"></p><p><strong>白化的缺点</strong></p><p>可能会夸大数据中的噪声。所有维度都拉到了相同的数值范围。可能有一些差异性小、但大多数是噪声的维度。可以使用平滑来解决。</p><h1 id="逐层归一化">逐层归一化</h1><h2 id="原因">原因</h2><p>深层神经网络，中间层的输入是上一层的输出。每次SGD参数更新，都会导致<strong>每一层的输入分布发生改变</strong>。</p><p>像高楼，低楼层发生较小偏移，就会导致高楼层发生较大偏移。</p><p>如果<strong>某个层的输入发生改变</strong>，其<strong>参数就需要重新学习</strong>，这也是<code>内部协变量偏移</code>问题。</p><p>在训练过程中，要使得每一层的输入分布保持一致。简单点，对每一个神经层进行归一化。</p><ul><li>批量归一化</li><li>层归一化</li><li>其它方法</li></ul><h2 id="批量归一化">批量归一化</h2><p>针对<strong>每一个维度</strong>，对<strong>每个batch的数据</strong>进行<strong>归一化+缩放平移</strong>。</p><p>批量归一化<code>Batch Normalization</code> ，<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#batch-normalization">我的BN详细笔记</a>。 对每一层（<strong>单个神经元</strong>）的输入进行归一化 <span class="math display">\[\begin{align}&amp; \mu = \frac{1}{m} \sum_{i=1}^m x_i &amp;  \text{求均值} \\&amp; \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 &amp; \text{求方差} \\&amp; \hat x = \frac{x - E(x)} {\sqrt{\sigma^2 + \epsilon}} &amp;  \text{标准归一化} \\&amp; y =  \gamma \hat x+ \beta &amp; \text{缩放和平移} \end{align}\]</span> <code>缩放参数</code><span class="math inline">\(\gamma\)</span> ，和<code>平移参数</code> <span class="math inline">\(\beta\)</span> 的作用</p><ul><li>强行归一化会破坏刚学习到的特征。用这两个变量去还原应该学习到的数据分布</li><li>归一化会聚集在0处，会减弱神经网络的非线性性质。缩放和平移解决这个问题</li></ul><p>注意：</p><ul><li>BN是对中间层的<strong>单个神经元</strong>进行归一化</li><li>要求<strong>批量样本数量不能太小</strong>，否则难以计算单个神经元的统计信息</li><li>如果层的净输入的分布是<strong>动态变化</strong>的，则<strong>无法使用批量归一化</strong>。如循环神经网络</li></ul><h2 id="层归一化">层归一化</h2><p>对每个样本，对所有维度做一个归一化，即对<strong>同层的所有神经元</strong>的输入做归一化。</p><ul><li><code>层归一化</code>是<strong>对一个中间层的所有神经元进行归一化</strong></li><li>批量归一化是对一个中间层的单个神经元进行归一化</li></ul><p>设第<span class="math inline">\(l\)</span>层的净输入为<span class="math inline">\(\mathbf z^{(l)}\)</span>， 求<strong>第<span class="math inline">\(l\)</span>层所有输入</strong>的<code>均值</code>和<code>方差</code> <span class="math display">\[\begin{align}&amp; \mu^{(l)} = \frac{1}{n^l} \sum_{i=1}^{n^l} z_i^{(l)} &amp;  \text{第l层输入的均值} \\&amp; \sigma^{(l)^2} = \frac{1}{n^l} \sum_{i=1}^{n^l} (z_i^{(l)} - \mu^{(l)})^2 &amp; \text{第l层输入的方差} \\ \end{align}\]</span> <code>层归一化</code> 如下，其中<span class="math inline">\(\gamma, \beta\)</span>是缩放和平移的参数向量，与<span class="math inline">\(\mathbf z^{(l)}\)</span>维数相同 <span class="math display">\[\hat {\mathbf z}^{(l)} = \rm{LayerNorm}_{\gamma, \beta} (\mathbf z^{(l)}) = \frac {\mathbf z^{(l) - \mu^{(l)}}}{\sqrt{\sigma ^{(l)^2} + \epsilon}} \cdot \gamma + \beta\]</span> <strong>层归一化的RNN</strong> <span class="math display">\[\mathbf z_t = U\mathbf h_{t-1} + W \mathbf x_t \\\mathbf h_t = f (\rm{LN}_{\gamma, \beta}(\mathbf z_t)))\]</span> RNN的净输入一般会随着时间慢慢变大或变小，导致梯度爆炸或消失。</p><p>层归一化的RNN可以有效缓解梯度消失和梯度爆炸。</p><h2 id="批归和层归对比">批归和层归对比</h2><p>思想类似，都是<code>标准归一化</code> + <code>缩放和平移</code>。</p><ul><li>批量归一化：针对每一个维度，对batch的所有数据做归一化</li><li>层归一化：针对每一个样本，对所有维度做归一化。可以用在RNN上，减小梯度消失和梯度爆炸。</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/layer-batch-norm.jpg" style="display:block; margin:auto" width="70%"></p><h2 id="其它归一化">其它归一化</h2><p><strong>权重归一化</strong></p><p>对神经网络的连接权重进行归一化。</p><p><strong>局部相应归一化</strong></p><p>对同层的神经元进行归一化。但是局部响应归一化，用在激活函数之后，对邻近的神经元进行局部归一化。</p><h1 id="梯度下降法的改进">梯度下降法的改进</h1><h2 id="梯度下降法">梯度下降法</h2><p><code>Mini-Batch</code>梯度下降法。设<span class="math inline">\(f(\mathbf x ^{(i)}, \theta)\)</span> 是神经网络。</p><p>在第<span class="math inline">\(t\)</span>次迭代(epoch)时，选取<span class="math inline">\(m\)</span>个训练样本<span class="math inline">\(\{\mathbf x^{(i)}, y^{(i)} \}_{i=1}^m\)</span>。 计算梯度<span class="math inline">\(\mathbf g_t\)</span> <span class="math display">\[\mathbf g_t = \frac{1}{m} \sum_{i \in I_t} \frac {\partial J(y^{(i)}, f(\mathbf x ^{(i)}, \theta))}{\partial \theta} + \lambda \|\theta\| ^2   \]</span> 更新参数，其中学习率<span class="math inline">\(\alpha \ge 0\)</span> ： <span class="math display">\[\theta_t = \theta_{t-1} - \alpha \mathbf g_t \]</span></p><p><span class="math display">\[\theta_t = \theta_{t-1}+ \Delta \theta_t\]</span></p><p><strong>1. BGD</strong></p><p>Batch Gradient Descent</p><p><code>意义</code>：每一轮选择所有整个数据集去计算梯度更新参数</p><p><code>优点</code></p><ul><li>凸函数，可以保证收敛到全局最优点；非凸函数，保证收敛到局部最优点</li></ul><p><code>缺点</code></p><ul><li>批量梯度下降非常慢。因为在整个数据集上计算</li><li>训练次数多时，耗费内存</li><li>不允许在线更新模型，例如更新实例</li></ul><p><strong>2. SGD</strong></p><p>Stochastic Gradient Descent</p><p><code>意义</code>：每轮值选择一条数据去计算梯度更新参数</p><p><code>优点</code></p><ul><li>算法收敛快（BGD每轮会计算很多相似样本的梯度，冗余的）</li><li>可以在线更新</li><li>有一定几率跳出比较差的局部最优而到达更好的局部最优或者全局最优</li></ul><p><code>缺点</code></p><ul><li>容易收敛到局部最优，并且容易困在鞍点</li></ul><p><strong>3. Mini-BGD</strong></p><p>Mini-Batch Gradient Descent</p><p><code>意义</code>： 每次迭代只计算一个mini-batch的梯度去更新参数</p><p>优点</p><ul><li>计算效率高，收敛较为稳定</li></ul><p><code>缺点</code></p><ul><li>更新方向依赖于当前batch算出的梯度，不稳定</li></ul><p><strong>4. 梯度下降法的难点</strong></p><ol style="list-style-type: decimal"><li>学习率<span class="math inline">\(\alpha\)</span>难以选择。太小，导致收敛缓慢；太大，造成较大波动妨碍收敛</li><li>学习率一直相同是不合理的。出现频率低的特征，大学习率；出现频率小的特征，小学习率</li><li>按迭代次数和loss阈值在训练时去调整学习率。然而次数和阈值难以设定，无法适应所有数据</li><li>很难逃离鞍点。梯度为0，一些特征是最高点（上升），一些特征是最低点（下降）</li><li>更新方向依赖于当前batch算出的梯度，不稳定</li></ol><p>主要通过<strong>学习率递减</strong>和<strong>动量法</strong>来优化梯度下降法。</p><p>可以看出</p><ul><li>SGD，整体下降，但局部会来回震荡</li><li>MBGD，一个batch来说，batch越大，下降越快，越平滑</li><li>MBGD，整体来说，batch越小，下降越明显</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd_batch.png" style="display:block; margin:auto" width="70%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd-whole.png" style="display:block; margin:auto" width="71%"></p><h2 id="学习率递减">学习率递减</h2><p><strong>0 指数加权平均</strong></p><p>求10天的平均温度，可以直接利用平均数求，每天的权值是一样的，且要保存所有的数值才能计算。 <span class="math display">\[v_{avg} = \frac {v_1 + \cdots + v_{100}}{100}\]</span> 设<span class="math inline">\(v_t\)</span>是到第t天的平均温度，<span class="math inline">\(\theta_t\)</span>是第t天的真实温度，<span class="math inline">\(\beta=0.9\)</span>是衰减系数。</p><p>则有<code>指数加权平均</code>：<br><span class="math display">\[v_t = \beta * v_{t-1} + (1-\beta) \theta_t\]</span></p><p><span class="math display">\[v_{100} = 0.1 \cdot \theta_{100} + 0.1(0.9)^1 \cdot \theta_{99} + 0.1 (0.9)^2 \cdot \theta_{98} + 0.1(0.9)^3 \cdot \theta_{97} + \ldots\]</span></p><p>离当前越近，权值越大。越远，权值越小（指数递减），也有一定权值。</p><p><strong>1. 按迭代次数递减</strong></p><p>设置<span class="math inline">\(\beta = 0.96\)</span>为衰减率</p><p><code>反时衰减</code> <span class="math display">\[\alpha_t = \alpha_0 \cdot \frac {1} {1 + \beta \times t}\]</span> <code>指数衰减</code> : <span class="math display">\[\alpha_t = \alpha_0 \cdot \beta^t\]</span> <code>自然指数衰减</code> <span class="math display">\[\alpha_t = \alpha_0 \cdot e^{-\beta \cdot t}\]</span> <strong>2. AdaGrad</strong></p><p><code>Adaptive Gradient</code></p><p><code>意义</code>：每次迭代时，根据历史梯度累积量来减小学习率，减小梯度。<strong>梯度平方的累计值</strong>来减小梯度</p><p>初始学习率<span class="math inline">\(\alpha_0\)</span>不变，实际学习率减小。<span class="math inline">\(\alpha = \frac {\alpha_0} {\sqrt {G_t + \epsilon}}\)</span> <span class="math display">\[G_t = \sum_{i=1}^t g_i^2\]</span></p><p><span class="math display">\[\Delta \theta_t = - \frac {\alpha_0}{\sqrt {G_t + \epsilon}} \cdot g_t\]</span></p><p><code>优点</code></p><ul><li>累积梯度<span class="math inline">\(G_t\)</span>的<span class="math inline">\(\frac{1}{\sqrt{G_t + \epsilon}}\)</span>实际上构成了一个约束项<br></li><li>前期<span class="math inline">\(G_t\)</span>较小， 约束值大，能够放大梯度</li><li>后期<span class="math inline">\(G_t\)</span>较大， 约束值小，能够约束梯度</li><li>适合处理稀疏梯度</li></ul><p><code>缺点</code></p><ul><li>经过一些迭代，学习率会变非常小，参数难以更新。过早停止训练</li><li>依赖于人工设置的全局学习率<span class="math inline">\(\alpha_0\)</span></li><li><span class="math inline">\(\alpha_0\)</span>设置过大，约束项大，则对梯度的调节太大</li></ul><p><strong>3. RMSprop</strong></p><p>意义：计算<strong>梯度<span class="math inline">\(\mathbf g_t\)</span>平方</strong>的<code>指数递减移动平均</code>， 即<strong>梯度平方的平均值</strong>来减小梯度 <span class="math display">\[G_t = \beta G_{t-1} + (1-\beta) \cdot \mathbf g_t^2\]</span></p><p><span class="math display">\[\Delta \theta_t = - \frac {\alpha_0}{\sqrt {G_t + \epsilon}} \cdot \mathbf g_t\]</span></p><p><code>优点</code></p><ul><li>解决了AdaGrad学习率一直递减过早停止训练的问题，学习率可大可小</li><li>训练初中期，加速效果不错，很快；训练后期，反复在局部最小值抖动</li><li><strong>适合处理非平稳目标</strong>，对于RNN效果很好</li></ul><p><code>缺点</code></p><ul><li>依然依赖于全局学习率<span class="math inline">\(\alpha_0\)</span></li></ul><p><strong>4. AdaDelta</strong></p><p><code>意义</code> 不初始化学习率。计算<strong>梯度更新差平方</strong>的<code>指数衰减移动平均</code>来作为分子学习率， <span class="math display">\[G_t = \beta G_{t-1} + (1-\beta) \cdot \mathbf g_t^2\]</span></p><p><span class="math display">\[\Delta X_{t-1}^2 = \beta \Delta X_{t-2}^2 + (1-\beta) \Delta \theta_{t-1}^2\]</span></p><p><span class="math display">\[\Delta \theta_t = - \frac { \sqrt {\Delta X_{t-1}^2 + \epsilon}}{\sqrt {G_t + \epsilon}} \cdot \mathbf g_t\]</span></p><p><code>优点</code></p><ul><li>初始学习率<span class="math inline">\(\alpha_0\)</span>改成了动态计算的<span class="math inline">\(\sqrt {\Delta X_{t-1}^2 + \epsilon}\)</span> ，一定程度上平抑了学习率的波动。</li></ul><h2 id="动量法">动量法</h2><p>结合<strong>前面更新的方向</strong>和<strong>当前batch的方向</strong>，来更新参数。</p><p>解决了MBGD的不稳定性，增加了<code>稳定性</code>。可以<code>加速</code>或者<code>减速</code>。</p><p><strong>1. 普通动量法</strong></p><p>设<span class="math inline">\(\rho = 0.9\)</span>为动量因子，计算<strong>负梯度</strong>的<code>移动加权平均</code> <span class="math display">\[\Delta \theta_t = \rho \cdot \Delta \theta_{t-1} - \alpha \cdot \mathbf g_t\]</span></p><p>当前梯度与最近时刻的梯度方向：</p><ul><li>前后<strong>梯度方向一致</strong>：参数更新幅度变大，<strong>会加速</strong></li><li>前后<strong>梯度方向不一致</strong>：参数更新幅度变小，<strong>会减速</strong></li></ul><p>优点：</p><ul><li>迭代初期，梯度方向一致，动量法加速，更快到达最优点</li><li>迭代后期，梯度方向不一致，在收敛值附近震荡，动量法会减速，增加稳定性</li></ul><p>当前梯度叠加上上次的梯度，可以近似地看成二阶梯度。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/momentumjpg.jpg" style="display:block; margin:auto" width="70%"></p><h2 id="adam">Adam</h2><p><code>Adaptive Momentum Estimation</code> = <code>RMSProp</code> + <code>Momentum</code>， 即<strong>自适应学习率+稳定性</strong>（动量法）。</p><p>意义：计算梯度<span class="math inline">\(\mathbf g_t\)</span>的指数权值递减移动平均(<code>动量</code>)，计算梯度平方<span class="math inline">\(\mathbf g_t^2\)</span>的指数权值递减移动平均(<code>自适应alpha</code>)</p><p>设<span class="math inline">\(\beta_1 = 0.9\)</span>， <span class="math inline">\(\beta_2 = 0.99\)</span> 为衰减率 <span class="math display">\[M_t = \beta_1M_{t-1} + (1-\beta_1) \mathbf g_t \quad \quad \sim E(\mathbf g_t)\]</span></p><p><span class="math display">\[G_t = \beta_2 G_{t-1} + (1-\beta_2) \mathbf g_t^2 \quad \quad \sim E(\mathbf g_t^2)\]</span></p><p><span class="math display">\[\hat M_t = \frac {M_t}{1 - \beta_1^t}, \quad \hat G_t = \frac{G_t}{1 - \beta_2^t} \quad \quad \text{初始化偏差修正}\]</span></p><p><span class="math display">\[\Delta \theta_t = - \frac {\alpha_0}{\sqrt{\hat G_t + \epsilon}} \hat M_t \]</span></p><p><code>优点</code></p><ul><li>有RMSprop的处理<strong>非稳态</strong>目标的优点，有Adagrad处理<strong>稀疏梯度</strong>的优点</li><li>对内存需求比较小，高效地计算</li><li>为不同的参数计算不同的自适应学习率</li><li>适用于大多数的非凸优化</li><li>超参数好解释，只需极少量的调参</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/optimizers.png" style="display:block; margin:auto" width="80%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/optimizer-1.gif" style="display:block; margin:auto" width="70%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/optimizer-2.gif" style="display:block; margin:auto" width="70%"></p><h2 id="梯度截断">梯度截断</h2><p>一般按模截断，如果<span class="math inline">\(\|\mathbf g_t\|^2 &gt; b\)</span>， 则 <span class="math display">\[\mathbf g_t = \frac{b}{\|\mathbf g_t\|} \mathbf g_t\]</span></p><h1 id="超参数优化">超参数优化</h1><h2 id="优化内容和难点">优化内容和难点</h2><p><strong>优化内容</strong></p><ul><li>网络结构：神经元之间连接关系、层数、每层的神经元数量、激活函数类型等</li><li>优化参数：优化方法、学习率、小批量样本数量</li><li>正则化系数</li></ul><p><strong>优化难点</strong></p><ul><li>参数优化是组合优化问题，没有梯度下降法来优化，没有通用的有效的方法</li><li>评估一组超参数配置的实际代价非常高</li></ul><p><strong>配置说明</strong></p><ul><li>有<span class="math inline">\(K\)</span>个超参数， 每个超参数配置表示为1个向量<span class="math inline">\(\mathbf x \in X\)</span></li><li><span class="math inline">\(f(\mathbf x)\)</span> 是衡量超参数配置<span class="math inline">\(\mathbf x\)</span>效果的函数</li><li><span class="math inline">\(f(\mathbf x)\)</span>不是<span class="math inline">\(\mathbf x\)</span>的连续函数，<span class="math inline">\(\mathbf x\)</span>也不同。 无法使用梯度下降等优化方法</li></ul><h2 id="超参数设置-搜索">超参数设置-搜索</h2><p>超参数设置：人工搜索、网格搜索、随机搜索。</p><p>缺点：没有利用到不同超参数组合之间的相关性，搜索方式都比较低效。</p><p><strong>1. 网格搜索</strong></p><p>对于<span class="math inline">\(K\)</span>个超参数，第<span class="math inline">\(k\)</span>个参数有<span class="math inline">\(m_k\)</span>种取值。总共的配置数量： <span class="math display">\[N = m_1 \times m_2 \times \cdots \times m_K\]</span> 如果超参数是连续的，可以根据经验选择一些经验值，比如学习率 <span class="math display">\[\alpha \in \{0.01, 0.1, 0.5, 1.0\}\]</span> 对这些超参数的不同组合，分别训练一个模型，测试在开发集上的性能。选取一组性能最好的配置。</p><p><strong>2. 随机搜索</strong></p><p>有的超参数对模型影响力有限（正则化），有的超参数对模型性能影响比较大。网格搜索会遍历所有的可能性。</p><p>随机搜索：对超参数进行随机组合，选择一个性能最好的配置。</p><p>优点：比网格搜索好，更容易实现，更有效。</p><h2 id="贝叶斯优化">贝叶斯优化</h2><p>根据当前已经试验的超参数组合，来预测下一个可能带来的最大收益的组合。</p><p>贝叶斯优化过程：根据已有的N组试验结果来建立高斯过程，计算<span class="math inline">\(f(\mathbf x)\)</span>的后验分布。</p><h2 id="动态资源分配">动态资源分配</h2><p>在早期阶段，估计出一组配置的效果会比较差，则中止这组配置的评估。把更多的资源留给其他配置。</p><p>这是多臂赌博机的泛化问题：最优赌博机。在给定有限次数的情况下，玩赌博机，找到收益最大的臂。</p><h2 id="神经架构搜索">神经架构搜索</h2><p>通过神经网络来自动实现网络架构的设计。</p><ul><li>变长字符串 -- 描述神经网络的架构</li><li>控制器 -- 生成另一个子网络的架构描述</li><li>控制器 -- RNN来实现</li><li>控制器训练 -- 强化学习来完成</li><li>奖励信号 -- 生成的子网络在开发集上的准确率</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;
&lt;h1 id=
      
    
    </summary>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="http://plmsmile.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="数据预处理" scheme="http://plmsmile.github.io/tags/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="归一化" scheme="http://plmsmile.github.io/tags/%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
      <category term="优化方法" scheme="http://plmsmile.github.io/tags/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>各种注意力总结</title>
    <link href="http://plmsmile.github.io/2018/03/25/33-attention-summary/"/>
    <id>http://plmsmile.github.io/2018/03/25/33-attention-summary/</id>
    <published>2018-03-25T06:14:12.000Z</published>
    <updated>2018-03-25T13:14:18.617Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一切都应该尽可能简单，但不能过于简单。</p></blockquote><blockquote><p>本文主要是总结：注意力机制、注意力机制的变体、论文中常见的注意力</p></blockquote><p><img src="" style="display:block; margin:auto" width="80%"></p><h1 id="注意力机制系统介绍">注意力机制系统介绍</h1><h2 id="问题背景">问题背景</h2><p><strong>计算能力不足</strong></p><p>神经网络有很强的能力。但是对于复杂任务，需要大量的输入信息和复杂的计算流程。计算机的计算能力是神经网络的一个瓶颈。</p><p><strong>减少计算复杂度</strong></p><p>常见的：局部连接、权值共享、汇聚操作。</p><p>但仍然需要：尽量<code>少增加模型复杂度</code>（参数），来<code>提高模型的表达能力</code>。</p><p><strong>简单文本分类可以使用单向量表达文本</strong></p><p>只需要一些关键信息即可，所以一个向量足以表达一篇文章，可以用来分类。</p><p><strong>阅读理解需要所有的语义</strong></p><p>文章比较长时，<strong>一个RNN很难反应出文章的所有语义信息</strong>。</p><p>对于阅读理解任务来说，编码时并不知道会遇到什么问题。这些问题可能会涉及到文章的所有信息点，如果丢失任意一个信息就可能导致无法正确回答问题。</p><p><strong>网络容量与参数成正比</strong></p><p>神经网络中可以存储的信息称为<code>网络容量</code>。 存储的多，参数也就越多，网络也就越复杂。 LSTM就是一个存储和计算单元。</p><p><strong>注意力和记忆力解决信息过载问题</strong></p><p>输入的信息太多(<code>信息过载问题</code>)，但不能同时处理这些信息。只能选择重要的信息进行计算，同时用额外空间进行信息存储。</p><ul><li><code>信息选择</code>：聚焦式自上而下地选择重要信息，过滤掉无关的信息。<strong>注意力机制</strong></li><li><code>外部记忆</code> ： 优化神经网络的记忆结构，使用额外的外部记忆，来提高网络的存储信息的容量。 <strong>记忆力机制 </strong></li></ul><p>比如，一篇文章，一个问题。答案只与几个句子相关。所以只需把相关的片段挑选出来交给后续的神经网络来处理，而不需要把所有的文章内容都给到神经网络。</p><h2 id="注意力">注意力</h2><p>注意力机制<code>Attention Mechanism</code> 是<code>解决信息过载</code>的一种资源分配方案，把<strong>计算资源分配给更重要的任务</strong>。</p><blockquote><p>注意力：人脑可以有意或无意地从大量的输入信息中，选择小部分有用信息来重点处理，并忽略其它信息</p></blockquote><p><strong>聚焦式注意力</strong></p><p>自上而下<code>有意识</code>的注意力。有预定目的、依赖任务、<code>主动有意识</code>的<code>聚焦于某一对象</code>的<code>注意力</code>。</p><p>一般注意力值聚焦式注意力。聚焦式注意力会根据环境、情景或任务的不同而选择不同的信息。</p><p><strong>显著性注意力</strong></p><p>自下而上<code>无意识</code>的注意力。由外界刺激驱动的注意力，无需主动干预，也和任务无关。如<code>Max Pooling</code>和<code>Gating</code>。</p><p><strong>鸡尾酒效应</strong></p><p>鸡尾酒效应可以理解这两种注意力。 在吵闹的酒会上</p><ul><li>噪音很多，依然可以听到朋友谈话的内容</li><li>没有关注背景声音，但是突然有人叫自己（重要信息），依然会马上注意到</li></ul><h2 id="普通注意力机制">普通注意力机制</h2><p>把目前的最大汇聚<code>Max Pooling</code>和门控<code>Gating</code> 近似地看做自下而上的基于显著性的注意力机制。</p><p>为了节省资源，选择重要的信息给到后续的神经网络进行计算，而不需要把所有的内容都给到后面的神经网络。</p><p><strong>输入N个信息</strong></p><p><span class="math inline">\(X_{1:N} = [\mathbf{x}_1, \cdots, \mathbf{x}_N]\)</span>， 问题<span class="math inline">\(\mathbf{q}\)</span>。 要从<span class="math inline">\(X\)</span>中选择一些和任务相关的信息输入给神经网络。</p><p><strong>计算注意力分布</strong></p><p><span class="math inline">\(\alpha_i\)</span> : 选择第<span class="math inline">\(i\)</span>个信息的概率，也称为<code>注意力分布</code> ，<span class="math inline">\(z\)</span>表示被选择信息的索引位置 <span class="math display">\[\alpha_i = p(z = i \mid X, \mathbf{q}) = \rm{softmax}\left(s(\mathbf{x}_i, \mathbf{q})\right) = \frac{\exp\left(s(\mathbf{x}_i, \mathbf{q})\right)}{\sum_{j=1}^N \exp\left(s(\mathbf{x}_j, \mathbf{q})\right)}\]</span></p><p><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#global-attention">NMT里面三种score打分函数</a> : <span class="math display">\[\color{blue}{\rm{score}(h_t, \bar h_s)} = \begin{cases}h_t^T \bar h_s &amp; \text{dot} \\h_t^T W_a \bar h_s  &amp; \text{general} \\v_a^T \tanh (W_a [h_t; \bar h_s]) &amp; \text{concat} \\\end{cases}\]</span> <code>加性模型</code> <span class="math display">\[s(\mathbf{x}_i, \mathbf{q}) = v^T\rm{tanh} (W\mathbf{x}_i + U\mathbf{q})\]</span> <code>点击模型</code> <span class="math display">\[s(\mathbf{x}_i, \mathbf{q}) = \mathbf{x}_i^T \mathbf{q}\]</span> <strong>计算注意力</strong></p><p><code>Soft Attention</code> 是对所有的信息进行加权求和。<code>Hard Attention</code>是选择最大信息的那一个。</p><p>使用软性注意力选择机制，对输入信息编码为，实际上也是一个期望。 <span class="math display">\[\rm{attn} (X, \mathbf q) = \sum_{i=1}^N \alpha_i \mathbf x_i = E_{z\sim p(z\mid X, \mathbf{q})} [X]\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/attention.png" style="display:block; margin:auto" width="80%"></p><h2 id="应用与优点">应用与优点</h2><p>传统机器翻译Encoder-Decoder的缺点：</p><ul><li>编码向量容量瓶颈问题：所有信息都需要保存在编码向量中</li><li>长距离依赖问题：长距离信息传递时，信息会丢失</li></ul><p><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/">注意力机制和PyTorch实现机器翻译</a></p><p>注意力机制直接从源语言信息中选择相关的信息作为辅助，有下面几个好处：</p><ul><li>解码过程中每一步都直接访问源语言所有位置上的信息。无需让所有信息都通过编码向量进行传递。</li><li>缩短了信息的传递距离。源语言的信息可以直接传递到解码过程中的每一步</li></ul><p>图像描述生成</p><h1 id="注意力机制变体">注意力机制变体</h1><h2 id="多头注意力">多头注意力</h2><p><code>Multi-head Attention</code>利用多个查询<span class="math inline">\(\mathbf{q}_{1:M}={\mathbf{q}_1, \cdots, \mathbf{q}_M}\)</span>来并行地从输入信息中选取多个信息。每个注意力关注输入信息的不同部分。比如<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/transformer.png" style="display:block; margin:auto" width="60%"> <img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/multi-head-attention.png" style="display:block; margin:auto" width="80%"></p><h2 id="硬性注意力">硬性注意力</h2><p>硬性注意力是<code>只关注到一个位置上</code>。</p><ul><li>选取最高概率的输入信息</li><li>在注意力分布上随机采样</li></ul><p>缺点：loss与注意力分布之间的函数关系不可导，无法使用反向传播训练。一般使用软性注意力。</p><p>需要：硬性注意力需要强化学习来进行训练。</p><h2 id="键值对注意力">键值对注意力</h2><p>输入信息：键值对<code>(Key, Value)</code>。 Key用来计算注意力分布<span class="math inline">\(\alpha_i\)</span>，值用来生成选择的信息。 <span class="math display">\[\rm{attn} (\mathbf{(K, V)}, \mathbf q) = \sum_{i=1}^N \alpha_i \mathbf v_i  =  \sum_{i=1}^N\frac{\exp\left(s(\mathbf{k}_i, \mathbf{q})\right)}{\sum_{j=1}^N \exp\left(s(\mathbf{k}_j, \mathbf{q})\right)} \mathbf v_i\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/attention.png" style="display:block; margin:auto" width="80%"></p><h2 id="结构化注意力">结构化注意力</h2><p>普通注意力是在输入信息上的一个多项分布，是一个扁平结构。</p><p>如果输入信息，本身就有<strong>层次化</strong>的结构，词、句子、段落、篇章等不同粒度的层次。这时用<code>层次化的注意力</code>来进行更好的信息选择。</p><p>也可以使用一种图模型，来构建更加复杂的结构化注意力分布。</p><h2 id="指针网络">指针网络</h2><p>前面的都是计算注意力对信息进行筛选：计算注意力分布，利用分布对信息进行加权平均。</p><p>指针网络<code>pointer network</code>是一种序列到序列的模型，用来指出相关信息的位置。也就是只做第一步。</p><p>输入： <span class="math inline">\(X_{1:n} = [\mathbf{x}_1, \cdots, \mathbf{x}_n]\)</span></p><p>输出：<span class="math inline">\(c_{1:m} = c_1, c_2, \cdots, c_m, \; c_i \in [1,n]\)</span>， 输出是序列的下标。如输入123，输出312</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/pointer-net.png" style="display:block; margin:auto" width="80%"></p><p>条件概率 <span class="math display">\[p(c_{1:m} \mid  \mathbf x_{1:n}) = \prod_{i=1}^m p(c_i \mid c_{1:i-1}, \mathbf x_{1:n})\approx \prod_{i=1}^m p(c_i \mid \mathbf x_{c_1}, \cdots,  \mathbf x_{c_{i-1}}\mathbf x_{1:n})\]</span></p><p><span class="math display">\[p(c_i \mid c_{1:i-1}, \mathbf x_{1:n}) = \rm{softmax}(s_{i,j})\]</span></p><p>第i步时，每个输入向量的得分（未归一化的注意力分布）： <span class="math display">\[s_{i,j} = v^T\rm{tanh} (W\mathbf{x}_j + U\mathbf{e}_i)\]</span> 其中向量<span class="math inline">\(\mathbf e_i\)</span>是第i个时刻，RNN对<span class="math inline">\(\mathbf x_{c_1}, \cdots, \mathbf x_{c_{i-1}}\mathbf x_{1:n}\)</span> 的编码。</p><h1 id="各种注意力计算模型">各种注意力计算模型</h1><h2 id="注意力的本质">注意力的本质</h2><p>有<span class="math inline">\(k\)</span>个<span class="math inline">\(d\)</span>维的特征向量<span class="math inline">\(\mathbf h_i \;(i \in [1,k])\)</span>，想要整合这k个特征向量的信息。得到一个向量<span class="math inline">\(\mathbf h^*\)</span>，一般也是d维。</p><ul><li>简单粗暴：对k个向量求平均。当然不合理啦。</li><li>加权平均：<span class="math inline">\(\mathbf h^* = \sum_{i=1}^k \alpha_i \mathbf h_i\)</span> 。<code>合理</code></li></ul><p>所以最重要的就是<strong>合理地求出<span class="math inline">\(\alpha_i\)</span></strong>，根据<code>所关心的对象</code><span class="math inline">\(\mathbf q\)</span>(可能是自身)去计算注意力分布</p><ul><li>针对每个<span class="math inline">\(\mathbf h_i\)</span>， 计算出一个<strong>得分</strong>，<span class="math inline">\(s_i\)</span>。 <span class="math inline">\(h_i\)</span>与<span class="math inline">\(q\)</span>越相关，得分越高。</li><li><span class="math inline">\(\alpha_i = \rm{softmax}(s_i)\)</span></li></ul><p><span class="math display">\[s_i = \rm{score}(\mathbf h_i, \mathbf q)\]</span></p><p>打分函数的计算：(<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#global-attention">NMT里面三种score打分函数</a> )</p><ul><li><code>Local-based Attention</code> ，没有外部的关注对象，自己关注自己。</li><li><code>General Attention</code>， 有外部的关注对象，直接乘积，全连接层。</li><li><code>Concatenation-based Attention</code>， 有关注的对象，先concat或相加再过连接层。</li></ul><h2 id="local-based">Local-based</h2><p>没有外部的信息，每个向量的得分<strong>与自己相关，与外部无关</strong>。</p><p>比如：<code>Where is the football?</code> ，<code>where</code>和<code>football</code>在句子中起总结性作用。Attention只与句子中的每个词有关。</p><p>一个句子，有多个词，多个向量。通过自己计算注意力分布，再对这些词的注意力进行加权求和，则可以得到这个句子的最终表达。 <span class="math display">\[s_i  = f(\mathbf h_i) = \rm{a}(W^T \mathbf h_i + b)\]</span></p><p><span class="math display">\[\mathbf h^* = \sum_{i=1}^n s_i \cdot \mathbf h_i\]</span></p><p>a是<a href="https://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">激活函数</a>。 <code>sigmoid</code>, <code>tanh</code>, <code>relu</code>, <code>maxout</code>， <code>y=x</code>（无激活函数）。</p><p><strong>1 一个得分简单求法</strong></p><p><a href="https://openreview.net/pdf?id=SkyQWDcex" target="_blank" rel="noopener">A Context-Aware Attention Network For Interactive Question Answering</a></p><p>利用自己计算注意力分布 <span class="math display">\[\gamma_j = \rm{softmax} (\mathbf v^T \mathbf g_j^q)\]</span> 利用新的注意力分布去计算最终的attention向量 <span class="math display">\[\mathbf u = W_{ch} \sum_{j=1}^N \gamma_j \mathbf g_j^q + \mathbf b_c ^q\]</span> <strong>2 两个得分合并为一个得分</strong></p><p><a href="http://wnzhang.net/papers/dadm-kdd.pdf" target="_blank" rel="noopener">Dynamic Attention Deep Model for Article Recommendation by Learning Human Editors’ Demonstration</a></p><p>计算两个得分 <span class="math display">\[\lambda _{m_t}^M = w_{m_t}^M \cdot \mathbf o + b_{m_t}^M , \quad \lambda_t ^T = w_t^T \cdot \mathbf o + b_t^T\]</span> 权值合并，求注意力分布： <span class="math display">\[p_t = \rm{softmax} (\alpha \lambda _{m_t}^M + (1-\alpha) \lambda_t ^T)\]</span> <strong>3 论文图片</strong></p><p>CAN for QA</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/CAN-1.png" style="display:block; margin:auto" width="80%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/CAN.png" style="display:block; margin:auto" width="80%"></p><p>Dynamic Attention</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dynamic-attention-1.png" style="display:block; margin:auto" width="80%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dynamic-attention-2.png" style="display:block; margin:auto" width="80%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dynamic-attention-3.png" style="display:block; margin:auto" width="80%"></p><h2 id="general-attention">General Attention</h2><p>有外部的信息，<span class="math inline">\(\mathbf h_i\)</span> 与 <span class="math inline">\(\mathbf q\)</span>进行乘积得分。 <a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#global-attention">机器翻译的应用</a> <span class="math display">\[\rm{score}(\mathbf h_i, \mathbf q) = \mathbf h_i^T W\mathbf q\]</span></p><h2 id="concatenation-based">Concatenation-based</h2><p>要关注的外部对象是<span class="math inline">\(\mathbf h_t\)</span>， 可以随时间变化，也可以一直不变(question)。 <span class="math display">\[s_i = f(\mathbf h_i, \mathbf h_t) = \mathbf v^T \rm a(W_1 \mathbf h_i + W_2 \mathbf h_t + b)\]</span> <strong>1 多个元素组成Attention</strong></p><p><a href="https://www.comp.nus.edu.sg/~xiangnan/papers/sigir17-AttentiveCF.pdf" target="_blank" rel="noopener">Attentive Collaborative Filtering Multimedia Recommendation with Item- and Component-Level Attention_sigir17</a></p><p>Item-Level Attention。可以看到需要加什么Attention，直接向公式里面一加就可以了。 <span class="math display">\[a(i, l) = w_1^T \phi(W_{1u} \mathbf u_i + W_{1v} \mathbf v_l + W_{1p} \mathbf p_l + W_{1x} \mathbf {\bar x}_l + \mathbf b_1) + \mathbf c_1\]</span></p><p><span class="math display">\[\alpha(i, l) = \frac {\exp (a(i, l))}{\sum_{n \in R(i)} \exp (a(i, n))}\]</span></p><h2 id="多层attention">多层Attention</h2><p>有<span class="math inline">\(m\)</span>个句子，每个句子有<span class="math inline">\(k\)</span>个词语。</p><p><code>Word-level Attention</code></p><p>每个句子，有k个词语，每个词语一个词向量，使用<code>Local-based Attention</code> ， 可以得到这个句子的向量表达<span class="math inline">\(\mathbf s_i\)</span>。</p><p><code>Sentence-level Attention</code></p><p>有<span class="math inline">\(m\)</span>个句子，每个句子是一个句子向量<span class="math inline">\(\mathbf s_i\)</span>。 可以再次Attention，得到文档的向量表达<span class="math inline">\(\mathbf d\)</span>， 也可以得到每个句子的权值<span class="math inline">\(\alpha_i\)</span>。</p><p>得到这些信息之后，再具体问题具体分析。</p><p><strong>1. 文章摘要生成</strong></p><p><a href="https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/ren-leveraging-2017.pdf" target="_blank" rel="noopener">Leveraging Contextual Sentence Relations for Extractive Summarization Using a Neural Attention Model_SIGIR2017</a></p><p>输入一篇文档，输出它的摘要。</p><ul><li>第一层：<code>Local-based Attention</code>， 生成每个句子的vector</li><li>第二层：当前句子作为中心，2n+1个句子。输入RNN（不明白）。将中心句子作为attention，来编码上下文。通过上下文对中心句子进行打分。作为该句子对整个文本的重要性</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/ap-bi-cnn-sentence-modeling.png" style="display:block; margin:auto" width="100%"></p><h2 id="can的实时问答">CAN的实时问答</h2><p><a href="https://openreview.net/pdf?id=SkyQWDcex" target="_blank" rel="noopener">A Context-Aware Attention Network For Interactive Question Answering</a></p><p>第一层Attention</p><p>对句子过GRU，每一时刻的output作为词的编码。再使用Local-Attention对这些词，得到<strong>问句的表达</strong><span class="math inline">\(\mathbf u\)</span>。</p><p>第二层Attention</p><p>由于上下文有多个句子。</p><p>首先，对一个句子进行过GRU，得到每一时刻单词的语义信息<span class="math inline">\(\alpha^t\)</span>， 然后利用Concat-Attention对这些单词计算，得到这句话的语义信息<span class="math inline">\(\mathbf y_t\)</span>。</p><p>再把当前句子的语义信息给到句子的GRU</p><p>第三次Attention</p><p>经过GRU，得到<strong>每个句子的表达</strong><span class="math inline">\(\mathbf s_t\)</span>。 再使用Concat-Attention来得到<strong>每个句子的注意力分配</strong><span class="math inline">\(\mathbf \beta_t\)</span>, 然后加权求和得到 <strong>整个Context的表达</strong><span class="math inline">\(\mathbf m\)</span>。</p><p>输出</p><p>结合<span class="math inline">\(\mathbf {m, u}\)</span>通过GRU去生成答案</p><ul><li><code>Period Symbol</code> ：是正确答案，直接输出</li><li><code>Question Mask</code>： 输出是一个问题，要继续问用户相应的信息</li></ul><p>用户重新给了反馈之后，对所有词汇信息使用<code>simple attention mechanism</code>， 即平均加权，所有的贡献都是一样的。得到反馈的向量表达<span class="math inline">\(\mathbf f\)</span>。</p><p>使用新的反馈向量和原始的问句向量，结合，重新生成新的context的语义表达<span class="math inline">\(\mathbf m\)</span>。 最终得到新的<span class="math inline">\(\mathbf {m, u}\)</span> 去重新回答。 <span class="math display">\[\mathbf r = \tanh (W_{rf}f + \mathbf b_r^{(f)})\]</span></p><p><span class="math display">\[\beta_t = \rm{softmax}(\mathbf u^T \mathbf s_t + \mathbf r^T \mathbf s_t)\]</span></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/CAN.png" style="display:block; margin:auto" width="100%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/ntm.png" style="display:block; margin:auto" width="80%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;一切都应该尽可能简单，但不能过于简单。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;本文主要是总结：注意力机制、注意力机制的变体、论文中常见的注意力&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Attention" scheme="http://plmsmile.github.io/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>Dynamic Coattention Network (Plus)</title>
    <link href="http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/"/>
    <id>http://plmsmile.github.io/2018/03/15/32-dynamic-coattention-network/</id>
    <published>2018-03-15T00:33:16.000Z</published>
    <updated>2018-03-18T14:06:35.276Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1611.01604" target="_blank" rel="noopener">Dynamic Coattention Networks For Question Answering</a></p><p><a href="https://arxiv.org/pdf/1711.00106" target="_blank" rel="noopener">DCN+: Mixed Objective and Deep Residual Coattention for Question Answering</a></p><blockquote><p>先放四张图，分别是DCN的Encoder、Decoder，DCN+的Encoder和Objective。后面再详细总结</p></blockquote><p><img src="" style="display:block; margin:auto" width="80%"></p><h1 id="dcn">DCN</h1><h2 id="coattention-encoder">Coattention Encoder</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dcn.png" style="display:block; margin:auto" width="100%"></p><h2 id="dynamic-pointing-decoder">Dynamic Pointing Decoder</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dcn-decoder.png" style="display:block; margin:auto" width="100%"></p><h2 id="hmn">HMN</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/hmn.png" style="display:block; margin:auto" width="60%"></p><h1 id="dcn-1">DCN+</h1><h2 id="dcn的问题">DCN的问题</h2><p><strong>loss没有判断真正的意义</strong></p><p>DCN使用传统交叉熵去优化<code>optimization</code>，只考虑答案字符串的匹配程度。但是实际上人的评判<code>evaluation</code>却是看回答的意义。如果只考虑span，则有下面两个问题：</p><ul><li>精确答案：没影响</li><li>但是对正确答案周围重叠的单词，却可能认为是错误的。</li></ul><p>句子：<code>Some believe that the Golden State Warriors team of 2017 is one of the greatest teams in NBA history</code></p><p>问题：<code>which team is considered to be one of the greatest teams in NBA history</code></p><p>正确答案：<code>the Golden State Warriors team of 2017</code></p><p>其实<code>Warriors</code>也是正确答案， 但是传统交叉熵却认为它还不如<code>history</code>。</p><p>DCN没有建立起<code>Optimization</code>和 <code>evaluation</code>的联系。 这也是Word Overlap。</p><p><strong>单层coattention表达力不强</strong></p><h2 id="dcn的优化点">DCN+的优化点</h2><p><strong>Mixed Loss</strong></p><p>交叉熵+自我批评学习（强化学习）。Word真正<strong>意义相似</strong>才会给一个好的<code>reward</code>。</p><ul><li>强化学习会鼓励意义相近的词语，而dis不相近的词语</li><li>交叉熵让强化学习朝着正确的轨迹发展</li></ul><p><strong>Deep Residual Coattention Encoder</strong></p><p>多层表达能力更强，详细看下面的优点。</p><h2 id="deep-residual-encoder">Deep Residual Encoder</h2><p><strong>优点</strong></p><p>两个别人得出的重要结论：</p><ul><li><code>stacked self-attention</code> 可以加速信号传递</li><li>减少信号传递路径，可以增加长依赖</li></ul><p>比DCN的两个优化点：</p><ul><li><code>coattention with self-attention</code>和多层<code>coattention</code> 。可以对输入有<code>richer representations</code></li><li>对每层的<code>coattention outputs</code>进行残差连接。缩短了信息传递路径。</li></ul><h2 id="coattention深层理解">Coattention深层理解</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/deep-residual-coattention-encoder.png" style="display:block; margin:auto" width="90%"></p><blockquote><p>当时理解了很久都不懂，后来一个下午，一直看，结合机器翻译实现和实际例子矩阵计算，终于理解了Attention、Coattention。</p></blockquote><p>参考了我的下面三篇笔记。</p><ul><li><a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#pytorch%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91">注意力机制在机器翻译中的思想和代码实现总结</a></li><li><a href="https://plmsmile.github.io/2017/10/10/attention-model/">图文介绍Attention</a></li><li><a href="https://plmsmile.github.io/2018/03/14/31-co-attention-vqa/#co-attention">Coattention的两种形式</a></li></ul><p><strong>单个Coattention层计算</strong></p><p>经过双向RNN后，得到两个语义编码：文档<span class="math inline">\(E_0^D \in \mathbb R^{m\times e}\)</span>， 问题编码<span class="math inline">\(E^Q_0 \in \mathbb R ^{n \times h}\)</span> 。 <span class="math display">\[E_1^D = \rm{biGRU_1}(E_0^D) \quad\in \mathbb R^{m \times h}\]</span></p><p><span class="math display">\[E_1^Q = \tanh(\rm{W \; \rm{biGRU_1(Q_E)+b)}} \quad \in \mathbb R^{n \times h}\]</span></p><p>计算<code>关联得分矩阵</code>A <span class="math display">\[A = E_1^D (E_1^Q)^T \in \mathbb R^{m \times n}\]</span></p><p><span class="math display">\[\begin{bmatrix} 0 &amp; 0 \\ 2 &amp; 3 \\0 &amp; 2 \\ 1 &amp; 1 \\3 &amp; 3 \\\end{bmatrix}_{5 \times 2}\cdot \begin{bmatrix} 1&amp; 3 \\ 1 &amp; 1 \\1&amp; 3 \\ \end{bmatrix}_{3 \times 2}^T=\begin{bmatrix} 0&amp; 0 &amp;0 \\ 11&amp; 5 &amp;11 \\ 6&amp; 2 &amp;6 \\ 4&amp; 2 &amp;4 \\ 12&amp; 6 &amp;12 \\ \end{bmatrix}_{5\times 3}\]</span></p><p>做<code>行Softmax</code>，得到Q对D的权值分配概率<span class="math inline">\(A^Q\)</span>， <code>attention_weights</code></p><ul><li>每一行是一个文档单词w</li><li>元素值是所有问句单词对当前文档单词w的注意力分配权值</li><li>元素值是每个问句单词的权值概率</li></ul><p><span class="math display">\[\begin{bmatrix} 0.3333 &amp; 0.3333 &amp; 0.3333 \\ 0.4994  &amp;0.0012 &amp; 0.4994 \\ 0.4955  &amp;0.0091  &amp;0.4955 \\ 0.4683  &amp;0.0634  &amp;0.4683\\ 0.4994  &amp;0.0012  &amp;0.4994 \\ \end{bmatrix}_{5\times 3}\]</span></p><p>计算D的summary， <span class="math inline">\(S^D = A^Q \cdot Q\)</span> <span class="math display">\[S^D = A^Q \cdot Q\]</span></p><ul><li>D所需要的新的语义，参考机器翻译的<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/#%E8%AE%A1%E7%AE%97%E6%96%B0%E7%9A%84%E8%AF%AD%E4%B9%89">新语义理解</a></li><li><span class="math inline">\(A^Q\)</span>的每一行去乘以Q的每一列去表达单词w</li><li>用Q去表达D，每个<span class="math inline">\(D_w\)</span>都是<strong>Q的所有单词对w的线性表达</strong>，权值就是<span class="math inline">\(A^Q\)</span><br></li><li>所以<span class="math inline">\(S^D\)</span>也是D的<code>summary</code>， 也称作D需要<code>context</code></li></ul><p>同理，对<code>列做softmax</code>， 得到D对Q的权值分配概率<span class="math inline">\(A^D\)</span>， 得到Q的<code>summary</code>， <span class="math inline">\(S^Q = A^D \cdot D\)</span></p><p>这时，借鉴<a href="https://plmsmile.github.io/2018/03/14/31-co-attention-vqa/#alternating-co-attention">alternation-coattention思想</a> 去计算对D的<code>Coattention context</code><span class="math inline">\(C^D\)</span> ： <span class="math display">\[C^D = S^Q \cdot A^Q\]</span> 实际上，<span class="math inline">\(C^D\)</span>与<span class="math inline">\(S^D\)</span>类似，都是<code>Summary</code>， 都是<code>context</code>。 只是<span class="math inline">\(C^D\)</span>使用的是新的<span class="math inline">\(S^Q\)</span>， 而不是<span class="math inline">\(E^Q_1\)</span>。</p><h2 id="coattention-encoder总结">Coattention Encoder总结</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/deep-residual-coattention-encoder.png" style="display:block; margin:auto" width="90%"></p><p>使用两层<code>coattention</code>， 最后再残差连接，经过LSTM输出。</p><p><strong>第一层</strong> <span class="math display">\[E_1^D = \rm{biGRU_1}(E_0^D) \quad\in \mathbb R^{m \times h} \\E_1^Q = \tanh(\rm{W \cdot \rm{biGRU_1(E_0^Q)+b)}} \quad \in \mathbb R^{n \times h}\]</span></p><p><span class="math display">\[\rm{coattn_1} (E_1^D, E_1^Q) =  S_1^D, S_1^Q, C_1^Q \\\]</span></p><p><strong>第二层</strong> <span class="math display">\[E_2^D = \rm{biGRU_2}(E_1^D) \quad\in \mathbb R^{m \times h} \\E_2^Q = \tanh (W \cdot \rm{biGRU_2}(E_1^Q) + b) \quad\in \mathbb R^{m \times h}\]</span></p><p><span class="math display">\[\rm{coattn_2} (E_2^D, E_2^Q) =  S_2^D, S_2^Q, C_2^Q \\\]</span></p><p><strong>残差连接所有的D</strong> <span class="math display">\[c = \rm {concat}((E_1^D, E_2^D, S_1^D, S_2^D, C_1^D, C_2^D)\]</span> <strong>LSTM编码输出，得到Encoder的输出</strong> <span class="math display">\[U = \rm{biGRU}(c) \quad \in \mathbb R^{m \times 2h}\]</span></p><h2 id="mixed-objective">Mixed Objective</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dcnplus-loss.png" style="display:block; margin:auto" width="100%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01604&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Dynamic Coattention Networks For Question Answering&lt;/a&gt;&lt;/p&gt;
&lt;p
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="QA" scheme="http://plmsmile.github.io/tags/QA/"/>
    
      <category term="DCN" scheme="http://plmsmile.github.io/tags/DCN/"/>
    
      <category term="coattention" scheme="http://plmsmile.github.io/tags/coattention/"/>
    
  </entry>
  
  <entry>
    <title>协同注意力简介</title>
    <link href="http://plmsmile.github.io/2018/03/14/31-co-attention-vqa/"/>
    <id>http://plmsmile.github.io/2018/03/14/31-co-attention-vqa/</id>
    <published>2018-03-14T08:56:27.000Z</published>
    <updated>2018-03-14T10:11:59.088Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>只是记录一下Co-Attention，后续再补上本篇论文的全部笔记吧。</p></blockquote><p>论文：<a href="https://arxiv.org/abs/1606.00061" target="_blank" rel="noopener">Hierarchical Question-Image Co-Attention for Visual Question Answering</a></p><p>我的相关笔记：<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/">Attention-based NMT阅读笔记</a>和<a href="https://plmsmile.github.io/2017/10/10/attention-model/#encoder-decoder">NLP中的Attention笔记</a></p><h1 id="co-attention">Co-Attention</h1><p>这里以VQA里面的两个例子记录一下Co-Attention。即图片和问题。</p><h2 id="注意力和协同注意力">注意力和协同注意力</h2><p><strong>注意力</strong></p><p><code>注意力机制</code>就像人<strong>带着问题去阅读</strong>， 先看问题，再去文本中有目标地阅读寻找答案。</p><p>机器阅读则是结合问题和文本的信息，生成一个关于文本段落各部分的<code>注意力权重</code>，再<strong>对文本信息进行加权</strong>。</p><p>注意力机制可以帮助我们更好地去捕捉段落中和问题相关的信息。</p><p><strong>协同注意力</strong></p><p><code>协同注意力</code>是一种<strong>双向的注意力</strong>， 再利用注意力去生成文本和问句的注意力。</p><ul><li>给文本生成注意力权值</li><li>给问句生成注意力权值</li></ul><p>协同注意力分为两种方式：</p><ul><li><strong>Parallel Co-Attention</strong> : 两种数据源A和B，先结合得到C，再基于结合信息C对A和B分别生成对应的Attention。<code>同时生成注意力</code></li><li><strong>Alternating Co-Attention</strong>： 先基于A产生B的attention，得到新的B；再基于新B去产生A的attention。两次<code>交替生成注意力</code></li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/coattention.png" style="display:block; margin:auto" width="100%"></p><h2 id="parallel-co-attention">Parallel Co-Attention</h2><p>图片特征：<span class="math inline">\(V \in \mathbb {R}^{d\times N}\)</span> ，问句特征：<span class="math inline">\(Q \in \mathbb R^{d \times T}\)</span> 。</p><p><code>同时</code>生成图片和问题的注意力。</p><p>先计算<code>关联矩阵</code>： <span class="math display">\[C = \rm{tanh}(Q^T W_b V) \in \mathbb R^{T \times N}\]</span> 计算<code>注意力权值</code> <span class="math inline">\(a^v\)</span>和<span class="math inline">\(a^q\)</span></p><p>方法1：直接选择最大值。<span class="math inline">\(a^v_n = \max \limits_i(C_{i, n})\)</span> ，<span class="math inline">\(a_t^q = \max \limits_i (C_{t, j})\)</span></p><p>方法2：把关联矩阵当做特征给到网络中，进行计算注意力权值，再进行<code>softmax</code>。<strong>更好</strong> <span class="math display">\[H^v = \rm{tanh} (W_vV + (W_qQ)C), \quad \quad H^q = \rm{tanh} (W_qQ + (W_vV)C^T)\]</span></p><p><span class="math display">\[a^v = \rm{softmax}(w_{hv}^TH^v), \quad \quad a^q = \rm{softmax}(w^T_{hq}H^q)\]</span></p><p>利用注意力和原特征向量去计算<code>新的特征向量</code> <span class="math display">\[\mathbf {\hat v} = \sum_{n=1}^N a^v_n \mathbf v_n, \quad \quad \mathbf { \hat q} = \sum_{t=1}^Tq_t^q \mathbf q_t\]</span></p><h2 id="alternating-co-attention">Alternating Co-Attention</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/coattention.png" style="display:block; margin:auto" width="100%"></p><p><code>交替</code>生成图片和问题的注意力。</p><ul><li>把问题归纳成一个单独向量<span class="math inline">\(\mathbf {q}\)</span></li><li>基于<span class="math inline">\(\mathbf q\)</span> 去和图片特征<span class="math inline">\(V\)</span>去生成图像特征<span class="math inline">\(\mathbf {\hat v}\)</span></li><li>基于<span class="math inline">\(\mathbf v\)</span>和问题特征<span class="math inline">\(Q\)</span>去生成问题特征<span class="math inline">\(\mathbf {\hat q}\)</span></li></ul><p>具体地，给一个<span class="math inline">\(X\)</span>和<code>attention guidance</code><span class="math inline">\(\mathbf g\)</span> ，通过<span class="math inline">\(\mathbf {\hat x} = f(X, \mathbf g)\)</span>去得到特征向量<span class="math inline">\(\mathbf {\hat x}\)</span> <span class="math display">\[H = \rm {tanh} (W_x X+ (W_g \mathbf g) \mathbb 1^T)\]</span> <span class="math inline">\(\mathbf a ^x\)</span> 是特征<span class="math inline">\(X\)</span>的<code>注意力权值</code> ： <span class="math display">\[\mathbf a^x = \rm(softmax)(w^T_{hx} H)\]</span> 新的<strong>注意力向量</strong> (<code>attended image (or question) vector)</code> : <span class="math display">\[\mathbf {\hat x} = \sum a_i^x \mathbf x_i\]</span> 对应本例子如下：</p><ul><li><span class="math inline">\(X = Q, \; g = 0 \to \mathbf q\)</span></li><li><span class="math inline">\(X = V, \; g = \mathbf q \to \mathbf {\hat v}\)</span></li><li><span class="math inline">\(X = Q, \; g = \mathbf {\hat v} \to \mathbf {\hat q}\)</span></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;只是记录一下Co-Attention，后续再补上本篇论文的全部笔记吧。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1606.00061&quot; target=&quot;_blank&quot; re
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="注意力" scheme="http://plmsmile.github.io/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
      <category term="VQA" scheme="http://plmsmile.github.io/tags/VQA/"/>
    
  </entry>
  
  <entry>
    <title>使用Dynamic Memory Network实现一个简单QA</title>
    <link href="http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/"/>
    <id>http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/</id>
    <published>2018-03-13T08:07:29.000Z</published>
    <updated>2018-03-13T13:15:10.221Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="noopener">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a></p><blockquote><p>本文概要：介绍DMN的基本原理，使用PyTorch进行实现一个简单QA</p></blockquote><p><img src="" style="display:block; margin:auto" width="50%"></p><h1 id="模型简介">模型简介</h1><h2 id="概要说明">概要说明</h2><p>许多NLP问题都可以看做一个Question-Answer问题。<code>Dynamic Memory Network</code> 由4部分组成。</p><p><strong>输入模块</strong></p><p>对输入的句子<code>facts</code>(先<code>embedding</code>)使用<a href="https://plmsmile.github.io/2017/10/18/rnn/#gru">GRU</a>进行编码，得到<code>encoded_facts</code>，给到后面的<code>情景记忆模块</code>。</p><p><strong>问题模块</strong></p><p>对输入的问题<code>question</code>使用<code>GRU</code>进行编码，得到<code>encoded_question</code>， 给到后面的<code>情景记忆模块</code> 和<code>回答模块</code> 。</p><p><strong>情景记忆模块</strong></p><p><code>Episodic Memory Module</code>由<code>memory</code>和<code>attention</code>组成。</p><ul><li>attention：会选择更重要的<code>facts</code></li><li>memory：根据<code>question</code>、<code>facts</code>和 <code>旧memory</code>来生成<code>新momery</code> 。初始：<code>memory=encoded_question</code></li></ul><p>会在<code>facts</code>上迭代多次去计算<code>memory</code>。 每一次迭代会提取出新的信息。</p><p>输出最终的<code>momery</code>， 给到<code>回答模块</code>。</p><p><strong>回答模块</strong></p><p><code>memory</code> + <code>question</code>， 在<code>GRUCell</code>上迭代<code>原本的回答长度</code>次， 得到最终的预测结果。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dmn-simple.png" style="display:block; margin:auto" width="50%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dmn-detail.png" style="display:block; margin:auto" width="90%"></p><h2 id="输入模块">输入模块</h2><p><strong>输入</strong></p><ul><li>一个句子，有<span class="math inline">\(T_I\)</span>个单词</li><li><span class="math inline">\(T_I\)</span>个句子，则把这些句子合并成一个大句子。在每个句子的末尾添加一个<code>句子结束标记&lt;/s&gt;</code>。如上图蓝色的部分</li></ul><p><strong>GRU计算隐状态</strong></p><p>句子过RNN时，对于每一时刻<span class="math inline">\(t\)</span>的单词<span class="math inline">\(w_t\)</span> ，有<span class="math inline">\(h_t\)</span> : <span class="math display">\[h_t = \rm{RNN}(w_t, h_{t-1})\]</span> <strong>输出</strong></p><p>使用RNN的<code>h = hidden states</code> 作为<code>输入句子的向量表达</code>，也就是<code>encoded_facts</code></p><ul><li>一个句子，输出所有时刻的<span class="math inline">\(h_t\)</span></li><li>多个句子，输出每个句子<code>结束标记&lt;/s&gt;</code>时刻的<span class="math inline">\(h_t\)</span>。</li></ul><h2 id="问题模块">问题模块</h2><p><strong>输入</strong></p><p>输入一个句子<code>question</code>，有<span class="math inline">\(T_Q\)</span>个单词。</p><p><strong>GRU计算隐状态</strong> <span class="math display">\[q_t = \rm{RNN}(w_t^Q, q_{t-1})\]</span> <strong>输出Q编码</strong></p><p><code>最后时刻的隐状态</code><span class="math inline">\(q_{T_Q}\)</span>作为句子的编码。</p><hr><h2 id="情景记忆模块">情景记忆模块</h2><p><strong>总体思路</strong></p><p>记忆模块收到两个编码表达：<code>encoded_facts</code>和<code>encoded_question</code> ， 也就是<span class="math inline">\(h\)</span>和<span class="math inline">\(q\)</span>。</p><p>模块会生成一个记忆<code>memory</code>，初始时<code>memory = encoded_question</code></p><p>记忆模块在<code>encoded_facts</code>上反复迭代多轮，每一轮去提取新的信息<code>episode</code>， 更新<code>memory</code></p><ul><li>遍历所有<code>facts</code>， 对于每一个的<code>fact</code>， 不停地更新当前轮的信息<code>e</code></li><li>计算新的信息：<span class="math inline">\(e_{new}=\rm{RNN}(fact, e)\)</span> ，使用当前fact和当前信息</li><li>计算新信息的保留比例注意门<span class="math inline">\(g\)</span></li><li><code>更新信息</code>：<span class="math inline">\(e = g * e_{new} + (1-g) * e\)</span></li><li><p>计算保留比例g：结合当前<code>fact</code> 、<code>memory</code>、 <code>question</code> 去生成多个特征，再过一个<code>两层前向网络G</code>得到一个比例数值</p></li><li><p><code>更新memory</code> ，<span class="math inline">\(m^i = \rm{GRU}(e, m^{i-1})\)</span></p></li></ul><p><strong>特征函数与前向网络</strong></p><p>保留比例门<code>g</code>充当着<code>attention</code>的作用 。</p><p>特征函数<span class="math inline">\(z(c, m, q)\)</span>， 其中c就是当前的<code>fact</code> ，（论文里面是9个特征）： <span class="math display">\[z(c, m, q) = [c \circ q,  c \circ m, \vert c-q\vert, \vert c-m\vert]\]</span> 前向网络<span class="math inline">\(g=G(c, m ,q)\)</span> ： <span class="math display">\[t = \rm{tanh}(W^1z(c, m, q) + b^1)  \\g = G(c, m, q) = \sigma(W^2 t + b^2)\]</span> <strong>e更新</strong></p><p>在每个fact遍历中，e会结合fact和旧e去生成新的信息<span class="math inline">\(e_{new}\)</span>，再结合旧<span class="math inline">\(e\)</span>和新<span class="math inline">\(e_{new}\)</span> 去生成最终的<span class="math inline">\(e^i\)</span> ： <span class="math display">\[e_{new}=\rm{RNN}(fact, e)\]</span></p><p><span class="math display">\[e = g * e_{new} + (1-g) * e\]</span></p><p><strong>记忆更新</strong></p><p>每一轮迭代后，结合旧记忆和当前轮的信息e去更新记忆： <span class="math display">\[m^i = \rm{GRU}(e, m^{i-1})\]</span> <strong>迭代停止条件</strong></p><ul><li>设置最大迭代次数<span class="math inline">\(T_M\)</span></li><li>在输入里面追加停止迭代信号，如果注意门选择它，则停止。</li></ul><h2 id="回答模块">回答模块</h2><p>回答模块结合memory和question，来生成对问题的答案。也是通过GRU来生成答案的。</p><p>设<code>a</code> 是<code>answer_gru</code>的hidden state，初始<span class="math inline">\(a_0= m^{T_M}\)</span> <span class="math display">\[y_t = \rm{softmax}(W^a a_t) \\a_t = \rm{GRU} ([y_{t-1}, q], a_{t-1})\]</span> 使用<code>交叉熵</code>去计算loss，进行优化。</p><h1 id="实现细节">实现细节</h1><p><a href="https://github.com/plmsmile/NLP-Demos/tree/master/question-answer-DMN" target="_blank" rel="noopener">我的github源代码</a> ，实现参考自<a href="https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/10.Dynamic-Memory-Network-for-Question-Answering.ipynb" target="_blank" rel="noopener">DSKSD的代码</a> 。</p><h2 id="数据处理">数据处理</h2><p><strong>原始数据</strong></p><p>使用过的数据是facebook的<a href="http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz" target="_blank" rel="noopener">bAbi Tasks Data 1-20</a>里面的 <code>en-10k</code>下的<code>qa5_three-arg-relations_train.txt</code> 和test数据。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> Bill travelled to the office.</span><br><span class="line"><span class="number">2</span> Bill picked up the football there.</span><br><span class="line"><span class="number">3</span> Bill went to the bedroom.</span><br><span class="line"><span class="number">4</span> Bill gave the football to Fred.</span><br><span class="line">5 What did Bill give to Fred?   football        4</span><br><span class="line"><span class="number">6</span> Fred handed the football to Bill.</span><br><span class="line"><span class="number">7</span> Jeff went back to the office.</span><br><span class="line">8 Who received the football?    Bill    6</span><br><span class="line"><span class="number">9</span> Bill travelled to the office.</span><br><span class="line"><span class="number">10</span> Bill got the milk there.</span><br><span class="line">11 Who received the football?   Bill    6</span><br><span class="line"><span class="number">12</span> Fred travelled to the garden.</span><br><span class="line"><span class="number">13</span> Fred went to the hallway.</span><br><span class="line"><span class="number">14</span> Bill journeyed to the bedroom.</span><br><span class="line"><span class="number">15</span> Jeff moved to the hallway.</span><br><span class="line"><span class="number">16</span> Jeff journeyed to the bathroom.</span><br><span class="line"><span class="number">17</span> Bill journeyed to the office.</span><br><span class="line"><span class="number">18</span> Fred travelled to the bathroom.</span><br><span class="line"><span class="number">19</span> Mary journeyed to the kitchen.</span><br><span class="line"><span class="number">20</span> Jeff took the apple there.</span><br><span class="line"><span class="number">21</span> Jeff gave the apple to Fred.</span><br><span class="line">22 Who did Jeff give the apple to?      Fred    21</span><br><span class="line"><span class="number">23</span> Bill went back to the bathroom.</span><br><span class="line"><span class="number">24</span> Bill left the milk.</span><br><span class="line">25 Who received the apple?      Fred    21</span><br><span class="line"><span class="number">1</span> Mary travelled to the garden.</span><br><span class="line"><span class="number">2</span> Mary journeyed to the kitchen.</span><br><span class="line"><span class="number">3</span> Bill went back to the office.</span><br><span class="line"><span class="number">4</span> Bill journeyed to the hallway.</span><br><span class="line"><span class="number">5</span> Jeff went back to the bedroom.</span><br><span class="line"><span class="number">6</span> Fred moved to the hallway.</span><br><span class="line"><span class="number">7</span> Bill moved to the bathroom.</span><br><span class="line"><span class="number">8</span> Jeff went back to the garden.</span><br><span class="line"><span class="number">9</span> Jeff went back to the kitchen.</span><br><span class="line"><span class="number">10</span> Fred went back to the garden.</span><br><span class="line"><span class="number">11</span> Mary got the football there.</span><br><span class="line"><span class="number">12</span> Mary handed the football to Jeff.</span><br><span class="line">13 What did Mary give to Jeff?  football        12</span><br></pre></td></tr></table></figure><p>比如1-25是一个大的情景</p><ul><li>没有问号的都是陈述句，是情景数据<code>fact</code>。只有<code>.</code>号， 都是简单句</li><li>带问号的：是问句，带有答案和答案所在句子。使用<code>tab</code>分割</li></ul><p><strong>加载原始数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_raw_data</span><span class="params">(file_path, seq_end=<span class="string">'&lt;/s&gt;'</span>)</span>:</span></span><br><span class="line">    <span class="string">''' 从文件中读取文本数据，并整合成[facts, question, answer]一条一条的可用数据，原始word形式</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file_path -- 数据文件</span></span><br><span class="line"><span class="string">        seq_end -- 句子结束标记</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        data -- list，元素是[facts, question, answer]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    source_data = open(file_path).readlines()</span><br><span class="line">    <span class="keyword">print</span> (file_path, <span class="string">":"</span>, len(source_data), <span class="string">"lines"</span>)</span><br><span class="line">    <span class="comment"># 去掉换行符号</span></span><br><span class="line">    source_data = [line[:<span class="number">-1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> source_data]</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> source_data:</span><br><span class="line">        index = line.split(<span class="string">' '</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> index == <span class="string">'1'</span>:</span><br><span class="line">            <span class="comment"># 一个新的QA开始</span></span><br><span class="line">            facts = []</span><br><span class="line">            <span class="comment">#qa = []</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'?'</span> <span class="keyword">in</span> line:</span><br><span class="line">            <span class="comment"># 当前QA的一个问句</span></span><br><span class="line">            <span class="comment"># 问题 答案 答案所在句子的编号 \t分隔</span></span><br><span class="line">            tmp = line.split(<span class="string">'\t'</span>)</span><br><span class="line">            question = tmp[<span class="number">0</span>].strip().replace(<span class="string">'?'</span>, <span class="string">''</span>).split(<span class="string">' '</span>)[<span class="number">1</span>:] + [<span class="string">'?'</span>]</span><br><span class="line">            answer = tmp[<span class="number">1</span>].split() + [seq_end]</span><br><span class="line">            facts_for_q = deepcopy(facts)</span><br><span class="line">            data.append([facts_for_q, question, answer])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 普通的事件描述，简单句，只有.和空格</span></span><br><span class="line">            sentence = line.replace(<span class="string">'.'</span>, <span class="string">''</span>).split(<span class="string">' '</span>)[<span class="number">1</span>:] + [seq_end]</span><br><span class="line">            facts.append(sentence)</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p><strong>把数据转成id格式</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triple_word2id</span><span class="params">(triple_word_data, th)</span>:</span></span><br><span class="line">    <span class="string">'''把文字转成id</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        triple_word_data -- [(facts, q, a)] word形式</span></span><br><span class="line"><span class="string">        th -- textheler</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        triple_id_data -- [(facts, q, a)]index形式</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 把各个word转成数字id</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> triple_word_data:</span><br><span class="line">        <span class="comment"># 处理facts句子</span></span><br><span class="line">        <span class="keyword">for</span> i, fact <span class="keyword">in</span> enumerate(t[<span class="number">0</span>]):</span><br><span class="line">            t[<span class="number">0</span>][i] = th.sentence2indices(fact)</span><br><span class="line">        <span class="comment"># 问题与答案</span></span><br><span class="line">        t[<span class="number">1</span>] = th.sentence2indices(t[<span class="number">1</span>])</span><br><span class="line">        t[<span class="number">2</span>] = th.sentence2indices(t[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> triple_word_data</span><br></pre></td></tr></table></figure><p><strong>根据batch_size取数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_loader</span><span class="params">(data, batch_size=<span class="number">1</span>, shuffle=False)</span>:</span></span><br><span class="line">    <span class="string">''' 以batch的格式返回数据</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data -- list格式的data</span></span><br><span class="line"><span class="string">        batch_size -- </span></span><br><span class="line"><span class="string">        shuffle -- 每一个epoch开始的时候，对数据进行shuffle</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        数据遍历的iterator</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.shuffle(data)</span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    end = batch_size</span><br><span class="line">    <span class="keyword">while</span> (start &lt; len(data)):</span><br><span class="line">        batch = data[start:end]</span><br><span class="line">        start, end = end, end + batch_size</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br><span class="line">    <span class="keyword">if</span> end &gt;= len(data) <span class="keyword">and</span> start &lt; len(data):</span><br><span class="line">        batch = data[start:]</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br></pre></td></tr></table></figure><p><strong>对每一个batch进行padding</strong></p><p>这部分有点复杂。要求问题、答案、fact的长度一致，每个问题的fact的数量也要一样。</p><blockquote><p>其实和模型也有关，模型写的有点坑，就是每条数据的所有fact应该连接在一起成为一个大的fact送进GRU里，在每个fact后面加上结束标记。但是我这却分开了，分成了多个标记好的fact，也怪当时没有仔细看好论文，这个也是参考别人的实现。循环也导致训练贼慢，但是现在忙着找实习，就先不改了。后面好好写DMNPLUS吧。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_batch_data</span><span class="params">(raw_batch_data, th)</span>:</span></span><br><span class="line">    <span class="string">''' 对数据进行padding，问题、答案、fact长度分别一致，同时每条数据的fact的数量一致。输入到网络的时候要用</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        raw_batch_data -- [[facts, q, a]]，都是以list wordid表示</span></span><br><span class="line"><span class="string">        th -- TextHelper</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        all_facts -- [b, nfact, flen]，pad后的facts，Variable</span></span><br><span class="line"><span class="string">        all_facts_mask -- [b, nfact, flen]，facts的mask，Variable</span></span><br><span class="line"><span class="string">        questions -- [b, qlen]，pad后的questions，Variable</span></span><br><span class="line"><span class="string">        questions_mask -- [b, qlen]，questions的mask，Variable</span></span><br><span class="line"><span class="string">        answers -- [b, alen]，pad后的answers，Variable</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    all_facts, questions, answers = [list(i) <span class="keyword">for</span> i <span class="keyword">in</span> zip(*raw_batch_data)]</span><br><span class="line">    batch_size = len(raw_batch_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 计算各种长度。一个QA的facts数量，fact、Q、A句子的最大长度</span></span><br><span class="line">    n_fact = max([len(facts) <span class="keyword">for</span> facts <span class="keyword">in</span> all_facts])</span><br><span class="line">    flen = max([len(f) <span class="keyword">for</span> f <span class="keyword">in</span> flatten(all_facts)])</span><br><span class="line">    qlen = max([len(q) <span class="keyword">for</span> q <span class="keyword">in</span> questions])</span><br><span class="line">    alen = max([len(a) <span class="keyword">for</span> a <span class="keyword">in</span> answers])</span><br><span class="line">    padid = th.word2index(th.pad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 对数据进行padding</span></span><br><span class="line">    all_facts_mask = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        <span class="comment"># 2.1 pad fact</span></span><br><span class="line">        facts = all_facts[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(facts)):</span><br><span class="line">            t = flen - len(facts[j])</span><br><span class="line">            <span class="keyword">if</span> t &gt; <span class="number">0</span>:</span><br><span class="line">                all_facts[i][j] = facts[j] + [padid] * t</span><br><span class="line">        <span class="comment"># fact数量pad</span></span><br><span class="line">        <span class="keyword">while</span> (len(facts) &lt; n_fact):</span><br><span class="line">            all_facts[i].append([padid] * flen)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算facts内容是否是填充给的，填充为1，不填充为0</span></span><br><span class="line">        mask = [tuple(map(<span class="keyword">lambda</span> v: v == padid, fact)) <span class="keyword">for</span> fact <span class="keyword">in</span> all_facts[i]]</span><br><span class="line">        all_facts_mask.append(mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2.2 pad question</span></span><br><span class="line">        q = questions[i]</span><br><span class="line">        <span class="keyword">if</span> len(q) &lt; qlen:</span><br><span class="line">            questions[i] = q + [padid] * (qlen - len(q))</span><br><span class="line">        <span class="comment"># 2.3 pad answer</span></span><br><span class="line">        a = answers[i]</span><br><span class="line">        <span class="keyword">if</span> len(a) &lt; alen:</span><br><span class="line">            answers[i] = a + [padid] * (alen - len(a))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 把list数据转成Variable</span></span><br><span class="line">    all_facts = get_variable(torch.LongTensor(all_facts))</span><br><span class="line">    all_facts_mask = get_variable(torch.ByteTensor(all_facts_mask))</span><br><span class="line">    answers = get_variable(torch.LongTensor(answers))</span><br><span class="line">    questions = torch.LongTensor(questions)</span><br><span class="line">    questions_mask = [(tuple(map(<span class="keyword">lambda</span> v: v == padid, q))) <span class="keyword">for</span> q <span class="keyword">in</span> questions]</span><br><span class="line">    questions_mask = torch.ByteTensor(questions_mask)</span><br><span class="line">    questions, questions_mask = get_variable(questions), get_variable(questions_mask)</span><br><span class="line">    <span class="keyword">return</span> all_facts, all_facts_mask, questions, questions_mask, answers</span><br></pre></td></tr></table></figure><h2 id="模型">模型</h2><p><strong>模型定义</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DMN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, hidden_size, padding_idx, seqbegin_id, dropout_p=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vocab_size -- 词汇表大小</span></span><br><span class="line"><span class="string">            embed_size -- 词嵌入维数</span></span><br><span class="line"><span class="string">            hidden_size -- GRU的输出维数</span></span><br><span class="line"><span class="string">            padding_idx -- pad标记的wordid</span></span><br><span class="line"><span class="string">            seqbegin_id -- 句子起始的wordid</span></span><br><span class="line"><span class="string">            dropout_p -- dropout比率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(DMN, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.seqbegin_id = seqbegin_id</span><br><span class="line">        </span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)</span><br><span class="line">        self.input_gru = nn.GRU(embed_size, hidden_size, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        self.question_gru = nn.GRU(embed_size, hidden_size, batch_first=<span class="keyword">True</span>)    </span><br><span class="line">        self.gate = nn.Sequential(</span><br><span class="line">                        nn.Linear(hidden_size * <span class="number">4</span>, hidden_size),</span><br><span class="line">                        nn.Tanh(),</span><br><span class="line">                        nn.Linear(hidden_size, <span class="number">1</span>),</span><br><span class="line">                        nn.Sigmoid()</span><br><span class="line">                    )</span><br><span class="line">        self.attention_grucell = nn.GRUCell(hidden_size, hidden_size)</span><br><span class="line">        self.memory_grucell = nn.GRUCell(hidden_size, hidden_size)</span><br><span class="line">        self.answer_grucell = nn.GRUCell(hidden_size * <span class="number">2</span>, hidden_size)</span><br><span class="line">        self.answer_fc = nn.Linear(hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        </span><br><span class="line">        self.init_weight()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="string">'''GRU的初始hidden。单层单向'''</span></span><br><span class="line">        hidden = torch.zeros(<span class="number">1</span>, batch_size, self.hidden_size)</span><br><span class="line">        hidden = get_variable(hidden)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weight</span><span class="params">(self)</span>:</span></span><br><span class="line">        nn.init.xavier_uniform(self.embed.state_dict()[<span class="string">'weight'</span>])</span><br><span class="line">        components = [self.input_gru, self.question_gru, self.gate, self.attention_grucell,</span><br><span class="line">                     self.memory_grucell, self.answer_grucell]</span><br><span class="line">        <span class="keyword">for</span> component <span class="keyword">in</span> components:</span><br><span class="line">            <span class="keyword">for</span> name, param <span class="keyword">in</span> component.state_dict().items():</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">                    nn.init.xavier_normal(param)</span><br><span class="line">        nn.init.xavier_uniform(self.answer_fc.state_dict()[<span class="string">'weight'</span>])</span><br><span class="line">        self.answer_fc.bias.data.fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><strong>前向计算参数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, allfacts, allfacts_mask, questions, questions_mask, alen, n_episode=<span class="number">3</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            allfacts -- [b, n_fact, flen]，输入的多个句子</span></span><br><span class="line"><span class="string">            allfacts_mask -- [b, n_fact, flen]，mask=1表示是pad的，否则不是</span></span><br><span class="line"><span class="string">            questions -- [b, qlen]，问题</span></span><br><span class="line"><span class="string">            questions_mask -- [b, qlen]，mask=1：pad</span></span><br><span class="line"><span class="string">            alen -- Answer len</span></span><br><span class="line"><span class="string">            seqbegin_id -- 句子开始标记的wordid</span></span><br><span class="line"><span class="string">            n_episodes -- </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            preds -- [b * alen,  vocab_size]，预测的句子。b*alen合在一起方便后面算交叉熵</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 0. 计算常用的信息，batch_size，一条数据nfact条句子，每个fact长度为flen，每个问题长度为qlen</span></span><br><span class="line">        bsize = allfacts.size(<span class="number">0</span>)</span><br><span class="line">        nfact = allfacts.size(<span class="number">1</span>)</span><br><span class="line">        flen = allfacts.size(<span class="number">2</span>)</span><br><span class="line">        qlen = questions.size(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>输入模块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 输入模块，用RNN编码输入的句子</span></span><br><span class="line"><span class="comment"># TODO 两层循环，待优化</span></span><br><span class="line">encoded_facts = []</span><br><span class="line"><span class="comment"># 对每一条数据，计算facts编码</span></span><br><span class="line"><span class="keyword">for</span> facts, facts_mask <span class="keyword">in</span> zip(allfacts, allfacts_mask):</span><br><span class="line">    facts_embeds = self.embed(facts)</span><br><span class="line">    facts.embeds = self.dropout(facts_embeds)</span><br><span class="line">    hidden = self.init_hidden(nfact)</span><br><span class="line">    <span class="comment"># 1.1 把输入(多条句子)给到GRU</span></span><br><span class="line">    <span class="comment"># b=nf, [nf, flen, h], [1, nf, h]</span></span><br><span class="line">    outputs, hidden = self.input_gru(facts_embeds, hidden)</span><br><span class="line">    <span class="comment"># 1.2 每条句子真正结束时(real_len)对应的输出，作为该句子的hidden。GRU：ouput=hidden</span></span><br><span class="line">    real_hiddens = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, o <span class="keyword">in</span> enumerate(outputs):</span><br><span class="line">        real_len = facts_mask[i].data.tolist().count(<span class="number">0</span>)</span><br><span class="line">        real_hiddens.append(o[real_len - <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 1.3 把所有单个fact连接起来，unsqueeze(0)是为了后面的所有batch的cat</span></span><br><span class="line">        hiddens = torch.cat(real_hiddens).view(nfact, <span class="number">-1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        encoded_facts.append(hiddens)</span><br><span class="line">        <span class="comment"># [b, nfact, h]</span></span><br><span class="line">        encoded_facts = torch.cat(encoded_facts)</span><br></pre></td></tr></table></figure><p><strong>问句模块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. 问题模块，对问题使用RNN编码</span></span><br><span class="line">questions_embeds = self.embed(questions)</span><br><span class="line">questions_embeds = self.dropout(questions_embeds)</span><br><span class="line">hidden = self.init_hidden(bsize)</span><br><span class="line"><span class="comment"># [b, qlen, h], [1, b, h]</span></span><br><span class="line">outputs, hidden = self.question_gru(questions_embeds, hidden)</span><br><span class="line">real_questions = []</span><br><span class="line"><span class="keyword">for</span> i, o <span class="keyword">in</span> enumerate(outputs):</span><br><span class="line">    real_len = questions_mask[i].data.tolist().count(<span class="number">0</span>)</span><br><span class="line">    real_questions.append(o[real_len - <span class="number">1</span>])</span><br><span class="line">    encoded_questions = torch.cat(real_questions).view(bsize, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Memory模块</span></span><br><span class="line">memory = encoded_questions</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_episode):</span><br><span class="line">    <span class="comment"># e</span></span><br><span class="line">    e = self.init_hidden(bsize).squeeze(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># [nfact, b, h]</span></span><br><span class="line">    encoded_facts_t = encoded_facts.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 根据memory, episode，计算每一时刻的e。最终的e和memory来计算新的memory</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(nfact):</span><br><span class="line">        <span class="comment"># [b, h]</span></span><br><span class="line">        bfact = encoded_facts_t[t]</span><br><span class="line">        <span class="comment"># TODO 计算4个特征，论文是9个</span></span><br><span class="line">        f1 = bfact * encoded_questions</span><br><span class="line">        f2 = bfact * memory</span><br><span class="line">        f3 = torch.abs(bfact - encoded_questions)</span><br><span class="line">        f4 = torch.abs(bfact - memory)</span><br><span class="line">        z = torch.cat([f1, f2, f3, f4], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [b, 1] 对每个fact的注意力</span></span><br><span class="line">        gt = self.gate(z)</span><br><span class="line">        e = gt * self.attention_grucell(bfact, e) + (<span class="number">1</span> - gt) * e</span><br><span class="line">        <span class="comment"># 每一轮的e和旧memory计算新的memory</span></span><br><span class="line">        memory = self.memory_grucell(e, memory)</span><br></pre></td></tr></table></figure><p><strong>回答模块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Answer模块</span></span><br><span class="line"><span class="comment"># [b, h]</span></span><br><span class="line">answer_hidden = memory</span><br><span class="line">begin_tokens = get_variable(torch.LongTensor([self.seqbegin_id]*bsize))</span><br><span class="line"><span class="comment"># [b, h]</span></span><br><span class="line">last_word = self.embed(begin_tokens)</span><br><span class="line">preds = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(alen):</span><br><span class="line">    inputs = torch.cat([last_word, encoded_questions], dim=<span class="number">1</span>)</span><br><span class="line">    answer_hidden = self.answer_grucell(inputs, answer_hidden)</span><br><span class="line">    <span class="comment"># to vocab_size</span></span><br><span class="line">    probs = self.answer_fc(answer_hidden)</span><br><span class="line">    <span class="comment"># [b, v]</span></span><br><span class="line">    probs = F.log_softmax(probs.float())</span><br><span class="line">    _, indics = torch.max(probs, <span class="number">1</span>)</span><br><span class="line">    last_word = self.embed(indics)</span><br><span class="line">    <span class="comment"># for cross entropy</span></span><br><span class="line">    preds.append(probs.view(bsize, <span class="number">1</span>, <span class="number">-1</span>))</span><br><span class="line">    <span class="comment">#print (preds[0].data.shape)</span></span><br><span class="line">    preds = torch.cat(preds, dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> preds.view(bsize * alen, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="配置信息">配置信息</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''配置文件'''</span></span><br><span class="line">    <span class="comment"># 数据信息</span></span><br><span class="line">    train_file = <span class="string">"./datasets/tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_train.txt"</span></span><br><span class="line">    test_file = <span class="string">"./datasets/tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_test.txt"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 一些特殊符号</span></span><br><span class="line">    seq_end = <span class="string">'&lt;/s&gt;'</span></span><br><span class="line">    seq_begin = <span class="string">'&lt;s&gt;'</span></span><br><span class="line">    pad = <span class="string">'&lt;pad&gt;'</span></span><br><span class="line">    unk = <span class="string">'&lt;unk&gt;'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># DataLoader信息</span></span><br><span class="line">    batch_size = <span class="number">128</span></span><br><span class="line">    shuffle = <span class="keyword">False</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    num_workers = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># model</span></span><br><span class="line">    embed_size = <span class="number">64</span></span><br><span class="line">    hidden_size = <span class="number">64</span></span><br><span class="line">    <span class="comment"># 对inputs推理的轮数</span></span><br><span class="line">    n_episode = <span class="number">3</span></span><br><span class="line">    dropout_p = <span class="number">0.1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    max_epoch = <span class="number">500</span></span><br><span class="line">    learning_rate = <span class="number">0.001</span></span><br><span class="line">    min_loss = <span class="number">0.01</span></span><br><span class="line">    print_every_epoch = <span class="number">5</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># cuda信息</span></span><br><span class="line">    use_cuda = <span class="keyword">True</span></span><br><span class="line">    device_id = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># model_path</span></span><br><span class="line">    model_path = <span class="string">"./models/DMN.pkl"</span></span><br></pre></td></tr></table></figure><h2 id="训练">训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(opt, th, train_data)</span>:</span></span><br><span class="line">    <span class="string">''' 训练</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        opt -- 配置信息</span></span><br><span class="line"><span class="string">        th -- TextHelper实例</span></span><br><span class="line"><span class="string">        train_data -- 训练数据，[[facts, question, answer]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 加载原始数据</span></span><br><span class="line">    seqbegin_id = th.word2index(th.seq_begin)</span><br><span class="line">    </span><br><span class="line">    model = DMN(th.vocab_size, opt.embed_size, opt.hidden_size, seqbegin_id, th.word2index(th.pad))</span><br><span class="line">    <span class="keyword">if</span> opt.use_cuda:</span><br><span class="line">        model = model.cuda(opt.device_id)</span><br><span class="line">    </span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr = opt.learning_rate)</span><br><span class="line">    loss_func = nn.CrossEntropyLoss(ignore_index=th.word2index(th.pad))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(opt.max_epoch):</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> batch_data <span class="keyword">in</span> get_data_loader(train_data, opt.batch_size, opt.shuffle):</span><br><span class="line">            <span class="comment"># batch内的数据进行pad，转成Variable</span></span><br><span class="line">            allfacts, allfacts_mask, questions, questions_mask, answers = \</span><br><span class="line">                    pad_batch_data(batch_data, th)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 前向</span></span><br><span class="line">            preds = model(allfacts, allfacts_mask, questions, questions_mask, </span><br><span class="line">                          answers.size(<span class="number">1</span>), opt.n_episode)</span><br><span class="line">            <span class="comment"># loss</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss = loss_func(preds, answers.view(<span class="number">-1</span>))</span><br><span class="line">            losses.append(loss.data.tolist()[<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># 反向</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        avg_loss = np.mean(losses)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> avg_loss &lt;= opt.min_loss <span class="keyword">or</span> e % opt.print_every_epoch == <span class="number">0</span> <span class="keyword">or</span> e == opt.max_epoch - <span class="number">1</span>:    </span><br><span class="line">            info = <span class="string">"e=&#123;&#125;, loss=&#123;&#125;"</span>.format(e, avg_loss)</span><br><span class="line">            losses = []</span><br><span class="line">            <span class="keyword">print</span> (info)</span><br><span class="line">            <span class="keyword">if</span> e == opt.max_epoch - <span class="number">1</span> <span class="keyword">and</span> avg_loss &gt; opt.min_loss:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"epoch finish, loss &gt; min_loss"</span>)</span><br><span class="line">                torch.save(model, opt.model_path)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">elif</span> avg_loss &lt;= opt.min_loss:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Early stop"</span>)</span><br><span class="line">                torch.save(model, opt.model_path)</span><br><span class="line">                <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h2 id="预测和效果">预测和效果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_test_accuracy</span><span class="params">(model, test_data, th, n_episode=DefaultConfig.n_episode)</span>:</span></span><br><span class="line">    <span class="string">'''测试，测试数据'''</span></span><br><span class="line">    batch_size = <span class="number">1</span></span><br><span class="line">    model.eval()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> get_data_loader(test_data, batch_size, <span class="keyword">False</span>):</span><br><span class="line">        facts, facts_mask, question, question_mask, answer = pad_batch_data(item, th)</span><br><span class="line">        preds = model(facts, facts_mask, question, question_mask, answer.size(<span class="number">1</span>), n_episode)</span><br><span class="line">        <span class="comment">#print (answer.data.shape, preds.data.shape)</span></span><br><span class="line">        preds = preds.max(<span class="number">1</span>)[<span class="number">1</span>].data.tolist()</span><br><span class="line">        answer = answer.view(<span class="number">-1</span>).data.tolist()</span><br><span class="line">        <span class="keyword">if</span> preds == answer:</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"acccuracy = "</span>, correct / len(test_data)) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one_data</span><span class="params">(model, item, th, n_episode=DefaultConfig.n_episode)</span>:</span></span><br><span class="line">    <span class="string">''' 测试一条数据</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model -- DMN模型</span></span><br><span class="line"><span class="string">        item -- [facts, question, answer]</span></span><br><span class="line"><span class="string">        th -- TextHelper</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># batch_size = 1</span></span><br><span class="line">    model.eval()</span><br><span class="line">    item = [item]</span><br><span class="line">    facts, facts_mask, question, question_mask, answer = pad_batch_data(item, th)</span><br><span class="line">    preds = model(facts, facts_mask, question, question_mask, answer.size(<span class="number">1</span>), n_episode)</span><br><span class="line">    </span><br><span class="line">    item = item[<span class="number">0</span>]</span><br><span class="line">    preds = preds.max(<span class="number">1</span>)[<span class="number">1</span>].data.tolist()</span><br><span class="line">    fact = item[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    facts = [th.indices2sentence(fact) <span class="keyword">for</span> fact <span class="keyword">in</span> item[<span class="number">0</span>]]</span><br><span class="line">    facts = [<span class="string">" "</span>.join(fact) <span class="keyword">for</span> fact <span class="keyword">in</span> facts]</span><br><span class="line">    q = <span class="string">" "</span>.join(th.indices2sentence(item[<span class="number">1</span>]))</span><br><span class="line">    a = <span class="string">" "</span>.join(th.indices2sentence(item[<span class="number">2</span>]))</span><br><span class="line">    preds = <span class="string">" "</span>.join(th.indices2sentence(preds))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Facts:"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"\n"</span>.join(facts))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Question:"</span>, q)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Answer:"</span>, a)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Predict:"</span>, preds)</span><br><span class="line">    <span class="keyword">print</span> ()</span><br></pre></td></tr></table></figure><p>在本数据集上效果较好，但是数据量小、句子简单，还没有在别的数据集上面进行测试。等忙完了测试一下。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1506.07285&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ask Me Anything: Dynamic Memory Networks for Natural Languag
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="DMN" scheme="http://plmsmile.github.io/tags/DMN/"/>
    
      <category term="QA" scheme="http://plmsmile.github.io/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>决策树笔记</title>
    <link href="http://plmsmile.github.io/2018/03/05/29-desicion-tree/"/>
    <id>http://plmsmile.github.io/2018/03/05/29-desicion-tree/</id>
    <published>2018-03-05T07:44:59.000Z</published>
    <updated>2018-03-08T09:12:47.010Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>决策树的特征选择、生成、剪枝。熵、信息增益、基尼指数。ID3、C4.5、CART。</p></blockquote><h1 id="决策树背景">决策树背景</h1><h2 id="概览">概览</h2><p><strong>树的意义</strong></p><p>决策树是一棵<code>if-then树</code>。 <code>内部节点</code>代表一个属性或<code>特征</code>，<code>叶节点</code>代表一个<code>类</code>。</p><p>决策树也是给定各个特征的情况下，某类别的概率。即条件概率<span class="math inline">\(P(Y \mid X)\)</span>。</p><p><strong>树的生成</strong></p><p>构建根节点，选择<code>最优特征</code>。按照特征<code>划分子集</code>，继续选择新的最优特征，直到没有或者全部被正确分类。</p><p><strong>剪枝</strong></p><p>决策树的生成对应于<code>模型的局部选择</code>，会尽量<code>拟合训练数据</code>，导致模型<code>复杂</code>和<code>过拟合</code>。</p><p>决策树的剪枝对应于<code>模型的全局选择</code>， 自下而上删掉一些节点。</p><h2 id="熵和信息增益">熵和信息增益</h2><p>在每个节点，要选择一个<code>最优特征</code>生成。</p><ul><li><code>ID3</code>使用<code>信息增益最大</code>选择最优特征</li><li><code>C4.5</code>使用<code>信息增益率最大</code>来选择最优特征</li><li><code>CART回归树</code> ，<code>平方误差最小</code></li><li><code>CART分类树</code>， <code>基尼指数最小</code></li></ul><p><strong>信息量</strong></p><p>信息量是随机变量<span class="math inline">\(X\)</span><code>不确定性</code>的度量。 <span class="math display">\[I(X) = - \log p(x)\]</span> <strong>熵</strong></p><p>熵是信息量的期望，也是<strong>随机变量不确定性的度量</strong>。熵偏向<code>离散属性</code>， 基尼指数偏向<code>连续属性</code>。<br><span class="math display">\[H(X) = - \sum_{x \in X} p(x) \log p(x)\]</span> <strong>条件熵</strong></p><p>条件熵是在给定随机变量<span class="math inline">\(X\)</span>的情况下，随机变量<span class="math inline">\(Y\)</span>的不确定性。<br><span class="math display">\[H(Y \mid X) = \sum_{i = 1}^K p(x_i) H(Y \mid X = x_i)\]</span> <span class="math inline">\(X\)</span>共有<code>K</code>类，<span class="math inline">\(p(x_i)\)</span>表示<span class="math inline">\(X\)</span>属于第<span class="math inline">\(i\)</span>类的概率。<span class="math inline">\(H(Y\mid X=x_i)\)</span>表示<span class="math inline">\(X=x_i\)</span>时<span class="math inline">\(Y\)</span>的子集的熵。</p><p><strong>经验熵和经验条件熵</strong></p><p>由数据估计（极大似然估计）得到的熵和条件熵。</p><p>如数据集D，有K个类别。<code>经验熵</code>是 <span class="math display">\[H(D) = -\sum_{k=1}^K \frac{\vert C_k\vert}{\vert D\vert} \log_2 \frac{\vert C_k\vert}{\vert D\vert}\]</span> 特征A根据取值把数据集D划分为n个子集，则<strong>给定特征A时数据集D的</strong><code>经验条件熵</code>是： <span class="math display">\[H(D \mid A) = \sum_{i=1}^n \frac{\vert D_i\vert}{\vert D\vert} H(D_i)= -\sum_{i=1}^n \frac{\vert D_i\vert}{\vert D\vert} \sum_{k=1}^K \frac{\vert D_{ik}\vert}{\vert D_i\vert} \log_2 \frac{\vert D_{ik}\vert}{\vert D_i\vert}\]</span> <strong>信息增益</strong></p><p><code>信息增益</code>是给定特征A，使得数据集D<strong>不确定性减少的程度</strong>。<code>信息增益 = 划分前熵 - 划分后熵 = 熵 - 条件熵</code> <span class="math display">\[g(D, A) = H(D) - H(D \mid A)\]</span> 特征A的<strong>信息增益越大</strong>，不确定性减少越多，A的<strong>分类能力就越强</strong>。</p><p><strong>信息增益的问题</strong></p><p>对于取值很多的特征，比如连续型数据(时间)。每一个取值几乎都可以确定一个样本。即这个特征就可以划分所有的样本数据。</p><ul><li><p>信息增益不适合<code>连续型</code>、<code>取值多</code>的特征</p></li><li>使得所有分支下的样本集合都是<code>纯的</code>，极端情况每一个叶子节点都是一个样本</li><li><p><strong>数据更纯，信息增益更大</strong>，选择它作为根节点，结果就是<strong>庞大且深度很浅的树</strong></p></li></ul><p><strong>信息增益比</strong></p><p><code>数据集</code><span class="math inline">\(D\)</span>关于<code>特征</code>A的<code>熵</code>，<span class="math inline">\(n\)</span>是特征A的取值个数： <span class="math display">\[H_A(D) = -\sum_{i=1}^n \frac{\vert D_i\vert}{\vert D\vert} \log_2 \frac{\vert D_i\vert}{\vert D\vert}\]</span> <code>信息增益比 = 信息增益 / 划分前熵 = 信息增益 / D关于特征A的熵</code> ： <span class="math display">\[g_R(D, A) = \frac {g(D, A)}{H_A(D)} = \frac {H(D) - H(D \mid A)}{H_A(D)}\]</span> 解决信息增益的问题：特征A分的类别越多，<span class="math inline">\(D​\)</span>关于A的熵就越大，作为分母，所以信息增益<span class="math inline">\(g_R(D, A)​\)</span> 就越小。在信息增益的基础上增加了一个<code>分母惩罚项</code>。</p><p>信息增益比的问题：实际上偏好可取类别数目较少的特征。</p><h2 id="基尼指数">基尼指数</h2><p><code>CART</code>分类树使用基尼指数来选择最优特征。 <code>基尼指数</code>也是度量<code>不确定性</code>。 熵偏向<code>离散属性</code>， 基尼指数偏向<code>连续属性</code>。</p><p><strong>概率分布基尼指数</strong></p><p>分类中，有<span class="math inline">\(K\)</span>类。 样本属于第<span class="math inline">\(k\)</span>类的概率为<span class="math inline">\(p_k\)</span>。 <span class="math display">\[\rm{Gini}(p) = \sum_{k=1}^K p_k(1-p_k) = 1 - \sum_{k=1}^Kp_k^2\]</span> <strong>样本集合基尼指数</strong></p><p>集合D，有<span class="math inline">\(K\)</span>类，<span class="math inline">\(D_k\)</span> 是第k类的样本子集。则D的基尼指数 <span class="math display">\[\rm{Gini}(D) = 1 - \sum_{k=1}^K \left(\frac{\vert D_k\vert}{\vert D\vert} \right)^2\]</span> <strong>特征A条件基尼指数</strong></p><p>特征A取值为某一可能取值为<code>a</code>。 根据<code>A是否取值为a</code>把D划分为<span class="math inline">\(D_1\)</span>和<span class="math inline">\(D_2\)</span><code>两个集合</code>。</p><p>在特征A的条件下，D的基尼指数如下： <span class="math display">\[\rm{Gini}(D, A) = \frac{\vert D_1\vert}{\vert D\vert} \rm{Gini}(D_1) + \frac{\vert D_2\vert}{\vert D\vert} \rm{Gini}(D_2)\]</span> <span class="math inline">\(\rm{Gini}(D, A)​\)</span>是<strong>集合D根据特征A分割后，集合D的不确定性</strong>。</p><h2 id="id3算法">ID3算法</h2><p>决策树的生成，ID3算法以<code>信息增益最大</code>为标准选择特征。递归构建，不断选择最优特征对训练集进行划分。</p><p>递归终止条件：</p><ul><li>当前节点的所有样本，属于同一类别<span class="math inline">\(C_k\)</span>，无需划分。该节点为叶子节点，类标记为<span class="math inline">\(C_k\)</span></li><li>当前属性集为空，或所有样本在属性集上取值相同</li><li>当前节点的样本集合为空，没有样本</li></ul><p>在集合D中，选择信息增益最大的特征<span class="math inline">\(A_g\)</span> ：</p><ul><li><code>增益小于阈值</code>，则<code>不继续向下分裂，到达叶子节点</code>。该节点的标记为该节点所有样本中的<code>majority class</code><span class="math inline">\(C_k\)</span>。 这也是<code>预剪枝</code></li><li><code>增益大于阈值</code>，按照特征<span class="math inline">\(A_g\)</span>的每一个取值<span class="math inline">\(A_g=a_i\)</span>把D划分为各个子集<span class="math inline">\(D_i\)</span>，去掉特征<span class="math inline">\(A_g\)</span></li></ul><p>继续对每个内部节点进行递归划分。</p><h2 id="c4.5算法">C4.5算法</h2><p>C4.5是ID3的改进，C4.5以<code>信息增益率最大</code>为标准选择特征。</p><h2 id="id3c4.5决策树剪枝">ID3/C4.5决策树剪枝</h2><p><code>决策树的生成</code>，会过多地考虑如何提高对训练数据的分类，从而<code>构建出非常复杂的决策树</code>。就容易<code>过拟合</code>。</p><p>剪枝就是裁掉一些子树和叶节点，并将其根节点或父节点作为叶节点。剪枝分为预剪枝和后剪枝。</p><p><strong>预剪枝</strong></p><p><code>在生成树的时候</code>，设定信息增益的<code>阈值</code>，如果某节点的<code>某特征的信息增益小于该阈值</code>，则<code>不继续分裂</code>，<code>直接设为叶节点</code>。选择该节点的D中类别<code>数量最多的类别</code> （<code>majority class</code>）作为<code>类别标记</code>。</p><p><strong>后剪枝</strong></p><p>树构建好以后，基于整体，<code>极小化损失函数</code>，自下而上地进行剪枝。</p><p>树T的参数表示</p><ul><li>叶节点的个数<span class="math inline">\(\vert T \vert\)</span></li><li>叶节点<span class="math inline">\(t\)</span></li><li>叶节点<span class="math inline">\(t\)</span>上有<span class="math inline">\(N_t\)</span>个样本</li><li>有<span class="math inline">\(K\)</span>类</li><li>叶节点t上的经验熵<span class="math inline">\(H_t(T)\)</span></li><li><span class="math inline">\(\alpha \ge 0\)</span> 为惩罚系数</li></ul><p><code>叶节点t</code>上的<code>经验熵</code> <span class="math display">\[H_t(T) = -\sum_{k=1}^K \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}\]</span> 模型<code>对训练数据的拟合程度</code><span class="math inline">\(C(T)\)</span> ，<code>所有叶节点的经验熵和</code>： <span class="math display">\[C(T) = \sum_{t=1}^{\vert T \vert} N_tH_t(T)\]</span> 最终<code>损失函数 = 拟合程度 + 惩罚因子</code>： <span class="math display">\[C_\alpha(T) = C(T) + \alpha \vert T\vert\]</span> 参数<span class="math inline">\(\alpha\)</span>权衡了训练数据的拟合程度和模型复杂度。</p><ul><li><span class="math inline">\(\alpha\)</span>大，决策树简单，拟合不好</li><li><span class="math inline">\(\alpha\)</span>小，决策树复杂，过拟合</li></ul><p><strong>剪枝步骤</strong></p><ol style="list-style-type: decimal"><li>计算每个节点的经验熵</li><li>递归从树的叶节点向上回缩。叶节点回缩到父节点：整体树：回缩前<span class="math inline">\(T_1\)</span> ，回缩后<span class="math inline">\(T_2\)</span></li></ol><ul><li><span class="math inline">\(C_\alpha(T_2) \le C_\alpha(T_1)\)</span>， 则<code>回缩到父节点</code>， <code>父节点变成新的叶节点</code>。</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/tree/cut-tree.jpeg" style="display:block; margin:auto" width="35%"></p><h2 id="cart-回归树">CART-回归树</h2><p><code>Classification and regression tree</code>分类与回归树。</p><ul><li>回归-平方误差最小</li><li>分类-基尼指数最小</li><li><code>二叉树</code></li><li>内部节点：是 - 否。如特征$A a $或 <span class="math inline">\(A &gt; a\)</span></li></ul><p><strong>模型</strong></p><p>把输入空间划分为<code>M个单元</code><span class="math inline">\(R_1,R_2,\cdots, R_M\)</span>， 每个单元有<code>多个样本</code>，有一个<code>固定的输出值</code><span class="math inline">\(c_m\)</span>。 <span class="math display">\[\hat c_m = \rm{avg} (y_i), \; y_i \in R_m\]</span> <code>树模型</code> ： <span class="math display">\[f(x) = \sum_{m=1}^M c_m I(x \in R_m)\]</span> <strong>划分单元</strong></p><p>寻找<code>最优切分变量j</code>和<code>最优切分点s</code> 。</p><p>选择第<span class="math inline">\(j\)</span>个变量<span class="math inline">\(x^{(j)}\)</span>和其取值<span class="math inline">\(s\)</span>， 作为<code>切分变量</code>和<code>切分点</code>，划分为<code>两个空间</code> <span class="math inline">\(R_1, R_2\)</span>，输出分别为<strong><span class="math inline">\(c_1, c_2\)</span></strong> : <span class="math display">\[R_1(j, s) = \{x \mid x^{(j)} \le s \}, \quad \quad R_2(j, s) = \{x \mid x^{(j)} &gt; s \}\]</span> 求<code>最优</code>，<code>平方误差最小</code> ： <span class="math display">\[\min_\limits{j, s} \left[\min_\limits{c_1} \sum_{x_i \in R_1(j, s)} (y_i - c_1)^2 +\min_\limits{c_2} \sum_{x_i \in R_1(j, s)} (y_i - c_1)^2 \right]\]</span> 对每个区域重复划分过程，直到停止。也叫作<code>最小二乘回归树</code>。</p><h2 id="cart-分类树">CART-分类树</h2><p><code>基尼指数最小原则</code> 。</p><p>对每一个数据集D，对每一个特征A，对每一个A的取值<span class="math inline">\(A=a\)</span> 是或者否，划分两个自己<span class="math inline">\(D_1\)</span>和<span class="math inline">\(D_2\)</span></p><ul><li>计算在特征<span class="math inline">\(A=a\)</span>条件下的基尼指数<span class="math inline">\(\color{blue} {\rm{Gini}(D, A=a)}\)</span></li><li>选择<code>基尼指数最小</code>特征A及其取值a，作为<code>最优特征</code>和<code>最优切分点</code></li><li>从现节点<code>划分为两个子节点</code></li></ul><h2 id="cart剪枝">CART剪枝</h2><p><strong>剪枝总体步骤</strong></p><ul><li><p>从生成的决策树<span class="math inline">\(T_0\)</span>开始， 从底端向上开始剪枝，直到<span class="math inline">\(T_0\)</span>的根节点。<code>损失函数决定是否剪枝</code></p></li><li>形成子树序列<span class="math inline">\(\{T_0, T_1, \cdots, T_n\}\)</span></li><li><p><code>交叉验证</code>子树序列，选择最优子树</p></li></ul><p><strong>K-折交叉验证法</strong></p><p>数据集划分为K个子集。每个子集分别做一次验证集，其余K-1组作为训练集。得到K个模型。</p><p><strong>剪枝损失函数</strong> <span class="math display">\[C_\alpha(T) = C(T) + \alpha \vert T\vert\]</span> <span class="math inline">\(C(T)\)</span>为<code>所有叶节点的经验熵和</code> ： <span class="math display">\[C(T) = \sum_{t=1}^{\vert T \vert} N_tH_t(T)\]</span> <span class="math inline">\(\alpha\)</span>权衡<code>训练数据拟合程度</code>和<code>模型复杂度</code>。</p><p>整体树<span class="math inline">\(T_0\)</span>的任意<code>内部节点t</code>， <span class="math inline">\(\alpha\)</span>从0开始，每次一个小区间<span class="math inline">\([\alpha_i, \alpha_{i+1})\)</span> ：</p><ul><li><code>t为单节点树时损失</code>：<span class="math inline">\(C\alpha(t) = C(t) + \alpha\)</span></li><li><code>t为根节点子树时损失</code>：<span class="math inline">\(C_\alpha(T_t) = C(T_t) + \alpha \vert T_t\vert\)</span></li><li><span class="math inline">\(\alpha=0\)</span>时， <span class="math inline">\(C\alpha(t) &lt; C_\alpha(T_t)\)</span> 。因为，树大，精确，损失小。</li><li>随着<span class="math inline">\(\alpha\)</span>的增大，会达到： <span class="math inline">\(C\alpha(t) = C_\alpha(T_t)\)</span></li></ul><p>求得<code>临界点</code><span class="math inline">\(\alpha​\)</span> <span class="math display">\[\alpha = \frac{C(T) - C(T_t)} {\vert T_t\vert - 1}\]</span> 对每个内部节点求： <span class="math display">\[g(t) = \frac{C(T) - C(T_t)} {\vert T_t\vert - 1}\]</span></p><ul><li>在<span class="math inline">\(T_0\)</span>中减去<code>最小的</code><span class="math inline">\(g(t)\)</span><code>对应的子树</code><span class="math inline">\(T_t\)</span> ， 作为<span class="math inline">\(T_1\)</span></li><li>t节点作为叶子节点，类标记为<code>majority class</code></li><li>最后再交叉验证所有的子树序列即可</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;决策树的特征选择、生成、剪枝。熵、信息增益、基尼指数。ID3、C4.5、CART。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;决策树背景&quot;&gt;决策树背景&lt;/h1&gt;
&lt;h2 id=&quot;概览&quot;&gt;概览&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;树的
      
    
    </summary>
    
      <category term="机器学习" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="决策树" scheme="http://plmsmile.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
      <category term="ID3" scheme="http://plmsmile.github.io/tags/ID3/"/>
    
      <category term="C4.5" scheme="http://plmsmile.github.io/tags/C4-5/"/>
    
      <category term="CART" scheme="http://plmsmile.github.io/tags/CART/"/>
    
      <category term="熵" scheme="http://plmsmile.github.io/tags/%E7%86%B5/"/>
    
      <category term="信息增益" scheme="http://plmsmile.github.io/tags/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A/"/>
    
      <category term="基尼指数" scheme="http://plmsmile.github.io/tags/%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习知识点汇总整理</title>
    <link href="http://plmsmile.github.io/2018/03/03/28-ml-interview-notes/"/>
    <id>http://plmsmile.github.io/2018/03/03/28-ml-interview-notes/</id>
    <published>2018-03-03T13:32:52.000Z</published>
    <updated>2018-03-05T01:39:41.174Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="tensorflow">Tensorflow</h1><p><strong>1 Tensorflow的计算图</strong></p><p>Tensorflow通过<code>计算图</code>的形式来表示计算。是一个有向图。<strong>节点</strong>代表一个<code>计算</code>，<strong>边</strong>代表计算之间的<code>依赖关系</code>。</p><ul><li>构建计算图</li><li>执行计算图，<code>session.run</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x_data = np.float32(np.random.rand(<span class="number">2</span>,<span class="number">100</span>))</span><br><span class="line">y_data = np.dot([<span class="number">0.1</span> , <span class="number">0.2</span>] , x_data) + <span class="number">0.3</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>,<span class="number">2</span>],<span class="number">-1.0</span>,<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">y = tf.matmul(W,x_data) + b</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure><p><img src="http://otafnwsmg.bkt.clouddn.com/image/itv/tf/flow-graph.jpg" style="display:block; margin:auto" width="30%"></p><h1 id="knn">KNN</h1><p>看中间的绿色属于哪一类？</p><ul><li>和谁近就属于谁</li><li>看k个</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/it/knn/knn.png" style="display:block; margin:auto" width="30%"></p><p><strong>1 欧式距离和曼哈顿距离</strong></p><p><code>欧式距离</code>是两点之间的距离 <span class="math display">\[d = \sqrt {(x_1-x_2)^2 +(y_1-y_2)^2  }\]</span> <code>曼哈顿距离</code>也称作<code>城市街区距离</code>，两个十字路口的实际要走的距离 <span class="math display">\[d = \vert x_1 - x_2\vert +  \vert y_1 -y_2 \vert\]</span> <strong>2 K值的选择</strong></p><ul><li>k较小：用较小范围进行预测。容易过拟合，模型复杂。</li><li>k较大：用较大范围进行预测。较远不相似的也会起作用，会发生错误。模型简单。</li><li>k=N：完全不可取。每个都是属于最多样本的类别</li><li>一般选择比较小的数值。如采用<code>交叉验证法</code>来选择最优的k值。 （一部分训练，一部分测试）</li></ul><h1 id="logistic-regression">Logistic Regression</h1><p><a href="https://plmsmile.github.io/2017/08/20/ml-ng-notes/#逻辑回归">我的LR笔记</a></p><table><thead><tr class="header"><th align="left">问题列表</th></tr></thead><tbody><tr class="odd"><td align="left">1 简介LR</td></tr><tr class="even"><td align="left">2 LR与SVM的区别和联系</td></tr><tr class="odd"><td align="left">3 LR与线性回归的区别和联系</td></tr></tbody></table><p><strong>1 简介LR</strong></p><p>问一个女生喜欢你吗，SVM会告诉你喜欢或者不喜欢。很粗暴。</p><p>LR则会告诉你，有多喜欢你，多不喜欢你。就是告诉你一个<code>可能性</code>。多喜欢你是取决于她的看重点权值和你身上有的东西x。 <span class="math display">\[p(y=1\mid x;\theta)= \sigma(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}} \quad\quad  = h_\theta(x)\]</span></p><p><span class="math display">\[p(y=1) = h(x), \quad p(y=0) = 1-h(x)\]</span></p><p>使用<code>极大似然法</code>进行参数估计：<strong>样本恰好使联合概率密度（似然函数）取得最大值</strong>。<strong>概率密度相乘</strong>。</p><p><code>对数似然函数</code>： <span class="math display">\[\begin{align}L(\theta) &amp; = \log \prod_{i=1}^N [h(x)^{y_i} ][1-h(x)]^{1-y_i} \\&amp; = \sum_{i=1}^N \left( y_i \log h(x) + (1-y_i)\log(1-h(x)) \right) \\&amp; = \sum_{i=1}^N (y_i \theta^Tx_i - \log(1+e^{\theta^Tx}))\end{align}\]</span> 使<span class="math inline">\(L(\theta)\)</span>最大，取负数就是其<code>损失函数</code> ： <span class="math display">\[J(\theta) = -\frac{1}{m} L(\theta) = -\frac{1}{m} \sum_{i=1}^N \left( y_i \log h(x) + (1-y_i)\log(1-h(x)) \right)\]</span> 令导数为0，发现无法解析求解。 <span class="math display">\[\begin{align}\frac{\partial L(\theta)}{\partial \theta} &amp; =\sum_{i=1}^N y_i x_i  - \sum_{i=1}^N \frac{e^{\theta^Tx}}{1 + e^{\theta^Tx}} x_i \\&amp; = \sum_{i=1}^N x_i (y_i - \sigma(\theta^Tx) )\end{align}\]</span> 只能借助迭代法，如<strong>梯度下降法和拟牛顿法来进行求解</strong>。</p><p><strong>2 LR和SVM的比较</strong></p><p><strong>相同点</strong></p><ul><li>都是分类算法，都是监督学习算法</li><li>如果不考虑核函数，LR和SVM都是<code>线性分类算法</code>，决策面都是线性的</li><li>LR和SVM都是<a href="https://plmsmile.github.io/2017/08/04/pgm-01/#产生式和判别式">判别式模型</a>。 不关心数据怎么生成，只关心数据之间的差别。用差别来进行分类。</li></ul><p>判别模型： <code>KNN</code>、 <code>LR</code> 、<code>SVM</code> 。 生成式模型：<code>朴素贝叶斯</code>、 <code>HMM</code> 。</p><p><strong>不同点</strong></p><ul><li>损失函数不同 <span class="math display">\[  J(\theta)  = -\frac{1}{m} \sum_{i=1}^N \left( y_i \log h(x) + (1-y_i)\log(1-h(x)) \right)  \]</span></li></ul><p><span class="math display">\[  L(w, b, \lambda) =\frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i \left(y_i(w^Tx_i+b) - 1\right)  \]</span></p><p>LR基于<code>概率理论</code>，用<code>sigmoid</code>函数表示，通过<code>极大似然法</code>估计出参数的值。</p><p>SVM基于<code>间隔最大化原理</code>，认为最大几何间隔的分类面为最优分类面。</p><ul><li>SVM只在乎边界线附近的点(SV)，LR在乎所有的点。</li><li>SVM不直接依赖于数据分布；LR受所有点影响，如果不同类别不平衡，要对数据做<code>balancing</code></li><li>处理非线性问题时，SVM使用核函数，LR不使用核函数</li><li>SVM只有少数SV进行核计算，LR如果用核函数，则所有的点都会进行计算，代价太高</li><li>线性SVM依赖数据的距离测度，要对数据做归一化，而LR不用</li><li>SVM损失函数自带L2正则项，而LR需要额外添加</li></ul><p><strong>3 LR与线性回归的联系</strong></p><p><strong>联系</strong></p><p>LR本质上是一个线性核回归模型</p><p><strong>区别</strong></p><ul><li>目标函数：线性回归最小二乘，LR是似然函数</li><li>线性回归整个实数范围内进行预测；LR预测值限定为<code>[0,1]</code> ，<code>sigmoid</code>的非线性形式。轻松处理<code>0/1分类问题</code></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;1 Tensorflow的计算图&lt;/strong
      
    
    </summary>
    
      <category term="机器学习" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Aim2offer4(51-64)</title>
    <link href="http://plmsmile.github.io/2018/03/02/aim2offer4/"/>
    <id>http://plmsmile.github.io/2018/03/02/aim2offer4/</id>
    <published>2018-03-02T13:34:02.000Z</published>
    <updated>2018-03-07T07:43:22.924Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数组中的逆序对-51">数组中的逆序对-51</h1><p><a href="https://www.nowcoder.com/practice/96bd6684e04a44eb80e6a68efc0ec6c5?tpId=13&amp;tqId=11188&amp;rp=2&amp;ru=%2Fta%2Fcoding-interviews&amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking" target="_blank" rel="noopener">牛客网数组中的逆序对</a></p><blockquote><p>逆序对，前面&gt;后面。给一个数组，求出所有逆序对的个数。如<code>{7,5,6,4}</code>， 有75-76-74-54-64这5对。</p></blockquote><p>使用<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#冒泡排序">冒泡排序</a>思想，每一次交换，就说明有一个逆序对，统计交换次数。</p><p>利用<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#归并排序">归并排序</a>思想，</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数组中的逆序对-51&quot;&gt;数组中的逆序对-51&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://www.nowcoder.com/practice/96bd6684e04a44eb80e6a68efc0ec6c5?tpId=13&amp;amp;tqId=11188&amp;a
      
    
    </summary>
    
      <category term="leetcode" scheme="http://plmsmile.github.io/categories/leetcode/"/>
    
    
      <category term="leetcode" scheme="http://plmsmile.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>SVM笔记</title>
    <link href="http://plmsmile.github.io/2018/03/01/27-svm-notes/"/>
    <id>http://plmsmile.github.io/2018/03/01/27-svm-notes/</id>
    <published>2018-03-01T12:42:20.000Z</published>
    <updated>2018-03-03T13:22:59.772Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Support Vector Machine简单笔记。 特征空间上的间隔最大的线性分类器。学习策略是间隔最大化，转化为一个凸二次规划问题的求解。</p></blockquote><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="svm概览">SVM概览</h1><h2 id="线性分类器">线性分类器</h2><p><a href="https://plmsmile.github.io/2017/08/20/ml-ng-notes/#逻辑回归">逻辑回归</a>的图像和公式如下，预测的分类为1的概率。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/lr/sigmoid%E5%87%BD%E6%95%B0.png" style="display:block; margin:auto" width="50%"> <span class="math display">\[h_\theta(x) = g(\theta^Tx), \quad g(z) = \frac{1}{1+e^{-z}},\quadg(z) = \begin{cases}1, &amp; z\ge 0 \\-1, &amp; z &lt; 0 \\\end{cases}\]</span></p><p><span class="math display">\[y = \begin{cases}    1, \;  &amp; h_\theta(x) \ge 0.5, \;即\; \theta^Tx \ge 0\\    0, \; &amp; h_\theta(x) &lt; 0.5,  \; 即 \; \theta^Tx &lt; 0 \\\end{cases}\]</span></p><p>其中<span class="math inline">\(\theta^Tx=w^Tx+b=0\)</span> 是一个<code>超平面</code>。 用<code>分类函数</code>表示<span class="math inline">\(f(x)=w^Tx+b\)</span> 。 <span class="math inline">\(w\)</span>是这个超平面的<strong>法向量</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/linear-desicion" style="display:block; margin:auto" width="35%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/linear-desicion2" style="display:block; margin:auto" width="60%"></p><p>即对于任意一个x，有如下<strong>预测类别</strong>： <span class="math display">\[\hat y=\begin{cases}1, &amp; f(x) \ge 0\\-1, &amp; f(x) &lt; 0 \\\end{cases}\]</span></p><h2 id="函数间隔与几何间隔">函数间隔与几何间隔</h2><p><strong>函数间隔</strong></p><p>超平面<span class="math inline">\(w^Tx+b=0\)</span>确定后， <span class="math inline">\(\vert w\cdot x+b\vert\)</span>表示点x到平面的<code>距离</code>，表示分类<strong>可靠性</strong>。<strong>距离越远，分类越可信</strong>。<span class="math inline">\(y\)</span>与<span class="math inline">\(w\cdot x+b\)</span>的<code>符号的一致性</code>表示分类的<strong>正确性</strong>。</p><p>超平面<span class="math inline">\((w,b)\)</span>关于样本点<span class="math inline">\((x_i, y_i)\)</span>的<strong>函数间隔<span class="math inline">\(\hat \gamma_i\)</span></strong>如下： <span class="math display">\[\hat \gamma_i = y_i (w^T \cdot x_i + b)\]</span> 超平面关于所有样本点的函数间隔<span class="math inline">\(\hat \gamma​\)</span> ： <span class="math display">\[\hat \gamma = \min \hat \gamma_i\]</span> 函数间隔的<strong>问题</strong>：w和b成比例改变，超平面未变，但函数间隔已变。</p><p><strong>几何间隔</strong></p><p>对函数间隔除以法向量的<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#范数">二范数</a>，则得到超平面与点<span class="math inline">\((x_i,y_i)\)</span>的<strong>几何间隔<span class="math inline">\(\gamma_i\)</span></strong> ： <span class="math display">\[\gamma_i = \frac{\hat \gamma_i}{\|w\|} = \frac{y_i(w^T\cdot x_i + b)}{\|w\|}\]</span> 超平面关于所有样本点的几何间隔： <span class="math display">\[\gamma = \min \gamma_i\]</span> <code>几何间隔</code>才是直观上<strong>点到超平面的距离</strong>。</p><h2 id="最大间隔分类器">最大间隔分类器</h2><p>分类时，超平面离数据点的<strong>间隔越大</strong>，<strong>分类的确信度也越大</strong>。 所以要<strong>最大化这个几何间隔</strong>，目标函数如下： <span class="math display">\[L = \max_\limits{w, b} \gamma, \quad s.t,\quad \gamma_i \ge \gamma\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/margin-1" style="display:block; margin:auto" width="40%"></p><p>用函数间隔<span class="math inline">\(\hat \gamma\)</span>描写为： <span class="math display">\[L = \max_\limits{w, b} \frac{\hat \gamma}{\|w\|}, \quad s.t, \quad \hat \gamma_i \ge \hat \gamma, \; \text{ 其中 }\hat \gamma_i = y_i(w^T \cdot x_i + b)\]</span> <strong>函数间隔</strong><span class="math inline">\(\hat \gamma​\)</span>的取值并<strong>不会影响最优化问题的解</strong>。 <span class="math inline">\(\lambda w, \lambda b \to \lambda \hat \gamma​\)</span></p><p><strong>目标函数</strong></p><p><strong>取函数间隔为1</strong>，<span class="math inline">\(\hat \gamma = 1\)</span>， 则有<strong>目标函数</strong>： <span class="math display">\[L = \max_\limits{w,b} \frac{1}{\|w\|}, \quad s.t, \quad y_i(w^Tx_i+b) \ge 1\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/margin-2" style="display:block; margin:auto" width="50%"></p><p><code>支持向量</code>是虚线边界上的点，则有： <span class="math display">\[\begin{cases}y_i(w^Tx_i+b)=1, &amp; 支持向量 \\y_i(w^Tx_i+b) &gt;1, &amp; 其他点 \\\end{cases}\]</span></p><p>分类 <span class="math display">\[\hat y=\begin{cases}1, &amp; f(x) \ge 0\\-1, &amp; f(x) &lt; 0 \\\end{cases}\]</span></p><h1 id="线性svm">线性SVM</h1><h2 id="拉格朗日对偶性">拉格朗日对偶性</h2><p><strong>1 原始问题</strong></p><p><span class="math inline">\(f(x), c_i(x), h_j(x)\)</span>都连续可微。</p><p>最优化： <span class="math display">\[\min_\limits{x\in R} f(x)\]</span> 有很多个约束条件（不等式约束和等式约束）： <span class="math display">\[c_i(x) \le 0 ,\quad h_j(x) = 0\]</span> <strong>求解原始问题</strong></p><p>将约束问题无约束化。</p><p>引入<strong>拉格朗日函数</strong>，其中<span class="math inline">\(\alpha_i (\ge 0)\)</span>和<span class="math inline">\(\beta_j\)</span>是<strong>拉格朗日乘子</strong><br><span class="math display">\[L(x, \alpha, \beta) = f(x) + \sum\alpha_ic_i(x) + \sum \beta_j h_j(x)\]</span> 定义关于<span class="math inline">\(x\)</span>的函数<strong><span class="math inline">\(\theta_p(x)\)</span></strong>： <span class="math display">\[\theta_p(x) = \max_\limits{\alpha,\beta:\alpha_i\ge0} L(x, \alpha, \beta) \]</span></p><p><span class="math display">\[\theta_p(x) = \begin{cases}f(x), &amp;x满足约束 \\+\infty, &amp; 其他 \\\end{cases}\]</span></p><p><span class="math inline">\(f(x)\)</span>求最小，则对<span class="math inline">\(\theta_p(x)\)</span>求最小。</p><p>原始问题： <strong>先固定x，优化出参数<span class="math inline">\(\alpha, \beta\)</span>，再优化x</strong>。 <span class="math display">\[\min_\limits{x} \; \theta_p(x) =  \min_\limits{x} \max_\limits{\alpha, \beta:\alpha_i\ge0} L(x, \alpha, \beta)\]</span> 所以<strong>原始最优化问题</strong> 变为 拉格朗日函数的<strong>极小极大问题</strong>。</p><p>定义原始问题的最优解<span class="math inline">\(p^*\)</span> ： <span class="math display">\[p^* = \min_\limits{x} \theta_p(x)\]</span> <strong>2 对偶问题</strong></p><p>定义关于<span class="math inline">\(\alpha, \beta\)</span>的函数<span class="math inline">\(\theta_d(\alpha, \beta)\)</span> <span class="math display">\[\theta_d(\alpha, \beta) = \min_x L(x, \alpha, \beta)\]</span> 对偶问题：<strong>先固定参数<span class="math inline">\(\alpha, \beta\)</span> ，优化出x，再优化出参数</strong>。 <strong>先优化x</strong>。 <span class="math display">\[\max_\limits{\alpha, \beta:\alpha_i\ge0} \theta_d(\alpha, \beta) = \max_\limits{\alpha, \beta:\alpha_i\ge0} \min_x L(x, \alpha, \beta)\]</span> 原始问题： <strong>先固定x，优化出参数<span class="math inline">\(\alpha, \beta\)</span>，再优化x</strong>。先优化参数。 <span class="math display">\[\min_\limits{x} \; \theta_p(x) =  \min_\limits{x} \max_\limits{\alpha, \beta:\alpha_i\ge0} L(x, \alpha, \beta)\]</span> 定义对偶问题的最优值： <span class="math display">\[d^* = \max_\limits{\alpha, \beta:\alpha_i\ge0} \theta_d(\alpha, \beta)\]</span></p><p><strong>3 原始问题与对偶问题的关系</strong></p><p>因为： <span class="math display">\[\theta_d(\alpha, \beta) = \min_x L(x, \alpha, \beta) \le \max_\limits{\alpha,\beta:\alpha_i\ge0} L(x, \alpha, \beta) = \theta_p(x)\]</span> 定理1：如果原始问题与对偶问题均有最优值，则有：<span class="math inline">\(d^* \le p^*\)</span> <span class="math display">\[d^* = \max_\limits{\alpha, \beta:\alpha_i\ge0} \min_x L(x, \alpha, \beta)  \le \min_\limits{x} \max_\limits{\alpha, \beta:\alpha_i\ge0} L(x, \alpha, \beta) = p^*\]</span> 推论1：如果<span class="math inline">\(d^* = p^*\)</span>， 那么<span class="math inline">\(x^*, \alpha^*, \beta^*\)</span>分别是原始问题和对偶问题的最优解。</p><p>通过对偶问题，来解决原始问题。</p><p><strong>4 KKT条件</strong></p><p>满足什么条件，才能使<span class="math inline">\(d^* = p^*\)</span>呢 ？</p><p>首先满足下面的大条件：</p><blockquote><p>假设<span class="math inline">\(f(x)\)</span>和<span class="math inline">\(c_i(x)\)</span>都是<a href="https://plmsmile.github.io/2017/08/13/em/#em算法">凸函数</a>， <span class="math inline">\(h_j(x)\)</span>是仿射函数；假设不等式约束<span class="math inline">\(c_i(x)\)</span>是严格可行的。</p></blockquote><p>定理2：则存在解，<span class="math inline">\(x^*\)</span>是原始问题的最优解，<span class="math inline">\(\alpha^*, \beta^*\)</span>是对偶问题的最优解。 并且： <span class="math display">\[d^* = p^* = L(x^*, \alpha^*, \beta^*)\]</span> KKT条件：则<span class="math inline">\(x^*\)</span>是原始问题、<span class="math inline">\(\alpha^*, \beta^*\)</span>是对偶问题的最优解的<code>充分必要条件</code>是<strong><span class="math inline">\(x^*, \alpha^*, \beta^*\)</span>满足下面的KKT条件</strong>： <span class="math display">\[\begin{align}&amp; 偏导为0条件\\&amp; \nabla_x L(x^*, \alpha^*, \beta^*)  = 0 \\&amp; \nabla_\alpha L(x^*, \alpha^*, \beta^*)  = 0 \\&amp; \nabla_\beta L(x^*, \alpha^*, \beta^*)  = 0 \\&amp; 约束条件 \\&amp; c_i(x^*) \le 0 \\&amp; h_j(x^*) = 0 \\&amp; \alpha_i^* \ge 0 \\&amp; \rm{KKT}对偶互补条件 \\&amp; \alpha_i^* c_i(x^*) = 0 \\\end{align}\]</span> 由<strong>KKT对偶互补条件</strong>可知，若<span class="math inline">\(\alpha_i^* &gt;0\)</span>， 则<span class="math inline">\(c_i(x^*)=0\)</span> 。SVM推导会用到。</p><p>若<span class="math inline">\(\alpha_i&gt;0\)</span>， <strong>则对应的<span class="math inline">\(x_i\)</span>是支持向量</strong>， 有<span class="math inline">\(y_i(w^*\cdot x+ b^*) = 1\)</span>。 所有的非支持向量，都有<span class="math inline">\(\alpha_i =0\)</span>。</p><h2 id="原始问题到对偶问题">原始问题到对偶问题</h2><p>先前的目标函数： <span class="math display">\[J = \max_\limits{w,b} \frac{1}{\|w\|}, \quad s.t, \quad y_i(w^Tx_i+b) \ge 1\]</span> 最大变为最小，则有<code>原始问题</code>如下。目标函数是二次的，约束条件是线性的。所以是个<code>凸二次规划问题</code>。 <span class="math display">\[J = \min_{w,b} \frac{1}{2} \|w\|^2, \quad s.t, \quad y_i(w^Tx_i+b) \ge 1\]</span> 构造<strong>拉格朗日函数</strong> ： <span class="math display">\[L(w, b, \lambda) =\frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i \left(y_i(w^Tx_i+b) - 1\right)\]</span> <strong>原始问题</strong> <span class="math display">\[\theta_p(w,b) = \max_{\lambda_i \ge 0} L(w, b, \alpha)\]</span></p><p><span class="math display">\[p^* = \min_{w, b} \theta_p(w, b) =  \min_{w, b} \max_{\lambda_i \ge 0} L(w, b, \alpha)\]</span></p><p><strong>对偶问题</strong> <span class="math display">\[\theta_d(\alpha) = \min_{w,b} L(w, b, \alpha)\]</span></p><p><span class="math display">\[d^* = \max_{\alpha_i \ge 0} \theta_d(\alpha) =  \max_{\alpha_i \ge 0} \min_{w,b} L(w, b, \alpha)\]</span></p><p>我们知道<span class="math inline">\(d^* \le p^*​\)</span>， 有时相等。原始问题可以转化为对偶问题求解，好处是：<strong>近似解</strong>，<strong>好求解</strong>。</p><h2 id="求解对偶问题">求解对偶问题</h2><p><strong>拉格朗日函数</strong>： <span class="math display">\[L(w, b, \lambda) =\frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i \left(y_i(w^Tx_i+b) - 1\right)\]</span> 化简后： <span class="math display">\[L(w, b, \lambda) =\frac{1}{2} \|w\|^2 -\sum_{i=1}^n\alpha_iy_iw^Tx_i- \sum_{i=1}^n\alpha_iy_ib + \sum_{i=1}^n\alpha_i\]</span> <strong>目标函数</strong>： <span class="math display">\[d^* = \max_{\alpha_i \ge 0} \theta_d(\alpha) =  \max_\limits{\alpha_i \ge 0} \min_\limits{w,b} L(w, b, \alpha)\]</span> 主要是三个步骤：</p><ul><li>固定参数<span class="math inline">\(\alpha\)</span>， 求 极小化<span class="math inline">\(\min_{w,b} L(w, b, \alpha)\)</span>的w和b<br></li><li>带入w和b，对<span class="math inline">\(L\)</span>求参数<span class="math inline">\(\alpha\)</span> 的极大化</li><li>利用SMO算法求解对偶问题中的拉格朗日乘子<span class="math inline">\(\alpha\)</span></li></ul><p><strong>1 极小求出w和b <span class="math inline">\(\min_\limits{w,b} L(w, b, \alpha)\)</span> </strong></p><p>对w和b求偏导，使其等于0。 <span class="math display">\[\frac{\partial L}{\partial w} = w -\sum_{i=1}^n\alpha_iy_ix_i \begin{equation}\xlongequal {令}{} 0  \end{equation}\quad \to \quad w = \sum_{i=1}^n\alpha_iy_ix_i\]</span></p><p><span class="math display">\[\frac{\partial L}{\partial b} = - \sum_{i=1}^n \alpha_iy_i \xlongequal {令}{} 0  \quad \to \quad  \sum_{i=1}^n \alpha_iy_i=0\]</span></p><p>特别地范式求导：<span class="math inline">\(\frac{\partial \|w\|^2}{\partial w} = 2w​\)</span> <span class="math display">\[\frac{\partial \|w\|^2}{\partial w} = w\]</span> <strong>把上面两个结论，带入原式进行化简</strong>，得到： <span class="math display">\[\begin{align}L(w, b, \alpha) &amp;=\frac{1}{2}w^Tw -  \sum_{i=1}^n\alpha_iy_iw^Tx_i- \sum_{i=1}^n\alpha_iy_ib + \sum_{i=1}^n\alpha_i \\&amp; = \frac{1}{2} w^T\sum_{i=1}^n\alpha_iy_ix_i - w^T \sum_{i=1}^n\alpha_iy_ix_i - b\sum_{i=1}^n\alpha_iy_i + \sum_{i=1}^n\alpha_i   \quad\text{(带入w，提出b，带入0)}\\&amp; = -\frac{1}{2}\left(\sum_{i=1}^n\alpha_iy_ix_i\right)^T\left(\sum_{i=1}^n\alpha_iy_ix_i\right) + \sum_{i=1}^n\alpha_i \quad{(只有x是向量，直接转置)}\\&amp;= -\frac{1}{2}\left(\sum_{i=1}^n\alpha_iy_ix_i^T\right)\left(\sum_{i=1}^n\alpha_iy_ix_i\right) + \sum_{i=1}^n\alpha_i \\&amp; = \sum_{i=1}^n\alpha_i -\frac{1}{2}\sum_{i=1}^n\sum_{i=1}^n\alpha_i \alpha_j y_i y_j x_i^T x_j\end{align}\]</span> 得到<strong>只用<span class="math inline">\(\alpha\)</span>表示的拉格朗日函数</strong>： <span class="math display">\[L(w, b, \alpha) =\sum_{i=1}^n\alpha_i -\frac{1}{2}\sum_{i=1}^n\sum_{i=1}^n\alpha_i \alpha_j y_i y_j x_i^T x_j\]</span> <strong>2 求出对<span class="math inline">\(\alpha​\)</span>的极大 <span class="math inline">\(\max_{\alpha_i \ge 0} \theta_d(\alpha) = \max_\limits{\alpha_i \ge 0} \min_\limits{w,b} L(w, b, \alpha)​\)</span></strong></p><p><strong>对偶问题</strong> 如下：</p><p>目标函数： <span class="math display">\[\max_\limits{\alpha} \; \sum_{i=1}^n\alpha_i -\frac{1}{2}\sum_{i=1}^n\sum_{i=1}^n\alpha_i \alpha_j y_i y_j x_i^T x_j\]</span> 约束条件： <span class="math display">\[\begin{align} &amp;\alpha_i \ge 0 \\&amp; \sum_{i=1}^n \alpha_iy_i = 0\\\end{align}\]</span> 利用<code>SMO算法</code>求出拉格朗日乘子<span class="math inline">\(\alpha^*\)</span>。</p><p><strong>3 求出w和b，得到分离超平面和决策函数</strong></p><p>根据前面的公式得到<strong><span class="math inline">\(w^*\)</span></strong>： <span class="math display">\[w* =\sum_{i=1}^n\alpha_i^*y_ix_i\]</span> 选一个<strong><span class="math inline">\(\alpha^*_j &gt; 0\)</span>对应的点</strong><span class="math inline">\((x_j, y_j)\)</span> 就是<strong>支持向量</strong>。由于支持向量<strong><span class="math inline">\(y_j(w^*\cdot x+ b^*) -1 = 0\)</span></strong> ，<span class="math inline">\(y_j^2=1\)</span></p><p>得到<strong><span class="math inline">\(b^*\)</span></strong> ： <span class="math display">\[b^*  = y_j - \sum_{i=1}^n\alpha_i^*y_i(x_i\cdot x_j), \quad\quad \text{($x_i\cdot x_j$是向量内积，后面同理)}\]</span> 通过公式可以看出，<strong>决定w和b的是支持向量</strong>， 其它点对超平面是没有影响的。</p><p><strong>分离超平面</strong> <span class="math display">\[f(x) = \sum_{i=1}^n\alpha_i^*y_i(x_i\cdot x) + b^* = 0\]</span> <strong>分类决策函数</strong> <span class="math display">\[f(x) = \rm{sign}\left(\sum_{i=1}^n\alpha_i^*y_i(x_i\cdot x) + b^* \right)\]</span></p><h2 id="简单总结">简单总结</h2><p>目标函数 <span class="math display">\[J = \min_{w,b} \frac{1}{2} \|w\|^2, \quad s.t, \quad y_i(w^Tx_i+b) \ge 1\]</span> 拉格朗日函数 <span class="math display">\[L(w, b, \lambda) =\frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i \left(y_i(w^Tx_i+b) - 1\right)\]</span> 转化为<code>对偶问题求解</code>， 需要<strong>会求解过程、会推导公式</strong>。 <span class="math display">\[\max_{\alpha_i \ge 0} \min_{w,b} L(w, b, \alpha)\]</span> 主要是下面4个求解步骤：<strong>十分重要!!!</strong></p><ol style="list-style-type: decimal"><li>固定<span class="math inline">\(\alpha\)</span>， <strong>L对w和b求偏导</strong>，得到两个等式</li><li>结果带入L，消去w和b，得到<strong>只有<span class="math inline">\(\alpha\)</span>的L</strong></li><li>利用<code>SMO</code>求出<span class="math inline">\(\alpha^*\)</span></li><li><strong>利用<span class="math inline">\(\alpha^*\)</span>和支持向量，算出w和b</strong>。得出分离超平面和分界函数。</li></ol><p>求导后消去w和b，得到L <span class="math display">\[L(w, b, \alpha) =\sum_{i=1}^n\alpha_i -\frac{1}{2}\sum_{i=1}^n\sum_{i=1}^n\alpha_i \alpha_j y_i y_j x_i^T x_j\]</span> 利用SMO求得<span class="math inline">\(\alpha^*\)</span>后， 带回原式，得到w和b： <span class="math display">\[w* =\sum_{i=1}^n\alpha_i^*y_ix_i, \quad b^*  = y_j - \sum_{i=1}^n\alpha_i^*y_i(x_i\cdot x_j),\]</span> 实际上最重要是<code>向量内积</code>来进行决策 <span class="math display">\[f(x) = \rm{sign}\left(\sum_{i=1}^n\alpha_i^*y_i \color{red}{(x_i\cdot x}) + b^* \right)\]</span> 目标函数 <span class="math display">\[\max_\limits{\alpha_i \ge 0}L(w, b, \lambda) =\max_\limits{\alpha_i \ge 0} \frac{1}{2} \|w\|^2 - \sum_{i=1}^n \alpha_i \color{red}{\left(y_i(w^Tx_i+b) - 1\right)}\]</span> 两种数据点</p><ul><li>支持向量：红色为0，<span class="math inline">\(\alpha_i &gt; 0\)</span>。 后面为0。</li><li>其它点：红色大于1，<span class="math inline">\(\alpha_i=0\)</span>。 后面为0。</li></ul><h1 id="非线性svm">非线性SVM</h1><h2 id="核函数">核函数</h2><p><strong>问题</strong></p><p>大部分数据不是线性可分的，前面的超平面根本不存在。可以用一个超曲面进行分离，这就是<code>非线性可分问题</code>。</p><p>SVM可以通过<code>核函数</code>把输入<strong>映射到高维特征空间</strong>，最终<strong>在高维特征空间中构造最优分离超平面</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/hyper_plane" style="display:block; margin:auto" width="60%"></p><p>需要映射和学习线性SVM：</p><ul><li>把输入<code>映射</code>到特征空间F</li><li>在特征空间F中使用<code>线性学习器</code>分类</li></ul><p><span class="math display">\[f(x) = \sum_{i=1}^n\alpha_i^*y_i \color{red}{(\phi(x_i)\cdot \phi(x)}) + b^*\]</span></p><p><strong>核函数的功能</strong></p><p><code>核函数</code>在特征空间中<strong>直接计算内积</strong>，就像在原始输入点的函数中一样，两个步骤合二为一： <span class="math display">\[K(x, z) = \phi(x) \cdot \phi(z)\]</span> 分类函数： <span class="math display">\[f(x) = \sum_{i=1}^n\alpha_i^*y_i \color{red}{k(x_i, x)} + b^*\]</span> 对偶问题： <span class="math display">\[\begin{align}&amp; \max_\limits{\alpha} \; \sum_{i=1}^n\alpha_i -\frac{1}{2}\sum_{i=1}^n\sum_{i=1}^n\alpha_i \alpha_j y_i y_j k(x_i, x) \\&amp; s.t, \quad \alpha_i \ge 0, \quad \sum_{i=1}^n \alpha_i y_i = 0\end{align}\]</span></p><h2 id="核函数处理非线性数据">核函数处理非线性数据</h2><p><strong>简单例子</strong></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/kernal1.png" style="display:block; margin:auto" width="40%"></p><p>上面的数据线性不可分，两个维度<code>(a, b)</code>。 应该用<code>二次曲线</code>(特殊圆)来区分： <span class="math display">\[w_1a + w_2a^2 +w_3b + w_4b^2 + w_5ab + b=0\]</span> 看做映<code>射到了五维空间</code>： <span class="math display">\[w_1z_1 + w_2z_2 + w_3z_3 + w_4z_4 + w_5z_5 + b = \sum_{i=1}^5 w_iz_i + b = 0\]</span> 如下图：（实际映射到了三维空间的图），<strong>可以使用一个平面来分开</strong>：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/kernal2.gif" style="display:block; margin:auto" width="40%"></p><p><strong>问题</strong></p><p>五维是由1维和2维进行<strong>组合</strong>，就可以解决问题。所以对输入数据无脑组合映射到高维可以吗？当然是不可以的。维数太高，根本没法计算，<strong>不能无脑组合映射</strong>。</p><p><strong>核函数的功能</strong></p><p>看<code>核函数</code>： <span class="math display">\[k(x_1, x_2) = (x_1 \cdot x_2 + 1)^2\]</span> 核函数和上面映射空间的结果是一样的！区别：</p><ul><li>映射计算：先映射到高维空间，然后根据内积进行计算</li><li><code>核函数</code>：<strong>直接在原来的低维空间中计算</strong>，而不需显示写出映射后的结果。<strong>避开了在高维空间中的计算</strong>！</li></ul><h2 id="常用核函数">常用核函数</h2><p><strong>1 线性核</strong> <span class="math display">\[k(x_1, x_2) = x_1 \cdot x_2 \quad\quad\text{(原始空间的内积)}\]</span> 目的：映射前和映射后，形式上统一了起来。写个通用模板，再带入不同的核就可以了。</p><p><strong>2 高斯核</strong> <span class="math display">\[k(x_1, x_2) = \exp \left( - \frac{\|x_1 - x_2\|^2}{2\sigma^2}\right)\]</span> 高斯核函数，非常灵活，应用很广泛。可以映射到无穷维。</p><p><span class="math inline">\(\sigma\)</span> 的选择</p><ul><li>太大：权重衰减快，相当于映射到低维子空间</li><li>太小：将任意数据线性可分，容易陷入严重过拟合</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/kernal3" style="display:block; margin:auto" width="60%"></p><p><strong>3 多项式核</strong> <span class="math display">\[k(x_1, x_2) = ((x_1, x_2) + R)^d\]</span></p><h2 id="核函数总结">核函数总结</h2><p>问题的出现</p><ul><li>数据线性不可分，要映射到高维空间中去</li><li>不能无脑低维组合映射到高维空间，维度太大根本没法计算</li></ul><p><strong>核函数的功能</strong></p><ul><li>将特征向由低维向高维的转换</li><li>直接在低位空间中进行计算</li><li>实际的分类效果却是在高维上</li><li>避免了直接在高维空间中的复杂计算</li></ul><p>SVM曲线，逻辑回归和决策树是直线。SVM的效果好。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/svm-logistic-tree" style="display:block; margin:auto" width="70%"></p><h1 id="松弛变量软间隔最大化">松弛变量软间隔最大化</h1><h2 id="定义">定义</h2><p>数据可能有一些噪声<code>特异点outlier</code>导致不是线性可分或者效果不好。 如果不处理outlier，则会非常影响SVM。因为本身支持向量就只有几个。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/Optimal-Hyper-Plane-2.png" style="display:block; margin:auto" width="30%"></p><p>给每个数据点加上<code>松弛变量</code><span class="math inline">\(\xi_i \ge 0\)</span>， 使<strong>函数间隔+松弛变量大于等于1</strong>，即约束条件： <span class="math display">\[y_i(w \cdot x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0\]</span> 为每个松弛变量<span class="math inline">\(\xi_i\)</span>支付一个代价<span class="math inline">\(\xi_i\)</span>， 新的目标函数和约束条件如下： <span class="math display">\[\min \frac{1}{2} \|w\|^2 + C\sum_{i=1}^n \xi_i\]</span></p><p><span class="math display">\[s.t, \quad \quad y_i(w \cdot x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0\]</span></p><p><code>惩罚系数</code>C是一个常数</p><ul><li>C大时，对误分类的惩罚增大</li><li>C来调节权衡：使间隔尽量大；误分类点个数尽量少</li></ul><h2 id="求解">求解</h2><p>定义新的拉格朗日函数： <span class="math display">\[L(w,b,\xi,\alpha, r) = \frac{1}{2} \|w\|^2 + C\sum_{i=1}^n \xi_i-\sum_{i=1}^n \alpha_i \left(y_i(w^Tx_i+b) - 1 + \xi_i\right) - \sum_{i=1}^nr_i\xi_i\]</span> 和前面对偶问题求解一样，求导求解： <span class="math display">\[\frac{\partial L}{\partial w} = 0\quad \to \quad w = \sum_{i=1}^n\alpha_iy_ix_i\]</span></p><p><span class="math display">\[\frac{\partial L}{\partial b} = 0\quad \to \quad \sum_{i=1}^n\alpha_iy_i = 0\]</span></p><p><span class="math display">\[\frac{\partial L}{\partial \xi} = 0\quad \to \quad C -\alpha_i - r_i = 0\]</span></p><p>带入，得到新的L <span class="math display">\[\max_{\alpha} L = \sum_{i=1}^n\alpha_i -\frac{1}{2}\sum_{i=1}^n\sum_{i=1}^n\alpha_i \alpha_j y_i y_j( x_i \cdot  x_j)\]</span> 约束条件： <span class="math display">\[0 \le \alpha_i \le C, \quad \sum_{i=1}^n \alpha_i y_i = 0\]</span></p><h1 id="svm的深层理解">SVM的深层理解</h1><h2 id="感知机算法">感知机算法</h2><p>感知机算法是一个二类分类的线性模型，也是找一个超平面进行划分数据： <span class="math display">\[f(x) = \rm{sign}(w\cdot x + b)\]</span> <code>损失函数</code>是<strong>所有误分类点到超平面的总距离</strong>： <span class="math display">\[\min_\limits{w, b} L(w, b) = - \sum_{x_i \in M}y_i(w\cdot x_i + b)\]</span> 可以使用<code>SGD</code>对损失函数进行优化。</p><p>当训练数据集线性可分时，感知机算法是<code>收敛的</code>。可以在一定迭代次数上，找到一个超平面，有很多个解。</p><p>感知机的超平面不是最优效果，<code>最优是SVM</code>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/svm/perceptron" style="display:block; margin:auto" width="50%"></p><h2 id="损失函数">损失函数</h2><p>数据<span class="math inline">\(x\)</span>， 预测值<span class="math inline">\(f(x)=\hat y\)</span>， 真实值<span class="math inline">\(y\)</span>。</p><p><strong>常见损失</strong></p><ol style="list-style-type: decimal"><li>01损失</li></ol><p><span class="math display">\[L(y, \hat y) = \begin{cases}1, &amp; y \neq \hat y \\0, &amp; y = \hat y\end{cases}\]</span></p><ol start="2" style="list-style-type: decimal"><li><p>平方损失 <span class="math display">\[   L(y, \hat y) = (y - \hat y)^ 2   \]</span></p></li><li><p>绝对损失 <span class="math display">\[   L(y, \hat y) = \vert y - \hat y\vert   \]</span></p></li><li><p>对数损失 <span class="math display">\[   L(y, \hat y) = -\log P(y \hat x)   \]</span></p></li></ol><p><strong>期望损失</strong></p><p><code>期望损失</code>也称为<code>风险函数</code>，需要知道联合概率分布<span class="math inline">\(P(X, Y)\)</span>， 一般不知道。 <span class="math display">\[R_{\rm{exp}} = E_p[L(y, \hat y)]  = \int_{(x,y)} L(y, \hat y) P(x, y) {\rm d}x{\rm d}y\]</span> <strong>经验损失</strong></p><p><code>经验损失</code>也成为<code>经验风险</code> ，所以<code>监督学习</code>就是要<code>经验风险最小化</code>。 <span class="math display">\[R_{\rm emp}(f) = \frac{1}{N} \sum_{i=1}^NL(y_i, \hat y_i)\]</span> <strong>结构风险最小化</strong></p><p>样本数量太小时，容易<code>过拟合</code>。需要加上<code>正则化项</code>，也称为<code>惩罚项</code>。 模型越复杂，越大。 <span class="math display">\[R_{\rm srm}(f) = \frac{1}{N} \sum_{i=1}^NL(y_i, \hat y_i) + \lambda J(f)\]</span> <span class="math inline">\(\lambda\ge0\)</span> 是系数，<code>权衡经验风险和模型复杂度</code>。 监督学习，就是要使结构风险最小化。</p><p>SVM也是<strong>最优化+损失最小</strong>。 可以从损失函数和优化算法角度去看SVM、boosting、LR，可能会有不同的收获。</p><h2 id="svm的合页损失函数">SVM的合页损失函数</h2><p>从最优化+损失最小的角度去理解SVM。</p><h2 id="最小二乘法">最小二乘法</h2><p>最小二乘法，就是通过<strong>最小化误差的平方</strong>来进行数学优化。对参数进行求偏导，进行求解。</p><h2 id="smo">SMO</h2><p><strong>模型</strong> <span class="math display">\[\min \frac{1}{2} \|w\|^2 + C\sum_{i=1}^n \xi_i\]</span></p><p><span class="math display">\[s.t, \quad \quad y_i(w \cdot x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0\]</span></p><p><code>序列最小最优化SMO</code> (Sequential minimal optimization)，解决求解<span class="math inline">\(\alpha\)</span>的问题： <span class="math display">\[\min_{\alpha} L = \frac{1}{2}\sum_{i=1}^n\sum_{i=1}^n\alpha_i \alpha_j y_i y_jK(x_i,  x_j) - \sum_{i=1}^n\alpha_i\]</span></p><p><span class="math display">\[s.t, \quad \quad0 \le \alpha_i \le C, \quad \sum_{i=1}^n \alpha_i y_i = 0\]</span></p><p>如果所有变量的解都满足KKT条件，则最优化问题的解已经得到。</p><p><strong>思想</strong></p><p>每次抽取两个乘子<span class="math inline">\(\alpha_1, \alpha_2\)</span>，然后固定其他乘子，针对这两个变量构建一个子二次规划问题，进行求解。不断迭代求解子问题，最终解得原问题。</p><p><strong>选择乘子</strong></p><p><span class="math inline">\(\alpha_1\)</span>选择违反KKT条件最严重的，<span class="math inline">\(\alpha_2\)</span>选择让<span class="math inline">\(\alpha_1\)</span>变化最大的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Support Vector Machine简单笔记。 特征空间上的间隔最大的线性分类器。学习策略是间隔最大化，转化为一个凸二次规划问题的求解。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:
      
    
    </summary>
    
      <category term="机器学习" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="SVM" scheme="http://plmsmile.github.io/tags/SVM/"/>
    
      <category term="机器学习" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="拉格朗日对偶性" scheme="http://plmsmile.github.io/tags/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7/"/>
    
      <category term="对偶问题" scheme="http://plmsmile.github.io/tags/%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98/"/>
    
      <category term="支持向量" scheme="http://plmsmile.github.io/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F/"/>
    
      <category term="核函数" scheme="http://plmsmile.github.io/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"/>
    
      <category term="感知机" scheme="http://plmsmile.github.io/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    
      <category term="损失函数" scheme="http://plmsmile.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>ide-envs</title>
    <link href="http://plmsmile.github.io/2018/02/10/ide-envs/"/>
    <id>http://plmsmile.github.io/2018/02/10/ide-envs/</id>
    <published>2018-02-10T13:39:42.000Z</published>
    <updated>2018-02-11T03:37:58.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="vscode-cpp">vscode-cpp</h1><p>使用vscode写cpp程序。参考自<a href="https://www.zhihu.com/question/30315894" target="_blank" rel="noopener">vscode-cpp知乎</a></p><p><strong>1 下载安装vscode</strong></p><p><strong>2 下载安装MINGW</strong></p><p><strong>3 VSCode安装cc++官方扩展</strong></p><p><strong>4 配置json文件</strong></p><p><code>c_cpp_properties.json</code>， 编译环境</p><p><code>ctrl+shift+p</code> ，选择<code>edit configurations</code>， 更新windows的配置</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">            <span class="attr">"name"</span>: <span class="string">"Win32"</span>,</span><br><span class="line">            <span class="attr">"intelliSenseMode"</span>: <span class="string">"clang-x64"</span>,</span><br><span class="line">            <span class="attr">"includePath"</span>: [</span><br><span class="line">                <span class="string">"$&#123;workspaceRoot&#125;"</span>,</span><br><span class="line">                <span class="string">"C:/MinGW/lib/gcc/mingw32/6.3.0/include/c++"</span>,</span><br><span class="line">                <span class="string">"C:/MinGW/lib/gcc/mingw32/6.3.0/include/c++/mingw32"</span>,</span><br><span class="line">                <span class="string">"C:/MinGW/lib/gcc/mingw32/6.3.0/include/c++/backward"</span>,</span><br><span class="line">                <span class="string">"C:/MinGW/lib/gcc/mingw32/6.3.0/include"</span>,</span><br><span class="line">                <span class="string">"C:/MinGW/include"</span>,</span><br><span class="line">                <span class="string">"C:/MinGW/lib/gcc/mingw32/6.3.0/include-fixed"</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"defines"</span>: [</span><br><span class="line">                <span class="string">"_DEBUG"</span>,</span><br><span class="line">                <span class="string">"UNICODE"</span>,</span><br><span class="line">                <span class="string">"__GNUC__=6"</span>,</span><br><span class="line">                <span class="string">"__cdecl=__attribute__((__cdecl__))"</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">"browse"</span>: &#123;</span><br><span class="line">                <span class="attr">"path"</span>: [</span><br><span class="line">                    <span class="string">"C:/MinGW/lib/gcc/mingw32/6.3.0/include"</span>,</span><br><span class="line">                    <span class="string">"C:/MinGW/lib/gcc/mingw32/6.3.0/include-fixed"</span>,</span><br><span class="line">                    <span class="string">"C:/MinGW/include/*"</span></span><br><span class="line">                ],</span><br><span class="line">                <span class="attr">"limitSymbolsToIncludedHeaders"</span>: <span class="literal">true</span>,</span><br><span class="line">                <span class="attr">"databaseFilename"</span>: <span class="string">""</span></span><br><span class="line">            &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>launch.json</code>， 执行结果文件。选择左边的调试<code>ctrl+shift+d</code>， 选择添加配置，选择gdbll什么的。粘贴下面的内容。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"0.2.0"</span>,</span><br><span class="line">    <span class="attr">"configurations"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            "name": "C++ Launch (GDB)",                 // 配置名称，将会在启动配置的下拉菜单中显示</span><br><span class="line">            "type": "cppdbg",                           // 配置类型，这里只能为cppdbg</span><br><span class="line">            "request": "launch",                        // 请求配置类型，可以为launch（启动）或attach（附加）</span><br><span class="line">            "targetArchitecture": "x86",                // 生成目标架构，一般为x86或x64，可以为x86, arm, arm64, mips, x64, amd64, x86_64</span><br><span class="line">            "program": "$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.out",                   // 将要进行调试的程序的路径</span><br><span class="line">            "miDebuggerPath":"c:\\MinGW\\bin\\gdb.exe", // miDebugger的路径，注意这里要与MinGw的路径对应</span><br><span class="line">            "args": [],                                 // 程序调试时传递给程序的命令行参数，一般设为空即可</span><br><span class="line">            "stopAtEntry": false,                       // 设为true时程序将暂停在程序入口处，一般设置为false</span><br><span class="line">            "cwd": "$&#123;workspaceRoot&#125;",                  // 调试程序时的工作目录，一般为$&#123;workspaceRoot&#125;即代码所在目录</span><br><span class="line">            "externalConsole": true,                    // 调试时是否显示控制台窗口，一般设置为true显示控制台</span><br><span class="line">            "preLaunchTask": "g++"　　                  // 调试会话开始前执行的任务，一般为编译程序，c++为g++, c为gcc</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>tasks.json</code>， 编译参数。F5运行调试代码， 选择配置任务，使用模板创建tasks文件，选择others。粘贴下面的内容。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"version"</span>: <span class="string">"0.1.0"</span>,</span><br><span class="line">    <span class="attr">"command"</span>: <span class="string">"g++"</span>,</span><br><span class="line">    "args": ["-g","$&#123;file&#125;","-o","$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.out", "-std=c++0x"],    // gcc编译命令参数</span><br><span class="line">    "problemMatcher": &#123;</span><br><span class="line">        "owner": "cpp",</span><br><span class="line">        "fileLocation": ["relative", "$&#123;workspaceRoot&#125;"],</span><br><span class="line">        "pattern": &#123;</span><br><span class="line">            "regexp": "^(.*):(\\d+):(\\d+):\\s+(warning|error):\\s+(.*)$",</span><br><span class="line">            "file": 1,</span><br><span class="line">            "line": 2,</span><br><span class="line">            "column": 3,</span><br><span class="line">            "severity": 4,</span><br><span class="line">            "message": 5</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;vscode-cpp&quot;&gt;vscode-cpp&lt;/h1&gt;
&lt;p&gt;使用vscode写cpp程序。参考自&lt;a href=&quot;https://www.zhihu.com/question/30315894&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;v
      
    
    </summary>
    
      <category term="编辑器配置" scheme="http://plmsmile.github.io/categories/%E7%BC%96%E8%BE%91%E5%99%A8%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="vscode" scheme="http://plmsmile.github.io/tags/vscode/"/>
    
  </entry>
  
  <entry>
    <title>linux-notes</title>
    <link href="http://plmsmile.github.io/2018/01/25/linux-notes/"/>
    <id>http://plmsmile.github.io/2018/01/25/linux-notes/</id>
    <published>2018-01-25T08:07:07.000Z</published>
    <updated>2018-01-26T07:55:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="tmux使用">Tmux使用</h1><p><code>tmux list-keys</code> 可以看到所有的按键。</p><p><strong>使用tmux进行复制粘贴</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 1. 复制</span><br><span class="line"><span class="meta">#</span> 进入vim模式</span><br><span class="line">ctrl x + [ </span><br><span class="line"><span class="meta">#</span> 输入Space开始选择</span><br><span class="line">space</span><br><span class="line"><span class="meta">#</span> 输入enter选择完成</span><br><span class="line">enter</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 2. 粘贴到另外一个窗口的vim文件里面</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> i进入vim的插入模式</span><br><span class="line">i</span><br><span class="line"><span class="meta">#</span> 粘贴到文件里</span><br><span class="line">ctrl x + ]</span><br></pre></td></tr></table></figure><p>即<code>ctrl x + [</code> 是进入复制模式，可以进行上下翻页。<code>ctrl x + ]</code> 是粘贴。</p><p><strong>交换window</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> &#123;&#125;实际操作是shift+[&#123;按键，和复制模式有点相似</span><br><span class="line">ctrl x + &#123;</span><br><span class="line">ctrl x + &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;tmux使用&quot;&gt;Tmux使用&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;tmux list-keys&lt;/code&gt; 可以看到所有的按键。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用tmux进行复制粘贴&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight s
      
    
    </summary>
    
      <category term="linux" scheme="http://plmsmile.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://plmsmile.github.io/tags/linux/"/>
    
      <category term="tmux" scheme="http://plmsmile.github.io/tags/tmux/"/>
    
  </entry>
  
  <entry>
    <title>Aim2offer3(21-40)</title>
    <link href="http://plmsmile.github.io/2018/01/07/aim2offer3/"/>
    <id>http://plmsmile.github.io/2018/01/07/aim2offer3/</id>
    <published>2018-01-07T04:22:49.000Z</published>
    <updated>2018-03-02T14:39:18.541Z</updated>
    
    <content type="html"><![CDATA[<h1 id="把奇数放在偶数的前面-21">把奇数放在偶数的前面-21</h1><p><strong>问题</strong></p><p>输入：一个数组，乱序，有奇数和偶数</p><p>输出：把所有的奇数放在前面，所有的偶数放在后面</p><p><strong>暴力思路</strong></p><ol style="list-style-type: decimal"><li>从头到尾遍历数组</li><li>遇到奇数，访问下一个</li><li>遇到偶数，把它后面的数字都向前挪动以为，该偶数放到末尾</li></ol><p><strong>冒泡思路</strong></p><ol style="list-style-type: decimal"><li><code>for (int i = n-1; i &gt; 0; i--)</code> 依次放置好后面n-1个数即可</li><li>从<code>for (int j = 0; j &lt; i; j++)</code> ，遇到j-1是偶数，j是奇数，则交换</li></ol><p><strong>辅助数组</strong></p><ol style="list-style-type: decimal"><li>新数组存偶数，原数组删除偶数，最后把新数组的偶数追加到原数组</li><li>遍历原数组，遇到偶数，存到新数组，删除原数组中的偶数</li></ol><p><strong>双指针快排思路</strong></p><p>类似于<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#快速排序">快速排序</a>的思路，但<strong>不是稳定</strong>的。</p><ol style="list-style-type: decimal"><li>指针1，在前面，向后移动，前面应是奇数</li><li>指针2，在后面，向前移动，后面应是偶数</li><li>指针1指偶数，指针2指奇数，交换两个数</li><li>直到指针相遇</li></ol><p><a href="https://github.com/plmsmile/aim2offer/blob/master/21_reorder_array/reorder_array.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 对数组进行重新排序，把奇数放在前面，偶数在后面</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a -- 数组</span></span><br><span class="line"><span class="comment"> *      f -- 函数指针，什么样的条件放在后面，如是偶数、是正数，解耦</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">void reorder_array(vector&lt;int&gt; &amp; a, bool (*f)(int)) &#123;</span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> r = a.size() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r) &#123;</span><br><span class="line">        <span class="comment">// 从左到右找到第一个偶数</span></span><br><span class="line">        <span class="keyword">while</span> (l &lt; r &amp;&amp; !f(a[l])) &#123;</span><br><span class="line">            l++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 从右到左，找到第一个奇数</span></span><br><span class="line">        <span class="keyword">while</span> (r &gt; l &amp;&amp; f(a[r])) &#123;</span><br><span class="line">            r--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 交换</span></span><br><span class="line">        <span class="keyword">if</span> (l &lt; r) &#123;</span><br><span class="line">            <span class="keyword">int</span> t = a[l];</span><br><span class="line">            a[l] = a[r];</span><br><span class="line">            a[r] = t;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>归并排序思路</strong></p><p>归并排序稳定，也很快，所以使用<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#归并排序">归并排序</a>。 分成长度为1、2、4的序列，各自都排好，依次两两合并。</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/21_reorder_array/reorder_array.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">/* </span></span><br><span class="line"><span class="comment">   * 奇偶性</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">get_parity</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> (n &amp; <span class="number">1</span>) == <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * a中前后有两个序列，分别有序，对其进行merge</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> start, <span class="keyword">int</span> mid, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 临时存储新的排序数据</span></span><br><span class="line">        <span class="keyword">int</span> *t = <span class="keyword">new</span> <span class="keyword">int</span>[end - start + <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> i = start;</span><br><span class="line">        <span class="keyword">int</span> j = mid + <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 临时数组的索引</span></span><br><span class="line">        <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &lt;= mid &amp;&amp; j &lt;= end) &#123;</span><br><span class="line">            <span class="comment">// 奇偶性</span></span><br><span class="line">            <span class="keyword">int</span> pi = get_parity(a[i]);</span><br><span class="line">            <span class="keyword">int</span> pj = get_parity(a[j]);</span><br><span class="line">            <span class="keyword">if</span> (pi == <span class="number">1</span> &amp;&amp; pj == <span class="number">0</span>) &#123;</span><br><span class="line">                t[k++] = a[i++];</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (pi == <span class="number">0</span> &amp;&amp; pj == <span class="number">1</span>)&#123;</span><br><span class="line">                t[k++] = a[j++];</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (pi == <span class="number">1</span> &amp;&amp; pj == <span class="number">1</span>) &#123;</span><br><span class="line">                t[k++] = a[i++];</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (pi == <span class="number">0</span> &amp;&amp; pj == <span class="number">0</span>) &#123;</span><br><span class="line">                t[k++] = a[i++];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (i &lt;= mid) &#123;</span><br><span class="line">            t[k++] = a[i++];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (j &lt;= end) &#123;</span><br><span class="line">            t[k++] = a[j++];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; end - start + <span class="number">1</span>; i++) &#123;</span><br><span class="line">            a[start + i] = t[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">delete</span> [] t;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 对a的长度为gap的子序列，两两合并</span></span><br><span class="line"><span class="comment">     * Args:</span></span><br><span class="line"><span class="comment">     *      a -- 数组</span></span><br><span class="line"><span class="comment">     *      gap -- 1个子序列的长度</span></span><br><span class="line"><span class="comment">     * Returns:</span></span><br><span class="line"><span class="comment">     *      None</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">merge_groups</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> gap)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 两组的长度</span></span><br><span class="line">        <span class="keyword">int</span> twolen = <span class="number">2</span> * gap;</span><br><span class="line">        <span class="keyword">int</span> i;</span><br><span class="line">        <span class="comment">// 对相邻的两个gap进行合并</span></span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i + twolen - <span class="number">1</span> &lt; a.size(); i += twolen) &#123;</span><br><span class="line">            <span class="keyword">int</span> start = i;</span><br><span class="line">            <span class="keyword">int</span> mid = start + gap - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">int</span> end = i + twolen - <span class="number">1</span>;</span><br><span class="line">            merge(a, start, mid, end);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 若最后一次不足两个gap，即1个gap和部分gap</span></span><br><span class="line">        <span class="keyword">if</span> (i + gap - <span class="number">1</span> &lt; a.size() - <span class="number">1</span>) &#123;</span><br><span class="line">            merge(a, i, i + gap - <span class="number">1</span>, a.size() - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">reOrderArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 分割为长度为i的子序列，两两进行合并</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.size(); i *= <span class="number">2</span>) &#123;</span><br><span class="line">            merge_groups(a, i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="链表中倒数第k个节点-22">链表中倒数第k个节点-22</h1><p>不知道链表长度，要求在<span class="math inline">\(O(n)\)</span>内找到倒数第k个节点，注意代码的鲁棒性。</p><p><strong>双指针思路</strong></p><p>两个指针，<code>l</code>先走k-1步，<code>r</code>再从头节点出发，始终<strong>保持距离为k-1</strong>。 <strong>r走到末尾</strong>时，<strong>l就是倒数第k个节点</strong>。</p><p>注意头结点为空和k非法(为0、k超出等)的情况。</p><p><strong>双指针求链表中间节点</strong></p><p>两个指针，<code>l</code><strong>走一步</strong>，<code>r</code><strong>走两步</strong>。<strong>r走到末尾</strong>的时候，<strong>l正好走到中间</strong>。</p><p><strong>双指针总结</strong></p><p>当一个指针遍历链表不能解决问题的时候，就使用两个指针。</p><ul><li>同时走、一个速度快</li><li>一个先走、速度一样</li></ul><p><a href="https://github.com/plmsmile/aim2offer/blob/master/22_kth_node_from_end/kth_node_from_end.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 返回链表中的倒数第k个节点</span></span><br><span class="line"><span class="comment"> * 双指针思路，注意代码的鲁棒性</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      phead -- 头指针</span></span><br><span class="line"><span class="comment"> *      k  -- 无符号整数</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      nullptr or 第k个节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">ListNode* <span class="title">kth_from_end</span><span class="params">(ListNode* phead, <span class="keyword">unsigned</span> <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. phead为空或者k不合法，都返回空，k不能为0，否则k-1是一个巨大的数</span></span><br><span class="line">    <span class="keyword">if</span> (phead == <span class="literal">nullptr</span> || k == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 双指针</span></span><br><span class="line">    ListNode* pr = phead;</span><br><span class="line">    ListNode* pl = phead;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. pl先走</span></span><br><span class="line">    <span class="keyword">while</span> (pl &amp;&amp; count &lt;= k - <span class="number">1</span>) &#123;</span><br><span class="line">        pl = pl-&gt;next;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4. 不足k个节点，返回空</span></span><br><span class="line">    <span class="keyword">if</span> (pl == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 左右指针一起走，保持k-1的距离</span></span><br><span class="line">    <span class="keyword">while</span> (pl-&gt;next) &#123;</span><br><span class="line">        pl = pl-&gt;next;</span><br><span class="line">        pr = pr-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="链表中环的入口节点-23">链表中环的入口节点-23</h1><blockquote><p>如果一个链表有环，则返回这个环的入口节点</p></blockquote><h2 id="双指针法">双指针法</h2><p><strong>确定有环</strong></p><p>双指针，同时走，l一次1步，r一次2步。</p><ul><li>r走到末尾，则无环</li><li>r与l相遇，则有环。相遇节点在环内</li></ul><p><strong>确定环内节点数量</strong></p><p>相遇节点在环内，从相遇节点开始遍历一圈，计数。再次回到相遇节点，就能知道<strong>环内节点数量k</strong>。</p><p><strong>找到环的入口节点</strong></p><p>双指针，r先走k步，l再走。l与r相遇时，就是环的入口节点</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/23_circle_entry/circle_entry.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 双指针判断链表是否有环时，返回两个指针相遇的节点</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      phead -- 头节点</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      pmeet -- nullptr or 相遇的节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">ListNode* <span class="title">meet_node</span><span class="params">(ListNode* phead)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 空节点 or 1个节点，无法成环</span></span><br><span class="line">    <span class="keyword">if</span> (phead == <span class="literal">nullptr</span> || phead-&gt;next == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 两个指针，l一次走一步，r一次走两步</span></span><br><span class="line">    ListNode* pl = phead;</span><br><span class="line">    ListNode* pr = phead-&gt;next-&gt;next;</span><br><span class="line">    ListNode* pmeet = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">while</span> (pl &amp;&amp; pr) &#123;</span><br><span class="line">        <span class="keyword">if</span> (pl == pr) &#123;</span><br><span class="line">            pmeet = pl;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        pl = pl-&gt;next;</span><br><span class="line">        pr = pr-&gt;next;</span><br><span class="line">        <span class="comment">// pr走第二步的时候，先判断一下</span></span><br><span class="line">        <span class="keyword">if</span> (pr) &#123;</span><br><span class="line">            pr = pr-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pmeet;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 获取链表中环内节点的数量，已经确保有环</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      pmeet -- 链表中环内的一个节点</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      count -- 环内的节点的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">get_circle_node_count</span><span class="params">(ListNode* pmeet)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (pmeet == <span class="literal">nullptr</span> || pmeet-&gt;next == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    ListNode* p = pmeet-&gt;next;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (p != pmeet) &#123;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 双指针法获得链表中环的入口节点</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      phead -- 头节点</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      pentry -- nullptr 或者 环的入口节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">ListNode* <span class="title">circle_entry_node</span><span class="params">(ListNode* phead)</span> </span>&#123;</span><br><span class="line">    ListNode* pmeet = meet_node(phead);</span><br><span class="line">    <span class="comment">// 无环</span></span><br><span class="line">    <span class="keyword">if</span> (pmeet == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 后面的操作已经确保有环</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 环内节点的数量</span></span><br><span class="line">    <span class="keyword">int</span> k = get_circle_node_count(pmeet);</span><br><span class="line">    <span class="comment">// pl先走k步</span></span><br><span class="line">    ListNode* pl = phead;</span><br><span class="line">    ListNode* pr = phead;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= k; i++) &#123;</span><br><span class="line">        pr = pr-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 同时走，直到相遇在入口节点</span></span><br><span class="line">    ListNode* pentry =  <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">while</span> (pl &amp;&amp; pr) &#123;</span><br><span class="line">        <span class="keyword">if</span> (pl == pr) &#123;</span><br><span class="line">            pentry = pl;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        pl = pl-&gt;next;</span><br><span class="line">        pr = pr-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pentry;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="推导法和断链法">推导法和断链法</h2><p><a href="https://leetcode.com/problems/linked-list-cycle-ii/discuss/44793" target="_blank" rel="noopener">leetcode推导法</a></p><h1 id="翻转链表-24">翻转链表-24</h1><p><a href="https://leetcode.com/problems/reverse-linked-list/description/" target="_blank" rel="noopener">Reverse Linked List</a></p><p><strong>思路</strong></p><p>每次遍历保存三个节点：<code>pre</code>, <code>pnow</code>, <code>pnext</code> ，遍历到pnow</p><ul><li>保存pnow的next</li><li>pnow指向pre</li><li>pre = pnow</li><li>把pnext赋值给pnow，下一次循环</li></ul><p>注意当pnext为空时，已走到末尾，此时的pnow应该是新的head。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">reverseList</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">ListNode* pre = <span class="literal">nullptr</span>;</span><br><span class="line">    ListNode* pnow = head;</span><br><span class="line">  ListNode* pnext = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="keyword">while</span> (pnow) &#123;</span><br><span class="line">    <span class="comment">// 保存pnow的next</span></span><br><span class="line">      pnext = pnow-&gt;next;</span><br><span class="line">      pnow-&gt;next = pre;</span><br><span class="line">      <span class="comment">// 新的头节点</span></span><br><span class="line">      <span class="keyword">if</span> (pnext == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">          head = pnow;</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="comment">// pnow作为新的pre</span></span><br><span class="line">      pre = pnow;</span><br><span class="line">      <span class="comment">// pnext作为下一轮遍历的pnow</span></span><br><span class="line">      pnow = pnext;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="合并两个有序链表-25">合并两个有序链表-25</h1><p><a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#合并两条有序链表-21">见这里</a></p><h1 id="树的子结构-26">树的子结构-26</h1><p><a href="https://leetcode.com/problems/subtree-of-another-tree/description/" target="_blank" rel="noopener">Subtree of Another Tree</a></p><blockquote><p>两棵树s和t，判断t是否是s的一个子结构</p></blockquote><p><strong>思路</strong></p><ul><li>层次遍历s的每个节点p</li><li>判断p是否和t完全相同</li></ul><p>判断两颗树相同</p><ul><li>两个都为空，相同</li><li>其中一个为空，不同</li><li>值不同，不同</li><li>则 return 左子树相同 &amp;&amp; 右子树相同</li></ul><p><a href="https://github.com/plmsmile/aim2offer/blob/master/26_sub_tree/sub_tree.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 判断两颗树是否相等</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">is_same</span><span class="params">(TreeNode* p1, TreeNode* p2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p1 == <span class="literal">nullptr</span> &amp;&amp; p2 == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (p1 == <span class="literal">nullptr</span> || p2 == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (p1-&gt;val != p2-&gt;val) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> is_same(p1-&gt;left, p2-&gt;left) &amp;&amp; is_same(p1-&gt;right, p2-&gt;right);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 层次遍历以s的每个节点为根节点的子树是否和t相同</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isSubtree</span><span class="params">(TreeNode* s, TreeNode* t)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">queue</span>&lt;TreeNode*&gt; nodes;</span><br><span class="line">    <span class="comment">// 放根节点</span></span><br><span class="line">    <span class="keyword">if</span> (s != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        nodes.push(s);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (!nodes.empty()) &#123;</span><br><span class="line">        <span class="comment">// 访问根节点</span></span><br><span class="line">        TreeNode* p = nodes.front();</span><br><span class="line">        nodes.pop();</span><br><span class="line">        <span class="keyword">if</span> (is_same(p, t)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 向队列中追加左右孩子</span></span><br><span class="line">        <span class="keyword">if</span> (p-&gt;left) &#123;</span><br><span class="line">            nodes.push(p-&gt;left);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (p-&gt;right) &#123;</span><br><span class="line">            nodes.push(p-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="二叉树的镜像-27">二叉树的镜像-27</h1><blockquote><p>二叉树的镜像，就是左右孩子交换节点嘛。</p></blockquote><p><strong>思路</strong></p><p>使用<a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#先序遍历-144">先序遍历</a>的思想，<a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#二叉树遍历">这里是二叉树各种遍历</a></p><ul><li>使用栈，根节点入栈</li><li>栈不为空，出栈一个元素p</li><li>交换其左右孩子节点</li><li>右孩子入栈，左孩子入栈</li></ul><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 先序遍历求二叉树的镜像</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">mirror</span><span class="params">(TreeNode *head)</span> </span>&#123;</span><br><span class="line">    TreeNode* p = head;</span><br><span class="line">    <span class="built_in">stack</span>&lt;TreeNode*&gt; st;</span><br><span class="line">    <span class="keyword">if</span> (p != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        st.push(p);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 栈不为空</span></span><br><span class="line">    <span class="keyword">while</span> (!st.empty()) &#123;</span><br><span class="line">        TreeNode* now = st.top();</span><br><span class="line">        st.pop();</span><br><span class="line">        <span class="comment">// 交换其左右节点</span></span><br><span class="line">        TreeNode* t = now-&gt;left;</span><br><span class="line">        now-&gt;left = now-&gt;right;</span><br><span class="line">        now-&gt;right = t;</span><br><span class="line">        <span class="comment">// 左右节点入栈</span></span><br><span class="line">        <span class="keyword">if</span> (now-&gt;right) &#123;</span><br><span class="line">            st.push(now-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (now-&gt;left) &#123;</span><br><span class="line">            st.push(now-&gt;left);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="对称的二叉树-28">对称的二叉树-28</h1><p><a href="https://leetcode.com/problems/symmetric-tree/description/" target="_blank" rel="noopener">leetcode对称的二叉树</a></p><blockquote><p>判断一个二叉树是不是对称的</p></blockquote><p><strong>思路</strong></p><p>普通的遍历都是<strong>先左后右</strong>， 对称的话，得用<strong>先右后左</strong>。</p><p>一棵树<strong>对称</strong>，则先左后右的先序序列、先右后左的<a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#先序遍历-144">先序序列</a>应该一样。 即<strong>左右子树对称相等</strong>。</p><p>遍历的时候</p><ul><li>根节点为空，对称</li><li>否则，看<code>sym(root.left, root.right)</code></li><li>两个节点其中一个为空，则看<code>p1 == p2</code></li><li>两个都不为空，则<strong>先看根节点的值</strong></li><li>最后则<strong>交叉看左右子树</strong>，<code>sym(p1.left, p2.right) &amp;&amp; sym(p1.right, p2.left)</code></li></ul><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 判断一棵树是否对称</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isSymmetric</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> symmetric_equal(root-&gt;left, root-&gt;right);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 判断两棵树对称相等</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      p1, p2 -- 一般是一棵树的左右子树</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      true or false</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">symmetric_equal</span><span class="params">(TreeNode* p1, TreeNode* p2)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 有空的</span></span><br><span class="line">    <span class="keyword">if</span> (p1 == <span class="literal">nullptr</span> || p2 == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> p1 == p2;</span><br><span class="line">    &#125;   </span><br><span class="line">    <span class="comment">// 先看根节点的值</span></span><br><span class="line">    <span class="keyword">if</span> (p1-&gt;val != p2-&gt;val) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;           </span><br><span class="line">    <span class="comment">// 看左右子树对称相等</span></span><br><span class="line">    <span class="keyword">return</span> symmetric_equal(p1-&gt;left, p2-&gt;right) &amp;&amp; symmetric_equal(p1-&gt;right, p2-&gt;left);   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>非递归代码</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isSymmetric</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    Stack&lt;TreeNode&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">    stack.push(root.left);</span><br><span class="line">    stack.push(root.right);</span><br><span class="line">    <span class="keyword">while</span> (!stack.empty()) &#123;</span><br><span class="line">        TreeNode n1 = stack.pop(), n2 = stack.pop();</span><br><span class="line">        <span class="keyword">if</span> (n1 == <span class="keyword">null</span> &amp;&amp; n2 == <span class="keyword">null</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span> (n1 == <span class="keyword">null</span> || n2 == <span class="keyword">null</span> || n1.val != n2.val) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        stack.push(n1.left);</span><br><span class="line">        stack.push(n2.right);</span><br><span class="line">        stack.push(n1.right);</span><br><span class="line">        stack.push(n2.left);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="顺时针打印矩阵-29">顺时针打印矩阵-29</h1><p><a href="https://www.nowcoder.com/practice/9b4c81a02cd34f76be2659fa0d54342a?tpId=13&amp;tqId=11172&amp;rp=1&amp;ru=/ta/coding-interviews&amp;qru=/ta/coding-interviews/question-ranking" target="_blank" rel="noopener">牛客网打印矩阵</a></p><p><strong>思路</strong></p><p>用<strong>左上</strong>和<strong>右下</strong>的坐标定位出一圈要打印的数据。一圈打完以后，分别沿着对角线向中间靠拢一个单位。</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/29_print_matrix/print_matrix.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; print_marix(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; matrix) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">    <span class="keyword">if</span> (matrix.empty() || matrix[<span class="number">0</span>].empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> row = matrix.size();</span><br><span class="line">    <span class="keyword">int</span> col = matrix[<span class="number">0</span>].size();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通过左上、右下去锁定当前圈的元素</span></span><br><span class="line">    <span class="keyword">int</span> left = <span class="number">0</span>, top = <span class="number">0</span>, right = col - <span class="number">1</span>, bottom = row - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (left &lt;= right &amp;&amp; top &lt;= bottom) &#123;</span><br><span class="line">        <span class="comment">// 左到右</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = left; i &lt;= right; i++)</span><br><span class="line">            res.push_back(matrix[top][i]);</span><br><span class="line">        <span class="comment">// 上到下</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = top + <span class="number">1</span>; i &lt;= bottom; i++)</span><br><span class="line">            res.push_back(matrix[i][right]);</span><br><span class="line">        <span class="comment">// 右到左，有多行时</span></span><br><span class="line">        <span class="keyword">if</span> (top != bottom)</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = right - <span class="number">1</span>; i &gt;= left; i--)</span><br><span class="line">            res.push_back(matrix[bottom][i]);</span><br><span class="line">        <span class="comment">// 下到上，有多列时</span></span><br><span class="line">        <span class="keyword">if</span> (left != right)</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = bottom - <span class="number">1</span>; i &gt; top; i--)</span><br><span class="line">            res.push_back(matrix[i][left]);</span><br><span class="line">        <span class="comment">// 左上角、右下角移动</span></span><br><span class="line">        left++, top++, right--, bottom--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="包含min函数的栈-30">包含min函数的栈-30</h1><p><a href="https://leetcode.com/problems/min-stack/description/" target="_blank" rel="noopener">Min Stack</a></p><blockquote><p>实现能够得到栈中最小元素的数据结构，要求入栈、出栈、获得最小元素都是O(1)</p></blockquote><p><strong>思考过程</strong></p><p>定义一个变量，去存储最小元素，每次对其更新。可是，当最小元素出栈以后呢，怎么去得到新的最小元素呢？这样就需要把次最小元素也存储下来。就<strong>需要把每次最小的元素，按顺序存储下来</strong>， 按照入栈的顺序</p><ul><li>入栈时，入栈当前最小的元素</li><li>出栈时，把当前最小的元素也出栈，留下的是下一个状态的最小元素</li></ul><p><strong>思路</strong></p><ul><li>数据栈，正常存数据</li><li><strong>最小栈</strong>，存放各个时刻的最小元素，栈顶一直是当前的最小元素</li><li>入栈： 当前最小元素：<code>min(min_st.top(), new)</code> ，最小栈压入当前的最小元素</li><li>出栈：数据栈元素出栈，同时最小栈出掉当前的最小元素</li></ul><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MinStack</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 数据栈</span></span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; st_data;</span><br><span class="line">    <span class="comment">// 存储每个状态最小元素的栈</span></span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; st_min;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 初始化数据结构</span></span><br><span class="line">    MinStack() &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">        st_data.push(x);</span><br><span class="line">        <span class="comment">// 当前的最小元素入栈</span></span><br><span class="line">        <span class="keyword">if</span> (st_min.empty() || x &lt; st_min.top()) &#123;</span><br><span class="line">            st_min.push(x);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            st_min.push(st_min.top());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (st_data.empty() || st_min.empty()) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 当前数据栈和最小元素栈都出栈</span></span><br><span class="line">        st_data.pop();</span><br><span class="line">        st_min.pop();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">top</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> st_data.top();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getMin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> st_min.top();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="栈的压入和弹出序列-31">栈的压入和弹出序列-31</h1><blockquote><p>输入两个序列，第一个为栈的入栈序列，判断第二个是否为其出栈序列。入栈所有数字都不相等</p></blockquote><p><strong>思路</strong></p><p>入栈序列，出栈序列。当前需要出栈元素为i</p><ol style="list-style-type: decimal"><li><strong>i在栈内</strong>，则直接出栈</li><li><strong>i不在栈内</strong>，则把如栈序列 <strong>前面到i</strong> 的元素全部入栈，再重复1</li><li>入栈序列全都入栈了，依然<strong>没有i</strong>，<strong>则不是弹出序列</strong></li></ol><p><strong>示例</strong></p><p>入栈：<code>1 2 3 4 5</code>， 出栈 <code>4 5 3 2 1</code></p><p>4不在栈顶，前面元素入栈</p><table><thead><tr class="header"><th align="center">操作</th><th align="center">栈内元素</th><th align="center">剩余出栈元素</th><th align="center">剩余入栈元素</th></tr></thead><tbody><tr class="odd"><td align="center">4不在栈顶，4入栈</td><td align="center">1 2 3 4</td><td align="center">4 , 5 3 2 1</td><td align="center">5</td></tr><tr class="even"><td align="center">4出栈</td><td align="center">1 2 3</td><td align="center">5 3 2 1</td><td align="center">5</td></tr><tr class="odd"><td align="center">5不在栈顶，5入栈</td><td align="center">1 2 3 5</td><td align="center">5, 3 2 1</td><td align="center">-</td></tr><tr class="even"><td align="center">5出栈</td><td align="center">1 2 3</td><td align="center">3 2 1</td><td align="center">-</td></tr><tr class="odd"><td align="center">3出栈</td><td align="center">1 2</td><td align="center">2 1</td><td align="center">-</td></tr><tr class="even"><td align="center">2出栈</td><td align="center">1</td><td align="center">1</td><td align="center">-</td></tr><tr class="odd"><td align="center">1出栈</td><td align="center">-</td><td align="center">-</td><td align="center">-</td></tr></tbody></table><p>入栈：<code>1 2 3 4 5</code>， 出栈 <code>4 3 5 1 2</code></p><table><thead><tr class="header"><th align="center">操作</th><th align="center">栈内元素</th><th align="center">剩余出栈元素</th><th align="center">剩余入栈元素</th></tr></thead><tbody><tr class="odd"><td align="center">4不在栈顶，4入栈</td><td align="center">1 2 3 4</td><td align="center">4 , 3 5 1 2</td><td align="center">5</td></tr><tr class="even"><td align="center">4出栈</td><td align="center">1 2 3</td><td align="center">3 5 1 2</td><td align="center">5</td></tr><tr class="odd"><td align="center">3出栈</td><td align="center">1 2</td><td align="center">5 1 2</td><td align="center">5</td></tr><tr class="even"><td align="center">5出栈，不在栈顶， 入栈</td><td align="center">1 2 5</td><td align="center">5, 1 2</td><td align="center"></td></tr><tr class="odd"><td align="center">5出栈</td><td align="center">1 2</td><td align="center">1 2</td><td align="center"></td></tr><tr class="even"><td align="center">1出栈，<strong>不在栈顶</strong>，已经<strong>无可入元素</strong>， <strong>终止</strong></td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p>[关键代码]</p><ol style="list-style-type: decimal"><li>遍历每个出栈元素now</li><li>使其在栈顶，对如栈序列进行入栈，直到now</li><li>如果now依然不在栈顶，则不是</li><li>如果now在栈顶，则出栈</li><li>继续遍历下一个出栈now</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 判断vpop是否是入栈序列vpush的出栈序列</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      vpush -- 入栈序列</span></span><br><span class="line"><span class="comment"> *      vpop -- 要判断的出栈序列</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      true or false</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">is_poporder</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; vpush, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; vpop)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">bool</span> res = <span class="literal">false</span>;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; st;</span><br><span class="line">    <span class="comment">// 入栈的元素</span></span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; vpop.size(); i++) &#123;</span><br><span class="line">        <span class="comment">// 当前要出栈的元素</span></span><br><span class="line">        <span class="keyword">int</span> now = vpop[i];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// now不在栈顶，则从入栈序列中入栈</span></span><br><span class="line">        <span class="keyword">if</span> (st.empty() || st.top() != now) &#123;</span><br><span class="line">            <span class="keyword">while</span> (k &lt; vpush.size()) &#123;</span><br><span class="line">                st.push(vpush[k]);</span><br><span class="line">                <span class="keyword">if</span> (vpush[k] == now) &#123;</span><br><span class="line">                    k++;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                k++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// now依然不在栈顶</span></span><br><span class="line">        <span class="keyword">if</span> (st.empty() || now != st.top()) &#123;</span><br><span class="line">            res =<span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// now 在栈顶，出栈</span></span><br><span class="line">        st.pop();</span><br><span class="line">        <span class="keyword">if</span> (i == vpop.size() - <span class="number">1</span>) &#123;</span><br><span class="line">            res = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="从上到下打印二叉树-32">从上到下打印二叉树-32</h1><p><a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#层次遍历-102">leetcode层次遍历</a></p><p>有3个题</p><p><strong>层次遍历序列</strong></p><p>每次遍历一层</p><p><strong>分行层次遍历打印</strong></p><p>每次打印一层</p><p><strong>z型遍历二叉树</strong></p><p>见<a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#z型打印二叉树-103">leetcode的z型遍历二叉树</a></p><h1 id="二叉搜索树的后序遍历-33">二叉搜索树的后序遍历-33</h1><blockquote><p>给一个数组，判断是否是二叉搜索树的后序遍历</p></blockquote><p>后序遍历：最后一个是根节点</p><p>BST：左 &lt; 根 &lt; 右</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">     <span class="number">8</span></span><br><span class="line">  <span class="number">6</span>    <span class="number">10</span></span><br><span class="line"><span class="number">5</span>  <span class="number">7</span>  <span class="number">9</span>  <span class="number">11</span></span><br></pre></td></tr></table></figure><p>给一个数组：<code>5 7 6  9 11 10  8</code></p><ul><li>根节点是8，</li><li><code>5 7 6</code>前面小于8的，是左子树，<strong>全部小于</strong>8</li><li><code>9 11 10</code>中间大于8的，是右子树，<strong>全部大于</strong>8</li><li>再依次取<strong>判断左右子树是否是BST</strong></li></ul><p><strong>思路</strong></p><ol style="list-style-type: decimal"><li>判断<code>nums[start, end]</code> 是否是BST的后序遍历序列</li><li>空返回false，一个元素返回true。</li><li>找到根节点<code>root = nums[end]</code></li><li>从<code>start</code>开始，<strong>找左子树</strong>的最后一个元素<code>i-1</code>， 直到<code>end-1</code>， 每个元素都<strong>小于根节点</strong></li><li>从<code>i</code>开始， <strong>找右子树</strong>，直到<code>end-1</code>， 每个元素都要<strong>大于根节点</strong>。</li><li>右子树时，前面左子树已经ok（有或者没有），所以<strong>后面的元素都是右子树的</strong>，<strong>都要大于根节点</strong>。</li><li>如果<strong>后面有小于根节点的，则不满足右子树，返回false</strong></li><li><strong>递归判断左右子树是否是BST</strong>，返回结果。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 是否是BST的后序遍历序列</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      nums: 要判断的目标序列，没有重复的数字</span></span><br><span class="line"><span class="comment"> *      start: 序列的起始值</span></span><br><span class="line"><span class="comment"> *      end: 序列的结束位置</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      true or false</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">is_bst_postorder</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;nums, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 不合法</span></span><br><span class="line">    <span class="keyword">if</span> (nums.empty() || start &lt; <span class="number">0</span> || end &gt; nums.size() - <span class="number">1</span> || start &gt; end) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 只有一个元素</span></span><br><span class="line">    <span class="keyword">if</span> (start == end) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> root = nums[end];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 找到左子树</span></span><br><span class="line">    <span class="keyword">int</span> i = start;</span><br><span class="line">    <span class="keyword">for</span> (; i &lt;= end - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums[i] &gt; root) </span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2. 找到右子树，全部都大于root</span></span><br><span class="line">    <span class="keyword">int</span> j = i;</span><br><span class="line">    <span class="keyword">for</span> (; j &lt;= end - <span class="number">1</span>; j++) &#123;</span><br><span class="line">        <span class="comment">// 右子树中有小于root的值，不合法</span></span><br><span class="line">        <span class="keyword">if</span> (nums[i] &lt; root) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 判断左右子树</span></span><br><span class="line">    <span class="keyword">bool</span> left = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span> (i &gt; start) &#123;</span><br><span class="line">        <span class="comment">// 判断右子树</span></span><br><span class="line">        left = is_bst_postorder(nums, start, i - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">bool</span> right = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">if</span> (j &gt; i) &#123;</span><br><span class="line">        <span class="comment">// 判断左子树</span></span><br><span class="line">        right = is_bst_postorder(nums, i, j - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> left &amp;&amp; right;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="二叉树中的路径求和-34">二叉树中的路径求和-34</h1><p><a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#二叉树路径求和">参考leetcode中各种pathsum汇总</a></p><h1 id="复杂链表的复制-35">复杂链表的复制-35</h1><blockquote><p>链表：值，下一个节点，任意一个节点。复制这样的一个链表。</p></blockquote><p><strong>思路1 暴力解法</strong></p><ul><li><strong>先把链表连接起来，不管任意指针</strong></li><li>遍历链表，<strong>为每个新节点找到任意指针</strong>的节点，连接起来</li><li>同时知道<strong>旧节点N、新节点N1</strong>、旧节点的<strong>任意指针节点S</strong></li><li>遍历新链表，<strong>找到任意指针S1</strong></li><li><strong>把N1和S1连接起来</strong></li></ul><p>时间复杂度<span class="math inline">\(O(n^2)\)</span>，遍历新链表，找到任意指针S1的节点。</p><p><strong>思路2 Hash解法</strong></p><p>上面耗时间：<strong>找任意指针S1</strong>。用<strong>HashMap</strong>建立<code>(N, N1)</code>的配对，就<strong>能够<span class="math inline">\(O(1)\)</span>查找到S1</strong>。从而总时间为<span class="math inline">\(O(n)\)</span></p><p><strong>思路3 最优-新旧链表先连再断</strong></p><ul><li><strong>连接新节点</strong>。把N1连接到N后面，<code>a-a1-b-b1-c-c1</code></li><li><strong>设置随机节点</strong>。设置N1的S1，实际上，<code>a-s</code>, <code>a-a1</code>，<code>s-s1</code> ，所以能够找到<code>a1-s1</code></li><li><strong>新旧节点断开</strong>。把N和N1断开，得到<code>a1-b1-c1</code></li></ul><p>注意：设置随机节点时，随机节点可能为空。断链时，下一个节点可能为空。</p><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">RandomListNode* <span class="title">clone</span><span class="params">(RandomListNode* head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (head == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 新建节点，连接到原节点的后面</span></span><br><span class="line">    RandomListNode* p = head;</span><br><span class="line">    <span class="keyword">while</span> (p) &#123;</span><br><span class="line">        RandomListNode* p1 = <span class="keyword">new</span> RandomListNode(p-&gt;label);</span><br><span class="line">        <span class="comment">// 连接</span></span><br><span class="line">        p1-&gt;next = p-&gt;next;</span><br><span class="line">        p-&gt;next = p1;</span><br><span class="line">        p = p1-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 为新节点设置随机节点</span></span><br><span class="line">    p = head;</span><br><span class="line">    <span class="keyword">while</span> (p) &#123;</span><br><span class="line">        RandomListNode* p1 = p-&gt;next;</span><br><span class="line">        RandomListNode* s = p-&gt;random;</span><br><span class="line">        <span class="comment">// 注意random可能为空</span></span><br><span class="line">        <span class="keyword">if</span> (s != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            p1-&gt;random = s-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        p = p1-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 新旧节点断开</span></span><br><span class="line">    p = head;</span><br><span class="line">    RandomListNode* head1 = p-&gt;next;</span><br><span class="line">    <span class="keyword">while</span> (p) &#123;</span><br><span class="line">        <span class="comment">// 新节点</span></span><br><span class="line">        RandomListNode* p1 = p-&gt;next;</span><br><span class="line">        <span class="comment">// 原节点，连接到原下一个节点</span></span><br><span class="line">        p-&gt;next = p1-&gt;next;</span><br><span class="line">        <span class="comment">// 新节点的下一个节点，可能下一个节点为空</span></span><br><span class="line">        <span class="keyword">if</span> (p-&gt;next) &#123;</span><br><span class="line">            p1-&gt;next = p-&gt;next-&gt;next;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            p1-&gt;next = <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="二叉搜索树转双向链表-36">二叉搜索树转双向链表-36</h1><blockquote><p>给一个二叉搜索树，转化为一个排好序的双向链表。左孩子-前一个节点，右孩子-后一个节点。</p></blockquote><p><a href="https://www.nowcoder.com/practice/947f6eb80d944a84850b0538bf0ec3a5?tpId=13&amp;tqId=11179&amp;tPage=2&amp;rp=2&amp;ru=%2Fta%2Fcoding-interviews&amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking" target="_blank" rel="noopener">牛客网二叉搜索树与双向链表</a> ， 类似题型：<a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#有序链表转平衡bst-109">有序链表转平衡BST</a></p><p>BST的<a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#中序遍历-94">中序遍历</a>就是自动有序的。</p><p><strong>递归思路</strong></p><p>中序遍历，使用<code>pre</code>来记录链表中的最后一个元素。</p><ul><li>遍历到根节点时，递归转换左子树</li><li>pre与root连接，<code>pre.right=root</code>, <code>root.left=pre</code>, <code>pre=root</code> 。注意pre为空时，<code>pre=root</code></li><li>递归创建右子树</li></ul><p>再从pre找到第一个节点。</p><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 把树转化为链表，递归版本</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      root -- 树</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      转换后的链表的头结点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">TreeNode* <span class="title">convert</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    TreeNode* pre = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="comment">// 转换为链表，pre为最后一个节点</span></span><br><span class="line">    convert_inorder(root, pre);</span><br><span class="line">    <span class="comment">// 从末尾找到第一个节点</span></span><br><span class="line">    <span class="keyword">while</span> (pre &amp;&amp; pre-&gt;left) &#123;</span><br><span class="line">        pre = pre-&gt;left;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pre;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 递归转换</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      root -- 当前节点</span></span><br><span class="line"><span class="comment"> *      pre -- 上一个节点，引用类型，改变值。</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      None</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">convert_inorder</span><span class="params">(TreeNode* root, TreeNode* &amp;pre)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 左子树</span></span><br><span class="line">    convert_inorder(root-&gt;left, pre);</span><br><span class="line">    <span class="comment">// 当前节点</span></span><br><span class="line">    <span class="keyword">if</span> (pre != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        root-&gt;left = pre;</span><br><span class="line">        pre-&gt;right = root;</span><br><span class="line">        pre = root;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        pre = root;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 右子树</span></span><br><span class="line">    convert_inorder(root-&gt;right, pre);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>非递归思路</strong></p><p>中序遍历时，记录<code>pre</code>节点，每次进行修改即可，访问到p时，则连接到pre即可。</p><ul><li>p不为空，p入栈，<code>p=p.left</code>， <strong>扫描左孩子</strong></li><li>p为空，p从栈顶获得，<code>p=st.top()</code>， 显然p没有左孩子或者左孩子已经出栈遍历过，<strong>p访问出栈</strong>。此时，<strong>把p与pre连接即可</strong>， 注意pre为空</li><li><strong>扫描右孩子</strong>，<code>p = p.right</code></li></ul><p>最后从末尾，找到头结点</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TreeNode* <span class="title">convert_stack</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">stack</span>&lt;TreeNode*&gt; st;</span><br><span class="line">    TreeNode* p = root;</span><br><span class="line">    <span class="comment">// 上一个节点</span></span><br><span class="line">    TreeNode* pre = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">while</span> (p || !st.empty()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (p) &#123;</span><br><span class="line">            <span class="comment">// p入栈</span></span><br><span class="line">            st.push(p);</span><br><span class="line">            <span class="comment">// 扫描左孩子</span></span><br><span class="line">            p = p-&gt;left;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// p为空，出栈元素，p为根节点，左孩子已经访问结束或者没有左孩子</span></span><br><span class="line">            p = st.top();</span><br><span class="line">            st.pop();</span><br><span class="line">            <span class="keyword">if</span> (pre == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                pre = p;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                pre-&gt;right = p;</span><br><span class="line">                p-&gt;left = pre;</span><br><span class="line">                pre = p;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 扫描右孩子</span></span><br><span class="line">            p = p-&gt;right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 找到头结点</span></span><br><span class="line">    <span class="keyword">while</span> (pre &amp;&amp; pre-&gt;left) &#123;</span><br><span class="line">        pre = pre-&gt;left;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pre;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="序列化和反序列化二叉树-37">序列化和反序列化二叉树-37</h1><p><a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#序列化二叉树-297">leetcode序列化和反序列化二叉树笔记</a></p><h1 id="数字全排列-38">数字全排列-38</h1><p><a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#数字全排列-046">leetcode数字全排列笔记</a></p><h1 id="数组中出现次数超过一半的数-39">数组中出现次数超过一半的数-39</h1><p><a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#数组中次数超过一半的数-169">Leetcode笔记</a></p><h1 id="最小的k个数-40">最小的k个数-40</h1><p><a href="https://www.nowcoder.net/practice/6a296eb82cf844ca8539b57c23e6e9bf?tpId=13&amp;tqId=11182&amp;rp=2&amp;ru=%2Fta%2Fcoding-interviews&amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking" target="_blank" rel="noopener">牛客最小的k个数</a></p><blockquote><p>给一个数组，找到最小的k个数</p></blockquote><p><strong>思路1 快排思路</strong></p><p>修改原数组。partition左边即可。<code>O(n)</code> 。<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#快速排序">快速排序</a> 。<a href="https://github.com/plmsmile/aim2offer/blob/master/40_min_kth_nums/min_k_nums.cpp" target="_blank" rel="noopener">关键代码</a> 如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 通过快速排序来找到最小的k个数，改变原数组</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a -- 数组</span></span><br><span class="line"><span class="comment"> *      k -- 最小的k个数</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      res -- 最小的k个数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; get_leastnums_by_partition(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> k) &#123;</span><br><span class="line">    <span class="comment">// 条件合法性判断</span></span><br><span class="line">    <span class="keyword">if</span> (a.empty() || k &lt; <span class="number">1</span> || k &gt; a.size()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">0</span>, r = a.size() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> i = partition(a, l, r);</span><br><span class="line">    <span class="comment">// 直到左边是最小的k个数，包含a[i]</span></span><br><span class="line">    <span class="keyword">while</span> (i + <span class="number">1</span> != k) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span> &gt; k) &#123;</span><br><span class="line">            <span class="comment">// 左边有超过k个数，左边继续划分</span></span><br><span class="line">            r = i - <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i + <span class="number">1</span> &lt; k) &#123;</span><br><span class="line">            <span class="comment">// 左边不足k个，划分右边，加一些给左边</span></span><br><span class="line">            l = i + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        i = partition(a, l, r);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 把左边给到res中</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(k);</span><br><span class="line">    <span class="built_in">std</span>::copy(a.begin(), a.begin() + k, res.begin());</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 快排的partition，左边小于，中间x，右边大于</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a -- 数组</span></span><br><span class="line"><span class="comment"> *      l -- 左边起始值</span></span><br><span class="line"><span class="comment"> *      r -- 右边结束值</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      a[l]的最终位置</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = a[l];</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r) &#123;</span><br><span class="line">        <span class="comment">// 从右向左，找到小于x的值，放到a[l]上</span></span><br><span class="line">        <span class="keyword">while</span> (l &lt; r &amp;&amp; a[r] &gt;= x) &#123;</span><br><span class="line">            r--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (l &lt; r) &#123;</span><br><span class="line">            a[l++] = a[r];</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 从左向右，找到大于x的值，放到a[r]上</span></span><br><span class="line">        <span class="keyword">while</span> (l &lt; r &amp;&amp; a[l] &lt;= x) &#123;</span><br><span class="line">            l++;</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">if</span> (l &lt; r) &#123;</span><br><span class="line">            a[r--] = a[l];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    a[l] = x;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>思路2 最大堆</strong></p><p>不修改原数组，用最大堆找到最小的k个数。<code>O(nlogk)</code> 。<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#堆排序">自己的堆操作</a>和<a href="https://github.com/plmsmile/aim2offer/blob/master/00_0_cpp_api/stl_test.cpp" target="_blank" rel="noopener">stl堆操作</a>。 <a href="https://github.com/plmsmile/aim2offer/blob/master/40_min_kth_nums/min_k_nums.cpp" target="_blank" rel="noopener">关键代码</a> 如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 通过最大堆来获得数组中最小的k个数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; get_leastknums_by_heap(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> k) &#123;</span><br><span class="line">    <span class="keyword">if</span> (a.empty() || k &lt;= <span class="number">0</span> || k &gt; a.size()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 选择前k个元素，初始化堆</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(k);</span><br><span class="line">    <span class="built_in">std</span>::copy(a.begin(), a.begin() + k, res.begin());</span><br><span class="line">    <span class="built_in">std</span>::make_heap(res.begin(), res.end());</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 剩余元素入堆</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> it = a.begin() + k; it != a.end(); it++) &#123;</span><br><span class="line">        <span class="keyword">auto</span> n = *it;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"n=%d, max=%d\n"</span>, n, res[<span class="number">0</span>]);</span><br><span class="line">        <span class="comment">// 大于最大值，无需入堆</span></span><br><span class="line">        <span class="keyword">if</span> (n &gt;= res[<span class="number">0</span>]) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// n小于最大堆的最大值，入堆</span></span><br><span class="line">        <span class="comment">// 最大元素出堆，默认放到末尾</span></span><br><span class="line">        <span class="built_in">std</span>::pop_heap(res.begin(), res.end());</span><br><span class="line">        <span class="comment">// 新元素入堆</span></span><br><span class="line">        res[k - <span class="number">1</span>] = n;</span><br><span class="line">        <span class="built_in">std</span>::push_heap(res.begin(), res.end());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="数据流中的中位数-41">数据流中的中位数-41</h1><blockquote><p>给一个数据流，找到数组排序之后的中间的数值。奇数，中间；偶数，中间两个数的平均值。</p></blockquote><p><a href="https://www.nowcoder.net/practice/9be0172896bd43948f8a32fb954e1be1?tpId=13&amp;tqId=11216&amp;rp=4" target="_blank" rel="noopener">牛客网数据流中的中位数</a></p><table><thead><tr class="header"><th align="center">数据结构</th><th align="center">插入时间</th><th align="center">获取中位数时间</th><th align="center">方法备注</th></tr></thead><tbody><tr class="odd"><td align="center">未排序数组</td><td align="center">O(1)</td><td align="center">O(n)</td><td align="center"><strong>快排partition方法</strong></td></tr><tr class="even"><td align="center">排序数组</td><td align="center">O(n)</td><td align="center">O(1)</td><td align="center"></td></tr><tr class="odd"><td align="center">排序链表</td><td align="center">O(n)</td><td align="center">O(1)</td><td align="center">两个指针指向中间节点</td></tr><tr class="even"><td align="center">二叉搜索树</td><td align="center">平均O(logn)，最差O(n)</td><td align="center">平均O(logn)，最差O(n)</td><td align="center"></td></tr><tr class="odd"><td align="center">AVL树</td><td align="center">O(logn)</td><td align="center">O(1)</td><td align="center"></td></tr><tr class="even"><td align="center">最大堆和最小堆</td><td align="center">O(logn)</td><td align="center">O(1)</td><td align="center"></td></tr></tbody></table><p><strong>堆思路</strong></p><p>两个容器（最大堆和最小堆），数据量<strong>平均分配</strong>，<strong>左边全部小于右边</strong>，左右两边无需排好序， 根据<strong>左边最大的</strong>，<strong>右边最小的</strong> 来找到<code>中位数</code>。</p><p>总体思路</p><ul><li>左边小，右边大。左边最大堆max，右边最小堆min。</li><li>先放右边，后方左边。</li><li>奇数：右边<code>min[0]</code>； 偶数：求平均<code>(max[0] + min[0]) / 2</code></li></ul><p>插入右边</p><ul><li>索引为偶数时，放右边。从0开始</li><li>num较大，直接插入右边min堆</li><li>num较小，插入左边max堆，再把max插入右边min堆。<code>num &lt;  左边max[0]</code></li></ul><p>插入左边</p><ul><li>索引为奇数时，放左边min堆。</li><li>num较小，直接插入左边min堆</li><li>num较大，插入右边min堆，再把min插入右边max堆。<code>num &gt; 右边min[0]</code></li></ul><p><a href="https://github.com/plmsmile/aim2offer/blob/master/41_stream_medium/stream_medium.cpp" target="_blank" rel="noopener">Githubd代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// 位于左边的最大堆</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; max;</span><br><span class="line">    <span class="comment">// 位于右边的最小堆</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; min;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序号是偶数时，写入右边的最小堆</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">insert2min</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// num较小，写到左边的最大堆</span></span><br><span class="line">        <span class="keyword">if</span> (max.size() &gt; <span class="number">0</span> &amp;&amp; num &lt; max[<span class="number">0</span>]) &#123;</span><br><span class="line">            max.push_back(num);</span><br><span class="line">            <span class="comment">// num入堆</span></span><br><span class="line">            <span class="built_in">std</span>::push_heap(max.begin(), max.end(), less&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">            <span class="comment">// max出堆</span></span><br><span class="line">            <span class="built_in">std</span>::pop_heap(max.begin(), max.end(), less&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">            num = max.back();</span><br><span class="line">            max.pop_back();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// num写入最小堆</span></span><br><span class="line">        min.push_back(num);</span><br><span class="line">        <span class="built_in">std</span>::push_heap(min.begin(), min.end(), greater&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序号是奇数时，写入左边的最大堆</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">insert2max</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// num较大，先写到右边的最小堆</span></span><br><span class="line">        <span class="keyword">if</span> (min.size() &gt; <span class="number">0</span> &amp;&amp; num &gt; min[<span class="number">0</span>]) &#123;</span><br><span class="line">            min.push_back(num);</span><br><span class="line">            <span class="comment">// num 入最小堆</span></span><br><span class="line">            <span class="built_in">std</span>::push_heap(min.begin(), min.end(), greater&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">            <span class="comment">// min 出堆</span></span><br><span class="line">            <span class="built_in">std</span>::pop_heap(min.begin(), min.end(), greater&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">            num = min.back();</span><br><span class="line">            min.pop_back();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// num写入最大堆</span></span><br><span class="line">        max.push_back(num);</span><br><span class="line">        <span class="built_in">std</span>::push_heap(max.begin(), max.end(), less&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 写入一个元素，先右后左，右边比左边多一个或相等</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Insert</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> idx = max.size() + min.size();</span><br><span class="line">        <span class="comment">// 奇数，写入左边的最大堆</span></span><br><span class="line">        <span class="keyword">if</span> ((idx &amp; <span class="number">1</span>) == <span class="number">1</span>) &#123;</span><br><span class="line">            insert2max(num);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            insert2min(num);</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/*</span></span><br><span class="line"><span class="comment">   * 获取中间元素，奇数在右边min[0]，偶数求平均</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">GetMedian</span><span class="params">()</span> </span>&#123; </span><br><span class="line">        <span class="keyword">int</span> size = min.size() + max.size();</span><br><span class="line">        <span class="keyword">if</span> (size == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (size &amp; <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> min[<span class="number">0</span>];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> (min[<span class="number">0</span>] + max[<span class="number">0</span>]) / <span class="number">2.0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h1 id="连续子数组的最大和-42">连续子数组的最大和-42</h1><blockquote><p>给一个数组，有正数有负数。求所有连续子数组和的最大值。O(n)</p></blockquote><p><a href="https://www.nowcoder.net/practice/459bd355da1549fa8a49e350bf3df484?tpId=13&amp;tqId=11183&amp;rp=2&amp;ru=%2Fta%2Fcoding-interviews&amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking" target="_blank" rel="noopener">牛客网连续子数组的最大和</a> ，<a href="https://github.com/plmsmile/aim2offer/blob/master/42_subarray_maxsum/maxsum_subarray.cpp" target="_blank" rel="noopener">github代码</a></p><p><strong>思路1 累加切换思路</strong></p><p>当前累加和，最大累加和。遍历遇到数n</p><ul><li>如果<code>cursum &lt;= 0</code>， 则<code>cursum = n</code>， 否则 <code>cursum += n</code></li><li>如果<code>cursum &gt; bestsum</code>， 则<code>bestsum = cursum</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxsum_subarray</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> cursum = a[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> bestsum = a[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (cursum &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            cursum = a[i];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            cursum += a[i];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (cursum &gt; bestsum) &#123;</span><br><span class="line">            bestsum = cursum;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> bestsum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>思路2 动态规划</strong></p><p>设<code>f[i]</code>表示以<span class="math inline">\(a_i\)</span> 结尾的子数组的最大和，我们要求<code>max[fi]</code> <span class="math display">\[f(i) = \begin{cases}&amp; a_i, &amp;  f(i-1) \le 0 \quad or \quad i = 0\\&amp; f(i-1) + a_i, &amp; f(i-1) &gt; 0 \quad and \quad i \ne 0  \\\end{cases}\]</span></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxsum_subarray_dp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// p[i]=k，以i结尾的所有连续子数组中的最大值为k</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; p(a.size(), <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">0</span> || p[i<span class="number">-1</span>] &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">            p[i] = a[i];</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (p[i<span class="number">-1</span>] &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            p[i] = p[i<span class="number">-1</span>] + a[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> *<span class="built_in">std</span>::max_element(p.begin(), p.end());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="n整数中1出现的次数-43">1-n整数中1出现的次数-43</h1><blockquote><p>输入一个n，求问1-n数字中，1出现的总个数。</p></blockquote><p><a href="https://www.nowcoder.net/practice/bd7f978302044eee894445e244c7eee6?tpId=13&amp;tqId=11184&amp;rp=2&amp;ru=%2Fta%2Fcoding-interviews&amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking" target="_blank" rel="noopener">牛客网1-n整数中1出现的次数</a> ，<a href="https://github.com/plmsmile/aim2offer/blob/master/43_count_1/numberof1.cpp" target="_blank" rel="noopener">github源码</a></p><p><strong>思路1 暴力无offer思路</strong></p><p>用个count计数，遍历所有的数字，求每一个数字的1的个数，再求和。<code>O(nlogn)</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 求余计算数字n中1的个数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">numberof1</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (n) &#123;</span><br><span class="line">        <span class="keyword">if</span> (n % <span class="number">10</span> == <span class="number">1</span>) &#123;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">        n = n / <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>思路2 分治递归思路</strong></p><p>例子，找到1-21345中1的个数。分为两段：<code>1346-21345</code>（当前解），<code>1-1345</code> （递归解）</p><p>思路总结</p><ol style="list-style-type: decimal"><li>21345，先求位数5和最高位2，</li><li>1346-21345，计算<strong>最高位为1</strong>的数量<code>h1</code>。<code>h &gt;= 2</code>时， <span class="math inline">\(10^4\)</span>。 <code>h==1</code>时， 做减法。</li><li>1346-21345，计算<strong>其余位为1</strong>的数量<code>o1</code>。选1位为1，剩余位0-9任选，<strong>排列组合</strong>。<span class="math inline">\(4\cdot10^3 \cdot 2\)</span></li><li><strong>递归计算</strong>1-1345中1的个数<code>r1</code></li><li>返回<code>h1+o1+r1</code></li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 递归计算从1-n中1的个数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">numberof1_between1andn</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">9</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> n &gt;= <span class="number">1</span> ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 以21345为例，分为1346-21345(本次求解)和1-1345(递归求解)两段</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 计算位数和最高位</span></span><br><span class="line">    <span class="built_in">string</span> ns = <span class="built_in">std</span>::to_string(n);</span><br><span class="line">    <span class="keyword">int</span> len = ns.size();</span><br><span class="line">    <span class="keyword">int</span> h = ns[<span class="number">0</span>] - <span class="string">'0'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 计算最高位为1的数量</span></span><br><span class="line">    <span class="keyword">int</span> h1 = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (h &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">        h1 = <span class="built_in">std</span>::<span class="built_in">pow</span>(<span class="number">10</span>, len<span class="number">-1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        h1 = n - <span class="built_in">pow</span>(<span class="number">10</span>, len<span class="number">-1</span>) + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 计算其他位为1的个数，选1位为1，其余位0-9任选</span></span><br><span class="line">    <span class="keyword">int</span> o1 = h * (len<span class="number">-1</span>) * <span class="built_in">pow</span>(<span class="number">10</span>, len<span class="number">-2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 递归求1-1346中1的位数</span></span><br><span class="line">    <span class="keyword">int</span> p = h * <span class="built_in">pow</span>(<span class="number">10</span>, len<span class="number">-1</span>);</span><br><span class="line">    <span class="keyword">int</span> r = n - h * <span class="built_in">pow</span>(<span class="number">10</span>, len<span class="number">-1</span>);</span><br><span class="line">    <span class="keyword">int</span> r1 = numberof1_between1andn(r);</span><br><span class="line">    <span class="keyword">return</span> h1 + o1 + r1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="数字序列中某一位的数字-44">数字序列中某一位的数字-44</h1><blockquote><p>数字序列：01234567891011121314.... 给位数，返回该位置上的数字。</p></blockquote><p><strong>例子</strong>， 第1001位</p><ul><li>1位数，10个，不在此，找后面的1001-10=991位</li><li>2位数，180个，不在此，找后面的991-180=881位</li><li>3位数，2700个，在此。881/3=270，881%3=1。即，从100开始的第270个数中的第2位。即370中的第2位7</li></ul><p><a href="https://github.com/plmsmile/aim2offer/blob/master/44_digit_in_sequence/digit_sequence.cpp" target="_blank" rel="noopener">github代码</a></p><p>注意每次递减index时，减的是位数*总数量，即<code>index -= numcount * digits</code></p><p>在某个d位数中时，先算出d位数，再算出在d位数字内的<code>res_index</code>，反正是<strong>除法求余</strong>。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 数字流012345...某一位的数字</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      index -- 数字序列中索引，从0开始</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      res -- 某一位的数字 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">digit_in_sequence</span><span class="params">(<span class="keyword">int</span> index)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (index &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 位数</span></span><br><span class="line">    <span class="keyword">int</span> digits = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">// d位数的总数量</span></span><br><span class="line">        <span class="keyword">int</span> numcount = count_num(digits);</span><br><span class="line">        <span class="keyword">if</span> (index &lt;= numcount * digits - <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="comment">// index在某个d位数中</span></span><br><span class="line">            <span class="comment">// d位数的某个数的index</span></span><br><span class="line">            <span class="keyword">int</span> num_index = index / digits;</span><br><span class="line">            <span class="comment">// 数字内的某位数</span></span><br><span class="line">            <span class="keyword">int</span> res_index = index % digits;</span><br><span class="line">            <span class="comment">// 目标d位数</span></span><br><span class="line">            <span class="keyword">int</span> num = num_in_digits(num_index, digits);</span><br><span class="line">            res = to_string(num)[res_index] - <span class="string">'0'</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// index在某个d+1位数中</span></span><br><span class="line">            index -= digits * numcount;</span><br><span class="line">            digits++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 获得几位数的所有数字数量</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      digits -- 位数</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      count -- 该位数所有数字的数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">count_num</span><span class="params">(<span class="keyword">int</span> digits)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (digits &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (digits == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 加0.5是这个版本的编译器pow的问题</span></span><br><span class="line">    <span class="keyword">int</span> count = (<span class="keyword">int</span>) (<span class="built_in">std</span>::<span class="built_in">pow</span>(<span class="number">10</span>, digits - <span class="number">1</span>) + <span class="number">0.5</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">9</span> * count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 第1个d位数 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">first_d_num</span><span class="params">(<span class="keyword">int</span> digits)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (digits == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">int</span>) (<span class="built_in">pow</span>(<span class="number">10</span>, digits - <span class="number">1</span>) + <span class="number">0.5</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * d位数中的第index个数</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      index -- 从0开始</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      num -- d位数中的第index个数字 </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">num_in_digits</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">int</span> digits)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> first = first_d_num(digits);</span><br><span class="line">    <span class="keyword">return</span> first + index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="把数组排成最小的数-45">把数组排成最小的数-45</h1><blockquote><p>给一个正整数数组，把它们拼接起来，求得一个最小的数字。</p></blockquote><p><a href="https://www.nowcoder.net/practice/8fecd3f8ba334add803bf2a06af1b993?tpId=13&amp;tqId=11185&amp;rp=2&amp;ru=%2Fta%2Fcoding-interviews&amp;qru=%2Fta%2Fcoding-interviews%2Fquestion-ranking" target="_blank" rel="noopener">牛客网排成最小的数</a></p><p><strong>1 全排列思路</strong></p><p>实际上可以获得所有的排列方法，然后选择最小的。但是这样会比较慢。一共<code>n!</code>个排列组合。参考<a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#数字全排列-046">全排列</a>。</p><p><strong>2 排序思路</strong></p><p>因为实际上最终是，<strong>按照一个规则把所有数字排列起来</strong>。</p><ul><li>把所有数字转换成字符串</li><li>定义一个<code>compare</code>方法，判断哪个数字应该排列在前面。<strong>字符串本身的比较</strong>即可。</li><li>对所有数字排序，使用<code>sort(nums.begin(), nums.end(), cmp)</code></li><li>按照排好序的数字来拼接到一个字符串</li></ul><p>注意：隐藏的大数问题。用<strong>字符串</strong>来保存。</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/45_array_to_minnum/array2minnum.cpp" target="_blank" rel="noopener">github源代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 把数组排列成最小的数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">array2minnum</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; nums;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> n: a) &#123;</span><br><span class="line">        nums.push_back(to_string(n));</span><br><span class="line">    &#125;</span><br><span class="line">    sort(nums.begin(), nums.end(), cmp);</span><br><span class="line">    <span class="built_in">ostringstream</span> res;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> s:nums) &#123;</span><br><span class="line">        res &lt;&lt; s;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res.str(); </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 用于从小到大排序的比较函数。n1&lt;n2，返回true</span></span><br><span class="line"><span class="comment"> * 小于的意义：谁小谁在前。</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      n1, n2 -- 两个数字，由字符串表示</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      true or false</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; n1, <span class="keyword">const</span> <span class="built_in">string</span>&amp; n2)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> n1n2 = n1 + n2;</span><br><span class="line">    <span class="built_in">string</span> n2n1 = n2 + n1;</span><br><span class="line">    <span class="keyword">if</span> (n1n2 &lt; n2n1) &#123;</span><br><span class="line">        <span class="comment">// n1 &lt; n2，n1应该在前面</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="把数字翻译成字符串-46">把数字翻译成字符串-46</h1><blockquote><p>基本规则：0-a, 1-b, 2-c, ... , 25-z。一个数字(可能是多位数)有多种翻译的可能。给一个数字字符串，求出有多少种翻译方法。比如12258有5种翻译方法。bccfi,bwfi,bczi,mcfi,mzi</p></blockquote><p><strong>1 递归计算</strong></p><p>1 - 2258, 12 - 258 。第一个数字有2种翻译方法，再递归地去翻译剩下的部分。</p><p>由于<strong>存在重复的子问题</strong>，所以递归不好。从上到下计算也不好。我们要用从<strong>最小的子问题自下而上</strong>解决问题</p><p>从上到下思考，自下而上循环计算。</p><p>设<code>c[i]</code> <strong>是<code>a[i]</code>开始到末尾的所有翻译种类的数量</strong>。</p><ul><li>每个a[i]都能单独翻译，<code>c[i] = c[i+1]</code></li><li>如果<code>a[i]a[i+1]</code>可以一起翻译，则<code>c[i] += c[i+2]</code></li></ul><p><span class="math display">\[f(i) = \begin{cases}f(i+1) + f(i+2), &amp;a_ia_{i+1} \text{可以一起翻译}\\f(i+1),&amp;a_i \text{只能单独翻译}\\\end{cases}\]</span> <a href="https://github.com/plmsmile/aim2offer/blob/master/46_num2letter/num_trans_letter.cpp" target="_blank" rel="noopener">github代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 把数字翻译成字母，找到有多少种翻译方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">get_trans_count</span><span class="params">(<span class="keyword">int</span> number)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> num = to_string(number);</span><br><span class="line">    <span class="keyword">if</span> (number &lt; <span class="number">0</span> || num.empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (num.length() == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// c[i]=k，i~n的翻译法有k种</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; c(num.size());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = c.size() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="comment">// 0. 计算a[i]能否和a[i+1]一起翻译</span></span><br><span class="line">        <span class="keyword">bool</span> flag = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span> &lt; c.size()) &#123;</span><br><span class="line">            <span class="keyword">int</span> now = num[i] - <span class="string">'0'</span>;</span><br><span class="line">            <span class="keyword">int</span> back = num[i+<span class="number">1</span>] - <span class="string">'0'</span>;</span><br><span class="line">            <span class="keyword">int</span> sum = now * <span class="number">10</span> + back;</span><br><span class="line">            <span class="keyword">if</span> (sum &gt;= <span class="number">10</span> &amp;&amp; sum &lt;= <span class="number">25</span>) &#123;</span><br><span class="line">                <span class="comment">// a[i]与a[i+1]一起翻译</span></span><br><span class="line">                flag = <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 每个a[i]都可以单独翻译</span></span><br><span class="line">        <span class="keyword">if</span> (i == c.size() - <span class="number">1</span>) &#123;</span><br><span class="line">            c[i] = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            c[i] = c[i+<span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. a[i]a[i+1]一起翻译，则与c[i+2]有关</span></span><br><span class="line">        <span class="keyword">if</span> (flag == <span class="literal">true</span>) &#123;</span><br><span class="line">           <span class="keyword">if</span> (i+<span class="number">2</span> &lt; c.size()) &#123;</span><br><span class="line">               c[i] += c[i+<span class="number">2</span>];</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               c[i] += <span class="number">1</span>;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回0-n的翻译数量</span></span><br><span class="line">    <span class="keyword">return</span> c[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="礼物的最大价值-47">礼物的最大价值-47</h1><blockquote><p>m*n的棋盘，每个格子上有一个礼物，每个礼物都有一个价值。从左上角开始走到右下角。每次只能向下-向右走。走一个格子，拿一个礼物。问，最大能拿多大的价值。</p></blockquote><p>设<code>v[i,j]</code>是格子ij上的价值，设<code>m[i,j]</code> 是到达ij后的总路线最大价值。只能从上面或左边到达。 <span class="math display">\[m[i, j] = \max(m[i-1,j], m[i, j-1]) +v[i, j]\]</span> 一行一行、一列一列的计算。初始化left和up为0，如果有，则赋值。计算<code>m[i,j] = max(left, up)+v[i,j]</code></p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/47_max_gift_value/max_gift_value.cpp" target="_blank" rel="noopener">github源码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 给一个礼物价值矩阵，找到最大价值 </span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      v -- 各个位置上的礼物矩阵</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      res -- (0,0)-(m,n)的最大礼物价值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">max_gifts_value</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; &amp;v)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (v.empty() || v[<span class="number">0</span>].empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; m;</span><br><span class="line">    <span class="keyword">int</span> row = v.size();</span><br><span class="line">    <span class="keyword">int</span> col = v[<span class="number">0</span>].size();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化为0</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; row; i++) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; cur_row(col, <span class="number">0</span>);</span><br><span class="line">        m.push_back(cur_row);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一行一行地计算</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; row; i++) &#123;</span><br><span class="line">        <span class="comment">// 一列一列地算</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; col; j++) &#123;</span><br><span class="line">            <span class="comment">// 从上边和左边来的</span></span><br><span class="line">            <span class="keyword">int</span> up = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                up = m[i<span class="number">-1</span>][j];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (j &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                left = m[i][j<span class="number">-1</span>];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 选大的+v</span></span><br><span class="line">            m[i][j] = <span class="built_in">std</span>::max(up, left) + v[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> m[row<span class="number">-1</span>][col<span class="number">-1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="最长不重复的子字符串-48">最长不重复的子字符串-48</h1><blockquote><p>给一个字符串，找到里面最长子字符串的长度。</p></blockquote><p><a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#最长不重复子字符串-003">leetcode最长子字符串笔记</a></p><h1 id="第n个丑数-49">第n个丑数-49</h1><p><a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#丑数-263-264-313">leetcode丑数笔记</a></p><h1 id="第一个只出现一次的字符-50">第一个只出现一次的字符-50</h1><blockquote><p>找到字符串中，第一个只出现一次的字符</p></blockquote><p>用空间换时间，<code>HashMap</code>， 统计出现次数；再遍历字符串，找出现次数为1的字符。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 字符串中出现次数为1的字符</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">char</span> <span class="title">first_char</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (s.empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'\0'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 每个字符的出现次数</span></span><br><span class="line">    <span class="built_in">map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; c;</span><br><span class="line">    <span class="comment">// 计算</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> ch:s) &#123;</span><br><span class="line">        <span class="keyword">if</span> (c.count(ch) == <span class="number">0</span>) &#123;</span><br><span class="line">            c[ch] = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            c[ch] += <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 遍历找</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> ch : s) &#123;</span><br><span class="line">        <span class="keyword">if</span> (c[ch] == <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ch;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'\0'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>字符流中出现的第一个不重复字符</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>&#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// char出现次数</span></span><br><span class="line">    <span class="built_in">map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt;m;</span><br><span class="line">    <span class="comment">// 保留字符串</span></span><br><span class="line">    <span class="built_in">string</span> s;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 写入一个字符</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">char</span> ch)</span> </span>&#123;</span><br><span class="line">        s += ch;</span><br><span class="line">        <span class="keyword">if</span> (m.count(ch) == <span class="number">0</span>) &#123;</span><br><span class="line">            m[ch] = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            m[ch] += <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 找到第一个出现一次的字符</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">char</span> <span class="title">first_char</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">char</span> ch = <span class="string">'#'</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> c:s) &#123;</span><br><span class="line">            <span class="keyword">if</span> (m[c] == <span class="number">1</span>) &#123;</span><br><span class="line">                ch = c;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ch;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;把奇数放在偶数的前面-21&quot;&gt;把奇数放在偶数的前面-21&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：一个数组，乱序，有奇数和偶数&lt;/p&gt;
&lt;p&gt;输出：把所有的奇数放在前面，所有的偶数放在后面&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;
      
    
    </summary>
    
      <category term="leetcode" scheme="http://plmsmile.github.io/categories/leetcode/"/>
    
    
      <category term="leetcode" scheme="http://plmsmile.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>dfs</title>
    <link href="http://plmsmile.github.io/2017/12/31/algorithm-dfs/"/>
    <id>http://plmsmile.github.io/2017/12/31/algorithm-dfs/</id>
    <published>2017-12-31T07:20:28.000Z</published>
    <updated>2018-01-07T04:49:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="搜索算法">搜索算法</h1><h2 id="解空间树">解空间树</h2><p><strong>子集树</strong></p><p>比如n种可选物品的01背包，对其进行选择。解空间为长度为n的01向量表示。解空间树是一颗完全二叉树。</p><p><strong>排列树</strong></p><p>比如旅行商问题，对n进行排列。</p><h2 id="解空间搜索方式">解空间搜索方式</h2><p><strong>DFS</strong></p><p>深度优先。<code>回溯法</code>。</p><p><strong>BFS</strong></p><p>广度优先或最小耗费优先。<code>分支限界法</code>。</p><p>在扩展节点处，先生成所有的儿子节点，再从活节点列表中选择一个最有利的节点作为新的扩展节点。</p><h2 id="回溯法">回溯法</h2><p>就是<strong>穷举</strong>，<strong>深度优先</strong>去找到解空间中满足约束条件的所有解，再选择一个最好的出来。</p><p>类似于走迷宫：走到死路或者已经达不到最优解就<strong>及时回头</strong>。要注意<strong>剪枝</strong>。</p><p><strong>搜索子集树</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> t)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (t &gt; n) &#123;</span><br><span class="line">    output(x);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 0和1两种选择</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">1</span>; i++) &#123;</span><br><span class="line">    x[t] = i;</span><br><span class="line">    <span class="keyword">if</span> (legal(t)) &#123;</span><br><span class="line">      bfs(t+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>搜索排列树</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> t)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (t &gt; n) &#123;</span><br><span class="line">    output(x);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 遍历所有t的可能：还剩下的可选择的内容，前面t-1已经确定了</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = t; i &lt;= n; i++) &#123;</span><br><span class="line">    swap(x[t], x[i]);</span><br><span class="line">    <span class="keyword">if</span> (legal(t)) &#123;</span><br><span class="line">      bfs(t + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    swap(x[t], x[i]);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="分支限界法">分支限界法</h2><p><strong>广度优先</strong>或者<strong>最小消费优先</strong>去搜索解空间树，去找到使目标函数达到极大或者极小解，某种意义下的最优解。</p><p>每个活节点只有一次机会成为扩展节点，成为时，就一次性产生其所有儿子节点。舍弃不可行或导致非最优解的儿子节点，其余儿子节点加入活节点列表。从活节点列表中取出下一个节点作为新的扩展节点。</p><p><strong>广度优先搜索</strong></p><p>队列式分支限界法</p><p><strong>最小消费优先搜索</strong></p><p>优先队列式分支限界法。每个活节点有一个优先级。通常用最大堆来实现最大优先队列。</p><h1 id="装载问题">装载问题</h1><h2 id="问题描述">问题描述</h2><p><span class="math inline">\(n\)</span>个集装箱质量分别为<span class="math inline">\(w_i\)</span>， 要装上总容量为<span class="math inline">\(c\)</span>的轮船。 问，怎样装，才能使装得最多？ <span class="math display">\[\begin{align}&amp; \max \sum_{i}^nx_i\cdot w_i \\&amp;  \sum_{i}^nx_i\cdot w_i \le c \\&amp; x_i \in\{0,1\}, \quad \text{表示装或不装}\end{align}\]</span></p><h2 id="贪心解法">贪心解法</h2><p>贪心策略：集装箱从轻到重排序，<strong>轻者先装</strong>， 直到超重。</p><h2 id="bfs">BFS</h2><p>有2艘轮船，<span class="math inline">\(\sum_i^n w_i \le c_1 + c_2\)</span>， 怎样才能把n个集装箱装到这2艘轮船。</p><p>可知最优方案：<strong>尽量把第一艘轮船装满</strong>，剩余的装到第二艘上。</p><p>定义Solution</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="comment">// 集装箱数量</span></span><br><span class="line">        <span class="keyword">int</span> n;</span><br><span class="line">        <span class="comment">// 集装箱的重量</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; w;</span><br><span class="line">        <span class="comment">// 船的载重</span></span><br><span class="line">        <span class="keyword">int</span> c;</span><br><span class="line">        <span class="comment">// 当前重量</span></span><br><span class="line">        <span class="keyword">int</span> cw;</span><br><span class="line">        <span class="comment">// 最优重量</span></span><br><span class="line">        <span class="keyword">int</span> bestw;</span><br><span class="line"><span class="comment">// 回溯遍历index=i的箱子</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">backtrack</span><span class="params">(<span class="keyword">int</span> i)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造函数</span></span><br><span class="line">        Solution(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;w, <span class="keyword">int</span> c):w(w), c(c) &#123;</span><br><span class="line">            <span class="keyword">this</span>-&gt;cw = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">this</span>-&gt;bestw = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">this</span>-&gt;n = w.size();</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">max_loading</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;w, <span class="keyword">int</span> c)</span> </span>&#123;</span><br><span class="line">    <span class="function">Solution <span class="title">solu</span><span class="params">(w, c)</span></span>;</span><br><span class="line">    solu.backtrack(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> solu.bestw;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>无剪枝的dfs</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> Solution::backtrack(<span class="keyword">int</span> i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (i == n) &#123;</span><br><span class="line">        <span class="keyword">if</span> (cw &gt; bestw) &#123;</span><br><span class="line">            bestw = cw;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// 选择i</span></span><br><span class="line">    <span class="keyword">if</span> (cw + w[i] &lt;= c) &#123;</span><br><span class="line">        cw = cw + w[i];</span><br><span class="line">        backtrack(i + <span class="number">1</span>);</span><br><span class="line">        cw = cw - w[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 不选择i</span></span><br><span class="line">    backtrack(i + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>剪枝的dfs</strong></p><p>进入下一步遍历之前，检查<span class="math inline">\(\rm{cw+rest &gt; bestw}\)</span> ，如果剩下的所有加上都小于最优解的话，那么就不用进入了。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 剪枝的dfs</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">void</span> Solution::dfs(<span class="keyword">int</span> i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (i == n) &#123;</span><br><span class="line">        <span class="keyword">if</span> (cw &gt; bestw) &#123;</span><br><span class="line">            bestw = cw;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 剩余</span></span><br><span class="line">    r = r - w[i];</span><br><span class="line">    <span class="comment">// 选择i</span></span><br><span class="line">    <span class="keyword">if</span> (cw + w[i] &lt;= c) &#123;</span><br><span class="line">        cw += w[i];</span><br><span class="line">        <span class="comment">// 剪枝</span></span><br><span class="line">        <span class="keyword">if</span> (cw + r &gt; bestw) &#123;</span><br><span class="line">            dfs(i + <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        cw -= w[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 不选择i，剪枝</span></span><br><span class="line">    <span class="keyword">if</span> (cw + r &gt; bestw) &#123;</span><br><span class="line">        dfs(i + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    r = r + w[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;搜索算法&quot;&gt;搜索算法&lt;/h1&gt;
&lt;h2 id=&quot;解空间树&quot;&gt;解空间树&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;子集树&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;比如n种可选物品的01背包，对其进行选择。解空间为长度为n的01向量表示。解空间树是一颗完全二叉树。&lt;/p&gt;
&lt;p
      
    
    </summary>
    
      <category term="算法" scheme="http://plmsmile.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="搜索" scheme="http://plmsmile.github.io/tags/%E6%90%9C%E7%B4%A2/"/>
    
  </entry>
  
  <entry>
    <title>树的总结</title>
    <link href="http://plmsmile.github.io/2017/12/29/trees/"/>
    <id>http://plmsmile.github.io/2017/12/29/trees/</id>
    <published>2017-12-29T06:25:09.000Z</published>
    <updated>2017-12-29T06:37:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="二叉树">二叉树</h1><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/tree/bstree_02.jpg" style="display:block; margin:auto" width="100%"></p><h2 id="二叉树的性质">二叉树的性质</h2><ul><li>第i层，节点最多<span class="math inline">\(2^{i-1}\)</span>个</li><li>深度为k的二叉树，最多有<span class="math inline">\(2^k-1\)</span>个 节点</li><li>二叉树有n个节点，高度至少为<span class="math inline">\(\log_2(n+1)\)</span></li><li><span class="math inline">\(n_0 =n_2 + 1\)</span>，叶子节点和度为2的节点的关系</li></ul><h2 id="种类">种类</h2><p><strong>满二叉树</strong></p><p>高为h，有<span class="math inline">\(2^h-1\)</span>个节点</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/tree/bstree_03.jpg" style="display:block; margin:auto" width="60%"></p><p><strong>完全二叉树</strong></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/tree/bstree_04.jpg" style="display:block; margin:auto" width="60%"></p><p><strong>二叉查找树</strong></p><p><span class="math inline">\(\rm{left &lt; root &lt; right}\)</span> ， 没有相等的节点</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/tree/bstree_05.jpg" style="display:block; margin:auto" width="60%"></p><p><strong>二叉平衡树</strong></p><p>左右子树的高度差的绝对值小于等于1</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/tree/avltree_01_compare.jpg" style="display:block; margin:auto" width="80%"></p><h1 id="二叉查找树">二叉查找树</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;二叉树&quot;&gt;二叉树&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://otafnwsmg.bkt.clouddn.com/ima
      
    
    </summary>
    
      <category term="leetcode" scheme="http://plmsmile.github.io/categories/leetcode/"/>
    
    
      <category term="leetcode" scheme="http://plmsmile.github.io/tags/leetcode/"/>
    
      <category term="树" scheme="http://plmsmile.github.io/tags/%E6%A0%91/"/>
    
      <category term="数据结构" scheme="http://plmsmile.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>leetcode-01</title>
    <link href="http://plmsmile.github.io/2017/12/29/leetcode-01/"/>
    <id>http://plmsmile.github.io/2017/12/29/leetcode-01/</id>
    <published>2017-12-29T06:08:03.000Z</published>
    <updated>2018-02-28T08:29:10.623Z</updated>
    
    <content type="html"><![CDATA[<h1 id="有序列表转二叉查找树-109">有序列表转二叉查找树-109</h1><p><a href="https://leetcode.com/problems/convert-sorted-list-to-binary-search-tree/description/" target="_blank" rel="noopener">Convert Sorted List to Binary Search Tree</a></p><h1 id="二叉树的最小深度-111">二叉树的最小深度-111</h1><p><a href="https://leetcode.com/problems/minimum-depth-of-binary-tree/description/" target="_blank" rel="noopener">Minimum Depth of Binary Tree</a></p><p>最小深度：根节点到某个叶子节点的最短路径。</p><ul><li>为空，返回0</li><li>左孩子为空，则结果在右孩子</li><li>右孩子为空，则结果在左孩子</li><li>左右均不为空，返回小的+1</li></ul><h1 id="二叉树遍历">二叉树遍历</h1><h2 id="先序遍历-144">先序遍历-144</h2><p><a href="https://leetcode.com/problems/binary-tree-preorder-traversal/description/" target="_blank" rel="noopener">Binary Tree Preorder Traversal</a></p><p>根、左、右。<strong>栈</strong>。</p><ul><li>先把根节点入栈</li><li>栈不为空时，出栈一个元素</li><li>访问该元素，<strong>右孩子进栈，左孩子进栈</strong>。 因为出栈，先出左孩子，再出右孩子。</li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/144_btree_pre_order/btree_preorder.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; pre_order(TreeNode* root) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vpre;</span><br><span class="line">    <span class="built_in">stack</span>&lt;TreeNode*&gt; st;</span><br><span class="line">    <span class="keyword">if</span> (root != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        st.push(root);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (!st.empty()) &#123;</span><br><span class="line">        TreeNode* p = st.top();</span><br><span class="line">        vpre.push_back(p-&gt;val);</span><br><span class="line">        st.pop();</span><br><span class="line">        <span class="comment">// 右进、左进；出时：左先出</span></span><br><span class="line">        <span class="keyword">if</span> (p-&gt;right) &#123;</span><br><span class="line">            st.push(p-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (p-&gt;left) &#123;</span><br><span class="line">            st.push(p-&gt;left);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> vpre;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="中序遍历-094">中序遍历-094</h2><p><a href="https://leetcode.com/problems/binary-tree-inorder-traversal/description/" target="_blank" rel="noopener">Binary Tree Inorder Traversal</a></p><p><strong>思路</strong></p><p>左、根、右。使用<strong>栈</strong>。</p><ol style="list-style-type: decimal"><li>p=root</li><li>p不为空，p入栈，一直向左走<code>p = p.left</code>，扫描它的左孩子，所有左孩子依次入栈</li><li>p为空时，<code>p = st.top()</code> ，<strong>p位于栈顶</strong>，显然<strong>没有左孩子或者左孩子已经遍历过</strong>，<strong>p访问出栈</strong>。</li><li><strong>扫描右孩子</strong> <code>p = p.right</code></li></ol><p>从根节点开始，一直向左，<strong>所有的左孩子入栈</strong>， 出栈一个节点，<strong>访问</strong>，<strong>它的右孩子入栈</strong>。</p><p><a href="https://github.com/plmsmile/leetcode/blob/master/94_btree_in_order/btree_inorder.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; inorder_traversal(TreeNode* root) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">    <span class="built_in">stack</span>&lt;TreeNode*&gt; st;</span><br><span class="line">    TreeNode* p = root;</span><br><span class="line">    <span class="keyword">while</span> (p || !st.empty()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (p) &#123;</span><br><span class="line">            <span class="comment">// 根节点入栈</span></span><br><span class="line">            st.push(p);</span><br><span class="line">            <span class="comment">// 扫描左孩子</span></span><br><span class="line">            p = p-&gt;left;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// p位于栈顶，左孩子已经被遍历过或者没有左孩子，直接出栈访问</span></span><br><span class="line">            p = st.top();</span><br><span class="line">            res.push_back(p-&gt;val);</span><br><span class="line">            st.pop();</span><br><span class="line">            <span class="comment">// 扫描右孩子</span></span><br><span class="line">            p = p-&gt;right;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="后序遍历-145">后序遍历-145</h2><p><a href="https://leetcode.com/problems/binary-tree-postorder-traversal/description/" target="_blank" rel="noopener">Binary Tree Postorder Traversal</a></p><p>思路</p><p>左孩子、右孩子、根节点。使用栈。使用<code>pre</code>记录<strong>上一次遍历的节点</strong>。</p><ul><li>根节点入栈</li><li>栈不为空，访问栈顶元素p</li><li><strong>直接访问</strong>p的条件：<strong>p没有左右孩子</strong> or <strong>左右孩子刚刚遍历结束</strong>，只要pre是左或者右孩子即可</li><li>p可以直接访问，则<strong>访问出栈</strong></li><li>p不能直接访问，则<strong>左右孩子入栈</strong></li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/145_btree_post_order/btree_postorder.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; post_order(TreeNode* root) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">    <span class="built_in">stack</span>&lt;TreeNode*&gt; st;</span><br><span class="line">    <span class="comment">// 前一次访问的节点</span></span><br><span class="line">    TreeNode* pre = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">if</span> (root != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        st.push(root);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (!st.empty()) &#123;</span><br><span class="line">        TreeNode* p = st.top();</span><br><span class="line">        <span class="comment">// 0. 检查是否可以直接访问p</span></span><br><span class="line">        <span class="keyword">bool</span> no_child = (p-&gt;left == <span class="literal">nullptr</span> &amp;&amp; p-&gt;right == <span class="literal">nullptr</span>);</span><br><span class="line">        <span class="keyword">bool</span> pre_is_child = (pre == p-&gt;left || pre == p-&gt;right);</span><br><span class="line">        <span class="keyword">if</span> (<span class="literal">nullptr</span> == pre) &#123;</span><br><span class="line">            pre_is_child = <span class="literal">false</span>;</span><br><span class="line">        &#125;   </span><br><span class="line">        <span class="comment">// 1. p无左右子树 or 左右子树刚刚遍历完，直接访问p</span></span><br><span class="line">        <span class="keyword">if</span> (no_child || pre_is_child) &#123;</span><br><span class="line">            res.push_back(p-&gt;val);</span><br><span class="line">            pre = p;</span><br><span class="line">            st.pop();</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="comment">// 2. 需要将p的左右孩子入栈</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (p-&gt;right) &#123;</span><br><span class="line">                st.push(p-&gt;right);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (p-&gt;left) &#123;</span><br><span class="line">                st.push(p-&gt;left);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="层次遍历-102">层次遍历-102</h2><p>层次遍历，使用队列。</p><p>从上到下 <a href="https://leetcode.com/problems/binary-tree-level-order-traversal/description/" target="_blank" rel="noopener">Binary Tree Level Order Traversal</a> 和从下到上 <a href="https://leetcode.com/problems/binary-tree-level-order-traversal-ii/description/" target="_blank" rel="noopener">Binary Tree Level Order Traversal II</a>。</p><p>如果只需要顺序放在一个数组里面，则不需要分层，直接层次遍历即可。</p><p>但是此题，需要分层构建vector。</p><p><strong>数量记录思路</strong></p><p>不是很好。</p><ul><li>队列层次遍历</li><li>当前层在队列中的数量：<code>cur_remain</code></li><li>下一层的数量：<code>next_level</code></li><li><code>cur_remain == 0</code>时， 就切换到下一层</li></ul><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; levelOrder(TreeNode* root) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">queue</span>&lt;TreeNode*&gt; q;</span><br><span class="line">    q.push(root);</span><br><span class="line">    res.push_back(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">    <span class="comment">// 当前层，在队列里面的元素数量</span></span><br><span class="line">    <span class="keyword">int</span> cur_remain = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 下一层的元素数量</span></span><br><span class="line">    <span class="keyword">int</span> next_level = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (!q.empty()) &#123;</span><br><span class="line">        TreeNode* now = q.front();</span><br><span class="line">        q.pop();</span><br><span class="line">        <span class="comment">// 存入队列</span></span><br><span class="line">        res[res.size() - <span class="number">1</span>].push_back(now-&gt;val);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 左右孩子入队</span></span><br><span class="line">        <span class="keyword">if</span> (now-&gt;left) &#123;</span><br><span class="line">            q.push(now-&gt;left);</span><br><span class="line">            next_level++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (now-&gt;right) &#123;</span><br><span class="line">            q.push(now-&gt;right);</span><br><span class="line">            next_level++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 当前层数量--</span></span><br><span class="line">        cur_remain--;</span><br><span class="line">        <span class="comment">// 切换到下一层</span></span><br><span class="line">        <span class="keyword">if</span> (cur_remain == <span class="number">0</span> &amp;&amp; !q.empty()) &#123;</span><br><span class="line">            res.push_back(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;());</span><br><span class="line">            cur_remain = next_level;</span><br><span class="line">            next_level = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>一次遍历一层的思路</strong></p><p>很好，<strong>掌握</strong>！</p><ul><li>一次while循环，保证<strong>当前队列里面只有当前层的元素</strong>，用vector记录当前层的序列</li><li><code>q.size()</code> 获得当前层元素数量，然后<strong>本次循环，只从队列里面出这么多元素</strong>。</li><li>依次<strong>遍历当前层的所有元素</strong>，<strong>出队，同时左右孩子入队</strong></li><li>q为空，则所有层遍历结束</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 保证当前队列的循环只有当前层的</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; level_order(TreeNode* root) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">queue</span>&lt;TreeNode*&gt; q;</span><br><span class="line">    q.push(root);</span><br><span class="line">    <span class="comment">// 一次while循环，出掉当前层的所有元素，下一层的元素全部入队</span></span><br><span class="line">    <span class="keyword">while</span> (!q.empty()) &#123;</span><br><span class="line">        <span class="comment">// 当前层的数量和遍历结果序列</span></span><br><span class="line">        <span class="keyword">int</span> level_num = q.size();</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curv;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; level_num; i++) &#123;</span><br><span class="line">            TreeNode* p = q.front();</span><br><span class="line">            <span class="comment">// p的左右孩子入队列</span></span><br><span class="line">            <span class="keyword">if</span> (p-&gt;left) q.push(p-&gt;left);</span><br><span class="line">            <span class="keyword">if</span> (p-&gt;right) q.push(p-&gt;right);</span><br><span class="line">            <span class="comment">// p出队，放到当前层的vector中</span></span><br><span class="line">            q.pop();</span><br><span class="line">            curv.push_back(p-&gt;val);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 放到末尾，就是从下到上</span></span><br><span class="line">        <span class="comment">// res.insert(res.begin(), curv); </span></span><br><span class="line">        res.push_back(curv);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="发现重复的数字-287">发现重复的数字-287</h1><p><a href="https://leetcode.com/problems/find-the-duplicate-number/description/" target="_blank" rel="noopener">Find the Duplicate Number</a>， 类似于<a href="https://plmsmile.github.io/2017/07/29/aim2offer/#题目2">aim2offer中查找重复的数字</a></p><blockquote><p>数组a，有n+1个数，都在[1,n]范围内，只有一个重复的元素。找到它</p></blockquote><p><strong>二分思路</strong></p><p><code>[1, n]</code>这个范围有n个数。划分为两个范围<code>[1, m]</code>和<code>[m+1, n]</code></p><ul><li><strong>每次去遍历整个数组</strong>，统计<strong>两个范围</strong>内的<strong>数字的数目</strong></li><li>统计整个数组中元素在<code>[1, m]</code>范围内的个数<span class="math inline">\(c_1\)</span></li><li>统计整个数组中元素在<code>[m+1, n]</code>范围内的个数<span class="math inline">\(c_2\)</span></li></ul><p>[1, m]这m个数字的数量是<code>c</code></p><ul><li>如果 <code>c &gt; m</code>， 则<code>1, m]</code>内一定存在重复的数，<code>e = m</code></li><li>否则，<code>[m+1, n]</code>一定存在重复的数，<code>s = m + 1</code></li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/287_find_the_duplicate_num/find_duplicate_num.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 找到一个重复的数字</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a -- 数组，n+1个元素，范围[1,n]，至少有一个重复的数字</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      dup -- 重复的数字</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find_duplicate</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = a.size() - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> r = n;</span><br><span class="line">    <span class="keyword">int</span> dup = <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">// 不断缩小范围</span></span><br><span class="line">    <span class="keyword">while</span> (l &lt;= r) &#123;</span><br><span class="line">        <span class="keyword">int</span> m = (l + r) &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 统计[l,m]在a中的出现次数</span></span><br><span class="line">        <span class="keyword">int</span> count = count_range(a, l, m);</span><br><span class="line">        <span class="keyword">if</span> (l == r) &#123;</span><br><span class="line">            <span class="keyword">if</span> (count &gt;= <span class="number">2</span>) &#123;</span><br><span class="line">                dup = l;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// [l, m]有重复的</span></span><br><span class="line">        <span class="keyword">if</span> (count &gt; (m - l + <span class="number">1</span>)) &#123;</span><br><span class="line">            r = m;</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="comment">// [m+1, r]有重复的</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            l = m + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dup;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 找到数组a中，[min, max]这些数的出现次数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">count_range</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> min, <span class="keyword">int</span> max)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a[i] &gt;= min &amp;&amp; a[i] &lt;= max) &#123;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="合并两条有序链表-021">合并两条有序链表-021</h1><p><a href="https://leetcode.com/problems/merge-two-sorted-lists/description/" target="_blank" rel="noopener"></a></p><p><a href="https://leetcode.com/problems/merge-two-sorted-lists/description/" target="_blank" rel="noopener">Merge Two Sorted Lists</a> ，和<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#归并排序">归并排序的Merge操作</a> 很类似。</p><p>考虑鲁棒性</p><p><strong>思路</strong></p><ul><li><span class="math inline">\(l_1\)</span>与<span class="math inline">\(l_2\)</span>若有一个为空的，则<strong>返回另一个</strong></li><li><strong>初始化新的head</strong>，选择<span class="math inline">\(l_1\)</span>与<span class="math inline">\(l_2\)</span>中第一个节点较小的那个</li><li>while循环，谁小选谁</li><li>结束之后，直接<strong>把未空的链表链接上</strong>即可</li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/21_merge_two_lists/merge_two_lists.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">mergeTwoLists</span><span class="params">(ListNode* l1, ListNode* l2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (l1 == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> l2;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (l2 == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> l1;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 初始化head</span></span><br><span class="line">    ListNode* head = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">if</span> (l1-&gt;val &lt; l2-&gt;val) &#123;</span><br><span class="line">        head = l1;</span><br><span class="line">        l1 = l1-&gt;next;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        head = l2;</span><br><span class="line">        l2 = l2-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 遍历两条链表，每次选择小的追加到p的后面</span></span><br><span class="line">    ListNode* p = head;</span><br><span class="line">    <span class="keyword">while</span> (l1 &amp;&amp; l2) &#123;</span><br><span class="line">        <span class="keyword">if</span> (l1-&gt;val &lt; l2-&gt;val) &#123;</span><br><span class="line">            p-&gt;next = l1;</span><br><span class="line">            l1 = l1-&gt;next;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            p-&gt;next = l2;</span><br><span class="line">            l2 = l2-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 某一条链表还有剩余</span></span><br><span class="line">    <span class="keyword">if</span> (l1) &#123;</span><br><span class="line">        p-&gt;next = l1;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (l2) &#123;</span><br><span class="line">        p-&gt;next = l2;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="合并多条有序链表-023">合并多条有序链表-023</h1><p><a href="https://leetcode.com/problems/merge-k-sorted-lists" target="_blank" rel="noopener">Merge k Sorted Lists</a></p><p>我们已经会合并2条链表了，可以使用<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#归并排序">归并排序</a>和<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#折半插入">二分查找</a>的思想来合并多个列表。</p><p><strong>示例</strong></p><p>现在有<code>1, 2, 3, 4, 5, 6</code>条链表</p><ul><li>第一步：<code>1-6</code>，<code>2-5</code>，<code>3-4</code>合并，得到新的<code>1, 2, 3</code></li><li>第二步：<code>1-3</code>合并，2不动，得到新的<code>1, 2</code></li><li>第三步：<code>1-2</code>合并， 得到新的<code>2</code>， 合并完成</li><li>返回<code>list[0]</code></li></ul><p><strong>总结</strong></p><ol style="list-style-type: decimal"><li>直到<code>len==1</code> <strong>合并到只有一条链表</strong>，合并到list[0]</li><li>对于当前len，<strong>折半两两合并</strong>，<code>i</code>和<code>len-i-1</code>合并，放到前面<code>lists[i]</code></li><li><strong>len缩减一半</strong>，<code>len=(len+1)/2</code></li></ol><p><a href="https://github.com/plmsmile/leetcode/blob/master/22_merge_k_sorted_list/merge_k_lists.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">mergeKLists</span><span class="params">(<span class="built_in">vector</span>&lt;ListNode*&gt;&amp; lists)</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (lists.empty()) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> len = lists.size();</span><br><span class="line">  <span class="comment">// 直到只有一条链表</span></span><br><span class="line">    <span class="keyword">while</span> (len &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="comment">// 依次合并前后两条链表</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len / <span class="number">2</span>; i++) &#123;</span><br><span class="line">          <span class="comment">// 合并放到list[i]</span></span><br><span class="line">        lists[i] = mergeTwoLists(lists[i], lists[len - i - <span class="number">1</span>]);</span><br><span class="line">      &#125;</span><br><span class="line">      len = (len + <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> lists[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="z型打印二叉树-103">Z型打印二叉树-103</h1><p><a href="https://leetcode.com/problems/binary-tree-zigzag-level-order-traversal/description/" target="_blank" rel="noopener">Binary Tree Zigzag Level Order Traversal</a></p><p>参考<a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#层次遍历-102">层次遍历</a> <strong>一次遍历一层</strong>的思路。</p><ul><li>一次遍历一层，得到当前层的遍历结果</li><li>单数，从左向右；偶数，从右向左</li><li>每次遍历一个元素，把左右孩子入队</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/tree/bstree_03.jpg" style="display:block; margin:auto" width="60%"></p><ul><li>根节点，向右走</li><li>第二层，向左走</li><li>向右走，从队头出，孩子先左后右，加到队尾</li><li>向左走，从队尾出，孩子先右后左，加到队首</li></ul><p><strong>两个栈的思路</strong></p><p>栈1初始存放根节点，栈2为空</p><ul><li><strong>向右</strong>走，<strong>栈1全部出栈</strong>，<strong>先左后右</strong>孩子依次压入栈2，栈底-栈顶，栈2为<code>2 3</code></li><li><strong>向左</strong>走，<strong>栈2全部出栈</strong>，<strong>先右后左</strong>孩子依次压入栈1，栈1为<code>7 6 5 4</code></li><li>向右走，栈1出栈，左右孩子依次压入栈2，栈2为<code>8 9 10 11 12 13 14 15</code></li><li>向左走，栈2出栈，结束</li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/103_zigzag_level_order/btree_zlevel_order.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 使用两个栈z型层次打印二叉树</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; zigzagLevelOrder(TreeNode* root) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">stack</span>&lt;TreeNode*&gt; st1;</span><br><span class="line">    <span class="built_in">stack</span>&lt;TreeNode*&gt; st2;</span><br><span class="line"></span><br><span class="line">    st1.push(root);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (!st1.empty() || !st2.empty()) &#123;</span><br><span class="line">        <span class="comment">// 向右走</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; curv;</span><br><span class="line">        <span class="keyword">if</span> (!st1.empty()) &#123;</span><br><span class="line">            <span class="keyword">while</span> (!st1.empty()) &#123;</span><br><span class="line">                TreeNode* p = st1.top();</span><br><span class="line">                st1.pop();</span><br><span class="line">                curv.push_back(p-&gt;val);</span><br><span class="line">                <span class="keyword">if</span> (p-&gt;left) st2.push(p-&gt;left);</span><br><span class="line">                <span class="keyword">if</span> (p-&gt;right) st2.push(p-&gt;right);</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">        &#125; </span><br><span class="line">        <span class="comment">// 向左走</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">while</span> (!st2.empty()) &#123;</span><br><span class="line">                TreeNode* p = st2.top();</span><br><span class="line">                st2.pop();</span><br><span class="line">                curv.push_back(p-&gt;val);</span><br><span class="line">                <span class="keyword">if</span> (p-&gt;right) st1.push(p-&gt;right);</span><br><span class="line">                <span class="keyword">if</span> (p-&gt;left) st1.push(p-&gt;left);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        res.push_back(curv);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="二叉树路径求和-112.113.437">二叉树路径求和-112.113.437</h1><p><strong>题目1 从根节点到叶子求和-112</strong></p><p><a href="https://leetcode.com/problems/path-sum/description/" target="_blank" rel="noopener">Path Sum</a>， <code>EASY</code></p><blockquote><p>给一颗二叉树和一个sum值，判断是否有从根节点到叶子节点的路径，使得路径上的节点求和等于sum</p></blockquote><p><strong>思路</strong></p><p><strong>做减法</strong></p><ul><li>根节点为空，False</li><li>没有孩子，判断<code>root.val == sum</code></li><li>有孩子，<strong>把sum减掉根节点的值，去判断左右子树是否有</strong> ，<code>sum = sum - root.val</code></li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/112_path_sum/root_leaf_path_sum.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">hasPathSum</span><span class="params">(TreeNode* root, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 节点为空</span></span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2. 没有左右子树，直接判断</span></span><br><span class="line">    <span class="keyword">if</span> (!root-&gt;left &amp;&amp; !root-&gt;right) &#123;</span><br><span class="line">        <span class="keyword">return</span> root-&gt;val == sum;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 减小sum，去递归判断左右子树</span></span><br><span class="line">    <span class="keyword">int</span> newsum = sum - root-&gt;val;</span><br><span class="line">    <span class="keyword">return</span> hasPathSum(root-&gt;left, newsum) || hasPathSum(root-&gt;right, newsum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>题目2 从根节点到叶子节点求和-保存路径-113</strong></p><p><a href="https://leetcode.com/problems/path-sum-ii/description/" target="_blank" rel="noopener">Path Sum-113</a>， <code>Medium</code></p><blockquote><p>给二叉树和sum值，找到root-to-leaf的路径，使得和为sum。保存该路径</p></blockquote><p><strong>思路</strong></p><p>用<code>vector&lt;int&gt; path</code> 来记录<strong>当前路径</strong>， <code>vector&lt;vector&lt;int&gt;&gt; res</code> 记录<strong>最终结果</strong></p><ul><li>根节点为空，返回</li><li>到达叶子节点，<code>val == sum</code>， 把<strong>当前节点加入path</strong>，<strong>当前path加入res</strong>， 否则返回</li><li>非叶子节点，<strong>把当前节点加入path</strong>，<strong>去左右子树中遍历</strong>，继续追加path直到叶子节点</li><li>非叶子节点，结束后，<strong>把当前节点从path中删除</strong></li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/113_path_sum_save/root2leaf_pathsum_save.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 找到树种从root到leaf的所有和为sum的路径</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; pathSum(TreeNode* root, <span class="keyword">int</span> sum) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; path;</span><br><span class="line">    find_path(root, sum, path, res);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 递归遍历节点，逐渐添加节点到当前的path，叶子节点，满足要求时，则把path追加到res中</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      root -- 当前节点</span></span><br><span class="line"><span class="comment"> *      path -- 当前路径</span></span><br><span class="line"><span class="comment"> *      res -- 所有路径</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      None</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">find_path</span><span class="params">(TreeNode* root, <span class="keyword">int</span> sum, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; path, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; &amp;res)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 到达叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> (!root-&gt;left &amp;&amp; !root-&gt;right) &#123;</span><br><span class="line">        <span class="keyword">if</span> (root-&gt;val == sum) &#123;</span><br><span class="line">            path.push_back(root-&gt;val);</span><br><span class="line">            res.push_back(path);</span><br><span class="line">            path.pop_back();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当前节点加到path中</span></span><br><span class="line">    path.push_back(root-&gt;val);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 更新sum，到左右子树中去添加path</span></span><br><span class="line">    <span class="keyword">int</span> newsum = sum - root-&gt;val;</span><br><span class="line">    <span class="keyword">if</span> (root-&gt;left) &#123;</span><br><span class="line">        find_path(root-&gt;left, newsum, path, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (root-&gt;right) &#123;</span><br><span class="line">        find_path(root-&gt;right, newsum, path, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 当前节点从path中移除</span></span><br><span class="line">    path.pop_back();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>题目3 求二叉树的所有和为sum的路径，任意起始节点-437</strong></p><blockquote><p>给一颗二叉树和sum，求出所有和为sum的路径数量，从任意节点开始和结束。</p></blockquote><p><strong>思路</strong></p><ul><li>层次遍历，<strong>以每一颗节点为起始值</strong>，找到以它开始的路径数量</li><li>节点为空，0</li><li>无孩子，不相等，0</li><li>无孩子，相等，1</li><li>有孩子，相等，<code>c = 1</code>， 不相等<code>c = 0</code>。 <strong>更新sum</strong>，继续<strong>递归查找左右孩子的count</strong>。返回<strong>c+count</strong></li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/437_all_path_sum/btree_all_path_sum.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 找到树中，和为sum的所有路径数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">pathSum</span><span class="params">(TreeNode* root, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">queue</span>&lt;TreeNode*&gt; q;</span><br><span class="line">    q.push(root);</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 前序遍历</span></span><br><span class="line">    <span class="keyword">while</span> (!q.empty()) &#123;</span><br><span class="line">        TreeNode* now = q.front();</span><br><span class="line">        q.pop();</span><br><span class="line">        count += count_from_root(now, sum);</span><br><span class="line">        <span class="keyword">if</span> (now-&gt;left) q.push(now-&gt;left);</span><br><span class="line">        <span class="keyword">if</span> (now-&gt;right) q.push(now-&gt;right);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 以root为起始节点，向下走，和为sum的路径的条数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">count_from_root</span><span class="params">(TreeNode* root, <span class="keyword">int</span> sum)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 空</span></span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 直接根节点就满足，无需看孩子</span></span><br><span class="line">    <span class="keyword">int</span> c = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (sum == root-&gt;val) &#123;</span><br><span class="line">        <span class="comment">// 相等</span></span><br><span class="line">        c = <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!root-&gt;left &amp;&amp; !root-&gt;right) &#123;</span><br><span class="line">        <span class="comment">// 无孩子，不相等</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// c+左右孩子的</span></span><br><span class="line">    <span class="keyword">int</span> newsum = sum - root-&gt;val;</span><br><span class="line">    <span class="keyword">return</span> c + count_from_root(root-&gt;left, newsum) + count_from_root(root-&gt;right, newsum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="有序链表转平衡bst-109">有序链表转平衡BST-109</h1><p><a href="https://leetcode.com/problems/convert-sorted-list-to-binary-search-tree/description/" target="_blank" rel="noopener">Convert Sorted List to Binary Search Tree</a>， <code>Medium</code>。 类似题型：<a href="https://plmsmile.github.io/2018/01/07/aim2offer3/#二叉搜索树转双向链表-36">BST转有序双向链表</a></p><blockquote><p>给一个有序链表，转化为平衡的二叉搜索树</p></blockquote><p><strong>思路</strong></p><p>有序 -- BST的中序遍历；平衡 -- 以中间节点为根节点，分为左右子树递归去创建。</p><ul><li>计算总结点数量 -- <code>size</code>， 递归去构建树<code>go(0, size - 1)</code></li><li><code>go(head, start, end)</code> ，计算出<strong>中间节点</strong><code>mid-node</code>， 构造<strong>树根</strong><code>root</code></li><li><strong>左孩子</strong><code>root.left = go(head, start, mid-1)</code>， <strong>右孩子</strong><code>root.right = go(now.next, mid+1, end)</code></li><li>返回当前树根节点root</li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/109_sortedlist_to_bst/list_to_bst.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 把有序链表转化为平衡的BST</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      head -- 链表</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      root -- 树的头结点</span></span><br><span class="line"><span class="comment"> */</span> </span><br><span class="line"><span class="function">TreeNode* <span class="title">sortedListToBST</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (head == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> size = <span class="number">0</span>;</span><br><span class="line">    ListNode* p = head;</span><br><span class="line">    <span class="keyword">while</span> (p) &#123;</span><br><span class="line">        ++size;</span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> create_tree(head, <span class="number">1</span>, size);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 递归中序创建树</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      head -- 链表头节点</span></span><br><span class="line"><span class="comment"> *      start -- 起始节点编号，从1开始</span></span><br><span class="line"><span class="comment"> *      end -- 结束节点编号</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      root -- 树的根节点</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">TreeNode* <span class="title">create_tree</span><span class="params">(ListNode* head, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 0. 递归终止条件</span></span><br><span class="line">    <span class="keyword">if</span> (head == <span class="literal">nullptr</span> || start &gt; end) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 1. 找到中间节点，构建根节点</span></span><br><span class="line">    <span class="keyword">int</span> mid = (end + start) / <span class="number">2</span>;</span><br><span class="line">    ListNode* node = head;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = start + <span class="number">1</span>; i &lt;= mid; i++) &#123;</span><br><span class="line">        node = node-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    TreeNode* root = <span class="keyword">new</span> TreeNode(node-&gt;val);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 递归构造左右子树</span></span><br><span class="line">    root-&gt;left = create_tree(head, start, mid - <span class="number">1</span>);</span><br><span class="line">    root-&gt;right = create_tree(node-&gt;next, mid + <span class="number">1</span>, end);</span><br><span class="line">    </span><br><span class="line">  <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="序列化二叉树-297">序列化二叉树-297</h1><p><a href="https://leetcode.com/problems/serialize-and-deserialize-binary-tree/description/" target="_blank" rel="noopener">Serialize and Deserialize Binary Tree</a>， <code>Hard</code></p><blockquote><p>把二叉树序列化为字符串，把字符串反序列化为一棵树</p></blockquote><p><strong>思路</strong></p><p><a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#先序遍历-144">前序遍历</a>来保存序列，保存成一颗<strong>完全二叉树</strong>，空节点用<code>$</code>表示，使用空格进行分割。</p><p><strong>序列化</strong></p><ul><li>序列化为一颗完全二叉树，先序递归。遇到空指针，则用<code>$</code>代替<br></li><li>先把字符放到<code>stringstream</code>里面，<code>&lt;&lt;</code>输入， 最后<code>s.str()</code>得到字符串</li></ul><p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 把一棵树序列化为一个字符串，前序完全二叉树序列</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      root -- 树</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      str -- 序列化后的字符串</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">serialize</span><span class="params">(TreeNode* root)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">stringstream</span> buf;</span><br><span class="line">    build_string(root, buf);</span><br><span class="line">    <span class="keyword">return</span> buf.str();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 递归把二叉树序列化到buf字符串中</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      root -- 当前的根节点</span></span><br><span class="line"><span class="comment"> *      buf -- 字符串buffer</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      None，都写到了buf中</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">build_string</span><span class="params">(TreeNode* root, <span class="built_in">stringstream</span>&amp; buf)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        buf &lt;&lt; <span class="string">"$"</span> &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    buf &lt;&lt; root-&gt;val &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">    build_string(root-&gt;left, buf);</span><br><span class="line">    build_string(root-&gt;right, buf);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>从字符串中解析得到序列，存到队列中</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 分割字符串，把字符写到容器q里面</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">split</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; str, <span class="built_in">queue</span>&lt;<span class="built_in">string</span>&gt; &amp;q, <span class="keyword">const</span> <span class="keyword">char</span> delim = <span class="string">' '</span>)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">istringstream</span> input;</span><br><span class="line">    input.str(str);</span><br><span class="line">    <span class="built_in">string</span> line;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">std</span>::getline(input, line, delim)) &#123;</span><br><span class="line">        q.push(line);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>反序列化</strong></p><p>得到<strong>队列</strong>序列之后，可以对其进行递归反序列化构建树。先序序列，不是层次序列。</p><ul><li><strong>根-左-右</strong>，队列。<strong>出队</strong>，<strong>建立根节点</strong></li><li><strong>左-右</strong>，队列，递归建立<strong>左孩子</strong></li><li><strong>右</strong>，队列，建立<strong>右孩子</strong></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 把String解析为一棵树</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      data -- 序列化后的字符串</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      root -- 树</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">TreeNode* <span class="title">deserialize</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; data)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (data.empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 先序序列</span></span><br><span class="line">    <span class="built_in">queue</span>&lt;<span class="built_in">string</span>&gt; preorder;</span><br><span class="line">    split(data, preorder);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> build_tree(preorder);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 递归先序构造树</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      prev -- 先序遍历序列</span></span><br><span class="line"><span class="comment"> * Retursn:</span></span><br><span class="line"><span class="comment"> *      root -- prev[i]为根构建的树</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">TreeNode* <span class="title">build_tree</span><span class="params">(<span class="built_in">queue</span>&lt;<span class="built_in">string</span>&gt;&amp; pres)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (pres.size() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">string</span> val = pres.front();</span><br><span class="line">    pres.pop();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当前为空节点</span></span><br><span class="line">    <span class="keyword">if</span> (val == <span class="string">"$"</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 有值</span></span><br><span class="line">    TreeNode* root = <span class="keyword">new</span> TreeNode(<span class="built_in">std</span>::stoi(val));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 递归按照顺序构建左右子树</span></span><br><span class="line">    root-&gt;left = build_tree(pres);</span><br><span class="line">    root-&gt;right = build_tree(pres);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="数字全排列-046">数字全排列-046</h1><p><a href="https://leetcode.com/problems/permutations/description/" target="_blank" rel="noopener">Permutations</a>, <code>Medium</code>。 <a href="https://plmsmile.github.io/2017/12/31/algorithm-dfs/#解空间树">搜索树</a></p><blockquote><p>给一个数组，返回全排列。每个数字都不相同</p></blockquote><p><strong>思路</strong></p><p>全排列<code>回溯法</code>搜索。一个数组，搜索第t层的时候</p><ul><li>前面t-1层都已经ok</li><li>遍历后面的所有元素，给到t层，去搜索</li><li>每次进行交换</li></ul><p><a href="https://github.com/plmsmile/leetcode/blob/master/046_num_permutations/permutations.cpp" target="_blank" rel="noopener">github代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 数组的全排列</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; permute(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;nums) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">    dfs(nums, <span class="number">0</span>, res);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 回溯搜索排列树，遍历当前第i层的所有可能性，前面i-1已经全部确定好</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      t -- 第几层，[0, n-1]</span></span><br><span class="line"><span class="comment"> *      path -- 当前路径，[0,i-1]已经确定好，[i,n-1]是剩余的数字，遍历每一种可能给到i</span></span><br><span class="line"><span class="comment"> *      res -- 总的结果</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      None</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; path, <span class="keyword">int</span> t, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; res)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (t &gt;= path.size()) &#123;</span><br><span class="line">        res.push_back(path);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = t; i &lt; path.size(); i++) &#123;</span><br><span class="line">        <span class="built_in">std</span>::swap(path[t], path[i]);</span><br><span class="line">        dfs(path, t + <span class="number">1</span>, res);</span><br><span class="line">        <span class="built_in">std</span>::swap(path[t], path[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="重复数字全排列-047">重复数字全排列-047</h1><p><a href="https://leetcode.com/problems/permutations-ii/description/" target="_blank" rel="noopener">重复数字全排列-047</a></p><blockquote><p>给一个数组，里面有一些重复的数字，给出所有的排列可能</p></blockquote><p><strong>思路</strong></p><p>重复的原因：当为t设置值的时候，遍历后面的所有元素给t赋值，但是后面都有一些重复的数值。</p><p>比如说 <code>1  2 1 1</code>， 开始是1，1会与最后的两个1再进行交换，然而其实是一样的。没必要了。</p><p>每次遍历交换的时候，<strong>只交换遍历后面不重复的元素</strong>。</p><p><a href="https://github.com/plmsmile/leetcode/blob/master/047_dup_nums_permutations/permutations.cpp" target="_blank" rel="noopener">github代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 回溯搜索排列树，遍历当前第i层的所有可能性，前面i-1已经全部确定好</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      t -- 第几层，[0, n-1]</span></span><br><span class="line"><span class="comment"> *      path -- 当前路径，[0,i-1]已经确定好，[i,n-1]是剩余的数字，遍历每一种可能给到i</span></span><br><span class="line"><span class="comment"> *      res -- 总的结果</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      None</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; path, <span class="keyword">int</span> t, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; res)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (t &gt;= path.size()) &#123;</span><br><span class="line">        res.push_back(path);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 不重复的元素与其索引</span></span><br><span class="line">    <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; vals;</span><br><span class="line">    <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; idx;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = t; i &lt; path.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (vals.find(path[i]) == vals.end()) &#123;</span><br><span class="line">            vals.insert(path[i]);</span><br><span class="line">            idx.insert(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for_each(idx.begin(), idx.end(), [&amp;](<span class="keyword">int</span> i) &#123;</span><br><span class="line">       <span class="built_in">std</span>::swap(path[t], path[i]);</span><br><span class="line">        dfs(path, t + <span class="number">1</span>, res);</span><br><span class="line">        <span class="built_in">std</span>::swap(path[t], path[i]);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>组合问题</strong></p><blockquote><p>给n个字符，要组成m个字符，问有多少种组成方法</p></blockquote><p>有点类似于01背包的选择。</p><ul><li>选择第一个字符，则在后面选择m-1个字符</li><li>不选择第一个字符，则在后面选择m个字符</li></ul><p><strong>正方体顶点和相等问题</strong></p><blockquote><p>给8个数字，正方体有8个顶点，数字放在顶点上。使得3对对面的顶点和相等。</p></blockquote><p>也就是搜索，然后限定一些条件。</p><p><strong>8皇后问题</strong></p><blockquote><p>8*8的象棋摆8个皇后，任意两个皇后不能在同一行、同一列或同一对角线上。问有多少种摆法</p></blockquote><h1 id="n皇后问题-051">N皇后问题-051</h1><blockquote><p>n*n的棋盘摆n个皇后，任意两个皇后不能在同一行、同一列或同一对角线上。返回摆法。</p></blockquote><p><a href="https://leetcode.com/problems/n-queens/description/" target="_blank" rel="noopener">N-Queens</a></p><ul><li>每一行一个皇后，每一行有n个选择。就去<strong>dfs搜索</strong>所有的排列树。</li><li><code>path[t]=k</code>， 第t行的皇后在第k列</li><li>k不能在前面皇后的：同一列、主对角线、<strong>副对角线</strong> 。别忘记副对角线。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 返回n皇后的解法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; solveNQueens(<span class="keyword">int</span> n) &#123;</span><br><span class="line">    <span class="comment">// 1. 获得所有可能的位置</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; locations;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; path(n);</span><br><span class="line">    <span class="built_in">std</span>::iota(path.begin(), path.end(), <span class="number">0</span>);</span><br><span class="line">    dfs(<span class="number">0</span>, n, path, locations);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2. 构造返回结果</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; res;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> loc : locations) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; solu;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i : loc) &#123;</span><br><span class="line">            <span class="function"><span class="built_in">string</span> <span class="title">line</span><span class="params">(n, <span class="string">'.'</span>)</span></span>;</span><br><span class="line">            line[i] = <span class="string">'Q'</span>;</span><br><span class="line">            solu.push_back(line);</span><br><span class="line">        &#125;</span><br><span class="line">        res.push_back(solu);</span><br><span class="line">    &#125;</span><br><span class="line">    show(res[<span class="number">0</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * dfs，设置t行的皇后位置</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      t -- 第t行，从0开始</span></span><br><span class="line"><span class="comment"> *      n -- n皇后</span></span><br><span class="line"><span class="comment"> *      path -- 当前的路径方案</span></span><br><span class="line"><span class="comment"> *      res -- 总的方案</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      None</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> t, <span class="keyword">int</span> n, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; path, <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; res)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (t == n) &#123;</span><br><span class="line">        res.push_back(path);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 前面t-1行已经ok，再后面的t-n个选择中选择遍历t</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = t; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (legal(t, path[i], path) == <span class="literal">true</span>) &#123;</span><br><span class="line">            swap(path[i], path[t]);</span><br><span class="line">            <span class="comment">//cout &lt;&lt; "t=" &lt;&lt; t &lt;&lt; ", k=" &lt;&lt; path[i] &lt;&lt; endl;</span></span><br><span class="line">            <span class="comment">//for_each(path.begin(), path.end(), [](int i)&#123;cout &lt;&lt; i &lt;&lt; " ";&#125;);</span></span><br><span class="line">            dfs(t + <span class="number">1</span>, n, path, res);</span><br><span class="line">            swap(path[i], path[t]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 合法性判断，同一列、主对角线、副对角线</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      t -- 第t行，从0开始</span></span><br><span class="line"><span class="comment"> *      k -- 放在第k个列，从0开始</span></span><br><span class="line"><span class="comment"> *      path -- 当前的路径，[0,t-1]行已经放好</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      true or false</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">legal</span><span class="params">(<span class="keyword">int</span> t, <span class="keyword">int</span> k, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; path)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= t - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        <span class="comment">// 1. 不能和之前的在同一列</span></span><br><span class="line">        <span class="keyword">if</span> (path[i] == k) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2. 不能在主对角线上</span></span><br><span class="line">        <span class="keyword">if</span> (t - i == k - path[i]) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 3. 不能在副对角线上</span></span><br><span class="line">        <span class="keyword">if</span> (t + k == i + path[i]) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"p[%d]=%d,p[%d]=%d\n"</span>, t, k, i, path[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>N皇后问题-052</strong></p><blockquote><p>返回有多少种解法</p></blockquote><p>做了上面的题，那这个就很简单了，返回数量就行了。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 返回n皇后的解法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">solveNQueens</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 所有的结果</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; locations;</span><br><span class="line">    <span class="comment">// 当前的位置</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; path(n);</span><br><span class="line">    <span class="comment">// 初始化为0-n-1</span></span><br><span class="line">    <span class="built_in">std</span>::iota(path.begin(), path.end(), <span class="number">0</span>);</span><br><span class="line">    <span class="comment">// dfs遍历搜索</span></span><br><span class="line">    dfs(<span class="number">0</span>, n, path, locations);</span><br><span class="line">    <span class="keyword">return</span> locations.size();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="查找第k大的数总结">查找第k大的数总结</h1><blockquote><p>给N个数，确定第k个最大值</p></blockquote><p><strong>1 排序</strong></p><p>排好序，取出第k大的值。<span class="math inline">\(O(n\log n + k)\)</span></p><p><strong>2 简单选择排序</strong></p><p><a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#简单选择">简单选择</a>。第k次选择，就是第k大的数字。<span class="math inline">\(O(n*k)\)</span></p><p><strong>3 快速排序思想</strong></p><p>每次partition，会把x放到位置i上。<strong>注意partition要从大到小排列，左大右小</strong>，而不是普通排序的左小右大。</p><ul><li><code>i == k</code>， 则就是<code>a[i]</code></li><li><code>k &gt; i</code>， 则在i的右边</li><li><code>k &lt; i</code>， 则在i的左边</li></ul><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 使用快排思想查找第k大的数字，从大到小排列！！</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a -- 数组</span></span><br><span class="line"><span class="comment"> *      l -- 范围的开始</span></span><br><span class="line"><span class="comment"> *      r -- 范围的结束</span></span><br><span class="line"><span class="comment"> *      k -- 该范围内第k大的数</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      第k大的数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">find_kth_num</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> l, <span class="keyword">int</span> r, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 划分。左边大，中间a[l]，右边小</span></span><br><span class="line">    <span class="keyword">int</span> i = partition(a, l, r);</span><br><span class="line">    <span class="comment">// 2. 通过i+1==k来判断是否是第k大的数</span></span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span> == k) &#123;</span><br><span class="line">        <span class="keyword">return</span> a[i];</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i + <span class="number">1</span> &gt; k) &#123;</span><br><span class="line">        <span class="comment">// 在左边</span></span><br><span class="line">        <span class="keyword">return</span> find_kth_num(a, l, i - <span class="number">1</span>, k);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 在右边</span></span><br><span class="line">        <span class="keyword">return</span> find_kth_num(a, i + <span class="number">1</span>, r, k);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>4 最大堆</strong></p><p><span class="math inline">\(O(4*n)\)</span>的空间建立最大堆，pop k次即可。<span class="math inline">\(O(4 \times n + k\times\log n)\)</span></p><p><strong>5 最小堆</strong></p><p>维护大小为k的最小堆，遍历数组</p><ul><li>堆顶元素大，则不管</li><li>堆顶元素小，则把当前值插入堆中</li><li>最后的堆顶，就是第k大的元素</li></ul><p><span class="math inline">\(O(n \times \log k)\)</span></p><p><strong>6 Hash法</strong></p><h1 id="查找最小的k个数的总结">查找最小的k个数的总结</h1><blockquote><p>给一个数组，找到最小的k个数。注意改变或不改变原数组</p></blockquote><p><strong>1 排序思路</strong></p><p>对n个数字从小到大排好序，再取前k个数。<code>O(nlogn + k)=O(nlogn)</code> 。<a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#总结比较">排序算法总结</a></p><p><strong>2 快速排序</strong></p><p>排好前k个即可，改变原数组。</p><p><strong>3 最大堆</strong></p><p>建立大小为k的最大堆。不改变原数组。遇到新的元素，小于堆顶，则加入</p><p><strong>4 堆排序</strong></p><p>对整个数组n进行堆排序，每次取堆顶，取k次。</p><h1 id="数组中次数超过一半的数-169">数组中次数超过一半的数-169</h1><p><a href="https://leetcode.com/problems/majority-element/description/" target="_blank" rel="noopener">Majority Element-169</a>， <code>easy</code></p><blockquote><p>一个数组，有一个元素出现次数超过一半，找到它。</p></blockquote><p><strong>思路0 先排序再找</strong></p><p><span class="math inline">\(O(n\log n)\)</span></p><p><strong>思路1 快速排序查找第k大元素思想</strong></p><p><a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/#快速排序">快速排序笔记</a> 。</p><ul><li>如果排好序，则该重复的数字应该在数组中间<span class="math inline">\(a_{\frac{n}{2}}\)</span>。 也就是中位数，第<code>n/2</code>大的数字</li><li>问题就转化为<strong>查找数组中的K大的元素</strong></li></ul><p>查找第k大的元素</p><ul><li><code>partition(a, l, r)</code> 会把<code>x=a[l]</code>放到中间去，小于的在右边，大于的在左边。返回x的最终位置<code>i</code></li><li><code>i == k</code>， 则x就是第k大的元素</li><li><code>k &lt; i</code>， 则k在右边</li><li><code>k &gt; i</code>， 则k在左边</li><li>继续查找，知道 <code>i == k</code></li></ul><p><strong>思路2 count加加减减思想</strong></p><ul><li>遇见友军（相同的），就++</li><li>遇见敌军（不同的），就--</li><li>最后剩余的肯定就是人数最多的那个（数字）</li></ul><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 查找主元素，阵地攻守思想。相同加价，不同减减，为0重新赋值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">majority_element</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.size() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 当前数值与计数</span></span><br><span class="line">    <span class="keyword">int</span> res = a[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a[i] == res) &#123;</span><br><span class="line">            <span class="comment">// 相同++</span></span><br><span class="line">            count++;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 不同--或者重置为1</span></span><br><span class="line">            <span class="keyword">if</span> (count == <span class="number">0</span>) &#123;</span><br><span class="line">                res = a[i];</span><br><span class="line">                count = <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                count--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="主元素2-229">主元素2-229</h1><blockquote><p>给一个数组，找到所有出现次数超过n/3的数</p></blockquote><p><a href="https://leetcode.com/problems/majority-element-ii/description/" target="_blank" rel="noopener">Majority Element II</a>, <code>medium</code> 。</p><p><strong>思路</strong></p><p>当然最终结果只有2个或1个。思路同阵地攻守。</p><ul><li>用两个变量去记录两个主元素</li><li>有一个相同，对应加1</li><li>两个都不同，有一个<code>count==0</code>， 则重置</li><li>两个都不听，两个都有count，则都减减</li><li>遍历之后，得到两个数，两个count</li><li>返回<code>count &gt; n/3</code>的数</li></ul><p>最后，一定要<strong>注意去重</strong>！<code>n1 != n2</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 查找出现次数超过n/3的元素</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      nums -- 数组</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      res -- 超过n/3的元素</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; majority_element(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">    <span class="keyword">if</span> (nums.empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 1. 找到出现次数最多的两个数</span></span><br><span class="line">    <span class="keyword">int</span> n1 = <span class="number">0</span>, c1 = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> n2 = <span class="number">0</span>, c2 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> n : nums) &#123;</span><br><span class="line">        <span class="keyword">if</span> (n == n1) &#123;</span><br><span class="line">            c1++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (n == n2) &#123;</span><br><span class="line">            c2++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c1 == <span class="number">0</span>) &#123;</span><br><span class="line">            n1 = n;</span><br><span class="line">            c1 = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (c2 == <span class="number">0</span>) &#123;</span><br><span class="line">            n2 = n;</span><br><span class="line">            c2 = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            c1--;</span><br><span class="line">            c2--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    c1 = <span class="number">0</span>, c2 = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 2. 重新计算出现次数</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> n : nums) &#123;</span><br><span class="line">        <span class="keyword">if</span> (n1 == n) &#123;</span><br><span class="line">            c1++;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (n2 == n) &#123;</span><br><span class="line">            c2++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 把出现次数超过n/3的数字放到res里面</span></span><br><span class="line">    <span class="keyword">if</span> (c1 &gt; nums.size() / <span class="number">3</span>) &#123;</span><br><span class="line">        res.push_back(n1);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 去重</span></span><br><span class="line">    <span class="keyword">if</span> (n2 != n1 &amp;&amp; c2 &gt; nums.size() / <span class="number">3</span>) &#123;</span><br><span class="line">        res.push_back(n2);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="最长不重复子字符串-003">最长不重复子字符串-003</h1><p><a href="https://leetcode.com/problems/longest-substring-without-repeating-characters/description/" target="_blank" rel="noopener">Longest Substring Without Repeating Characters-003</a></p><blockquote><p>给一个字符串，找到里面最长子字符串的长度。</p></blockquote><p><strong>1 DP思路</strong></p><p>设<code>l[i]=k</code>， 是以<code>s[i]</code><strong>结尾</strong>最长字符串的长度是k。<strong>从左到右进行计算</strong>。遍历到<code>s[i]</code></p><p>1、 <code>s[i]</code>在前面<strong>没有出现过</strong>，<code>l[i] = l[i-1] + 1</code></p><p>2 、<code>s[i]</code>在<strong>前面出现过</strong>，计算该字符现在与前面<strong>两次出现的距离d</strong>，比较d和<code>s[i-1]</code>的最大长度<code>l[i-1]</code></p><ul><li><code>d &gt; l[i-1]</code>， <strong>d很远</strong>（不在i-1的最长字符串里），<code>l[i] = l[i-1] + 1</code></li><li><code>d &lt;= l[i-1]</code>，<strong>d很近</strong>（在i-1的最长字符串里），<code>l[i]=d</code></li></ul><p>使用<code>HashMap&lt;Char, Int&gt;</code>去记录位置信息，保留<strong>最近时间的出现位置</strong>。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 字符串中，最长不重复子串的长度，是连续</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      s -- 原字符串</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      len -- 最长子串长度</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">long_substr_len</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (s.empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 长度，l[i]=k，以s[i]结尾的最大子串长度为k</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; l(s.length(), <span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 位置，m['a']=i，最近，字符a出现的位置是i</span></span><br><span class="line">    <span class="built_in">map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); i++) &#123;</span><br><span class="line">        <span class="keyword">char</span> ch = s[i];</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">0</span>) &#123;</span><br><span class="line">            l[i] = <span class="number">1</span>;</span><br><span class="line">            m[ch] == i;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (m.find(ch) == m.end()) &#123;</span><br><span class="line">            <span class="comment">// ch 没出现过</span></span><br><span class="line">            l[i] = l[i<span class="number">-1</span>] + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// ch 出现过，距离比较</span></span><br><span class="line">            <span class="keyword">int</span> d = i - m[ch];</span><br><span class="line">            <span class="keyword">if</span> (d &gt; l[i<span class="number">-1</span>]) &#123;</span><br><span class="line">                <span class="comment">// 上一个ch距离很远</span></span><br><span class="line">                l[i] = l[i<span class="number">-1</span>] + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 上一个ch距离很近</span></span><br><span class="line">                l[i] = d;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 更新出现位置</span></span><br><span class="line">        m[ch] = i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> *max_element(l.begin(), l.end());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="丑数-263-264-313">丑数-263-264-313</h1><p><a href="https://leetcode.com/problems/ugly-number/description/" target="_blank" rel="noopener">263-判断是否是丑数</a> ，<code>easy</code></p><blockquote><p>只包含2、3、5作为因子的正整数是丑数，1也是。判断是否是丑数</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isUgly</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 特殊情况</span></span><br><span class="line">    <span class="keyword">if</span> (num &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (num == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 不断分解2,3,5</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; a = &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> i : a) &#123;</span><br><span class="line">        <span class="keyword">while</span> (num % i == <span class="number">0</span>) &#123;</span><br><span class="line">            num /= i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> num == <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://leetcode.com/problems/ugly-number-ii/description/" target="_blank" rel="noopener">264-找到第n个丑数</a> ，<code>medium</code></p><blockquote><p>找到第n个丑数</p></blockquote><p>如果依次找，所有的数都要分解求余，效率低。**丑数=丑数*{2,3,5}<strong>。 用数组去存放丑数，</strong>从1开始依次向前面递推计算。**</p><p>用<strong>3个索引</strong>，去分别保存<strong>乘以</strong><code>2,3,5</code>的基数，每次<strong>选择最小的</strong>作为下一个丑数，同时更新<strong>对应的索引++</strong>。</p><p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 第n个丑数，从已有的丑数不断地向前乘得到新的丑数。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">nthUglyNumber</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; u(n);</span><br><span class="line">    u[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> t2 = <span class="number">0</span>, t3 = <span class="number">0</span>, t5 = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 计算丑数</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="comment">// 计算新的丑数，选择最小的</span></span><br><span class="line">        <span class="keyword">int</span> cur = min(min(u[t2] * <span class="number">2</span>, u[t3] * <span class="number">3</span>), u[t5]*<span class="number">5</span>);</span><br><span class="line">        u[i] = cur;</span><br><span class="line">        <span class="comment">// 更新基数索引</span></span><br><span class="line">        <span class="keyword">if</span> (cur == u[t2] * <span class="number">2</span>) &#123;</span><br><span class="line">            t2++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (cur == u[t3] * <span class="number">3</span>) &#123;</span><br><span class="line">            t3++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (cur == u[t5] * <span class="number">5</span>) &#123;</span><br><span class="line">            t5++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> u[n<span class="number">-1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><a href="https://leetcode.com/problems/super-ugly-number/description/" target="_blank" rel="noopener">313-超级丑数</a>， <code>medium</code></p><p>超级丑数是，<strong>给一个素数列表去计算丑数</strong>，不再局限于2,3,5去计算丑数。</p><blockquote><p>给一个素数列表和n，返回第n个丑数</p></blockquote><p>和上一个思路一致，用数组保存。注意t数组的初始大小是primes.size()，而不是n。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 给定素数，返回第n个丑数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">nthSuperUglyNumber</span><span class="params">(<span class="keyword">int</span> n, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; primes)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">0</span> || primes.empty()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; u(n);</span><br><span class="line">    u[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; t(primes.size(), <span class="number">0</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; ++i) &#123;</span><br><span class="line">        <span class="comment">// 计算新的丑数，选择最小的</span></span><br><span class="line">        <span class="keyword">int</span> cur = INT_MAX;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; primes.size(); ++j) &#123;</span><br><span class="line">            cur = min(cur, u[t[j]] * primes[j]);</span><br><span class="line">        &#125;</span><br><span class="line">        u[i] = cur;</span><br><span class="line">        <span class="comment">// 更新索引，向前推进</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; primes.size(); ++j) &#123;</span><br><span class="line">            <span class="keyword">if</span> (cur == u[t[j]] * primes[j]) &#123;</span><br><span class="line">                t[j]++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> u[n<span class="number">-1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;有序列表转二叉查找树-109&quot;&gt;有序列表转二叉查找树-109&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://leetcode.com/problems/convert-sorted-list-to-binary-search-tree/description
      
    
    </summary>
    
      <category term="leetcode" scheme="http://plmsmile.github.io/categories/leetcode/"/>
    
    
      <category term="leetcode" scheme="http://plmsmile.github.io/tags/leetcode/"/>
    
      <category term="树" scheme="http://plmsmile.github.io/tags/%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>剑指offer算法题(11-20)</title>
    <link href="http://plmsmile.github.io/2017/12/29/aim2offer2/"/>
    <id>http://plmsmile.github.io/2017/12/29/aim2offer2/</id>
    <published>2017-12-29T03:57:48.000Z</published>
    <updated>2018-01-06T10:05:40.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://plmsmile.github.io/2017/07/29/aim2offer/">剑指offer算法题(03-10)</a></p><h1 id="旋转数组中的最小值-11">旋转数组中的最小值-11</h1><h2 id="排序算法">排序算法</h2><p><a href="https://plmsmile.github.io/2017/12/26/sort-algorithms/">我的排序算法总结</a></p><h2 id="旋转数组最小值">旋转数组最小值</h2><blockquote><p>原有序数组：1,2,3,4,5,6,7，旋转一下，把一部分放到后面去：4,5,6,7, 1,2,3。</p></blockquote><blockquote><p>求：在<span class="math inline">\(O(logn)\)</span>内找到数组中最小的元素</p></blockquote><p><a href="https://leetcode.com/problems/find-minimum-in-rotated-sorted-array-ii/description/" target="_blank" rel="noopener">leetcode旋转数组</a></p><p><strong>思路</strong></p><p>特点：左边序列全部大于等于右边序列。</p><p><strong>最小的值在中间</strong>，顺序查找肯定不行，那就只能<code>二分查找</code>了。</p><p>设两个指针，<code>left</code>从<strong>左边</strong>向中间靠拢，<code>right</code>从<strong>右边</strong>向中间靠拢。</p><p><strong>循环条件</strong>：<code>a[l] &gt;= a[r]</code></p><p>直到<code>l+1==r</code> 为止，那么<code>a[r]</code>就是我们要的最小值。</p><p>计算中间值 <code>a[m]</code></p><ul><li><code>a[m] &gt;= a[l]</code> ：m在左边序列，<code>l = m</code>， l向右走</li><li><code>a[m] &lt;= a[r]</code> ： m在右边序列，<code>r == m</code>， r向左走</li></ul><p><strong>陷阱</strong></p><ul><li>可能一个数字都不旋转，即a为有序序列，直接返回<code>a[0]</code></li><li>有重复的数字，<code>a[l]==a[m] &amp;&amp; a[m]==[r]</code>， 则只能<strong>顺序查找</strong>了</li></ul><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/11_min_rotate_array.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 查找旋转数组中的最小值。两个指针指向前后两个递增序列，向中间靠拢</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">minum_rotate_array</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (a.size() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> r = a.size() - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 特殊情况，旋转0个，原数组，小于不能等于</span></span><br><span class="line">    <span class="keyword">if</span> (a[l] &lt; a[r]) &#123;</span><br><span class="line">        <span class="keyword">return</span> a[l];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> res = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (a[l] &gt;= a[r]) &#123;</span><br><span class="line">        <span class="comment">// 两个指针已经相邻</span></span><br><span class="line">        <span class="keyword">if</span> (l + <span class="number">1</span> == r) &#123;</span><br><span class="line">            res = a[r];</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 中间指针</span></span><br><span class="line">        <span class="keyword">int</span> m = (l + r) / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 三个数相等，无法确定中间数在前后那个序列</span></span><br><span class="line">        <span class="keyword">if</span> (a[l] == a[m] &amp;&amp; a[m] == a[r]) &#123;</span><br><span class="line">            res = get_min(a, l, r);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (a[m] &gt;= a[l]) &#123;</span><br><span class="line">            l = m;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (a[m] &lt;= a[r]) &#123;</span><br><span class="line">            r = m;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">get_min</span><span class="params">(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp;a, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> min = a[start];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = start + <span class="number">1</span>; i &lt;= end; i++) </span><br><span class="line">        <span class="keyword">if</span> (a[i] &lt; min) &#123;</span><br><span class="line">            min = a[i];</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">return</span> min;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="矩阵中的路径-12">矩阵中的路径-12</h1><p>给一个字符数组，手动给行和列进行分割。再给一个字符串，判断该字符串是否在该字符矩阵中。使用<strong>回溯法</strong>进行搜索。</p><p>其实就是遍历所有的点开始，然后依次进行上下左右继续查找。用<code>visited</code>去记录点是否已经走过，用<code>pathlen</code>记录目标字符串的遍历长度。</p><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 判断str在字符矩阵matrix中是否有路径</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      matrix -- 字符数组，由rows和cols切分为矩阵</span></span><br><span class="line"><span class="comment"> *      rows -- 行</span></span><br><span class="line"><span class="comment"> *      cols -- 列</span></span><br><span class="line"><span class="comment"> *      str -- 字符串</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      true -- 包含，false -- 不包含</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">has_path</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* matrix, <span class="keyword">int</span> rows, <span class="keyword">int</span> cols, <span class="keyword">const</span> <span class="keyword">char</span>* str)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (matrix== <span class="literal">nullptr</span> || rows &lt; <span class="number">1</span> || cols &lt; <span class="number">1</span> || str == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> *visited = <span class="keyword">new</span> <span class="keyword">bool</span>[rows * cols];</span><br><span class="line">    <span class="built_in">memset</span>(visited, <span class="number">0</span>, rows * cols);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> pathlen = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 从每个点开始</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> row = <span class="number">0</span>; row &lt; rows; row++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> col = <span class="number">0</span>; col &lt; cols; col++) &#123;</span><br><span class="line">            <span class="keyword">bool</span> has = go_find(matrix, rows, cols, row, col, str, pathlen, visited);</span><br><span class="line">            <span class="keyword">if</span> (has) &#123;</span><br><span class="line">                <span class="keyword">delete</span>[] visited;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">delete</span>[] visited;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 上下左右去回溯查询</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      matrix -- 字符矩阵</span></span><br><span class="line"><span class="comment"> *      rows -- 矩阵的行数</span></span><br><span class="line"><span class="comment"> *      rows -- 矩阵的列数</span></span><br><span class="line"><span class="comment"> *      row -- 当前要判断的行</span></span><br><span class="line"><span class="comment"> *      col -- 当前要判断的列</span></span><br><span class="line"><span class="comment"> *      str -- 原字符串</span></span><br><span class="line"><span class="comment"> *      pathlen -- 判断到第几个字符</span></span><br><span class="line"><span class="comment"> *      visited -- 位置访问与否</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      true -- </span></span><br><span class="line"><span class="comment"> *      false --</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">go_find</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* matrix, <span class="keyword">int</span> rows, <span class="keyword">int</span> cols, <span class="keyword">int</span> row, <span class="keyword">int</span> col, </span></span></span><br><span class="line"><span class="function"><span class="params">                    <span class="keyword">const</span> <span class="keyword">char</span>* str, <span class="keyword">int</span> &amp;pathlen, <span class="keyword">bool</span> *visited)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (str[pathlen] == <span class="string">'\0'</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">bool</span> ok = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">int</span> cur = row * cols + col;</span><br><span class="line">    <span class="keyword">if</span> (row &gt;= <span class="number">0</span> &amp;&amp; row &lt; rows &amp;&amp; col &gt;= <span class="number">0</span> &amp;&amp; col &lt; cols</span><br><span class="line">        &amp;&amp; matrix[cur] == str[pathlen] &amp;&amp; visited[cur] == <span class="literal">false</span>) &#123;</span><br><span class="line">        ++pathlen;</span><br><span class="line">        visited[cur] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">bool</span> left = go_find(matrix, rows, cols, row, col - <span class="number">1</span>, str, pathlen, visited);</span><br><span class="line">        <span class="keyword">bool</span> right = go_find(matrix, rows, cols, row, col + <span class="number">1</span>, str, pathlen, visited);</span><br><span class="line">        <span class="keyword">bool</span> down = go_find(matrix, rows, cols, row + <span class="number">1</span>, col, str, pathlen, visited);</span><br><span class="line">        <span class="keyword">bool</span> up = go_find(matrix, rows, cols, row - <span class="number">1</span>, col, str, pathlen, visited);</span><br><span class="line">        ok = left || right || down || up;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!ok) &#123;</span><br><span class="line">            --pathlen;</span><br><span class="line">            visited[cur] = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ok;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="机器人的运动范围-13">机器人的运动范围-13</h1><p>给一个<span class="math inline">\(\rm{rows \times cols}\)</span>的矩阵，和一个阈值<span class="math inline">\(k\)</span>。 机器人从<span class="math inline">\((0, 0)\)</span>出发，进入矩阵的格子。</p><p>能进入格子：(35, 38)，因为3+5+3+8<k 不能进入格子：(35,="" 56)，因为3+5+5+5="">k</k></p><p><strong>思路</strong></p><p><code>回溯法</code>， <span class="math inline">\(\rm{count = 1 + left + right + up + down}\)</span></p><p>[关键代码]</p><p><strong>定义Solution</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        <span class="comment">// 阈值</span></span><br><span class="line">        <span class="keyword">int</span> threshold;</span><br><span class="line">        <span class="comment">// 矩阵行</span></span><br><span class="line">        <span class="keyword">int</span> rows;</span><br><span class="line">        <span class="comment">// 矩阵列</span></span><br><span class="line">        <span class="keyword">int</span> cols;</span><br><span class="line">        <span class="comment">// 记录格子有没有被走过</span></span><br><span class="line">        <span class="keyword">bool</span> *visited;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="comment">// 外部接口</span></span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">movingCount</span><span class="params">(<span class="keyword">int</span> threshold, <span class="keyword">int</span> rows, <span class="keyword">int</span> cols)</span></span>;</span><br><span class="line">        <span class="comment">// 回溯计算</span></span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> row, <span class="keyword">int</span> col)</span></span>;</span><br><span class="line">        <span class="comment">// 检查该点是否可以进入</span></span><br><span class="line">        <span class="function"><span class="keyword">bool</span> <span class="title">check_point</span><span class="params">(<span class="keyword">int</span> row, <span class="keyword">int</span> col)</span></span>;</span><br><span class="line">        <span class="comment">// 把各个位的数字加起来</span></span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">resolve_num</span><span class="params">(<span class="keyword">int</span> n)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>movingcount</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 在这个矩阵和阈值上，从(0, 0)进入统计能进入多少个格子</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      threshold -- 各个位的阈值</span></span><br><span class="line"><span class="comment"> *      rows -- 矩阵的行数</span></span><br><span class="line"><span class="comment"> *      cols -- 矩阵的列数</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      count -- 可以进入的总的格子数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> Solution::movingCount(<span class="keyword">int</span> threshold, <span class="keyword">int</span> rows, <span class="keyword">int</span> cols) &#123;</span><br><span class="line">    <span class="comment">// 参数校验</span></span><br><span class="line">    <span class="keyword">if</span> (threshold &lt; <span class="number">0</span> || rows &lt; <span class="number">1</span> || cols &lt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 变量初始化</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;threshold = threshold;</span><br><span class="line">    <span class="keyword">this</span>-&gt;rows = rows;</span><br><span class="line">    <span class="keyword">this</span>-&gt;cols = cols;</span><br><span class="line">    <span class="keyword">this</span>-&gt;visited = <span class="keyword">new</span> <span class="keyword">bool</span>[rows * cols];</span><br><span class="line">    <span class="built_in">memset</span>(<span class="keyword">this</span>-&gt;visited, <span class="number">0</span>, rows * cols);</span><br><span class="line">    <span class="keyword">int</span> count = dfs(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">delete</span>[] <span class="keyword">this</span>-&gt;visited;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>回溯法上下左右格子累加</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 回溯查询</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      row -- 当前行，索引</span></span><br><span class="line"><span class="comment"> *      col -- 当前列，索引</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      count -- 从当前点(row,col)开始向上下左右走能走的格子之和</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> Solution::dfs(<span class="keyword">int</span> row, <span class="keyword">int</span> col) &#123;</span><br><span class="line">  <span class="comment">// 可以进入该点</span></span><br><span class="line">    <span class="keyword">if</span> (check_point(row, col) == <span class="literal">false</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> cur = row * cols + col;</span><br><span class="line">    visited[cur] = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">int</span> left = dfs(row, col - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">int</span> right = dfs(row, col + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">int</span> up = dfs(row - <span class="number">1</span>, col);</span><br><span class="line">    <span class="keyword">int</span> down = dfs(row + <span class="number">1</span>, col);</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">1</span> + left + right + up + down;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>检查点是否可进入和分解数</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 把n的各个位上的数加起来</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">int</span> Solution::resolve_num(<span class="keyword">int</span> n) &#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        sum += n % <span class="number">10</span>;</span><br><span class="line">        n = n / <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 检查一个点是否可以进入。索引、访问状态、阈值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">bool</span> Solution::check_point(<span class="keyword">int</span> row, <span class="keyword">int</span> col) &#123;</span><br><span class="line">    <span class="keyword">int</span> cur = row * cols + col;</span><br><span class="line">    <span class="keyword">if</span> (row &lt; <span class="number">0</span> || row &gt;= rows || col &lt; <span class="number">0</span> || col &gt;= cols || visited[cur] == <span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (resolve_num(row) + resolve_num(col) &gt; threshold) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="剪绳子-14">剪绳子-14</h1><h2 id="动态规划">动态规划</h2><p>求最优解（最大值or最小值），大问题分解成若干个小问题。使用动态规划四个特点，如下</p><ul><li>求最优解</li><li>整体问题的最优解依赖于各个子问题的最优解</li><li>若干个子问题，之间有相互重叠的更小的子问题。如f(2)是f(4)和f(6)共同的子问题</li><li>从上往下分析问题，从下网上求解问题。用数组存下小问题的解嘛。</li></ul><h2 id="贪心算法">贪心算法</h2><p>每一步做一个贪婪的选择，基于这个选择，可以得到最优解。需要数学证明。</p><h2 id="剪绳子-动态规划">剪绳子-动态规划</h2><p>条件</p><p>长度为<span class="math inline">\(n\)</span>的绳子，把绳子剪成<span class="math inline">\(m, \; (m \ge 2)\)</span>段， 每段长度为<span class="math inline">\(k[0], k[1], \cdots, k[m]\)</span>。 m和n都是整数，都大于1。</p><p>问题</p><p>长度连乘积最大是多少？</p><p><strong>思路</strong></p><p>定义： <span class="math inline">\(f(n)\)</span> 为把绳子切成若干段后，各段长度乘积的最大值</p><p>第一刀的时候，在长度为<span class="math inline">\(i\)</span>的地方剪，分成<span class="math inline">\(i\)</span>和<span class="math inline">\(n-i\)</span>两段。 递推公式如下： <span class="math display">\[f(n) = \max (f(i) \cdot f(n-i)), \quad 0 &lt; i &lt; n\]</span></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自下而上计算f[i]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算单个f[i]</span></span><br><span class="line"><span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= i / <span class="number">2</span>; j++) &#123;</span><br><span class="line">  <span class="keyword">int</span> t = f[j] * f[i - j];</span><br><span class="line">  <span class="keyword">if</span> (t &gt; max) &#123;</span><br><span class="line">    max = t;</span><br><span class="line">  &#125;</span><br><span class="line">&#125; </span><br><span class="line">f[i] = max;</span><br></pre></td></tr></table></figure><p><a href="https://github.com/plmsmile/aim2offer/blob/master/14_cutting_rope/cutting_rope.cpp" target="_blank" rel="noopener">关键代码</a></p><ul><li>长度在3以内的特殊返回</li><li>构建f[i]，f[0,1,2,3]手动赋值处理，意义不同</li><li>自下而上计算f[4, ..., length]</li><li>单独计算f[i]，找到<span class="math inline">\(\max(f_j \cdot f_{i-j})\)</span>， 其中<span class="math inline">\(j \in \{1, \cdots, i/2\}\)</span></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">max_product_dp</span><span class="params">(<span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 特殊处理，长度在3以内，自动返回</span></span><br><span class="line">    <span class="keyword">if</span> (length &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (length == <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (length == <span class="number">3</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. f[i]=k，长度为i的绳子剪成若干段最大乘积为k，i&gt;=4</span></span><br><span class="line">    <span class="keyword">int</span> * f = <span class="keyword">new</span> <span class="keyword">int</span>[length + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. f[i]特殊值处理。比如4切一刀：1-3和2-2，f[1]=1,f[3]=3, f[2]=2。因为2*2&gt;1*3，f[4]=4</span></span><br><span class="line">    f[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    f[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    f[<span class="number">2</span>] = <span class="number">2</span>;</span><br><span class="line">    f[<span class="number">3</span>] = <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 4. 自下而上计算</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">4</span>; i &lt;= length; i++) &#123;</span><br><span class="line">        max = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 找最大的分割</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt;= i / <span class="number">2</span>; j++) &#123;</span><br><span class="line">            <span class="keyword">int</span> t = f[j] * f[i - j];</span><br><span class="line">            <span class="keyword">if</span> (t &gt; max) &#123;</span><br><span class="line">                max = t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        f[i] = max;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    max = f[length];</span><br><span class="line">    <span class="keyword">delete</span>[] f;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> max;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="剪绳子-贪心">剪绳子-贪心</h2><p>贪心策略</p><ul><li>当<span class="math inline">\(n \ge 5\)</span>时， 尽可能多地剪成长度为3的绳子</li><li>当剩余长度为4的时候，要剪成2*2的绳子</li></ul><p><a href="https://github.com/plmsmile/aim2offer/blob/master/14_cutting_rope/cutting_rope.cpp" target="_blank" rel="noopener">关键代码</a></p><ol style="list-style-type: decimal"><li>特殊处理，长度在3以内，自动返回</li><li>计算3的个数</li><li>计算2的个数</li><li>计算最终结果 pow(3, t3)*pow(2,t2)</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">max_product_greedy</span><span class="params">(<span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 特殊处理，长度在3以内，自动返回</span></span><br><span class="line">    <span class="keyword">if</span> (length &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (length == <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (length == <span class="number">3</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (length == <span class="number">4</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">4</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 计算3的个数</span></span><br><span class="line">    <span class="keyword">int</span> t3 = length / <span class="number">3</span>;</span><br><span class="line">    <span class="comment">// 最后剩1，则补成4</span></span><br><span class="line">    <span class="keyword">if</span> (length - t3*<span class="number">3</span> == <span class="number">1</span>) &#123;</span><br><span class="line">        t3--;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 计算2的个数</span></span><br><span class="line">    <span class="keyword">int</span> t2 = (length - t3*<span class="number">3</span>) / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 计算最终结果 </span></span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">int</span>) <span class="built_in">pow</span>(<span class="number">3</span>, t3) * (<span class="keyword">int</span>) (<span class="built_in">pow</span>(<span class="number">2</span>, t2));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="二进制中1的个数-15">二进制中1的个数-15</h1><h2 id="基础知识">基础知识</h2><p><strong>位运算</strong></p><table><thead><tr class="header"><th align="center">操作</th><th align="center">意义</th></tr></thead><tbody><tr class="odd"><td align="center">与&amp;</td><td align="center">都为1，才为1</td></tr><tr class="even"><td align="center">或|</td><td align="center">有一个为1，就为1</td></tr><tr class="odd"><td align="center">异或^</td><td align="center">不同为1，相同为0</td></tr></tbody></table><p><strong>移位</strong></p><table><thead><tr class="header"><th align="center">操作</th><th align="center">意义</th><th align="center">注意</th></tr></thead><tbody><tr class="odd"><td align="center">左移n位</td><td align="center">丢弃左边n位，右边n位补0</td><td align="center"></td></tr><tr class="even"><td align="center">右移n位</td><td align="center">无符号数&amp;正数：右移n位，左边补n个0</td><td align="center">负数：右移n位，左边补n个1</td></tr></tbody></table><p><strong>重要结论</strong></p><table><thead><tr class="header"><th align="center">操作</th><th align="center">意义</th></tr></thead><tbody><tr class="odd"><td align="center">n &amp; (n-1)</td><td align="center">把n的二进制中，最右边的1变成0</td></tr><tr class="even"><td align="center">n &amp; 1</td><td align="center">检测n的二进制，末尾位是否是1</td></tr><tr class="odd"><td align="center">n &amp; 2</td><td align="center">检测n的二进制，倒数第二位是否是1</td></tr></tbody></table><h2 id="二进制中1的个数">二进制中1的个数</h2><p>问题：给一个整数，判断它的二进制中，有多少个1</p><p><strong>正数思路</strong></p><p>n &amp; 1，n不停右移。每次判断 n &amp; 1，即最末尾位是否是1。</p><p>但是负数有问题，负数右移，左边会补1，陷入死循环。</p><p><strong>正数负数思路</strong></p><p>n不变，n &amp; flag，flag每次向左移。即从右到左依次判断n的各个位是否是1。一共循环n的二进制位数次。</p><p><strong>最优思路</strong></p><p>n = n &amp; (n - 1)，每次把n的最右边的1变成0。看看能执行多少次这样的操作，就有多少个1。</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/15_binary_1_count/binary_1_count.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * flag不断左移</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">count_1_by_flag</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">unsigned</span> <span class="keyword">int</span> flag = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (flag != <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (n &amp; flag) &#123;</span><br><span class="line">            count++;</span><br><span class="line">        &#125;</span><br><span class="line">        flag = flag &lt;&lt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 把n的最左边1变成0，看看能变几次，则有几个1</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">count_1</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (n) &#123;</span><br><span class="line">        n = n &amp; (n - <span class="number">1</span>);</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="数值的整数次方-16">数值的整数次方-16</h1><h2 id="代码风格">代码风格</h2><p>代码的规范性：清晰的书写、清晰的布局、合理的命名</p><p>代码的完整性：功能测试、边界测试、负面测试</p><p>处理错误的方法</p><table><thead><tr class="header"><th align="center">方法</th><th align="center">优点</th><th align="center">缺点</th></tr></thead><tbody><tr class="odd"><td align="center">返回值</td><td align="center">和系统API一致</td><td align="center">不能方便使用计算结果</td></tr><tr class="even"><td align="center">全局变量</td><td align="center">能够方便使用计算结果</td><td align="center">用户可能会忘记检查全局变量</td></tr><tr class="odd"><td align="center">异常</td><td align="center">不同错误，抛出不同异常，可自定义。逻辑清晰</td><td align="center">有的语言不支持；对性能负面影响</td></tr></tbody></table><h2 id="数值的整数次方">数值的整数次方</h2><p>要求实现</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">power</span><span class="params">(<span class="keyword">double</span> base, <span class="keyword">int</span> exponent)</span></span>;</span><br></pre></td></tr></table></figure><p>注意特殊情况条件</p><ul><li>指数为0</li><li>指数为负数</li><li>指数为负数，底数不能为0或者约等于0。用fabs(a-b) &lt; theta</li></ul><p><strong>思路1</strong></p><p>条件考虑全面，一个一个乘</p><p><strong>思路2递归</strong></p><p>使用这个公式 <span class="math display">\[a^n = \begin{cases}&amp; a^{n/2} \cdot a^{n/2}  &amp;\text{n为偶数} \\&amp; a^{(n-1)/2} \cdot a^{(n-1)/2}  &amp; \text{n为奇数} \\\end{cases}\]</span> <strong>思路3位运算</strong></p><p>例如<span class="math inline">\(a^{13}\)</span>， 13的二进制是<code>1101</code>。</p><p>从右向左，依次读1，<code>flag &amp; 1</code>, <code>b = b &gt;&gt; 1</code></p><p>遇到1就累乘， <code>res *= a</code></p><p>每读一位，倍数增加， <code>a *= a</code></p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/16_power/num_power.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">power_normal_binary</span><span class="params">(<span class="keyword">double</span> base, <span class="keyword">int</span> exponent)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (exponent == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (exponent == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> base;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">double</span> res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (exponent != <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> ((exponent &amp; <span class="number">1</span>) == <span class="number">1</span>) &#123;</span><br><span class="line">            res *= base;</span><br><span class="line">        &#125;</span><br><span class="line">        base *= base;</span><br><span class="line">        exponent = exponent &gt;&gt; <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 递归计算</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">power_normal_recur</span><span class="params">(<span class="keyword">double</span> base, <span class="keyword">int</span> exponent)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (exponent == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (exponent == <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> base;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">double</span> res = power_normal_recur(base, exponent &gt;&gt; <span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 翻次方倍</span></span><br><span class="line">    res *= res;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 奇数</span></span><br><span class="line">    <span class="keyword">if</span> ((exponent &amp; <span class="number">1</span>) == <span class="number">1</span>) &#123;</span><br><span class="line">        res *= base;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">power_normal</span><span class="params">(<span class="keyword">double</span> base, <span class="keyword">int</span> exponent)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span> res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; exponent; i++) &#123;</span><br><span class="line">        res *= base;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">double</span> <span class="title">power</span><span class="params">(<span class="keyword">double</span> base, <span class="keyword">int</span> exponent)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 指数为0，返回1</span></span><br><span class="line">    <span class="keyword">if</span> (exponent == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 指数为负数，底数不能为0</span></span><br><span class="line">    <span class="keyword">if</span> (exponent &lt; <span class="number">0</span> &amp;&amp; equal(base, <span class="number">0.0</span>)) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"error. exponent &lt; 0, base should != 0"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 分正负计算</span></span><br><span class="line">    <span class="keyword">double</span> res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (exponent &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        res = power_normal_binary(base, exponent);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        res = power_normal_binary(base, -exponent);</span><br><span class="line">        res = <span class="number">1.0</span> / res;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="打印1到最大的n位数-17">打印1到最大的N位数-17</h1><p>问题：输出打印1-999。但是n不确定，非常大，<strong>要注意溢出的问题</strong>。</p><p><strong>思路</strong></p><ul><li>开辟n+1位的字符串，来存储数字。末尾位是''</li><li>字符串模拟数字的自增、进位和溢出</li><li>长度超过n+1，或者第0位产生进位，则溢出</li><li>输出的时候，要人性化点。比如0087，要打印87</li></ul><p><a href="https://github.com/plmsmile/aim2offer/blob/master/17_print_1_to_maxn/print_string.cpp" target="_blank" rel="noopener">关键代码</a></p><p>开始的时候，个位加1。要注意<strong>进位</strong>， 清楚当前数字存在哪些位置上面的。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 当前数加1，字符串模拟加法、进位、溢出</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">void</span> Solution::increment() &#123;</span><br><span class="line">    <span class="comment">// 当前位的值</span></span><br><span class="line">    <span class="keyword">int</span> now = <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">// 前一位的进位</span></span><br><span class="line">    <span class="keyword">int</span> take = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">// 当前位数是n-1~n-real_len</span></span><br><span class="line">    <span class="keyword">for</span> (i = n - <span class="number">1</span>; i &gt;= n - real_len; i--) &#123;</span><br><span class="line">        <span class="keyword">int</span> now = num[i] - <span class="string">'0'</span> + take;</span><br><span class="line">        <span class="keyword">if</span> (i == n - <span class="number">1</span>) &#123;</span><br><span class="line">            <span class="comment">// 实现自增，末尾加1</span></span><br><span class="line">            now = now + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (now &gt;= <span class="number">10</span>) &#123;</span><br><span class="line">            num[i] = <span class="string">'0'</span> + (now - <span class="number">10</span>);</span><br><span class="line">            take = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            num[i] = <span class="string">'0'</span> + now;</span><br><span class="line">            take = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 需要新增一位</span></span><br><span class="line">    <span class="keyword">if</span> (take &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">"num char* over flow"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">this</span>-&gt;over_flow = <span class="literal">true</span>;</span><br><span class="line">            <span class="comment">// 注意释放空间</span></span><br><span class="line">            <span class="keyword">delete</span> <span class="keyword">this</span>-&gt;num;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        num[i] = <span class="string">'0'</span> + <span class="number">1</span>;</span><br><span class="line">        real_len++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="删除链表中的元素-18">删除链表中的元素-18</h1><h2 id="删除一个节点">删除一个节点</h2><p>给一个链表和一个节点，删除这个节点i，要求<span class="math inline">\(O(1)\)</span></p><p><strong>思路</strong></p><p>从头到尾遍历找到i的前驱节点，时间复杂度为<span class="math inline">\(O(n)\)</span>， 显然不行。</p><p><strong>把i的后继节点的值拷贝到i上，然后删除i的后继节点</strong>。</p><p>但，要注意：头结点、中间节点、末尾节点（顺序遍历）。</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/18_del_node_in_list/delete_node.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">delete_node</span><span class="params">(ListNode ** phead, ListNode* pdeleted)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!phead || !pdeleted) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 只有一个节点，删除头节点/尾节点</span></span><br><span class="line">    <span class="keyword">if</span> (*phead == pdeleted &amp;&amp; pdeleted-&gt;next == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">delete</span> pdeleted;</span><br><span class="line">        pdeleted = <span class="literal">nullptr</span>;</span><br><span class="line">        *phead = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 有多个节点，删除尾节点</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (pdeleted-&gt;next == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        ListNode* p = *phead;</span><br><span class="line">        <span class="keyword">while</span> (p-&gt;next != pdeleted) &#123;</span><br><span class="line">            p = p-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        p-&gt;next = <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="keyword">delete</span> pdeleted;</span><br><span class="line">        pdeleted = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 有多个节点，删除中间的节点</span></span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 直接把下一个节点的值赋值到当前节点，再删除下一个节点</span></span><br><span class="line">        ListNode* pnext = pdeleted-&gt;next;</span><br><span class="line">        pdeleted-&gt;val = pnext-&gt;val;</span><br><span class="line">        pdeleted-&gt;next = pnext-&gt;next;</span><br><span class="line">        <span class="keyword">delete</span> pnext;</span><br><span class="line">        pnext = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="删除有序链表中的重复元素">删除有序链表中的重复元素</h2><p><a href="https://leetcode.com/problems/remove-duplicates-from-sorted-list/description/" target="_blank" rel="noopener">Remove Duplicates from Sorted List</a> 有序的链表。</p><p><strong>留下一个</strong></p><p>每个重复的元素，留下一个。</p><p><code>思路</code></p><p>从head开始循环，遍历每一个节点。</p><p>对于当前节点c，再进行遍历，直到下一个节点不等于它或者为空。中间删除掉重复的节点。</p><p><strong>全部删除</strong></p><p>把所有重复的元素，都删除，有序链表。</p><p><code>思路</code></p><p>当前节点<code>cur</code>， 需要保留前一个元素的指针<code>pre</code>。</p><p><strong>我的思路</strong></p><p>循环遍历到cur，head应该为第一个pre。</p><ul><li>cur无重复<code>cur!=next || next==null</code></li><li>pre为空，则<code>pre=cur</code> ； pre不为空， 则<code>pre.next=cur</code> ，<code>pre=pre.next</code> 。实际上pre.next指向下一轮的cur</li><li>cur重复 <code>cur=next</code></li><li>遍历删除所有与cur相同的元素</li><li>删除cur</li><li>但由于原本pre.next指向当前cur，cur又被删除。所以<code>pre.next=nullptr</code></li></ul><p>其实最重要是：<strong>当前不重复，则续接到pre后面，pre后移；当前重复，则删除，pre后面设为空</strong>。</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/18_del_node_in_list/del_dups_sortedlist.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode* <span class="title">del_dups_nosave</span><span class="params">(ListNode* head)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (head == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> head;</span><br><span class="line">    &#125;</span><br><span class="line">    ListNode* cur = head;</span><br><span class="line">    ListNode* pre = <span class="literal">nullptr</span>;</span><br><span class="line">    ListNode* next = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">while</span> (cur) &#123;</span><br><span class="line">        <span class="comment">// 1. cur无重复</span></span><br><span class="line">        <span class="keyword">if</span> (cur-&gt;next == <span class="literal">nullptr</span> || cur-&gt;val != cur-&gt;next-&gt;val) &#123;</span><br><span class="line">            <span class="keyword">if</span> (pre == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">                pre = cur;</span><br><span class="line">                head = pre;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                pre-&gt;next = cur;</span><br><span class="line">                pre = cur;</span><br><span class="line">            &#125;</span><br><span class="line">            cur = cur-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 2. cur重复，删掉重复的元素</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            next = cur-&gt;next;</span><br><span class="line">            <span class="comment">// 2.1 删除与cur相同的</span></span><br><span class="line">            <span class="keyword">while</span> (next &amp;&amp; cur-&gt;val == next-&gt;val) &#123;</span><br><span class="line">                cur-&gt;next = next-&gt;next;</span><br><span class="line">                <span class="keyword">delete</span> next;</span><br><span class="line">                next = cur-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 2.2 删除cur</span></span><br><span class="line">            <span class="keyword">delete</span> cur;</span><br><span class="line">            cur = next;</span><br><span class="line">            <span class="comment">// 原本pre-&gt;next=cur，但是当前cur已经被删除，所以重新置为空</span></span><br><span class="line">            <span class="keyword">if</span> (pre != <span class="literal">nullptr</span>)</span><br><span class="line">                pre-&gt;next = <span class="literal">nullptr</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (pre == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        head = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="正则表达式匹配-19">正则表达式匹配-19</h1><p><strong>问题</strong></p><p>正则表达式：普通字符、<code>.</code> 任意字符 、<code>*</code> 前面的字符出现0次或多次。</p><p>给定字符串，去判断该字符串是否符合正则表达式。</p><p>例如：<code>aaa</code> 与<code>a.a</code>和<code>ab*ac*a</code>匹配， 与<code>aa.a</code>和<code>ab*a</code>不匹配。</p><p><strong>思路</strong></p><p><strong>递归</strong>去判断。用<span class="math inline">\(s_1\)</span>和<span class="math inline">\(p_1\)</span>去 代表第一个字符和第一个模式。</p><p><span class="math inline">\(s_1\)</span>与<span class="math inline">\(p_1\)</span>匹配成功： <code>s1==p1 || (p1 == '.' &amp;&amp; s1 != '\0')</code></p><p>必须<strong>先判断<span class="math inline">\(p2\)</span>是否是</strong>*</p><p>如果，第二个模式<span class="math inline">\(p_2\)</span>是*</p><ul><li><span class="math inline">\(s_1\)</span>与<span class="math inline">\(p_1\)</span>匹配成功</li><li>*代表出现1次，<code>go(s2, p3)</code></li><li>*代表出现0次，<code>go(s1, p3)</code></li><li>*代表出现多次，<code>go(s2, p1)</code></li><li><span class="math inline">\(s_1\)</span>与<span class="math inline">\(p_1\)</span>匹配失败</li><li>*只能代表出现0次，<code>go(s1, p3)</code></li></ul><p>如果<span class="math inline">\(s_1\)</span>与<span class="math inline">\(p_1\)</span>匹配成功</p><ul><li>字符串和模式都向后挪一次，<code>go(s2, p2)</code></li></ul><p><strong>总结</strong></p><ol style="list-style-type: decimal"><li>p结束，s结束。返回true</li><li>p结束，s还有。 返回false</li><li>p未结束，<span class="math inline">\(p_2\)</span> == *。<span class="math inline">\(s_1\)</span>与<span class="math inline">\(p_1\)</span>匹配成功， p2作为0、1、多次，<strong>合并返回</strong>。匹配失败，作为0次，返回。</li><li>p未结束，<span class="math inline">\(p_2\)</span> != *，单独match成功。<code>go(s2, p2)</code></li><li>p未结束，<span class="math inline">\(p_2\)</span> != *，单独match失败。返回false</li></ol><p><a href="https://github.com/plmsmile/aim2offer/blob/master/19_regular_expression_matching/regular_expression_string.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 单个字符是否能匹配</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">single_char_match</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span> &amp;str, <span class="keyword">int</span> s, <span class="keyword">const</span> <span class="built_in">string</span> &amp;pattern, <span class="keyword">int</span> p)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 超出</span></span><br><span class="line">    <span class="keyword">if</span> (s &gt;= str.size() || p &gt;= pattern.size()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 匹配成功</span></span><br><span class="line">    <span class="keyword">if</span> (str[s] ==  pattern[p] || (pattern[p] == <span class="string">'.'</span> &amp;&amp; s &lt; str.size())) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 匹配失败</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 递归判断str和pattern是否匹配，从snow和pbgin开始</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      str -- 字符串</span></span><br><span class="line"><span class="comment"> *      snow -- 从字符串的第几个开始</span></span><br><span class="line"><span class="comment"> *      pattern -- 正则表达式</span></span><br><span class="line"><span class="comment"> *      pnow -- 模式的开始</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      true or false</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">match_core</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span> &amp;str, <span class="keyword">int</span> snow, <span class="keyword">const</span> <span class="built_in">string</span> &amp;pattern, <span class="keyword">int</span> pnow)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. p结束，s结束</span></span><br><span class="line">    <span class="keyword">if</span> (pnow &gt;= pattern.size() &amp;&amp; snow &gt;= str.size()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. p结束，s还有</span></span><br><span class="line">    <span class="keyword">if</span> (pnow &gt;= pattern.size() &amp;&amp; snow &lt; str.size()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. p未结束，p2 == *</span></span><br><span class="line">    <span class="keyword">int</span> p2 = pnow + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (p2 &lt; pattern.size() &amp;&amp; pattern[p2] == <span class="string">'*'</span>) &#123;</span><br><span class="line">        <span class="comment">// s1与p1匹配成功</span></span><br><span class="line">        <span class="keyword">if</span> (single_char_match(str, snow, pattern, pnow) == <span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="keyword">bool</span> t0 = match_core(str, snow, pattern, pnow + <span class="number">2</span>);</span><br><span class="line">            <span class="keyword">bool</span> t1 = match_core(str, snow+<span class="number">1</span>, pattern, pnow + <span class="number">2</span>);</span><br><span class="line">            <span class="keyword">bool</span> t_many = match_core(str, snow+<span class="number">1</span>, pattern, pnow);</span><br><span class="line">            <span class="keyword">return</span> t0 || t1 || t_many;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// s1与p1匹配失败</span></span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// *只能代表0</span></span><br><span class="line">            <span class="keyword">return</span> match_core(str, snow, pattern, pnow + <span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 4. p未结束，p2 != *，单独match成功</span></span><br><span class="line">    <span class="keyword">if</span> (single_char_match(str, snow, pattern, pnow) == <span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> match_core(str, snow+<span class="number">1</span>, pattern, pnow+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. p未结束，p2 != *，单独match失败</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="表示数值的字符串-20">表示数值的字符串-20</h1><p>数值：+100、5e2、-123、3.1416、-1E-16 、1.5e2</p><p>非数值：12e、1a3.14、1.2.3、+-5、12e+5.4</p><p>判断字符串是否是一个正确的数值。<code>A[.[B]][e|EC]</code>或者<code>.B[e|EC]</code></p><p>（符号）、整数、小数点、（整数）、e、整数</p><ul><li><strong>小数点前后，必须有一个整数</strong></li><li><strong>e前面是一个数，后面是一个整数</strong></li></ul><p><a href="">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span> Solution::isNumeric(<span class="keyword">const</span> <span class="built_in">string</span> &amp;str) &#123;</span><br><span class="line">    <span class="keyword">int</span> start = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">bool</span> pre_int = scan_integer(str, start);</span><br><span class="line">    <span class="keyword">bool</span> numeric = pre_int;</span><br><span class="line">    <span class="comment">// 有小数点</span></span><br><span class="line">    <span class="keyword">if</span> (start &lt; str.size() &amp;&amp; str[start] == <span class="string">'.'</span>) &#123;</span><br><span class="line">        start++;</span><br><span class="line">        <span class="keyword">bool</span> dot_int = scan_unsigned_integer(str, start);</span><br><span class="line">        <span class="comment">// 小数点前面或者后面至少有一个整数</span></span><br><span class="line">        numeric = pre_int || dot_int;</span><br><span class="line">    &#125; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// 有指数符号</span></span><br><span class="line">    <span class="keyword">if</span> (start &lt; str.size() &amp;&amp; (str[start] == <span class="string">'E'</span> ||str[start] == <span class="string">'e'</span>)) &#123;</span><br><span class="line">        start++;</span><br><span class="line">        <span class="keyword">bool</span> e_int = scan_integer(str, start);</span><br><span class="line">        <span class="comment">// e前面是一个数值，e后面是一个整数</span></span><br><span class="line">        numeric = numeric &amp;&amp; e_int;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 没有走完，还剩余字符</span></span><br><span class="line">    <span class="keyword">if</span> (start &lt; str.size()) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> numeric;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://plmsmile.github.io/2017/07/29/aim2offer/&quot;&gt;剑指offer算法题(03-10)&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;旋转数组中的最小值-11&quot;&gt;旋转数组中的最小值-11&lt;/h1&gt;
&lt;h2 id=&quot;排序
      
    
    </summary>
    
      <category term="leetcode" scheme="http://plmsmile.github.io/categories/leetcode/"/>
    
    
      <category term="leetcode" scheme="http://plmsmile.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>排序算法总结</title>
    <link href="http://plmsmile.github.io/2017/12/26/sort-algorithms/"/>
    <id>http://plmsmile.github.io/2017/12/26/sort-algorithms/</id>
    <published>2017-12-26T08:06:53.000Z</published>
    <updated>2018-03-22T09:02:41.190Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="80%"></p><h1 id="插入排序">插入排序</h1><h2 id="直接插入">直接插入</h2><blockquote><p>前面的已经有序，把后面的插入到前面有序的元素中。</p></blockquote><p><strong>步骤</strong></p><ul><li>找到待插入位置</li><li>给插入元素腾出空间，<strong>边比较边移动</strong></li></ul><p>如对<code>4  5  1  2  6  3</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5插入4</span></span><br><span class="line"><span class="number">4</span> <span class="number">5</span>   <span class="number">1</span> <span class="number">2</span> <span class="number">6</span> <span class="number">3</span></span><br><span class="line"><span class="comment"># 1插入 4 5</span></span><br><span class="line"><span class="number">1</span> <span class="number">4</span> <span class="number">5</span>   <span class="number">2</span> <span class="number">6</span> <span class="number">3</span></span><br><span class="line"><span class="comment"># 2插入 1 4 5</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">5</span>   <span class="number">6</span> <span class="number">3</span></span><br><span class="line"><span class="comment"># 6插入 1 2 4 5</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span>   <span class="number">3</span></span><br><span class="line"><span class="comment"># 3插入到前面</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span></span><br></pre></td></tr></table></figure><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/01_insert_shell_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 直接插入排序，先比较找位置，再移动</span></span><br><span class="line"><span class="comment"> **/</span> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="comment">// 最大，追加在末尾即可</span></span><br><span class="line">        <span class="keyword">if</span> (a[i] &gt; a[i<span class="number">-1</span>]) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 找到待插入的位置</span></span><br><span class="line">        <span class="keyword">int</span> k = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (a[i] &lt; a[j]) &#123;</span><br><span class="line">                k = j;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> t = a[i];</span><br><span class="line">        <span class="comment">// 先挪动元素，向后移动</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &gt; k; j--) &#123;</span><br><span class="line">            a[j] = a[j<span class="number">-1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        a[k] = t;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 直接插入排序，边比较边移动</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert_sort2</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a[i] &lt; a[i<span class="number">-1</span>]) &#123;</span><br><span class="line">            <span class="keyword">int</span> t = a[i];</span><br><span class="line">            <span class="keyword">int</span> j = i - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span> (a[j] &gt; t &amp;&amp; j &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                a[j+<span class="number">1</span>] = a[j];</span><br><span class="line">                j--;</span><br><span class="line">            &#125;</span><br><span class="line">            a[j+<span class="number">1</span>] = t;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>总结分析</strong></p><table><thead><tr class="header"><th align="center">名称</th><th align="center">时间</th><th align="center">最好</th><th align="center">最差</th><th align="center">空间</th><th align="center">稳定</th><th align="center">适用性</th></tr></thead><tbody><tr class="odd"><td align="center">直接插入</td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(n)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(1)\)</span></td><td align="center">是</td><td align="center">顺序存储和链式存储的<strong>线性表</strong></td></tr></tbody></table><h2 id="折半插入">折半插入</h2><blockquote><p>先<code>折半查找</code>出位置，再统一的移动。 若m为1个数字，则<span class="math inline">\(a[i]&gt;a[m]\)</span>，该插入位置为<span class="math inline">\(l=m+1\)</span>。</p></blockquote><p>仅仅减少了元素的比较次数，元素的移动次数依然没有改变。时间复杂度仍然为<span class="math inline">\(\mathrm{O}(n)\)</span>。</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/01_insert_shell_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 插入排序，折半查找出位置，再统一移动</span></span><br><span class="line"><span class="comment"> **/</span> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert_sort_bisearch</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a[i] &gt; a[i<span class="number">-1</span>]) &#123;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 折半查找，a[i]要插入的位置为l</span></span><br><span class="line">        <span class="keyword">int</span> l = <span class="number">0</span>, r = i - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (l &lt;= r) &#123;</span><br><span class="line">            <span class="keyword">int</span> m = (l + r) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span> (a[i] &gt; a[m]) &#123;</span><br><span class="line">                <span class="comment">// 查找右边</span></span><br><span class="line">                l = m + <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (a[i] &lt; a[m])&#123;</span><br><span class="line">                <span class="comment">// 查找左边</span></span><br><span class="line">                r = m - <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                l = m + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> t = a[i];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &gt; l; j--) &#123;</span><br><span class="line">            a[j] = a[j<span class="number">-1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        a[l] = t;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="希尔排序">希尔排序</h2><blockquote><p>希尔排序又称为<code>缩小增量排序</code>， 把整个列表，分成多个<span class="math inline">\(\rm{L[i, i+d,i+2d,\cdots, i+kd]}\)</span>这样的列表，每个进行直接插入排序。每一轮不断缩小d的值，直到全部有序。</p></blockquote><p>实际例子 <code>4 5 1 2 6 3</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># d = 3</span></span><br><span class="line"><span class="number">4</span> _ _ <span class="number">2</span> _ _</span><br><span class="line">_ <span class="number">5</span> _ _ <span class="number">6</span> _</span><br><span class="line">_ _ <span class="number">1</span> _ _ <span class="number">3</span></span><br><span class="line"><span class="comment"># 化为3个列表，分别进行直接插入排序，得到</span></span><br><span class="line"><span class="number">2</span> <span class="number">5</span> <span class="number">1</span> <span class="number">4</span> <span class="number">6</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># d = 2</span></span><br><span class="line"><span class="number">2</span> _ <span class="number">1</span> _ <span class="number">6</span> _</span><br><span class="line">_ <span class="number">5</span> _ <span class="number">4</span> _ <span class="number">3</span></span><br><span class="line"><span class="comment"># 排序，得到</span></span><br><span class="line"><span class="number">1</span> <span class="number">3</span> <span class="number">2</span> <span class="number">4</span> <span class="number">6</span> <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># d = 1</span></span><br><span class="line"><span class="number">1</span> <span class="number">3</span> <span class="number">2</span> <span class="number">4</span> <span class="number">6</span> <span class="number">5</span></span><br><span class="line"><span class="comment"># 直接插入排序，得到</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span></span><br></pre></td></tr></table></figure><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/01_insert_shell_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 希尔排序，按照步长，去划分为多个组。对这些组分别进行插入排序</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">shell_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 步长gap==组的个数</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> gap = a.size() / <span class="number">2</span>; gap &gt; <span class="number">0</span>; gap = gap / <span class="number">2</span>) &#123;</span><br><span class="line">        <span class="comment">// 对各个组进行排序</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; gap; i++) &#123;</span><br><span class="line">            group_sort(a, i, gap);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 对希尔排序中的单个组进行排序，直接插入</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a -- 数组</span></span><br><span class="line"><span class="comment"> *      start -- 该组的起始地址</span></span><br><span class="line"><span class="comment"> *      gap -- 组的步长，也是组的个数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">group_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> start, <span class="keyword">int</span> gap)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = start + gap; i &lt; a.size(); i += gap) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a[i] &lt; a[i - gap]) &#123;</span><br><span class="line">            <span class="keyword">int</span> t = a[i];</span><br><span class="line">            <span class="keyword">int</span> j = i - gap;</span><br><span class="line">            <span class="comment">// 从后向前比较，边比较，边移动</span></span><br><span class="line">            <span class="keyword">while</span> (a[j] &gt; t &amp;&amp; j &gt;= start) &#123;</span><br><span class="line">                a[j + gap] = a[j];</span><br><span class="line">                j -= gap;</span><br><span class="line">            &#125;</span><br><span class="line">            a[j + gap] = t;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>总结分析</strong></p><p>最好的增量<span class="math inline">\(d_1 = \frac{n}{2}, d_{i+1} = \lfloor \frac{d_i}{2}\rfloor\)</span></p><table><thead><tr class="header"><th align="center">名称</th><th align="center">时间</th><th align="center">最好</th><th align="center">最差</th><th align="center">空间</th><th align="center">稳定</th><th align="center">适用性</th></tr></thead><tbody><tr class="odd"><td align="center">希尔排序</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"><span class="math inline">\({O}(1)\)</span></td><td align="center">不稳定</td><td align="center">线性存储的线性表</td></tr></tbody></table><h1 id="交换排序">交换排序</h1><h2 id="冒泡排序">冒泡排序</h2><blockquote><p>执行n-1轮，每一轮把<span class="math inline">\(a[0, \ldots, i]\)</span>的最大的向下沉。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2</span> <span class="number">4</span> <span class="number">3</span> <span class="number">1</span> <span class="number">6</span> <span class="number">5</span></span><br><span class="line"><span class="comment"># 第一趟</span></span><br><span class="line"><span class="number">2</span> <span class="number">4</span> _ _ _ _</span><br><span class="line"><span class="number">2</span> <span class="number">3</span> <span class="number">4</span> _ _ _<span class="comment"># 4-3 to 3-4</span></span><br><span class="line"><span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">4</span> _ _<span class="comment"># 4-1 to 1-4 </span></span><br><span class="line"><span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">4</span> <span class="number">6</span> _</span><br><span class="line"><span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span><span class="comment"># 6-5 to 5-6</span></span><br><span class="line"><span class="comment"># 第二趟</span></span><br><span class="line"><span class="number">2</span> <span class="number">3</span> _ _ _  <span class="number">6</span></span><br><span class="line"><span class="number">2</span> <span class="number">1</span> <span class="number">3</span> _ _  <span class="number">6</span><span class="comment"># 3-1 to 1-3</span></span><br><span class="line"><span class="number">2</span> <span class="number">1</span> <span class="number">3</span> <span class="number">4</span> _  <span class="number">6</span></span><br><span class="line"><span class="number">2</span> <span class="number">1</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>  <span class="number">6</span></span><br><span class="line"><span class="comment"># 第三趟</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> _ _  <span class="number">5</span> <span class="number">6</span><span class="comment"># 2-1 to 1-2</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> _  <span class="number">5</span> <span class="number">6</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>  <span class="number">5</span> <span class="number">6</span></span><br></pre></td></tr></table></figure><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/02_bubble_quick_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubble_sort1</span><span class="params">(<span class="keyword">int</span>* a, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = n<span class="number">-1</span>; i &gt; <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="comment">// 每一轮把a[0,...,i]中最大的向下沉</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (a[j] &gt; a[j+<span class="number">1</span>]) &#123;</span><br><span class="line">                <span class="keyword">int</span> t = a[j];</span><br><span class="line">                a[j] = a[j+<span class="number">1</span>];</span><br><span class="line">                a[j+<span class="number">1</span>] = t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 若当前轮，已经没有发生交换，说明已经全部有序</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubble_sort2</span><span class="params">(<span class="keyword">int</span>* a, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> swapped = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = n - <span class="number">1</span>; i &gt; <span class="number">0</span>; i--) &#123;</span><br><span class="line">        swapped = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (a[j] &gt; a[j+<span class="number">1</span>]) &#123;</span><br><span class="line">                <span class="keyword">int</span> t = a[j];</span><br><span class="line">                a[j] = a[j+<span class="number">1</span>];</span><br><span class="line">                a[j+<span class="number">1</span>] = t;</span><br><span class="line">                swapped = <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (swapped = <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>总结分析</strong></p><table><thead><tr class="header"><th align="center">名称</th><th align="center">时间</th><th align="center">最好</th><th align="center">最差</th><th align="center">空间</th><th align="center">稳定</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">冒泡排序</td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(n)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(1)\)</span></td><td align="center">是</td><td align="center">每一轮都有一个元素到最终位置上</td></tr></tbody></table><h2 id="快速排序">快速排序</h2><p><strong>思想</strong></p><blockquote><p>把一个序列，选一个数（第一个数），进行划分。左边小于x，中间x，右边大于x。再依次递归划分左右两边。</p></blockquote><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/02_bubble_quick_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><p><strong>划分</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 划分，左边小于x，中间x，右边大于x</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a:</span></span><br><span class="line"><span class="comment"> *      l:</span></span><br><span class="line"><span class="comment"> *      r:</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      i: x=a[l]的最终所在位置</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="keyword">int</span>* a, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = a[l];</span><br><span class="line">    <span class="keyword">int</span> i = l;</span><br><span class="line">    <span class="keyword">int</span> j = r;</span><br><span class="line">    <span class="comment">// 划分</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt; j) &#123;</span><br><span class="line">        <span class="comment">// 从右到左，找到第一个小于x的a[j]，放到a[i]上</span></span><br><span class="line">        <span class="keyword">while</span> (a[j] &gt;= x &amp;&amp; j &gt; i) &#123;</span><br><span class="line">            j--;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 把a[j]放到左边i上</span></span><br><span class="line">        <span class="keyword">if</span> (j &gt; i) &#123;</span><br><span class="line">            a[i++] = a[j];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 从左到右，找到一个大于x的[i]</span></span><br><span class="line">        <span class="keyword">while</span> (a[i] &lt;= x &amp;&amp; i &lt; j) &#123;</span><br><span class="line">            i++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 把a[i]放到右边j上</span></span><br><span class="line">        <span class="keyword">if</span> (i &lt; j) &#123;</span><br><span class="line">            a[j--] = a[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// x放在中间</span></span><br><span class="line">    a[i] = x;</span><br><span class="line">    <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>递归快排</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quick_sort</span><span class="params">(<span class="keyword">int</span>* a, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 递归终止</span></span><br><span class="line">    <span class="keyword">if</span> (l &gt;= r) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2. 划分，左边小于x，中间x，右边大于x</span></span><br><span class="line">    <span class="keyword">int</span> k = partition(a, l, r);</span><br><span class="line">    <span class="comment">// 3. 递归快排左右两边</span></span><br><span class="line">    quick_sort(a, l, k - <span class="number">1</span>);</span><br><span class="line">    quick_sort(a, k + <span class="number">1</span>, r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>非递归快排</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quick_sort_stack</span><span class="params">(<span class="keyword">int</span>* a, <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; st;</span><br><span class="line">    <span class="comment">// 注意进栈和出栈的顺序</span></span><br><span class="line">    st.push(r);</span><br><span class="line">    st.push(l);</span><br><span class="line">    <span class="keyword">while</span> (st.empty() == <span class="literal">false</span>) &#123;</span><br><span class="line">      <span class="comment">// 每次出栈一组</span></span><br><span class="line">        i = st.top();</span><br><span class="line">        st.pop();</span><br><span class="line">        j = st.top();</span><br><span class="line">        st.pop();</span><br><span class="line">        <span class="keyword">if</span> (i &lt; j) &#123;</span><br><span class="line">            <span class="keyword">int</span> k = partition(a, i, j);</span><br><span class="line">          <span class="comment">// 左边的</span></span><br><span class="line">            <span class="keyword">if</span> (k &gt; i) &#123;</span><br><span class="line">                st.push(k - <span class="number">1</span>);</span><br><span class="line">                st.push(i);</span><br><span class="line">            &#125;</span><br><span class="line">          <span class="comment">// 右边的</span></span><br><span class="line">            <span class="keyword">if</span> (k &lt; j) &#123;</span><br><span class="line">                st.push(j);</span><br><span class="line">                st.push(k + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>总结分析</strong></p><p>第i趟完成后，最少有i个元素在最终位置。</p><table><thead><tr class="header"><th align="center">名称</th><th align="center">时间</th><th align="center">最好</th><th align="center">最差</th><th align="center">空间</th><th align="center">稳定</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">快排</td><td align="center"><span class="math inline">\({O}(n\log n)\)</span></td><td align="center"><span class="math inline">\(O(n\log n)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(\log_2n)\)</span>栈的深度</td><td align="center">不稳定</td><td align="center">基本有序或者逆序，效果最差</td></tr></tbody></table><h1 id="选择排序">选择排序</h1><h2 id="简单选择">简单选择</h2><blockquote><p>前面已经有序，<strong>从后面选择最小的</strong>与前面末尾最大的进行交换。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4</span>  <span class="number">5</span> <span class="number">1</span> <span class="number">2</span> <span class="number">6</span> <span class="number">3</span></span><br><span class="line"><span class="comment"># 选择1与4交换</span></span><br><span class="line"><span class="number">1</span>  <span class="number">5</span> <span class="number">4</span> <span class="number">2</span> <span class="number">6</span> <span class="number">3</span></span><br><span class="line"><span class="comment"># 选择2与5交换</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span>  <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">3</span></span><br><span class="line"><span class="comment"># 选择3与4交换</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span>  <span class="number">5</span> <span class="number">6</span> <span class="number">4</span></span><br><span class="line"><span class="comment"># 选择4与5交换</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span>  <span class="number">6</span> <span class="number">5</span></span><br><span class="line"><span class="comment"># 选择5与6交换</span></span><br><span class="line"><span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span>  <span class="number">6</span></span><br></pre></td></tr></table></figure><p><strong>总结分析</strong></p><table><thead><tr class="header"><th align="center">名称</th><th align="center">时间</th><th align="center">最好</th><th align="center">最差</th><th align="center">空间</th><th align="center">稳定</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">简单选择</td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\(O(1)\)</span></td><td align="center">不稳定</td><td align="center"></td></tr></tbody></table><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/03_select_heap_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 简单选择排序，选择后面最小的来与当前有序的最后一个（最大的）交换</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">select_sort</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> k = min_index(a, i, n<span class="number">-1</span>);</span><br><span class="line">        <span class="keyword">if</span> (a[i<span class="number">-1</span>] &gt; a[k]) &#123;</span><br><span class="line">            swap(a, i<span class="number">-1</span>, k);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="堆">堆</h2><p><strong>堆</strong></p><p>堆是一颗完全二叉树。</p><p>有n个节点，堆的索引从0开始，<strong>节点i的</strong></p><ul><li>左孩子：<span class="math inline">\(2\cdot i+1\)</span></li><li>右孩子：<span class="math inline">\(2\cdot i +2\)</span></li><li>父亲：<span class="math inline">\(\lceil \frac{i-1}{2}\rceil\)</span></li><li>最后一个节点是：下标为<span class="math inline">\(\lfloor n/2\rfloor-1\)</span> 的节点的孩子，即第<span class="math inline">\(\lfloor n/2\rfloor\)</span>个节点的孩子。</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap_02.jpg" style="display:block; margin:auto" width="100%"></p><p><strong>大根堆</strong></p><blockquote><p>大根堆：最大元素在根节点。小根堆：最小元素在根节点。</p></blockquote><p><code>90 70 80 60 10 40 50 30 20</code> 是一个大根堆，<code>10 20 70 30 50 90 80 60 40</code> 是一个小根堆，如下</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/max-heap.jpg" style="display:block; margin:auto" width="40%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/min-heap.jpg" style="display:block; margin:auto" width="40%"></p><p><strong>堆添加</strong></p><p>先把元素<strong>添加到末尾</strong>，再<strong>依次向上面调整</strong>，若大于父节点，就<strong>和父节点交换</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap_04.jpg" style="display:block; margin:auto" width="100%"></p><p><strong>堆删除</strong></p><p><strong>删除堆顶元素</strong>：把末尾元素，放到堆顶，再依次向下调整，每次<strong>选择较大的子节点进行交换</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap_05.jpg" style="display:block; margin:auto" width="100%"></p><p><strong>删除堆中元素</strong>：把末尾元素，放入空白处，再调整堆即可。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap_06.jpg" style="display:block; margin:auto" width="100%"></p><h2 id="堆排序">堆排序</h2><p><strong>思路</strong></p><ol style="list-style-type: decimal"><li>初始化堆。把<span class="math inline">\(a_1, \cdots, a_n\)</span>构造成为最大堆</li><li>取出最大值。每次出堆根最大元素。出<span class="math inline">\(a_1\)</span>，把<span class="math inline">\(a_n\)</span>放到<span class="math inline">\(a_1\)</span>上，再把<span class="math inline">\(a_1, \cdots. a_{n-1}\)</span>调整为最大堆。再出元素</li><li>重复2</li></ol><p><strong>建立堆</strong></p><p>n个元素，<strong>最后一个父亲节点</strong><span class="math inline">\(\lfloor n/2\rfloor\)</span>， 下标为<span class="math inline">\(k=\lfloor n/2\rfloor-1\)</span>。</p><p>对这些节点<span class="math inline">\(a_k, \cdots, a_0\)</span>， <strong>依次调整它们的子树，使子树成堆</strong>。即，若<strong>根节点小于左右节点的较大值，则交换</strong>。</p><p><strong>建立堆实例</strong></p><p>对于数据{20,30,90,40,70,110,60,10,100,50,80}，建立为最大堆{110,100,90,40,80,20,60,10,30,50,70}</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap/heap_p1_01.jpg" style="display:block; margin:auto" width="100%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap/heap_p1_02.jpg" style="display:block; margin:auto" width="100%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap/heap_p1_03.jpg" style="display:block; margin:auto" width="100%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap/heap_p1_04.jpg" style="display:block; margin:auto" width="100%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap/heap_p1_05.jpg" style="display:block; margin:auto" width="100%"></p><p><strong>取出最大值</strong></p><p>把根节点（最大值）和当前堆的末尾值进行交换，最大值放到最后。再对剩余的数据进行成堆，再依次取最大值交换。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/heap/heap_p2.jpg" style="display:block; margin:auto" width="100%"></p><p>每一次取出最大值重新恢复堆，要<span class="math inline">\(O(\log n)\)</span>，有n个数，一共是<span class="math inline">\(O(n \log n)\)</span>。</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/03_select_heap_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 保证以start为根节点的子树是一个最大堆，末尾元素为end</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a: 存放堆的数组</span></span><br><span class="line"><span class="comment"> *      start: 根节点</span></span><br><span class="line"><span class="comment"> *      end: 子树的末尾元素</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      None</span></span><br><span class="line"><span class="comment"> **/</span> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">max_heap_down</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 当前节点</span></span><br><span class="line">    <span class="keyword">int</span> c = start;</span><br><span class="line">    <span class="comment">// 左孩子</span></span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">2</span> * c + <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 当前父亲节点</span></span><br><span class="line">    <span class="keyword">int</span> t = a[c];</span><br><span class="line">    <span class="keyword">for</span> (; l &lt;= end; c = l, l = <span class="number">2</span>*c + <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="comment">// 选择较大的孩子与父亲交换</span></span><br><span class="line">        <span class="keyword">if</span> (l + <span class="number">1</span> &lt;= end &amp;&amp; a[l] &lt; a[l + <span class="number">1</span>]) &#123;</span><br><span class="line">            <span class="comment">// 有右孩子，并且右孩子比左孩子大，则选择右孩子</span></span><br><span class="line">            l++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (t &gt;= a[l]) &#123;</span><br><span class="line">            <span class="comment">// 父亲大于孩子</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 交换</span></span><br><span class="line">            a[c] = a[l];</span><br><span class="line">            a[l] = t;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 堆排序，升序</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">heap_sort_asc</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = a.size();</span><br><span class="line">    <span class="comment">// 初始化一个最大堆</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = n / <span class="number">2</span> - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        max_heap_down(a, i, n - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 依次取堆顶元素放到末尾</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="comment">// max放到a[i]</span></span><br><span class="line">        <span class="keyword">int</span> t = a[i];</span><br><span class="line">        a[i] = a[<span class="number">0</span>];</span><br><span class="line">        a[<span class="number">0</span>] = t;</span><br><span class="line">        <span class="comment">// 保证a[0...i-1]依然是个最大堆</span></span><br><span class="line">        max_heap_down(a, <span class="number">0</span>, i<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>总结分析</strong></p><table><thead><tr class="header"><th align="center">名称</th><th align="center">时间</th><th align="center">最好</th><th align="center">最差</th><th align="center">空间</th><th align="center">稳定</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">堆排序</td><td align="center"><span class="math inline">\({O}(n \log n)\)</span></td><td align="center"><span class="math inline">\({O}(n\log n)\)</span></td><td align="center"><span class="math inline">\({O}( \log n)\)</span></td><td align="center"><span class="math inline">\(O(n)\)</span></td><td align="center">不稳定</td><td align="center"></td></tr></tbody></table><h1 id="归并排序">归并排序</h1><h2 id="从上往下-递归">从上往下-递归</h2><p><strong>思路</strong></p><ul><li>分解 -- 将区间一分为二，求分裂点<span class="math inline">\(\rm{mid} = \frac{\rm{start +end}}{2}\)</span></li><li>递归求解，<code>sort</code>， <code>sort</code> -- <strong>递归对两个无序子区间</strong> <span class="math inline">\(a_s,\cdots , a_m\)</span>和<span class="math inline">\(a_{m+1}, \cdots, a_{e}\)</span>进行<strong>归并排序</strong>。<strong>终结条件是子区间长度为1</strong></li><li>合并，<code>merge</code> -- <strong>把两个有序的子区间</strong> <span class="math inline">\(a_s,\cdots , a_m\)</span>和<span class="math inline">\(a_{m+1}, \cdots, a_{e}\)</span> <strong>合并</strong>为一个完整的有序区间<span class="math inline">\(a_s, \cdots, a_e\)</span></li></ul><p><strong>分解</strong></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/merge/merge_02.jpg" style="display:block; margin:auto" width="80%"></p><p><strong>分解&amp;递归&amp;合并</strong></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/merge/merge_01.jpg" style="display:block; margin:auto" width="80%"></p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/04_merge_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 归并排序，从上到下</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge_sort_up2down</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> start, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (start &gt;= end) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> mid = (start + end) / <span class="number">2</span>;</span><br><span class="line">    <span class="comment">// 递归排序a[start...mid]</span></span><br><span class="line">    merge_sort_up2down(a, start, mid);</span><br><span class="line">    <span class="comment">// 递归排序a[mid+1...end]</span></span><br><span class="line">    merge_sort_up2down(a, mid + <span class="number">1</span>, end);</span><br><span class="line">    <span class="comment">// 两个有序序列merge在一起</span></span><br><span class="line">    merge(a, start, mid, end);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将a中前后两个有序序列合并在一起</span></span><br><span class="line"><span class="comment"> **/</span> </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> start, <span class="keyword">int</span> mid, <span class="keyword">int</span> end)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 把有序序列临时存放到t中</span></span><br><span class="line">    <span class="keyword">int</span> * t = <span class="keyword">new</span> <span class="keyword">int</span> [end - start + <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> i = start;</span><br><span class="line">    <span class="keyword">int</span> j = mid + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 依次合并</span></span><br><span class="line">    <span class="keyword">while</span> (i &lt;= mid &amp;&amp; j &lt;= end) &#123;</span><br><span class="line">        <span class="keyword">if</span> (a[i] &lt; a[j]) &#123;</span><br><span class="line">            t[k++] = a[i++];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            t[k++] = a[j++];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (i &lt;= mid) &#123;</span><br><span class="line">        t[k++] = a[i++];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (j &lt;= end) &#123;</span><br><span class="line">        t[k++] = a[j++];</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">    <span class="comment">// 把新的有序列表复制回a中</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; k; i++) &#123;</span><br><span class="line">        a[start + i] = t[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">delete</span> [] t;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="从下往上-非递归">从下往上-非递归</h2><p><strong>思想</strong></p><p>把数组分成若干个长度为1的子数组，再两两合并；得到长度为2的数组，再两两合并；依次反复，直到形成一个数组。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/merge/merge_03.jpg" style="display:block; margin:auto" width="80%"></p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/04_merge_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 归并排序，从下到上</span></span><br><span class="line"><span class="comment"> **/</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge_sort_down2up</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.size() &lt;= <span class="number">0</span>) </span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; a.size(); i = i * <span class="number">2</span>)</span><br><span class="line">        merge_groups(a, i);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 对a做若干次合并，分为若干个gap。对每相邻的两个gap进行合并排序</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a: 数组</span></span><br><span class="line"><span class="comment"> *      gap: 一个子数组的长度</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge_groups</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a, <span class="keyword">int</span> gap)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> twolen = <span class="number">2</span> * gap;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i + twolen - <span class="number">1</span> &lt; a.size(); i += twolen) &#123;</span><br><span class="line">        <span class="keyword">int</span> start = i;</span><br><span class="line">        <span class="keyword">int</span> mid = i + gap - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> end = i + twolen - <span class="number">1</span>;</span><br><span class="line">        merge(a, start, mid, end);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 最后还有一个gap</span></span><br><span class="line">    <span class="keyword">if</span> (i + gap - <span class="number">1</span> &lt; a.size() - <span class="number">1</span>) &#123;</span><br><span class="line">        merge(a, i, i + gap - <span class="number">1</span>, a.size() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>总结分析</strong></p><table><thead><tr class="header"><th align="center">名称</th><th align="center">时间</th><th align="center">最好</th><th align="center">最差</th><th align="center">空间</th><th align="center">稳定</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">归并排序</td><td align="center"><span class="math inline">\({O}(n \log n)\)</span></td><td align="center"><span class="math inline">\({O}(n\log n)\)</span></td><td align="center"><span class="math inline">\({O}(n \log n)\)</span></td><td align="center"><span class="math inline">\(O(n)\)</span>， merge占用</td><td align="center">稳定</td><td align="center"></td></tr></tbody></table><p>归并排序的形式是一颗二叉树，遍历的次数就是二叉树的深度<span class="math inline">\(O(\log n)\)</span>， 而一共n个数。</p><h1 id="桶排序">桶排序</h1><p>桶排序很简单。数组a有n个数，最大值为max。则，建立一个长度为max的数组b，初始化为0。</p><p>遍历a，遇到<span class="math inline">\(a_i = k\)</span>， 则<span class="math inline">\(b_k += 1\)</span> 。即在对应的桶里计数加1。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/bucket_01.jpg" style="display:block; margin:auto" width="80%"></p><h1 id="基数排序">基数排序</h1><p>基数排序分为最高位优先和最低位优先。</p><p>基数排序是桶排序的扩展。把所有的数，统一位数。然后，按照每一位进行，从低位到高位跑排序。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/radix_01.jpg" style="display:block; margin:auto" width="80%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/radix_02.jpg" style="display:block; margin:auto" width="80%"></p><p>关键是找到output和buckets的对应关系。每个bucket存储前面累积的元素的数量。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">buckets[<span class="number">2</span>] = <span class="number">1</span>;</span><br><span class="line"><span class="comment">// 说明排序数是2的元素有1个</span></span><br><span class="line">buckets[<span class="number">3</span>] = <span class="number">4</span>;</span><br><span class="line"><span class="comment">// 说明排序数是3的元素有 4-1=3个</span></span><br><span class="line">buckets[<span class="number">4</span>] = <span class="number">7</span>;</span><br><span class="line"><span class="comment">// 说明排序数是4的元素有 7-4=3个</span></span><br></pre></td></tr></table></figure><p>后面在进行根据<code>排序数</code>找到当前数的最终所在位置的时候，就会利用这个关系。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 比如排序数是3的数字，会出现3个</span></span><br><span class="line"><span class="comment">// 则id就为 buckets[3]-1</span></span><br><span class="line"><span class="comment">// 每出现一个，则buckets[3]--</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 举个例子</span></span><br><span class="line"><span class="comment">// 初始buckets[2]=1，则这1个数字的最终序号是：0</span></span><br><span class="line"><span class="comment">// 初始buckets[3]=4，则这3个数字的最终序号是：3,2,1</span></span><br></pre></td></tr></table></figure><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ds/radix_03.jpg" style="display:block; margin:auto" width="80%"></p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/11_sort/05_radix_sort.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 基数排序</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">radix_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;a)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.size() &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> max = *max_element(a.begin(), a.end());</span><br><span class="line">    <span class="comment">// exp=1, 10, 100, 1000...</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> <span class="built_in">exp</span> = <span class="number">1</span>; max / <span class="built_in">exp</span> &gt; <span class="number">0</span>; <span class="built_in">exp</span> *= <span class="number">10</span>) &#123;</span><br><span class="line">        count_sort(a, <span class="built_in">exp</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 对数组按照某个位数进行排序</span></span><br><span class="line"><span class="comment"> * Args:</span></span><br><span class="line"><span class="comment"> *      a -- 数组</span></span><br><span class="line"><span class="comment"> *      exp -- 指数，1, 10, 100... 分别按照个位、十位、百位排序</span></span><br><span class="line"><span class="comment"> * Returns:</span></span><br><span class="line"><span class="comment"> *      None</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">count_sort</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; a, <span class="keyword">int</span> <span class="built_in">exp</span>)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 存储被排序数据的临时数组</span></span><br><span class="line">    <span class="keyword">int</span> output [a.size()];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 桶 数据的出现次数</span></span><br><span class="line">    <span class="keyword">int</span> buckets[<span class="number">10</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> t = (a[i] / <span class="built_in">exp</span>) % <span class="number">10</span>;</span><br><span class="line">        buckets[t]++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据前面的出现次数，推算出当前数字在原数组中的index</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">10</span>; i++) </span><br><span class="line">        buckets[i] += buckets[i - <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将数据存储到output中</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = a.size() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="keyword">int</span> j = (a[i] / <span class="built_in">exp</span>) % <span class="number">10</span>;</span><br><span class="line">        <span class="keyword">int</span> k = buckets[j];</span><br><span class="line">        output[k - <span class="number">1</span>] = a[i];</span><br><span class="line">        buckets[j]--;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 赋值给a</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.size(); i++) &#123;</span><br><span class="line">        a[i] = output[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>总结分析</strong></p><table><thead><tr class="header"><th align="center">名称</th><th align="center">时间</th><th align="center">最好</th><th align="center">最差</th><th align="center">空间</th><th align="center">稳定</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">基数排序</td><td align="center"><span class="math inline">\({O(d(n+r))}\)</span></td><td align="center"><span class="math inline">\({O(d(n+r))}\)</span></td><td align="center"><span class="math inline">\({O(d(n+r))}\)</span></td><td align="center"><span class="math inline">\(O(r)\)</span>， r个队列</td><td align="center">稳定</td><td align="center"></td></tr></tbody></table><h1 id="总结比较">总结比较</h1><h2 id="总结">总结</h2><p><strong>思想总结</strong></p><table><thead><tr class="header"><th align="center">名称</th><th align="center">一句话描述</th></tr></thead><tbody><tr class="odd"><td align="center">直接插入</td><td align="center">前面有序，为新来的，在前面找到合适的位置，进行插入</td></tr><tr class="even"><td align="center">折半插入</td><td align="center">前面有序，为新来的，使用<strong>折半查找</strong>到插入位置，进行插入</td></tr><tr class="odd"><td align="center">希尔排序</td><td align="center"><strong>gap个间隔为gap的子序列</strong>，<strong>每个进行直接插入排序</strong>；减小gap，依次排序，直至为1</td></tr><tr class="even"><td align="center">冒泡排序</td><td align="center">交换n-1趟，<span class="math inline">\(a_{i-1}&gt;a_i\)</span>，则进行交换，每一趟都有个最大的沉到末尾</td></tr><tr class="odd"><td align="center">快排</td><td align="center">第一个数x，先<strong>划分</strong>，左边小于x，中间x，右边大于x。再依次<strong>递归排序左右两边</strong></td></tr><tr class="even"><td align="center">简单选择</td><td align="center">前面有序，从后面选择最小的与前面末尾的（最大的）进行交换</td></tr><tr class="odd"><td align="center">堆排序</td><td align="center">初始化<strong>大根堆</strong>，<strong>堆顶和末尾元素交换</strong>，再调整<strong>使剩下的元素成堆</strong>， 重复</td></tr><tr class="even"><td align="center">归并排序</td><td align="center"><strong>分解</strong>为左右两个序列，对左右两个序列进行<strong>递归归并排序</strong>，<strong>再合并</strong>。即sort,sort,merge</td></tr><tr class="odd"><td align="center">基数排序</td><td align="center">位数一样，从低位到高位，分别按照每一位进行排序</td></tr></tbody></table><p><strong>时空复杂度总结</strong></p><table><thead><tr class="header"><th align="center">名称</th><th align="center">时间</th><th align="center">最好</th><th align="center">最差</th><th align="center">空间</th><th align="center">稳定</th></tr></thead><tbody><tr class="odd"><td align="center">直插</td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(n)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(1)\)</span></td><td align="center">是</td></tr><tr class="even"><td align="center">希尔</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"><span class="math inline">\({O}(1)\)</span></td><td align="center">不稳定</td></tr><tr class="odd"><td align="center">冒泡</td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(n)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(1)\)</span></td><td align="center">是</td></tr><tr class="even"><td align="center">快排</td><td align="center"><span class="math inline">\({O}(n\log n)\)</span></td><td align="center"><span class="math inline">\(O(n\log n)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(\log_2n)\)</span>，栈的深度</td><td align="center">不稳定</td></tr><tr class="odd"><td align="center">简选</td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\({O}(n^2)\)</span></td><td align="center"><span class="math inline">\(O(1)\)</span></td><td align="center">不稳定</td></tr><tr class="even"><td align="center">堆排序</td><td align="center"><span class="math inline">\({O}(n \log n)\)</span></td><td align="center"><span class="math inline">\({O}(n\log n)\)</span></td><td align="center"><span class="math inline">\({O}( \log n)\)</span></td><td align="center"><span class="math inline">\(O(n)\)</span></td><td align="center">不稳定</td></tr><tr class="odd"><td align="center">归并排序</td><td align="center"><span class="math inline">\({O}(n \log n)\)</span></td><td align="center"><span class="math inline">\({O}(n\log n)\)</span></td><td align="center"><span class="math inline">\({O}(n \log n)\)</span></td><td align="center"><span class="math inline">\(O(n)\)</span>， merge占用</td><td align="center">稳定</td></tr><tr class="even"><td align="center">基数排序</td><td align="center"><span class="math inline">\({O(d(n+r))}\)</span></td><td align="center"><span class="math inline">\({O(d(n+r))}\)</span></td><td align="center"><span class="math inline">\({O(d(n+r))}\)</span></td><td align="center"><span class="math inline">\(O(r)\)</span>， r个队列</td><td align="center">稳定</td></tr></tbody></table><p>稳定的：<strong>插、冒、归、基</strong></p><p>较快的：<strong>快、些、归、堆</strong>，<strong>插得好、冒得好</strong></p><p>比较次数与初始状态无关：选择、归并</p><p>排序趟数与初始状态无关：选择、插入、基数</p><p>冒、选、堆：每趟都有1个在最终的位置，最大or最小</p><p>直接插入：第<span class="math inline">\(i\)</span>趟，前<span class="math inline">\(i+1\)</span>个有序，但不是最终序列</p><p>快排： <span class="math inline">\(i\)</span>趟后， 至少有<span class="math inline">\(i\)</span>个在最终位置</p><p><strong>时间复杂度</strong>：快些归堆<span class="math inline">\({O(n\log n)}\)</span>，基数排序<span class="math inline">\(O(d(n+r))\)</span> ， 其余<span class="math inline">\(O(n^2)\)</span> ，</p><p><strong>空间复杂度</strong>：归<span class="math inline">\(O(n)\)</span>， 快<span class="math inline">\(O(\log n)\)</span>， 基<span class="math inline">\(O(r)\)</span>， 其余<span class="math inline">\(O(1)\)</span></p><h2 id="算法选择">算法选择</h2><table><thead><tr class="header"><th align="center">条件</th><th align="center">可选算法</th></tr></thead><tbody><tr class="odd"><td align="center">n较小， <span class="math inline">\(n \le 50\)</span></td><td align="center">直接插入、简单选择</td></tr><tr class="even"><td align="center">基本有序</td><td align="center">直接插入、冒泡</td></tr><tr class="odd"><td align="center">n较大，要<span class="math inline">\(O(n \log n)\)</span></td><td align="center">快排、堆排（不稳定），归并排序（稳定）</td></tr><tr class="even"><td align="center">n较大，要快，要稳定</td><td align="center">归并排序，与直插结合的改进的归并排序</td></tr><tr class="odd"><td align="center">n很大，位数很少，可以分解</td><td align="center">基数排序</td></tr><tr class="even"><td align="center">记录本身信息量太大</td><td align="center">为了避免移动，可以使用链表作为存储结构</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;80%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;插入排序&quot;&gt;插入排序&lt;/h1&gt;
&lt;h2 id=&quot;直接插入&quot;&gt;直接插入&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;前面的已经
      
    
    </summary>
    
      <category term="leetcode" scheme="http://plmsmile.github.io/categories/leetcode/"/>
    
    
      <category term="排序" scheme="http://plmsmile.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
      <category term="插入" scheme="http://plmsmile.github.io/tags/%E6%8F%92%E5%85%A5/"/>
    
      <category term="快排，快速排序" scheme="http://plmsmile.github.io/tags/%E5%BF%AB%E6%8E%92%EF%BC%8C%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"/>
    
      <category term="堆排序" scheme="http://plmsmile.github.io/tags/%E5%A0%86%E6%8E%92%E5%BA%8F/"/>
    
      <category term="归并排序" scheme="http://plmsmile.github.io/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/"/>
    
      <category term="基数排序" scheme="http://plmsmile.github.io/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/"/>
    
      <category term="冒泡排序" scheme="http://plmsmile.github.io/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>cpp-pointer-object-reference</title>
    <link href="http://plmsmile.github.io/2017/12/25/cpp-pointer-object-reference/"/>
    <id>http://plmsmile.github.io/2017/12/25/cpp-pointer-object-reference/</id>
    <published>2017-12-25T13:17:36.000Z</published>
    <updated>2017-12-26T06:52:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="http://www.cnblogs.com/flylong0204/p/4731318.html" target="_blank" rel="noopener">类对象和指针的区别</a></p></blockquote><h1 id="类对象和指针">类对象和指针</h1><h2 id="代码">代码</h2><p>类</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>&#123;</span></span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">      <span class="keyword">int</span> a;</span><br><span class="line">      Test()&#123;</span><br><span class="line">          a = <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>类指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">test1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 1. 两个类的指针，使用了new</span></span><br><span class="line">    Test* t1 = <span class="keyword">new</span> Test();</span><br><span class="line">    t1-&gt;a = <span class="number">10</span>;</span><br><span class="line">    Test* t2 = <span class="keyword">new</span> Test();</span><br><span class="line">    t2-&gt;a = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 两个指针的所指向的地址不一样</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t1:"</span> &lt;&lt; t1 &lt;&lt; <span class="string">" a = "</span> &lt;&lt; t1-&gt;a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t2:"</span> &lt;&lt; t2 &lt;&lt;  <span class="string">" a = "</span> &lt;&lt; t2-&gt;a &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 指针t1的值赋值给了t2，两个指针指向的地址相同，都是t1的地址</span></span><br><span class="line">    t2 = t1;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t1:"</span> &lt;&lt; t1 &lt;&lt; <span class="string">" a = "</span> &lt;&lt; t1-&gt;a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t2:"</span> &lt;&lt; t2 &lt;&lt;  <span class="string">" a = "</span> &lt;&lt; t2-&gt;a &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    t1-&gt;a = <span class="number">111</span>;</span><br><span class="line">    t2-&gt;a = <span class="number">222</span>;</span><br><span class="line">    <span class="comment">// 4. 修改了同样的地方，输出为222</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t1:"</span> &lt;&lt; t1 &lt;&lt; <span class="string">" a = "</span> &lt;&lt; t1-&gt;a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t2:"</span> &lt;&lt; t2 &lt;&lt;  <span class="string">" a = "</span> &lt;&lt; t2-&gt;a &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类对象</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">test2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 两个类对象</span></span><br><span class="line">    Test t1;</span><br><span class="line">    t1.a = <span class="number">10</span>;</span><br><span class="line">    Test t2;</span><br><span class="line">    t2.a = <span class="number">5</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 2. 对象的内容不一样</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t1:"</span> &lt;&lt; &amp;t1 &lt;&lt; <span class="string">" a = "</span> &lt;&lt; t1.a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t2:"</span> &lt;&lt; &amp;t2 &lt;&lt;  <span class="string">" a = "</span> &lt;&lt; t2.a &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 3. 把t1对象的内容赋值给t2。t2的内容和t1相同</span></span><br><span class="line">    t2 = t1;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t1:"</span> &lt;&lt; &amp;t1 &lt;&lt; <span class="string">" a = "</span> &lt;&lt; t1.a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t2:"</span> &lt;&lt; &amp;t2 &lt;&lt;  <span class="string">" a = "</span> &lt;&lt; t2.a &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4. 再分别修改两个不同的对象，输出分别为111， 222</span></span><br><span class="line">    t1.a = <span class="number">111</span>;</span><br><span class="line">    t2.a = <span class="number">222</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t1:"</span> &lt;&lt; &amp;t1 &lt;&lt; <span class="string">" a = "</span> &lt;&lt; t1.a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"&amp;t2:"</span> &lt;&lt; &amp;t2 &lt;&lt;  <span class="string">" a = "</span> &lt;&lt; t2.a &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="不同点">不同点</h2><table><thead><tr class="header"><th align="center"></th><th align="center">类指针</th><th align="center">类对象</th></tr></thead><tbody><tr class="odd"><td align="center">内存分配</td><td align="center">定义类指针，不分配内存</td><td align="center">定义类对象，分配内存</td></tr><tr class="even"><td align="center">关系</td><td align="center">内存地址，指向类对象。利用构造函数分配一块内存</td><td align="center"></td></tr><tr class="odd"><td align="center">访问</td><td align="center">间接访问</td><td align="center">直接访问</td></tr><tr class="even"><td align="center">多态</td><td align="center">实现多态，父类指针调用子类对象</td><td align="center">不能实现多态，声明即调用构造函数，已分配内存</td></tr><tr class="odd"><td align="center">堆栈</td><td align="center">内存堆，永久变量，手动释放，new-delete搭配</td><td align="center">内存栈，局部临时变量</td></tr><tr class="even"><td align="center"></td><td align="center">new的对象在堆中</td><td align="center">在栈中</td></tr><tr class="odd"><td align="center">使用</td><td align="center">new： <code>S* s = new S()</code> , <code>s-&gt;name</code></td><td align="center">声明即可： <code>S s;</code>，<code>s.name</code></td></tr><tr class="even"><td align="center">生命期</td><td align="center">需要delete</td><td align="center">类的析构函数来释放空间</td></tr><tr class="odd"><td align="center">传参</td><td align="center">指针4个字节</td><td align="center">对象，参数传递资源占用太大</td></tr><tr class="even"><td align="center">虚函数f</td><td align="center">调用分配给它空间时那种类的func</td><td align="center">调用自己的func</td></tr><tr class="odd"><td align="center">其它</td><td align="center">父类指针指向子类对象</td><td align="center"></td></tr></tbody></table><p>推荐使用<code>const &amp;引用</code>，安全系数高。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://www.cnblogs.com/flylong0204/p/4731318.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;类对象和指针的区别&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
      
    
    </summary>
    
      <category term="cpp" scheme="http://plmsmile.github.io/categories/cpp/"/>
    
    
      <category term="cpp" scheme="http://plmsmile.github.io/tags/cpp/"/>
    
      <category term="指针，对象，引用" scheme="http://plmsmile.github.io/tags/%E6%8C%87%E9%92%88%EF%BC%8C%E5%AF%B9%E8%B1%A1%EF%BC%8C%E5%BC%95%E7%94%A8/"/>
    
  </entry>
  
  <entry>
    <title>cs224n-assignment-1</title>
    <link href="http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/"/>
    <id>http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/</id>
    <published>2017-12-17T04:47:06.000Z</published>
    <updated>2017-12-21T14:13:14.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="softmax">Softmax</h1><h2 id="softmax常数不变性">Softmax常数不变性</h2><p><span class="math display">\[\rm{softmax}(\mathbf{x})_i = \frac{e^{\mathbf x_i}}{\sum_{j}e^{\mathbf{x}_j}}\]</span></p><p>一般在计算<code>softmax</code>的时候，<strong>避免太大的数，要加一个常数</strong>。 一般是减去最大的数。 <span class="math display">\[\rm{softmax}(x) = \rm{softmax}(x+c)\]</span></p><h2 id="关键代码">关键代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    exp_func = <span class="keyword">lambda</span> x: np.exp(x - np.max(x))</span><br><span class="line">    sum_func = <span class="keyword">lambda</span> x: <span class="number">1.0</span> / np.sum(x)</span><br><span class="line">    x = np.apply_along_axis(exp_func, <span class="number">-1</span>, x)</span><br><span class="line">    denom = np.apply_along_axis(sum_func, <span class="number">-1</span>, x)</span><br><span class="line">    denom = denom[..., np.newaxis]</span><br><span class="line">    x = x * denom</span><br><span class="line">   <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h1 id="神经网络基础">神经网络基础</h1><h2 id="sigmoid实现">Sigmoid实现</h2><p><a href="https://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/#sigmoid">我的sigmoid笔记</a> <span class="math display">\[\begin{align}&amp; \sigma (z) = \frac {1} {1 + \exp(-z)}, \; \sigma(z) \in (0,1) \\ \\&amp; \sigma^\prime (z) = \sigma(z) (1 - \sigma(z)) \\\end{align}\]</span> <strong>关键代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    s = <span class="number">1.0</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_grad</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">""" 对sigmoid的函数值，求梯度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ds = s * (<span class="number">1</span> - s)</span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure><h2 id="softmax求梯度">Softmax求梯度</h2><p>交叉熵和softmax如下，记softmax的输入为<span class="math inline">\(\theta\)</span> ，<span class="math inline">\(y\)</span>是真实one-hot向量。 <span class="math display">\[\begin{align}&amp; \rm{CE}(y, \hat y) = - \sum_{i} y_i \times \log (\hat y_i) \\ \\&amp; \hat y = \rm{softmax} (\theta)\\\end{align}\]</span> <strong>softmax求导</strong></p><p>引入记号： <span class="math display">\[\begin{align}&amp; f_i = e^{\theta_i} &amp; \text{分子} \\&amp; g_i = \sum_{k=1}^{K}e^{\theta_k} &amp; \text{分母，与i无关} \\&amp; \hat y_i = S_i = \frac{f_i}{g_i} &amp; \text{softmax}\\\end{align}\]</span> 则有<span class="math inline">\(S_i​\)</span>对其中的一个数据<span class="math inline">\(\theta_j​\)</span> 求梯度： <span class="math display">\[\frac{\partial S_i}{\partial \theta_j} = \frac{f_i^{\prime} g_i - f_i g_i^{\prime}}{g_i^2}\]</span> 其中两个导数 <span class="math display">\[f^{\prime}_i(\theta_j) =\begin{cases}&amp; e^{\theta_j}, &amp; i = j\\&amp; 0, &amp; i \ne j \\\end{cases}\]</span></p><p><span class="math display">\[g^{\prime}_i(\theta_j) = e^{\theta_j}\]</span></p><p><strong><span class="math inline">\(i=j\)</span>时</strong> <span class="math display">\[\begin{align}\frac{\partial S_i}{\partial \theta_j}&amp; = \frac{e^{\theta_j} \cdot \sum_{k}e^{\theta_k}-  e^{\theta_i} \cdot e^{\theta_j}}{\left( \sum_ke^{\theta_k}\right)^2} \\ \\&amp; = \frac{e^{\theta_j}}{\sum_ke^{\theta_k}} \cdot \left( 1 - \frac{e^{\theta_j}}{\sum_k e^{\theta_k}} \right)  \\ \\&amp; = S_i \cdot (1 - S_i)\end{align}\]</span> <strong><span class="math inline">\(i \ne j\)</span>时</strong> <span class="math display">\[\begin{align}\frac{\partial S_i}{\partial \theta_j}&amp; = \frac{ -  e^{\theta_i} \cdot e^{\theta_j}}{\left( \sum_ke^{\theta_k}\right)^2}  = - S_i \cdot  S_j\end{align}\]</span></p><h2 id="交叉熵求梯度">交叉熵求梯度</h2><p><span class="math display">\[\begin{align}&amp; \rm{CE}(y, \hat y) = - \sum_{i} y_i \times \log (\hat y_i) \\ \\&amp; \hat y = \rm{S} (\theta)\\\end{align}\]</span></p><p>只关注有关系的部分，带入<span class="math inline">\(y_i =1\)</span> ： <span class="math display">\[\begin{align}\frac{\partial CE}{\partial \theta_i}&amp; =  -\frac{\partial \log \hat y_i}{\partial \theta_i}  = - \frac{1}{\hat y_i} \cdot  \frac{\partial \hat y_i}{\partial \theta_i} \\ \\&amp; = - \frac{1}{S_i} \cdot \frac{\partial S_i}{\partial \theta_i}  = S_i - 1  \\ \\ &amp; = \hat y_i - y_i\end{align}\]</span> 不带入求导 <span class="math display">\[\begin{align}\frac{\partial CE}{\partial \theta_i}&amp; = - \sum_{k}y_k \times \frac{\partial \log S_k}{\partial \theta_i} \\&amp; = - \sum_{k}y_k \times \frac{1}{S_k}\times \frac{\partial S_k}{\partial \theta_i}   \\ &amp; = - y_i (1 - S_i) - \sum_{k \ne i} y_k \cdot \frac{1}{S_k} \cdot (- S_i \cdot S_k) \\ &amp; = - y_i (1 - S_i) + \sum_{k \ne i} y_k \cdot S_i \\&amp; =  S_i - y_i \end{align}\]</span> 所以，交叉熵的导数是 <span class="math display">\[\frac{\partial CE}{\partial \theta_i} = \hat y_i  - y_i, \quad \quad \frac{\partial CE(y, \hat y)}{\partial \theta} = \hat y  - y\]</span></p><p>即 <span class="math display">\[\frac{\partial CE(y, \hat y)}{\partial \theta_i} = \begin{cases}&amp; \hat y_i - 1, &amp; \text{i是label} \\&amp;\hat y_i, &amp; \text{其它}\\\end{cases}\]</span></p><h2 id="简单网络">简单网络</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/note3/three_layer.png" style="display:block; margin:auto" width="40%"></p><p><strong>前向计算</strong> <span class="math display">\[\begin{align}&amp; z_1 = xW_1 + b_1 \\ \\&amp; h = \rm{sigmoid}(z1) \\ \\&amp; z_2 = hW_2 + b_2 \\ \\&amp; \hat y = \rm{softmax}(z_2) \end{align}\]</span> 关键代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></span><br><span class="line">    h = sigmoid(np.dot(data, W1) + b1)</span><br><span class="line">    yhat = softmax(np.dot(h, W2) + b2)</span><br></pre></td></tr></table></figure><p><strong>loss函数</strong> <span class="math display">\[J = \rm{CE}(y, \hat y)\]</span> 关键代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></span><br><span class="line">    <span class="comment"># yhat[labels==1]实际上是boolean索引，见我的numpy_api.ipynb</span></span><br><span class="line">    cost = np.sum(-np.log(yhat[labels == <span class="number">1</span>])) / data.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>反向传播</strong> <span class="math display">\[\begin {align}&amp; \delta_2 = \frac{\partial J}{\partial z_2} = \hat y - y \\ \\&amp; \frac{\partial J}{\partial h} = \delta_2 \cdot \frac{\partial z_2}{\partial h} = \delta_2  W_2^T \\ \\&amp; \delta_1 = \frac{\partial J}{\partial z_1} = \frac{\partial J}{\partial h} \cdot \frac{\partial h}{\partial z_1} = \delta_2  W_2^T \circ \sigma^{\prime}(z_1) \\ \\&amp;  \frac{\partial J}{\partial x} = \delta_1 W_1^T\end{align}\]</span> 一共有<span class="math inline">\((d_x + 1) \cdot d_h + (d_h +1) \cdot d_y\)</span> 个参数。</p><p>关键代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></span><br><span class="line">    <span class="comment"># 前面推导的softmax梯度公式</span></span><br><span class="line">    gradyhat = (yhat - labels) / data.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 链式法则</span></span><br><span class="line">    gradW2 = np.dot(h.T, gradyhat)</span><br><span class="line">    <span class="comment"># 本地导数是1，把第1维的所有加起来</span></span><br><span class="line">    gradb2 = np.sum(gradyhat, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    gradh = np.dot(gradyhat, W2.T)</span><br><span class="line">    gradz1 = gradh * sigmoid_grad(h)</span><br><span class="line">    gradW1 = np.dot(data.T, gradz1)</span><br><span class="line">    gradb1 = np.sum(gradz1, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),</span><br><span class="line">            gradW2.flatten(), gradb2.flatten()))</span><br><span class="line">    <span class="keyword">return</span> cost, grad</span><br></pre></td></tr></table></figure><h2 id="梯度检查">梯度检查</h2><p><a href="https://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/#梯度检查">我的梯度检查</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradcheck_naive</span><span class="params">(f, x)</span>:</span></span><br><span class="line">    fx, grad = f(x) <span class="comment"># Evaluate function value at original point</span></span><br><span class="line">    h = <span class="number">1e-4</span>        <span class="comment"># Do not change this!</span></span><br><span class="line">    <span class="comment"># Iterate over all indexes in x</span></span><br><span class="line">    it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line">        ix = it.multi_index</span><br><span class="line">        <span class="comment"># 关键代码</span></span><br><span class="line">        x[ix] += h</span><br><span class="line">        random.setstate(rndstate)</span><br><span class="line">        new_f1 = f(x)[<span class="number">0</span>]</span><br><span class="line">        x[ix] -= <span class="number">2</span> * h</span><br><span class="line">        random.setstate(rndstate)</span><br><span class="line">        new_f2 = f(x)[<span class="number">0</span>]</span><br><span class="line">        x[ix] += h</span><br><span class="line">        numgrad = (new_f1 - new_f2) / (<span class="number">2</span> * h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compare gradients</span></span><br><span class="line">        reldiff = abs(numgrad - grad[ix]) / max(<span class="number">1</span>, abs(numgrad), abs(grad[ix]))</span><br><span class="line">        <span class="keyword">if</span> reldiff &gt; <span class="number">1e-5</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Gradient check failed."</span>)</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"First gradient error found at index %s"</span> % str(ix))</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Your gradient: %f \t Numerical gradient: %f"</span> % (</span><br><span class="line">                grad[ix], numgrad))</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        it.iternext() <span class="comment"># Step to next dimension</span></span><br></pre></td></tr></table></figure><h1 id="word2vec">Word2Vec</h1><p><a href="https://plmsmile.github.io/2017/11/12/cs224n-notes1-word2vec/#word2vec">我的word2vec笔记</a></p><h2 id="词向量的梯度">词向量的梯度</h2><p>符号定义</p><ul><li><span class="math inline">\(v_c\)</span> 中心词向量，输入词向量，<span class="math inline">\(V\)</span>， <span class="math inline">\(\mathbb{R}^{W\times d}\)</span><br></li><li><span class="math inline">\(u_o\)</span> 上下文词向量，输出词向量，<span class="math inline">\(U=[u_1, u_2, \cdots, u_w]\)</span> , <span class="math inline">\(\mathbb{R}^{d\times W}\)</span></li></ul><p><strong>前向</strong></p><p>预测o是c的上下文概率，o为正确单词 <span class="math display">\[\hat y_o = p(o \mid c) = \rm{softmax}(o) = \frac{\exp(u_o^T v_c)}{\sum_{w} \exp(u_w^T v_c)}\]</span> 得分向量： <span class="math display">\[z=U^T \cdot v_c, \quad  [W,d] \times[ d] \in ,\mathbb{R}^{W }\]</span> <strong>loss及梯度</strong> <span class="math display">\[J_{\rm{softmax-CE}}(v_c, o, U) = CE(y, \hat y), \quad \text{其中} \; \frac{\partial CE(y, \hat y)}{\partial \theta} = \hat y  - y\]</span></p><table><colgroup><col width="35%"><col width="8%"><col width="42%"><col width="13%"></colgroup><thead><tr class="header"><th align="center">梯度</th><th align="center">中文</th><th align="center">计算</th><th align="center">维数</th></tr></thead><tbody><tr class="odd"><td align="center"><span class="math inline">\(\frac{\partial J}{\partial z}\)</span></td><td align="center">softmax</td><td align="center"><span class="math inline">\(\hat y - y\)</span></td><td align="center"><span class="math inline">\(W\)</span></td></tr><tr class="even"><td align="center"><span class="math inline">\(\frac{\partial J}{\partial v_c}\)</span></td><td align="center">中心词</td><td align="center"><span class="math inline">\(\frac{\partial J}{\partial z} \cdot \frac{\partial z}{\partial v_c} = (\hat y - y) \cdot U^T\)</span></td><td align="center"><span class="math inline">\(d\)</span></td></tr><tr class="odd"><td align="center"><span class="math inline">\(\frac{\partial J}{\partial U}\)</span></td><td align="center">上下文</td><td align="center"><span class="math inline">\(\frac{\partial J}{\partial z} \cdot \frac{\partial z}{\partial U^T}= (\hat y - y) \cdot v_c\)</span></td><td align="center"><span class="math inline">\(d \times W\)</span></td></tr></tbody></table><p><strong>关键代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmaxCostAndGradient</span><span class="params">(predicted, target, outputVectors, dataset)</span>:</span></span><br><span class="line">    <span class="string">""" Softmax cost function for word2vec models</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        predicted: 中心词vc</span></span><br><span class="line"><span class="string">        target: 上下文uo, index</span></span><br><span class="line"><span class="string">        outputVectors: 输出，上下文矩阵U，W*d，未转置</span></span><br><span class="line"><span class="string">        dataset: </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cost: 交叉熵loss</span></span><br><span class="line"><span class="string">        gradv: 一维向量</span></span><br><span class="line"><span class="string">        gradU: W*d</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    vhat = predicted</span><br><span class="line">    z = np.dot(outputVectors,vhat)</span><br><span class="line">    preds = softmax(z)</span><br><span class="line">    <span class="comment">#  Calculate the cost:</span></span><br><span class="line">    cost = -np.log(preds[target])</span><br><span class="line">    <span class="comment">#  Gradients</span></span><br><span class="line">    gradz = preds.copy()</span><br><span class="line">    gradz[target] -= <span class="number">1.0</span></span><br><span class="line">    gradU = np.outer(z, vhat)</span><br><span class="line">    gradv = np.dot(outputVectors.T, z)</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line">    <span class="keyword">return</span> cost, gradv, gradU</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;softmax&quot;&gt;Softmax&lt;/h1&gt;
&lt;h2 id=&quot;softmax常数不变性&quot;&gt;Softmax常数不变性&lt;/h2&gt;

      
    
    </summary>
    
      <category term="cs224n" scheme="http://plmsmile.github.io/categories/cs224n/"/>
    
    
      <category term="cs224n" scheme="http://plmsmile.github.io/tags/cs224n/"/>
    
      <category term="assignment" scheme="http://plmsmile.github.io/tags/assignment/"/>
    
      <category term="word2vec" scheme="http://plmsmile.github.io/tags/word2vec/"/>
    
      <category term="cbow" scheme="http://plmsmile.github.io/tags/cbow/"/>
    
      <category term="skip-gram" scheme="http://plmsmile.github.io/tags/skip-gram/"/>
    
  </entry>
  
  <entry>
    <title>nlp-labels</title>
    <link href="http://plmsmile.github.io/2017/12/03/nlp-labels/"/>
    <id>http://plmsmile.github.io/2017/12/03/nlp-labels/</id>
    <published>2017-12-03T07:31:23.000Z</published>
    <updated>2017-12-12T03:50:30.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>词性标注和句法依存的符号</p></blockquote><h1 id="词性标注">词性标注</h1><p><a href="https://www.ibm.com/support/knowledgecenter/zh/SS5RWK_3.5.0/com.ibm.discovery.es.ta.doc/iiysspostagset.htm" target="_blank" rel="noopener">IBM英语和中文标注集</a></p><h2 id="书上的">书上的</h2><p>斯坦福<code>Stanford coreNLP</code> 中 <code>宾州树库</code> 中的<code>汉语词性标注规范</code>， 如下</p><table><thead><tr class="header"><th align="center">词性标记</th><th align="center">英文名称</th><th align="center">中文名称</th><th align="center">示例</th></tr></thead><tbody><tr class="odd"><td align="center">AD</td><td align="center">adverbs</td><td align="center">副词</td><td align="center">还</td></tr><tr class="even"><td align="center">AS</td><td align="center">aspect marker</td><td align="center">体表词</td><td align="center">了，着，过，的（我是去年来过的）</td></tr><tr class="odd"><td align="center">BA</td><td align="center">in ba-const</td><td align="center">把，将</td><td align="center">把，将</td></tr><tr class="even"><td align="center">CC</td><td align="center">coordinating conjunction</td><td align="center">并列连词</td><td align="center">和，与，或，或者</td></tr><tr class="odd"><td align="center">CD</td><td align="center">cardinal conj</td><td align="center">数词，基数词</td><td align="center">一百</td></tr><tr class="even"><td align="center">CS</td><td align="center">subordinating conj</td><td align="center">从数连词</td><td align="center">若，如果，如</td></tr><tr class="odd"><td align="center">DEC</td><td align="center">for relative-clause etc</td><td align="center">标句词，关系从句<code>的</code></td><td align="center">我买<code>的</code>书</td></tr><tr class="even"><td align="center">DEG</td><td align="center">associative</td><td align="center">所有格、连接作用<code>的</code></td><td align="center">我<code>的</code>书</td></tr><tr class="odd"><td align="center">DT</td><td align="center">determiner</td><td align="center">限定词</td><td align="center">这</td></tr><tr class="even"><td align="center">ETC</td><td align="center">tag for words in coordination phrase</td><td align="center">等，等等</td><td align="center">科技文教<code>等</code>领域，等，等等</td></tr><tr class="odd"><td align="center">IJ</td><td align="center">Interjection</td><td align="center">感叹词</td><td align="center">啊</td></tr><tr class="even"><td align="center">JJ</td><td align="center">noun-modifier other than nouns</td><td align="center">其他名词修饰语</td><td align="center"><code>共同</code>的/DEG目的/NN他/PN是/VC<code>男</code>的/DEG</td></tr><tr class="odd"><td align="center">LB</td><td align="center">in long bei-construction</td><td align="center">长被</td><td align="center"><code>被</code>他打了</td></tr><tr class="even"><td align="center">LC</td><td align="center">localizer</td><td align="center">方位词</td><td align="center">桌子<code>上</code></td></tr><tr class="odd"><td align="center">M</td><td align="center">measure word（including classifiers）</td><td align="center">量词</td><td align="center">一<code>块</code>糖</td></tr><tr class="even"><td align="center">MSP</td><td align="center">some particles</td><td align="center">其他结构助词</td><td align="center">他/PN <code>所</code> 需要/VV 的/DEC <code>所</code>，<code>而</code>，<code>以</code></td></tr><tr class="odd"><td align="center">NN</td><td align="center">common nouns</td><td align="center">普通名词</td><td align="center"><code>桌子</code></td></tr><tr class="even"><td align="center">NR</td><td align="center">proper nouns</td><td align="center">专有名词</td><td align="center"><code>天安门</code></td></tr><tr class="odd"><td align="center">NT</td><td align="center">temporal nouns</td><td align="center">时间名词</td><td align="center">清朝，一月</td></tr><tr class="even"><td align="center">OD</td><td align="center">ordinal numbers</td><td align="center">序数词</td><td align="center">第一</td></tr><tr class="odd"><td align="center">ON</td><td align="center">onomatopoeia</td><td align="center">拟声词</td><td align="center">哗啦啦</td></tr><tr class="even"><td align="center">P</td><td align="center">prepositions</td><td align="center">介词</td><td align="center">在</td></tr><tr class="odd"><td align="center">PN</td><td align="center">pronouns</td><td align="center">代词</td><td align="center">你，我，他</td></tr><tr class="even"><td align="center">PU</td><td align="center">punctuations</td><td align="center">标点</td><td align="center">， 。</td></tr><tr class="odd"><td align="center">SB</td><td align="center">In long bei-consturction</td><td align="center">短被</td><td align="center">他/PN 被/SB 训了/AS</td></tr><tr class="even"><td align="center">SP</td><td align="center">Sentence-final particle</td><td align="center">句末助词</td><td align="center">你好吧、SP吧 呢 啊 吗</td></tr><tr class="odd"><td align="center">VA</td><td align="center">Predicative adjective</td><td align="center">谓词形容词</td><td align="center">太阳 红彤彤/VA 雪白 丰富</td></tr><tr class="even"><td align="center">VC</td><td align="center">Copula</td><td align="center">系动词</td><td align="center">是 为 非</td></tr><tr class="odd"><td align="center">VE</td><td align="center">as the main verb</td><td align="center">“有”作为主要动词</td><td align="center">有，无</td></tr><tr class="even"><td align="center">VV</td><td align="center">verbs</td><td align="center">普通动词</td><td align="center">喜欢，走</td></tr></tbody></table><h2 id="自己总结的">自己总结的</h2><table><thead><tr class="header"><th align="center">标记</th><th align="center">英文</th><th align="center">中文</th></tr></thead><tbody><tr class="odd"><td align="center">NP</td><td align="center">noun phrase</td><td align="center">名词短语</td></tr><tr class="even"><td align="center">PP</td><td align="center">prepositional phrase</td><td align="center">介词短语</td></tr><tr class="odd"><td align="center">VP</td><td align="center">verb phrase</td><td align="center">动词短语</td></tr><tr class="even"><td align="center">NNS</td><td align="center"></td><td align="center">名词（复数）</td></tr><tr class="odd"><td align="center">NNP</td><td align="center"></td><td align="center">专有名词（单数）</td></tr><tr class="even"><td align="center">NNPS</td><td align="center"></td><td align="center">专有名词（复数）</td></tr><tr class="odd"><td align="center">PRP</td><td align="center"></td><td align="center">人称代词</td></tr><tr class="even"><td align="center">JJ</td><td align="center"></td><td align="center">形容词</td></tr><tr class="odd"><td align="center">JJS</td><td align="center"></td><td align="center">形容词（最高级）</td></tr><tr class="even"><td align="center">JJR</td><td align="center"></td><td align="center">形容词（比较级）</td></tr><tr class="odd"><td align="center">MD</td><td align="center"></td><td align="center">情态动词</td></tr><tr class="even"><td align="center">VB</td><td align="center"></td><td align="center">动词</td></tr><tr class="odd"><td align="center">VBP</td><td align="center"></td><td align="center">动词，现在时，非第三人称单数</td></tr><tr class="even"><td align="center">VBZ</td><td align="center"></td><td align="center">动词，现在时，第三人称单数</td></tr><tr class="odd"><td align="center">VBG</td><td align="center"></td><td align="center">动词，动名词，现在分词</td></tr><tr class="even"><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr class="odd"><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><h2 id="英语标记">英语标记</h2><p><strong>复杂版</strong></p><table><thead><tr class="header"><th align="center">词性标记</th><th align="center">描述</th></tr></thead><tbody><tr class="odd"><td align="center">UNKNOW</td><td align="center">未知词</td></tr><tr class="even"><td align="center">DT</td><td align="center">限定词</td></tr><tr class="odd"><td align="center">QT</td><td align="center">量词</td></tr><tr class="even"><td align="center">CD</td><td align="center">基数</td></tr><tr class="odd"><td align="center">NN</td><td align="center">名词，单数</td></tr><tr class="even"><td align="center">NNS</td><td align="center">名词，复数</td></tr><tr class="odd"><td align="center">NNP</td><td align="center">专有名词，单数</td></tr><tr class="even"><td align="center">NNPS</td><td align="center">专有名词，复数</td></tr><tr class="odd"><td align="center">EX</td><td align="center">存在性的There</td></tr><tr class="even"><td align="center">PRP</td><td align="center">人称代词，PP</td></tr><tr class="odd"><td align="center">PRP$</td><td align="center">物主代词，PP$</td></tr><tr class="even"><td align="center">POS</td><td align="center">所有格结束词</td></tr><tr class="odd"><td align="center">RB</td><td align="center">副词</td></tr><tr class="even"><td align="center">RBS</td><td align="center">副词，最高级</td></tr><tr class="odd"><td align="center">RBR</td><td align="center">副词，比较级</td></tr><tr class="even"><td align="center"></td><td align="center"></td></tr><tr class="odd"><td align="center"></td><td align="center"></td></tr><tr class="even"><td align="center"></td><td align="center"></td></tr><tr class="odd"><td align="center"></td><td align="center"></td></tr></tbody></table><h1 id="句法依存">句法依存</h1><p>中心语为谓语</p><table><thead><tr class="header"><th align="center">符号</th><th align="center">意义</th><th align="center">英语</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">subj</td><td align="center">主语</td><td align="center">subject</td><td align="center"></td></tr><tr class="even"><td align="center">nsubj</td><td align="center">名词性主语</td><td align="center">nominal subject</td><td align="center">同步、建设</td></tr><tr class="odd"><td align="center">top</td><td align="center">主题</td><td align="center">topic</td><td align="center">是，建筑</td></tr><tr class="even"><td align="center">npsubj</td><td align="center">被动型主语</td><td align="center">nominal passive subject</td><td align="center">被句子 中的主语</td></tr><tr class="odd"><td align="center">csubj</td><td align="center">从句主语</td><td align="center">clausal subject</td><td align="center">中文里无</td></tr><tr class="even"><td align="center">xsubj</td><td align="center">x主语</td><td align="center"></td><td align="center">一般一个主语下含多个从句</td></tr></tbody></table><p>中心语为谓语或介词</p><table><thead><tr class="header"><th align="center">符号</th><th align="center">意义</th><th align="center">英语</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">obj</td><td align="center">宾语</td><td align="center">object</td><td align="center"></td></tr><tr class="even"><td align="center">dobj</td><td align="center">直接宾语</td><td align="center"></td><td align="center">颁布，文件</td></tr><tr class="odd"><td align="center">iobj</td><td align="center">间接宾语</td><td align="center">indirect object</td><td align="center">基本不存在</td></tr><tr class="even"><td align="center">range</td><td align="center">间接宾语为数量词，格</td><td align="center"></td><td align="center">成交，元</td></tr><tr class="odd"><td align="center">pobj</td><td align="center">介词宾语</td><td align="center"></td><td align="center">根据，要求</td></tr><tr class="even"><td align="center">lobj</td><td align="center">时间介词</td><td align="center"></td><td align="center">来，今年</td></tr></tbody></table><p>中心语为谓词</p><table><thead><tr class="header"><th align="center">符号</th><th align="center">意义</th><th align="center">英语</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">comp</td><td align="center">补语</td><td align="center"></td><td align="center"></td></tr><tr class="even"><td align="center">ccomp</td><td align="center">从句补语</td><td align="center"></td><td align="center">一般由两个动词组成，</td></tr><tr class="odd"><td align="center">xcomp</td><td align="center">x从句补语</td><td align="center">xclausal complement</td><td align="center">不存在</td></tr><tr class="even"><td align="center">acomp</td><td align="center">形容词补语</td><td align="center">adjectival complement</td><td align="center"></td></tr><tr class="odd"><td align="center">tcomp</td><td align="center">时间补语</td><td align="center">temporal complement</td><td align="center">遇到，以前</td></tr><tr class="even"><td align="center">lccomp</td><td align="center">位置补语</td><td align="center">localizer complement</td><td align="center">占，以上</td></tr><tr class="odd"><td align="center">rscomp</td><td align="center">结果补语</td><td align="center">resultative complement</td><td align="center"></td></tr></tbody></table><p>中心语为名词</p><table><thead><tr class="header"><th align="center">符号</th><th align="center">意义</th><th align="center">英语</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">mod</td><td align="center">修饰语</td><td align="center">modifier</td><td align="center"></td></tr><tr class="even"><td align="center">pass</td><td align="center">被动修饰</td><td align="center">passive</td><td align="center"></td></tr><tr class="odd"><td align="center">tmod</td><td align="center">时间修饰</td><td align="center">temporal modifier</td><td align="center"></td></tr><tr class="even"><td align="center">remod</td><td align="center">关系从句修饰</td><td align="center">relative clause modifier</td><td align="center">问题，遇到</td></tr><tr class="odd"><td align="center">numod</td><td align="center">数量修饰</td><td align="center">numeric modifier</td><td align="center">规定，若干</td></tr><tr class="even"><td align="center">ornmod</td><td align="center">序数修饰</td><td align="center">numeric modifier</td><td align="center"></td></tr><tr class="odd"><td align="center">clf</td><td align="center">类别修饰</td><td align="center">classifier modifier</td><td align="center">文件，件</td></tr><tr class="even"><td align="center">nmod</td><td align="center">符合名词修饰</td><td align="center">noun compound modifier</td><td align="center">浦东，上海</td></tr><tr class="odd"><td align="center">amod</td><td align="center">形容词修饰</td><td align="center">adjective modifier</td><td align="center">情况，新</td></tr><tr class="even"><td align="center">advmod</td><td align="center">副词修饰</td><td align="center">adverbial modifier</td><td align="center">做到，基本</td></tr><tr class="odd"><td align="center">vmod</td><td align="center">动词修饰</td><td align="center">verb modifier, participle modifier</td><td align="center"></td></tr><tr class="even"><td align="center">prnmod</td><td align="center">插入词修饰</td><td align="center">parenthetical modifier</td><td align="center"></td></tr><tr class="odd"><td align="center">neg</td><td align="center">不定修饰</td><td align="center">negative modifier</td><td align="center">遇到，不</td></tr><tr class="even"><td align="center">det</td><td align="center">限定词修饰</td><td align="center">determiner modifier</td><td align="center">活动，这些</td></tr><tr class="odd"><td align="center">possm</td><td align="center">所属标记</td><td align="center">possessive maker</td><td align="center">NP</td></tr><tr class="even"><td align="center">poss</td><td align="center">所属修饰</td><td align="center">possessive modifier</td><td align="center">NP</td></tr><tr class="odd"><td align="center">dvpm</td><td align="center">DVP标记</td><td align="center">DVP maker</td><td align="center">DVP(简单，的)</td></tr><tr class="even"><td align="center">dvpmod</td><td align="center">DVP修饰</td><td align="center">DVP modifier</td><td align="center">DVP(采取，简单)</td></tr><tr class="odd"><td align="center">assm</td><td align="center">关联标记</td><td align="center">associative marker</td><td align="center">DNP(开发，的)</td></tr><tr class="even"><td align="center">assmod</td><td align="center">关联修饰</td><td align="center">associative modifier</td><td align="center">NP|QP (教训，特区)</td></tr><tr class="odd"><td align="center">prep</td><td align="center">介词修饰</td><td align="center">prepositional modifier</td><td align="center">NP|VP|IP (采取，对)</td></tr><tr class="even"><td align="center">clmod</td><td align="center">从句修饰</td><td align="center">clause modifier</td><td align="center">因为，开始</td></tr><tr class="odd"><td align="center">plmod</td><td align="center">介词性地点修饰</td><td align="center">prepositional localizer modifier</td><td align="center">在，上</td></tr><tr class="even"><td align="center">asp</td><td align="center">时态修饰</td><td align="center">aspect marker</td><td align="center">做到，了</td></tr><tr class="odd"><td align="center">partmod</td><td align="center">分词修饰</td><td align="center">participial modifier</td><td align="center">中文不存在</td></tr><tr class="even"><td align="center">etc</td><td align="center">等关系</td><td align="center"></td><td align="center">办法，等</td></tr></tbody></table><p>中心语为实词</p><table><thead><tr class="header"><th align="center">符号</th><th align="center">意义</th><th align="center">英语</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">conj</td><td align="center">联合</td><td align="center">conjunct</td><td align="center"></td></tr><tr class="even"><td align="center">cop</td><td align="center">系动双指助动词</td><td align="center">copula</td><td align="center"></td></tr><tr class="odd"><td align="center">cc</td><td align="center">连接</td><td align="center">coordination</td><td align="center">指中心词与连词</td></tr></tbody></table><p>其他</p><table><thead><tr class="header"><th align="center">符号</th><th align="center">意义</th><th align="center">英语</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">attr</td><td align="center">属性关系</td><td align="center"></td><td align="center">是，过程</td></tr><tr class="even"><td align="center">cordmod</td><td align="center">并列联合动词</td><td align="center">coordinated verb compound</td><td align="center">颁布，实行</td></tr><tr class="odd"><td align="center">mmod</td><td align="center">清潭洞次</td><td align="center">modal verb</td><td align="center">得到，能</td></tr><tr class="even"><td align="center">ba</td><td align="center">把字关系</td><td align="center"></td><td align="center"></td></tr><tr class="odd"><td align="center">tclaus</td><td align="center">时间从句</td><td align="center"></td><td align="center">以后，积累</td></tr><tr class="even"><td align="center"></td><td align="center">补语化成分</td><td align="center">complementizer</td><td align="center">一般指</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;词性标注和句法依存的符号&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;词性标注&quot;&gt;词性标注&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://www.ibm.com/support/knowledgecenter/zh/SS5RWK
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="词性标注" scheme="http://plmsmile.github.io/tags/%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"/>
    
      <category term="句法依存" scheme="http://plmsmile.github.io/tags/%E5%8F%A5%E6%B3%95%E4%BE%9D%E5%AD%98/"/>
    
  </entry>
  
  <entry>
    <title>cs231n线性分类器和损失函数</title>
    <link href="http://plmsmile.github.io/2017/11/27/cs231n-linear-notes/"/>
    <id>http://plmsmile.github.io/2017/11/27/cs231n-linear-notes/</id>
    <published>2017-11-27T03:24:40.000Z</published>
    <updated>2018-03-20T08:11:01.233Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="40%"></p><blockquote><p>线性分类器，svm和交叉熵损失函数</p></blockquote><h1 id="线性分类器">线性分类器</h1><h2 id="得分函数">得分函数</h2><p>图片是三维数组，元素在0-255的整数。<code>宽度-高度-3</code> ，3代表RGB颜色通道。 对图像进行<code>零中心化</code>。</p><p>输入图片<span class="math inline">\(D=32 \times 32 \times 3 = 3072\)</span>个像素，压缩成1维向量，一共有<span class="math inline">\(K=10\)</span>个类别。<br><span class="math display">\[f(x_i, W, b) = Wx_i + b\]</span> 分析</p><ul><li><span class="math inline">\(W\)</span>的每一行都是一个类别的分类器，一共10个</li><li>得到分为每个类的<code>score</code></li><li>改变<span class="math inline">\(W, b\)</span> ，使得分类准确，正确的score高，错误的score低</li></ul><p>为x添加一维，<span class="math inline">\(x \in \mathbb R^{D+1}\)</span> ， 写为： <span class="math display">\[f (x_i, W) = Wx_i\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs231n/linear/linear.jpg" style="display:block; margin:auto" width="60%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs231n/linear/linear2.jpg" style="display:block; margin:auto" width="60%"></p><h2 id="理解">理解</h2><p><strong>权重</strong></p><p>输入4个像素，<strong>函数会根据权重对某些位置的某些颜色表现出喜好或者厌恶</strong>（正负）。</p><p>比如船类别，一般周围有很多蓝色的水，那么蓝色通道的权值就会很大（正）。绿色和红色就比较低（负）。那么如果出现绿色和红色的像素，就会降低是船的概率。</p><p><strong>权重解释2</strong></p><p><span class="math inline">\(W\)</span>的每一行对应于一个分类的<code>模板</code>(原型)， 用图像和模板去比较，计算得分（<code>点积</code>），找到最相似的模板。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs231n/linear/10template.jpg" style="display:block; margin:auto" width="100%"></p><p><strong>线性函数</strong></p><p>实际上，每个输入<span class="math inline">\(x_i\)</span>就是<strong>3072维空间</strong>中的一个<code>点</code>，线性函数就是对这些点进行<code>边界决策分类</code>。 与线的距离越大，得分越高。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs231n/linear/linear-decision.jpg" style="display:block; margin:auto" width="60%"></p><h1 id="损失函数">损失函数</h1><p>最常用两个分类器：<code>SVM</code>和<code>Softmax</code>。分别使用<code>SVM loss</code>和<code>交叉熵loss</code>。</p><h2 id="svm">SVM</h2><p><code>多类支持向量积损失</code>（Multiclass Support Vector Machine）。<strong>正确分类比错误分类的得分高出一个边界值</strong><span class="math inline">\(\Delta (一般= 1)\)</span> 。</p><p>记<span class="math inline">\(x_i\)</span>分为第j个类别的<code>得分</code>为<span class="math inline">\(s_j = f(x_i, W)_j\)</span> ，单个<code>折叶损失 hinge loss</code>， 也称作<code>max margin loss</code> ，如下： <span class="math display">\[\begin {align}loss &amp; = \max(0, s_j  - s_{y_i}+ \Delta)=\begin {cases}&amp; 0, &amp; s_{y_i} - s_j &gt;  \Delta  \\&amp;  s_j  - s_{y_i}+ \Delta, &amp; 其它 \\\end{cases} \\ \\&amp; =\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \\\end{align}\]</span> 如果错误分类进入红色区域，就开始计算loss。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs231n/linear/hinge-loss.jpg" style="display:block; margin:auto" width="80%"></p><p>第<span class="math inline">\(i\)</span>个数据的loss就是<strong>把所有错误类别的loss加起来</strong>： <span class="math display">\[L_i = \sum_{j \neq y_i} \max(0, s_j  - s_{y_i}+ \Delta)\]</span></p><h2 id="正则化">正则化</h2><p><a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#l2正则化权重衰减">plmsmile的L2正则化</a></p><p>使用L2正则化，对<span class="math inline">\(W\)</span>的元素进行平方惩罚， 抑制大数值的权值。<code>正则化loss</code>（正则化惩罚）如下： <span class="math display">\[R(W) = \sum_k\sum_l W_{kl}^2\]</span> 最终loss就是<strong>数据损失+正则化损失</strong>， <span class="math inline">\(\lambda\)</span> 是<code>正则化强度</code>。 <span class="math display">\[L = \underbrace {\frac{1}{N}\sum_{i}L_i}_{\text{data loss}} + \underbrace {\lambda R(W)}_{\text{正则化 loss}}\]</span> 引入正则化以后，SVM就有了<code>最大边界max margin</code> 这一个良好性质。（不是很懂，后面再解决）</p><h2 id="softmax">Softmax</h2><p><a href="https://plmsmile.github.io/2017/07/31/nlp-notes/#相对熵和交叉熵">plmsmile的交叉熵</a></p><p>每个类别的得分 <span class="math display">\[s_j = f(x_i, W)_j = f_j\]</span> <code>Softmax函数</code>求得<span class="math inline">\(x_i\)</span>分为第<span class="math inline">\(j\)</span> 类的概率，这样会求得所有类别的概率，即预测的结果。 <span class="math display">\[p(j \mid x_i) = \frac {\exp(f_j)} {\sum_{k} \exp(f_k)}\]</span> 单个数据的<code>loss</code>，就是取<strong>其概率的负对数 </strong> ： <span class="math display">\[L_i = - \log p(y_i \mid x_i) = - \log \left( \frac{e^{f_{y_i}}}{\sum e^{f_k}}\right) = -f_{y_i} + \log \sum_{k}e^{f_k}\]</span> 从直观上看，最小化loss就是要最大化正确的概率（最小化正确分类的负对数概率），最小化其它分类的概率。</p><p><strong>交叉熵的体现</strong></p><p>程序会预测一个所有类别的概率分布<span class="math inline">\(q = (p(1 \mid x_i), \cdots, p(K \mid x_i))\)</span> 。真实label概率<span class="math inline">\(p = (0, \cdots, 1, 0,\cdots, 0)\)</span> ，交叉熵： <span class="math display">\[\begin{align}H(p, q) &amp; = - \sum_{x} p(x) \log q(x) \\&amp; = - (p(y_i) \cdot \log q(y_i)) = - (1 \cdot  \log  p(y_i \mid x_i) ) =  - \log p(y_i \mid x_i) \end{align}\]</span> 由于<span class="math inline">\(H(p) = 0\)</span>， 唯一确定，熵为0。交叉熵就等于真实和预测的分布的<code>KL距离</code> 。也就是说想要两个概率分布一样，即预测的所有概率密度都在正确类别上面。 <span class="math display">\[H(p, q) = H(p) + D_{KL}(p || q) = D_{KL}(p || q) \]</span> <strong>结合正则化</strong></p><p>结合正则化 <span class="math display">\[L = \underbrace {\frac{1}{N}\sum_{i}L_i}_{\text{data loss}} + \underbrace {\lambda R(W)}_{\text{正则化 loss}}\]</span> 最小化正确概率分类的负对数概率，就是在进行最大似然估计。正则化部分就是对W的高斯先验，这里进行的是最大后验估计。（不懂）</p><h2 id="svm和softmax比较">SVM和Softmax比较</h2><p>SVM loss：希望正确分类比其他分类的得分高出一个边界值。</p><p>Softmax 交叉熵loss：希望正确分类概率大，其它分类概率小。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs231n/linear/svm-softmax.jpg" style="display:block; margin:auto" width="60%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;线性分类器，svm和交叉熵损失函数&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;线性分类器&quot;&gt;线性
      
    
    </summary>
    
      <category term="cs231n" scheme="http://plmsmile.github.io/categories/cs231n/"/>
    
    
      <category term="cs231n" scheme="http://plmsmile.github.io/tags/cs231n/"/>
    
      <category term="线性分类" scheme="http://plmsmile.github.io/tags/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/"/>
    
      <category term="svm" scheme="http://plmsmile.github.io/tags/svm/"/>
    
  </entry>
  
  <entry>
    <title>神经网络-过拟合-预处理-BN</title>
    <link href="http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/"/>
    <id>http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/</id>
    <published>2017-11-26T08:21:23.000Z</published>
    <updated>2018-03-30T06:23:39.828Z</updated>
    
    <content type="html"><![CDATA[<h1 id="过拟合">过拟合</h1><h2 id="过拟合-1">过拟合</h2><p>训练数据很少，或者训练次数很多，会导致<code>过拟合</code>。避免过拟合的方法有如下几种：</p><ul><li>early stop</li><li>数据集扩增</li><li>正则化 （L1， <strong>L2权重衰减</strong>）</li><li>Dropout</li><li>决策树剪枝（尽管不属于神经网络）</li></ul><p>现在一般用L2正则化+Dropout。</p><p><strong>过拟合时</strong>，<strong>拟合系数一般都很大</strong>。过拟合需要顾及到所有的数据点，意味着<strong>拟合函数波动很大</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/overfit.png" style="display:block; margin:auto" width="50%"></p><p>看到，在某些很小的区间内里，函数值的<strong>变化很剧烈</strong>。意味着这些小区间的<strong>导数值（绝对值）非常大</strong>。由于自变量值可大可小，所以只有<strong>系数足够大</strong>，才能保证导数值足够大。</p><p>所以：<strong>过拟合时，参数一般都很大</strong>。<strong>参数较小时，意味着模型复杂度更低，对数据的拟合刚刚好</strong>， 这也是<code>奥卡姆剃刀</code>法则。</p><h2 id="范数">范数</h2><p><strong>向量范数</strong></p><p><span class="math inline">\(x \in \mathbb {R}^d\)</span></p><table><colgroup><col width="19%"><col width="80%"></colgroup><thead><tr class="header"><th>范数</th><th>定义</th></tr></thead><tbody><tr class="odd"><td>1-范数</td><td><span class="math inline">\(\left \| x\right\|_1 = \sum_i^d \|x_i\|\)</span>， <strong>绝对值之和</strong></td></tr><tr class="even"><td>2-范数</td><td><span class="math inline">\(\left \| x\right\|_2 = \left(\sum_i^d \vert x_i\vert^2\right)^{\frac{1}{2}}\)</span>， <strong>绝对值平方之和再开方</strong></td></tr><tr class="odd"><td>p-范数</td><td><span class="math inline">\(\left \| x\right\|_p = \left(\sum_i^d \|x_i\|^p\right)^{\frac{1}{p}}\)</span>， <strong>绝对值的p次方之和的<span class="math inline">\(\frac{1}{p}​\)</span>次幂</strong></td></tr><tr class="even"><td><span class="math inline">\(\infty\)</span>-范数</td><td><span class="math inline">\(\left \| x\right\|_\infty = \max_\limits i \|x_i\|\)</span> ，绝对值的最大值</td></tr><tr class="odd"><td>-<span class="math inline">\(\infty\)</span>-范数</td><td><span class="math inline">\(\left \| x\right\|_{-\infty} = \min_\limits i \|x_i\|\)</span> ，绝对值的最小值</td></tr></tbody></table><p><strong>矩阵范数</strong></p><p><span class="math inline">\(A \in \mathbb R^{m \times n}\)</span></p><table style="width:74%;"><colgroup><col width="16%"><col width="56%"></colgroup><thead><tr class="header"><th>范数</th><th>定义</th></tr></thead><tbody><tr class="odd"><td>1-范数</td><td><span class="math inline">\(\left \| A\right\|_1 = \max \limits_{j}\sum_i^m \|a_{ij}\|\)</span>，<strong>列和范数</strong>，矩阵列向量绝对值之和的最大值。</td></tr><tr class="even"><td><span class="math inline">\(\infty\)</span>-范数</td><td><span class="math inline">\(\left \| A\right\|_\infty = \max_\limits i \sum_{j}^{n}\|a_{ij}\|\)</span> ，<strong>行和范数</strong>，所有行向量绝对值之和的最大值。</td></tr><tr class="odd"><td>2-范数</td><td><span class="math inline">\(\left \| A\right\|_2 = \sqrt{\lambda_{m}}\)</span> ， 其中<span class="math inline">\(\lambda_m\)</span>是<span class="math inline">\(A^TA\)</span>的<strong>最大特征值</strong>。</td></tr><tr class="even"><td>F-范数</td><td><span class="math inline">\(\left \| A\right\|_F = \left(\sum_i^m \sum_j^n a_{ij}^2\right)^{\frac{1}{2}}\)</span>，<strong>所有元素的平方之和，再开方。或者不开方</strong>， L2正则化就直接平方，不开方。</td></tr></tbody></table><h2 id="l2正则化权重衰减">L2正则化权重衰减</h2><p><strong>为了避免过拟合</strong>，使用L2正则化参数。<span class="math inline">\(\lambda\)</span>是正则项系数，<strong>用来权衡正则项和默认损失的比重</strong>。<span class="math inline">\(\lambda\)</span> 的选取很重要。 <span class="math display">\[J_R = J + \lambda \sum_{i=1}^L \left \| W^{(i)}\right \|_F\]</span> L2惩罚更倾向于<strong>更小更分散</strong>的权重向量，鼓励使用所有维度的特征，而不是只依赖其中的几个，这也避免了过拟合。</p><p><strong>标准L2正则化</strong></p><p><span class="math inline">\(\lambda\)</span> 是<code>正则项系数</code>，<span class="math inline">\(n\)</span>是数据数量，<span class="math inline">\(w\)</span>是模型的参数。 <span class="math display">\[C = C_0 + \frac {\lambda} {2n} \sum_w w^2\]</span> <span class="math inline">\(C\)</span>对参数<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>的<code>偏导</code>： <span class="math display">\[\begin {align}&amp; \frac{\partial C}{\partial w}  =  \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} w  \\&amp; \frac{\partial C}{\partial b}  =  \frac{\partial C_0}{\partial b} \\\end{align}\]</span> <strong>更新参数</strong> ：可以看出，正则化<span class="math inline">\(C\)</span>对<span class="math inline">\(w\)</span>有影响，对<span class="math inline">\(b\)</span>无影响。<br><span class="math display">\[\begin{align}w &amp;= w - \alpha \cdot  \frac{\partial C}{\partial w} \\&amp;=  (1 - \frac{\alpha \lambda}{n})w - \alpha \frac{\partial C_0}{\partial w} \\\end{align}\]</span> 从上式可以看出：</p><ul><li>不使用正则化时，<span class="math inline">\(w\)</span>的系数是1</li><li>使用正则化时</li><li><span class="math inline">\(w\)</span>的系数是<span class="math inline">\(1 - \frac{\alpha \lambda}{n} &lt; 1\)</span> ，效果是减小<span class="math inline">\(w\)</span>， 所以是<strong>权重衰减</strong> <code>weight decay</code></li><li>当然，<span class="math inline">\(w\)</span>具体增大或减小，还取决于后面的导数项</li></ul><p><strong>mini-batch随机梯度下降</strong></p><p>设<span class="math inline">\(m\)</span> 是这个batch的样本个数，有更新参数如下，即求<strong>batch个C对w的平均偏导值</strong> <span class="math display">\[\begin {align}&amp; w =  (1 - \frac{\alpha \lambda}{n})w - \frac{\alpha}{m} \cdot \sum_{i=1}^{m}\frac{\partial C_i}{\partial w} \\&amp; b = b - \frac{\alpha}{m} \cdot \sum_{i=1}^{m}\frac{\partial C_i}{\partial b} \\\end{align}\]</span> 所以，权重衰减后一般可以减小过拟合。 L2正则化比L1正则化<strong>更加发散</strong>，权值也会被限制的更小。 一般使用L2正则化。</p><p>还有一种方法是<code>最大范数限制</code>：给范数一个上界<span class="math inline">\(\left \| w \right \| &lt; c\)</span> ， 可以在学习率太高的时候网络不会爆炸，因为更新总是有界的。</p><h2 id="实例说明">实例说明</h2><p>增加网络的层的数量和尺寸时，网络的容量上升，多个神经元一起合作，可以表达各种复杂的函数。</p><p>如下图，2分类问题，有噪声数据。</p><p>一个隐藏层。神经元数量分别是3、6、20。很明显20<strong>过拟合</strong>了，拟合了所有的数据。<code>正则化</code>就是<strong>处理过拟合</strong>的非常好的办法。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/compare-neurons-num.jpg" style="display:block; margin:auto" width="60%"></p><p>对20个神经元的网络，使用正则化，解决过拟合问题。正则化强度<span class="math inline">\(\lambda\)</span>很重要。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/compare-lambda-regularize.jpg" style="display:block; margin:auto" width="60%"></p><h2 id="l1正则化">L1正则化</h2><p>正则化loss如下： <span class="math display">\[C = C_0 + \frac {\lambda} {n} \sum_w |w|\]</span> 对<span class="math inline">\(w\)</span>的<strong>偏导</strong>， 其中<span class="math inline">\(\rm{sgn}(w)\)</span>是<code>符号函数</code>： <span class="math display">\[\frac{\partial C}{\partial w}  =  \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} \cdot \rm{sgn}(w)\]</span> 更新参数： <span class="math display">\[w =  w  - \frac{\alpha \lambda}{n} \cdot \rm{sgn}(w) - \alpha \frac{\partial C_0}{\partial w} \]</span> 分析：<span class="math inline">\(w\)</span>为正，减小；<span class="math inline">\(w\)</span>为负，增大。所以<strong>L1正则化就是使参数向0靠近</strong>，是权重尽可能为0，减小网络复杂度，防止过拟合。</p><p>特别地：当<span class="math inline">\(w=0\)</span>时，不可导，就不要正则化项了。L1正则化更加稀疏。</p><h2 id="随机失活dropout">随机失活Dropout</h2><p>Dropout是<strong>非常有用</strong>的<strong>正则化</strong>的办法，它<strong>改变了网络结构</strong>。一般采用<strong>L2正则化+Dropout来防止过拟合</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/dropout.png" style="display:block; margin:auto" width="50%"></p><p>训练的时候，输出不变，<strong>随机以概率<span class="math inline">\(p\)</span>保留神经元，<span class="math inline">\(1-p\)</span>删除神经元</strong>（<strong>置位0</strong>）。每次迭代删除的神经元都不一样。</p><p>BP的时候，<strong>置位0的神经元的参数就不再更新</strong>， <strong>只更新前向时alive的神经元</strong>。</p><p>预测的时候，要保留所有的神经元，即不使用Dropout。</p><p>相当于训练了很多个（<strong>指数级数量</strong>）小网络（<code>半数网络</code>），在预测的时候综合它们的结果。随着训练的进行，大部分的半数网络都可以给出正确的分类结果。</p><h1 id="数据预处理">数据预处理</h1><p>用的很多的是0中心化。CNN中很少用PCA和白化。</p><p>应该：线划分训练、验证、测试集，<strong>只是从训练集中求平均值</strong>！<strong>然后各个集再减去这个平均值</strong>。</p><h2 id="中心化">中心化</h2><p>也称作<code>均值减法</code>， 把数据所有维度变成<code>0均值</code>，其实就是减去均值。就是将<strong>数据迁移到原点</strong>。 <span class="math display">\[x = x - \rm{avg}(x) = x - \bar x\]</span></p><h2 id="标准化">标准化</h2><p>也称作<code>归一化</code>， 数据所有维度都归一化，使其数值<strong>变化范围都近似相等</strong>。</p><ul><li><strong>除以标准差</strong></li><li>最大值和最小值按照比例缩放到<span class="math inline">\((-1 ,1)\)</span> 之间</li></ul><p><code>方差</code><span class="math inline">\(s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2\)</span> ，<code>标准差</code>就是<span class="math inline">\(s\)</span> 。数据<strong>除以标准差</strong>，接近<code>标准高斯分布</code>。 <span class="math display">\[x = \frac{x}{s}\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-process.jpg" style="display:block; margin:auto" width="70%"></p><h2 id="pca">PCA</h2><p><a href="http://ufldl.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90" target="_blank" rel="noopener">斯坦福PCA</a> ，<a href="http://blog.csdn.net/wangjian1204/article/details/50642732" target="_blank" rel="noopener">CSDNPCA和SVD的区别和联系</a></p><p><strong>协方差</strong></p><p><code>协方差</code>就是<strong>乘积的期望-期望的乘积</strong>。 <span class="math display">\[\rm{Cov}(X, Y) = E(XY) - E(X)E(Y)\]</span> 协方差的性质如下： <span class="math display">\[\begin{align}&amp; \rm{Cov}(X, Y) = \rm{Conv}(Y, X) \\ \\&amp; \rm{Cov}(aX, bY) = ab \cdot \rm{Conv}(Y, X) \\ \\&amp; \rm{Cov}(X, X) = E(X^2) - E^2(X) = D(X) , \quad \text{三方公式}\\ \\&amp; \rm{Cov}(X, C)  = 0 \\ \\ &amp; \rm{Cov}(X, Y) = 0 \leftrightarrow X与Y独立\end{align}\]</span> 还有别的性质就看考研笔记吧。</p><p><strong>奇异值分解</strong> <span class="math display">\[A_{m \times n} = U_{m \times m} \Sigma_{m \times n}  V^T_{n \times n}\]</span> <span class="math inline">\(V_{n \times n}\)</span> ：<span class="math inline">\(V\)</span>的列，一组对A正交输入或分析的基向量（线性无关）。这些向量是<span class="math inline">\(M^TM\)</span> 的特征向量。</p><p><span class="math inline">\(U_{m \times m}\)</span> ：<span class="math inline">\(U\)</span>的列，一组对A正交输出的<code>基向量</code> 。是<span class="math inline">\(MM^T\)</span>的特征向量。</p><p><span class="math inline">\(\Sigma_{m \times n}\)</span>：<code>对角矩阵</code>。对角元素按照从小到大排列，这些<strong>对角元素</strong>称为<code>奇异值</code>。 是<span class="math inline">\(M^TM, MM^T\)</span> 的特征值的非负平方根，并且与U和V的行向量对应。</p><p>记<span class="math inline">\(r\)</span>是<strong>非0奇异值的个数</strong>，则A中仅有<strong><span class="math inline">\(r\)</span>个重要特征</strong>，其余特征都是噪声和冗余特征。</p><p><a href="https://www.zhihu.com/question/22237507" target="_blank" rel="noopener">奇异值的物理意义</a></p><p><strong>利用SVD进行PCA</strong></p><p><strong>先将数据中心化</strong>。输入是<span class="math inline">\(X \in \mathbb R^ {N \times D}\)</span> ，则<code>协方差矩阵</code> 如下： <span class="math display">\[\mathrm{Cov}(X) = \frac{X^TX}{N}  \;  \in  \mathbb R^{D \times D}\]</span> 比如X有a和b两维，<strong>均值均是0</strong>。那么<span class="math inline">\(\rm{Cov}(ab)=E(ab)-0=(a_0b_0+a_1b_1+\cdots + a_nb_n) /n\)</span> ，就得到了协方差值。</p><ul><li>中心化</li><li>计算<span class="math inline">\(x\)</span>的<code>协方差矩阵cov</code></li><li>对协方差矩阵cov进行<code>svd分解</code>，得到<code>u, s, v</code></li><li><strong>去除x的相关性，旋转</strong>，<span class="math inline">\(xrot = x \cdot u\)</span> ，此时xrot的协方差矩阵只有对角线才有值，其余均为0</li><li>选出大于0的奇异值</li><li>数据降维</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_pca</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.random.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="comment"># 中心化</span></span><br><span class="line">    x -= np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">print</span> (x.shape)</span><br><span class="line">    <span class="comment"># 协方差</span></span><br><span class="line">    conv = np.dot(x.T, x) / x.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">print</span> (conv.shape)</span><br><span class="line">    <span class="keyword">print</span> (conv)</span><br><span class="line">    u, s, v = np.linalg.svd(conv)</span><br><span class="line">    <span class="keyword">print</span> (s)</span><br><span class="line">    <span class="keyword">print</span> (u.shape, s.shape, v.shape)</span><br><span class="line">    <span class="comment"># 大于0的奇异值</span></span><br><span class="line">    n_sv = np.where(s &gt; <span class="number">1e-5</span>)[<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">    print(n_sv)</span><br><span class="line">    <span class="comment"># 对数据去除相关性</span></span><br><span class="line">    xrot = np.dot(x, u)</span><br><span class="line">    <span class="keyword">print</span> (xrot.shape)</span><br><span class="line">    <span class="comment"># 数据降维</span></span><br><span class="line">    xrot_reduced = np.dot(x, u[:, :n_sv])</span><br><span class="line">    <span class="comment"># 降到了4维</span></span><br><span class="line">    <span class="keyword">print</span> (xrot_reduced.shape)</span><br></pre></td></tr></table></figure><h2 id="白化">白化</h2><p><a href="http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96" target="_blank" rel="noopener">斯坦福白化</a></p><p>白化希望特征之间的相关性较低，所有特征具有相同的协方差。白化后，得到均值为0，协方差相等的矩阵。对<span class="math inline">\(xrot\)</span>除以特征值。 <span class="math display">\[x_{white} = \frac{x_{rot}}{\sqrt{\lambda + \epsilon}}\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_white = xrot / np.sqrt(s + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure><p>缺陷是：可能会夸大数据中的早上，因为把所有维度都拉伸到了相同的数值范围。可能有一些极少差异性（方差小）但大多数是噪声的维度。可以使用平滑来解决。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-pca-process.jpg" style="display:block; margin:auto" width="60%"></p><h1 id="权重初始化">权重初始化</h1><p>如果数据恰当归一化以后，可以假设所有权重数值中大约一半为正数，一半为负数。所以期望参数值是0。</p><p><strong>千万不能够全零初始化</strong>。因为每个神经元的输出相同，BP时梯度也相同，参数更新也<strong>相同</strong>。神经元之间就<strong>失去了不对称性的源头</strong>。</p><h2 id="小随机数初始化">小随机数初始化</h2><blockquote><p>如果神经元刚开始的时候是随机且不相等的，那么它们将<strong>计算出不同的更新</strong>，并<strong>成为网络的不同部分</strong>。</p></blockquote><p>参数接近于0单不等于0。使用<strong>零均值和标准差的高斯分布</strong>来生成随机数初始化参数，这样就<strong>打破了对称性</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)</span><br></pre></td></tr></table></figure><p>注意：不是参数值初始越小就一定好。参数小，意味着会减小BP中的梯度信号，在深度网络中，就会有问题。</p><p><strong>校准方差</strong></p><p>随着数据集的增长，随机初始化的神经元的<strong>输出数据分布的方差也会增大</strong>。可以<strong>使用<span class="math inline">\(\frac{1}{\sqrt{n}}\)</span> 校准方差</strong>。n是数据的数量。这样就能保证网络中<strong>所有神经元起始时有近似同样的输出分布</strong>。这样也能够提高收敛的速度。 感觉实际上就是做了一个归一化。</p><p>数学详细推导见<code>cs231n</code> ， <span class="math inline">\(s = \sum_{i}^nw_ix_i\)</span> ，假设w和x都服从同样的分布。想要输出s和输入x有同样的方差。 <span class="math display">\[\begin {align}&amp; \because D(s) = n \cdot D(w)D(x),  \; D(s) = D(x)    \\&amp; \therefore D(w) = \frac{1}{n} \\&amp; \because D(w_{old}) = 1, \; D(aX) = a^2 D(X) \\&amp; \therefore D(w) = \frac{1}{n}D(w_{old}) = D(\frac{1}{\sqrt n} w_{old}) \\&amp; \therefore w = \frac{1}{\sqrt n} w_{old}\end{align}\]</span> 所以要使用<span class="math inline">\(\frac{1}{\sqrt{n}}\)</span>来标准化参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)/ sqrt(n)</span><br></pre></td></tr></table></figure><p><strong>经验公式</strong></p><p>对于某一层的方差，应该取决于两层的输入和输出神经元的数量，如下： <span class="math display">\[\rm{D}(w) = \frac{2}{n_{in} + n_{out}}\]</span> <code>ReLU</code>来说，<strong>方差应该是<span class="math inline">\(\frac{2}{n}\)</span></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H) * sqrt(<span class="number">2.0</span> / n)</span><br></pre></td></tr></table></figure><h2 id="稀疏和偏置初始化">稀疏和偏置初始化</h2><p>一般稀疏初始化用的比较少。一般偏置都初始化为0。</p><h1 id="batch-normalization">Batch Normalization</h1><p><a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-08-batch-normalization/" target="_blank" rel="noopener">莫凡python BN讲解</a> 和 <a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">CSDN-BN论文介绍</a> 。Batch Normalization和普通数据标准化类似，是将分散的数据标准化。</p><p><code>Batch Normalization</code>在神经网络非常流行，<strong>已经成为一个标准了</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB1.png" style="display:block; margin:auto" width="40%"></p><h2 id="训练速度分析">训练速度分析</h2><p>网络训练的时候，每一层<strong>网络参数更新</strong>，会导致<strong>下一层输入数据分布的变化</strong>。这个称为<code>Internal Convariate Shift</code>。</p><p>需要对<strong>数据归一化的原因</strong> ：</p><ul><li>神经网络的本质是<strong>学习数据分布</strong>。如果训练数据与测试数据的分布不同，那么<code>泛化能力</code>也大大降低</li><li>如果每个batch数据分布不同（batch 梯度下降），每次迭代都要去<strong>学习适应不同的分布</strong>，会大大<strong>降低训练速度</strong></li></ul><p>深度网络，前几层数据微小变化，后面几层数据<strong>差距会积累放大</strong>。</p><p>一旦某一层网络输入<strong>数据发生改变</strong>，这层网络就需要去<strong>适应学习这个新的数据分布</strong>。如果训练数据的分布一直变化，那么就会<strong>影响网络的训练速度</strong>。</p><h2 id="敏感度问题">敏感度问题</h2><p>神经网络中，如果使用<code>tanh</code>激活函数，初始权值是0.1。</p><p>输入<span class="math inline">\(x=1\)</span>， 正常更新： <span class="math display">\[z = wx = 0.1, \quad a(z_1) = 0.1 \quad  \to \quad a^\prime(z) = 0.99\]</span> 但是如果一开始输入 <span class="math inline">\(x=20\)</span> ，会导致梯度消失，不更新参数。 <span class="math display">\[z = wx = 2 ,\quad a(z) \approx 1  \quad \to \quad   a^\prime(z) = 0\]</span> 同样地，如果再输入<span class="math inline">\(x=100\)</span> ，神经元的输出依然是接近于1，不更新参数。 <span class="math display">\[z = wx = 10 ,\quad a(z) \approx 1  \quad \to \quad   a^\prime(z) = 0\]</span> 对于一个<strong>变化范围比较大</strong>特征维度，神经网络在初始阶段<strong>对它已经不敏感没有区分度</strong>了！</p><p>这样的问题，在神经网络的输入层和中间层都存在。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB2.png" style="display:block; margin:auto" width="50%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB3.png" style="display:block; margin:auto" width="50%"></p><h2 id="bn算法">BN算法</h2><p>BN算法在每一次迭代中，对每一层的输入都进行归一化。<strong>把数据转换为均值为0、方差为1的高斯分布</strong>。 <span class="math display">\[\hat x = \frac{x - E(x)} {\sqrt{D(x) + \epsilon}}\]</span> 非常大的<code>缺陷</code>：<strong>强行归一化会破坏掉刚刚学习到的特征</strong>。 把每层的数据分布都固定了，但不一定是前面一层学习到的数据分布。</p><p><code>牛逼的地方</code> ：设置两个可以学习的变量<code>扩展参数</code><span class="math inline">\(\gamma\)</span> ，和<code>平移参数</code> <span class="math inline">\(\beta\)</span> ，<strong>用这两个变量去还原上一层应该学习到的数据分布</strong>。（但是芳芳说，这一步其实可能没那么重要，要不要都行，CNN的本身会处理得更好）。 <span class="math display">\[y = \gamma \hat x+ \beta\]</span> 这样理解：用这两个参数，让神经网络自己去学习琢磨是前面的标准化是否有优化作用，<strong>如果没有优化效果，就用<span class="math inline">\(\gamma, \beta\)</span>来抵消标准化的操作。</strong></p><p>这样，BN就把原来不固定的数据分布，全部转换为固定的数据分布，而这种数据分布恰恰就是要学习到的分布。从而<strong>加速了网络的训练</strong>。</p><p>对一个<code>mini-batch</code>进行更新， 输入一个<span class="math inline">\(batchsize=m\)</span>的数据，学习两个参数，输出<span class="math inline">\(y\)</span> <span class="math display">\[\begin{align}&amp; \mu = \frac{1}{m} \sum_{i=1}^m x_i &amp;  \text{求均值} \\&amp; \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 &amp; \text{求方差} \\&amp; \hat x = \frac{x - E(x)} {\sqrt{\sigma^2 + \epsilon}} &amp;  \text{标准化} \\&amp; y =  \gamma \hat x+ \beta &amp; \text{scale and shfit} \end{align}\]</span> 其实就是对输入数据做个归一化： <span class="math display">\[z = wx+b \to z = \rm{BN}(wx + b) \to a = f(z)\]</span> 一般<strong>在全连接层和激活函数之间</strong>添加BN层。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB4.png" style="display:block; margin:auto" width="50%"></p><p>在测试的时候，由于是没有batch，所以使用固定的均值和标准差，也就是对训练的各个batch的均值和标准差做批处理。 <span class="math display">\[E(x) = E(\mu), \quad D(x) = \frac{b}{b-1} E(\sigma^2)\]</span></p><h2 id="bn的优点">BN的优点</h2><p><strong>1 训练速度快</strong></p><p><strong>2 选择大的初始学习率</strong></p><p>初始大学习率，学习率的衰减也很快。<strong>快速训练收敛</strong>。小的学习率也可以。</p><p><strong>3 不再需要Dropout</strong></p><p>BN本身就可以<strong>提高网络泛化能力</strong>，可以不需要Dropout和L2正则化。源神说，<strong>现在主流的网络都没有dropout了</strong>。但是<strong>会使用L2正则化</strong>，比较小的正则化。</p><p><strong>4 不再需要局部相应归一化</strong></p><p><strong>5 可以把训练数据彻底打乱</strong></p><h2 id="效果图片展示">效果图片展示</h2><p>对所有数据标准化到一个范围，这样<strong>大部分的激活值都不会饱和</strong>，都不是-1或者1。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB5.png" style="display:block; margin:auto" width="50%"></p><p>大部分的激活值在各个分布区间都有值。再传递到后面，数据更有价值。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB6.png" style="display:block; margin:auto" width="50%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB8.png" style="display:block; margin:auto" width="50%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;过拟合&quot;&gt;过拟合&lt;/h1&gt;
&lt;h2 id=&quot;过拟合-1&quot;&gt;过拟合&lt;/h2&gt;
&lt;p&gt;训练数据很少，或者训练次数很多，会导致&lt;code&gt;过拟合&lt;/code&gt;。避免过拟合的方法有如下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;early stop&lt;/li&gt;
&lt;li&gt;数据
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="数据预处理" scheme="http://plmsmile.github.io/tags/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="正则化" scheme="http://plmsmile.github.io/tags/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    
      <category term="范数" scheme="http://plmsmile.github.io/tags/%E8%8C%83%E6%95%B0/"/>
    
      <category term="Dropout" scheme="http://plmsmile.github.io/tags/Dropout/"/>
    
      <category term="PCA" scheme="http://plmsmile.github.io/tags/PCA/"/>
    
      <category term="白化" scheme="http://plmsmile.github.io/tags/%E7%99%BD%E5%8C%96/"/>
    
      <category term="BN" scheme="http://plmsmile.github.io/tags/BN/"/>
    
  </entry>
  
  <entry>
    <title>神经网络基础-反向传播-激活函数</title>
    <link href="http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/"/>
    <id>http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/</id>
    <published>2017-11-23T04:01:08.000Z</published>
    <updated>2018-03-20T08:14:48.112Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="40%"></p><h1 id="神经网络基础">神经网络基础</h1><p>很多数据都是非线性分割的，所以需要一种<code>非线性non-linear决策边界</code> 来分类。<code>神经网络</code>包含很多这样的<strong>非线性的决策函数</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NonlinearBoundary.png" style="display:block; margin:auto" width="30%"></p><h2 id="神经元">神经元</h2><p>神经元其实就是一个<code>计算单元</code>。</p><ul><li>输入向量 <span class="math inline">\(x \in \mathbb R^n\)</span></li><li><span class="math inline">\(z = w^T x + b\)</span></li><li><span class="math inline">\(a = f(z)\)</span> 激活函数，<code>sigmoid</code>, <code>relu</code>等，后文有讲。</li></ul><p><strong>Sigmoid神经元</strong></p><p>传统用<code>sigmoid</code>多，但是现在一定不要使用啦。大多使用<code>Relu</code>作为<code>激活函数</code>。<br><span class="math display">\[z = \mathbf{w}^T \mathbf{x} + b , \;  a = \frac {1}{1 + \exp (-z)}\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/sigmoidneuron.png" style="display:block; margin:auto" width="40%"></p><h2 id="网络层">网络层</h2><p><strong>一个网络层有很多个神经元</strong>。输入<span class="math inline">\(\mathbf x\)</span>向量，<strong>会传递到多个神经元</strong>。如</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/SingleLayerNeuralNetwork.png" style="display:block; margin:auto" width="30%"></p><p>输入是<span class="math inline">\(n\)</span>维，隐层是<span class="math inline">\(m\)</span>维，有<span class="math inline">\(m\)</span>个神经元。则有 <span class="math display">\[\begin{align}&amp; z = W x + b , &amp; W \in \mathbb{R}^{m \times n}, x \in \mathbb R^n, b \in \mathbb R^m\\&amp; a = f (z)  &amp; a \in \mathbb R^m\\&amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\ \end{align}\]</span> <strong>激活函数的意义</strong></p><p>每个神经元</p><ul><li>输入<span class="math inline">\(z = w^Tx+b\)</span> ：对特征进行加权组合的结果</li><li>激活<span class="math inline">\(a = f(z)\)</span>： 对<span class="math inline">\(z\)</span>是否继续保留</li></ul><p>最后会把所有的神经元的<strong>所有<span class="math inline">\(z\)</span>的激活信息<span class="math inline">\(a\)</span>综合起来</strong>，得到最终的分类结果。比如<span class="math inline">\(s = U^T a\)</span>。</p><h2 id="前向计算">前向计算</h2><p>输入<span class="math inline">\(x \in \mathbb R^n\)</span>， 激活信息<span class="math inline">\(a \in \mathbb R^m\)</span>。一般前向计算如下： <span class="math display">\[\begin{align}&amp; z = W x + b , &amp; W \in \mathbb{R}^{m \times n}, x \in \mathbb R^n, b \in \mathbb R^m\\\\&amp; a = f (z)  &amp; a \in \mathbb R^m\\\\&amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\ \end{align}\]</span> 下面是一个简单的<code>全连接</code>，最后的圆圈里的1代表<strong>等价输出</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/SimpleFF.png" style="display:block; margin:auto" width="30%"></p><p><strong>NER例子</strong></p><p><code>NER</code>(named-entity recognition)，<code>命名实体识别</code>。对于一个句子<code>Museums in Paris are amazing</code>。 要判断中心单词<code>Paris</code><strong>是否是个命名实体</strong>。</p><p>既要看window里的所有<strong>词向量</strong>，也要看这些词的<strong>交互关系</strong>。比如：<code>Paris</code>出现在<code>in</code>的后面。 因为可能有<code>Paris</code>和<code>Paris Hilton</code>。这就需要<code>non-linear decisions</code>。</p><p>如果直接把input给到softmax，是很难获取到非线性决策的。所以需要添加中间层使用神经网络。如上图所示。</p><p><strong>维数分析</strong></p><p>每个单词4维，输入整个窗口就是20维。在隐层使用8个神经元。计算过程如下，最终得到一个分类的得分。 <span class="math display">\[\begin {align}&amp; z = Wx + b \\&amp; a = f(z) \\&amp; s = U^T a \\\end{align}\]</span> 维数如下： <span class="math display">\[x \in \mathbb R^{20}, \; W \in \mathbb R^{8\times20}, \; U \in \mathbb R^{8\times1}, s \in R\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ner1.png" style="display:block; margin:auto" width="40%"></p><h2 id="max-magin目标函数">Max magin目标函数</h2><p>正样本<span class="math inline">\(s\)</span> ：<code>Museums in Paris are amazing</code> ，负样本<span class="math inline">\(s_c\)</span>： <code>Not all museums in Paris</code> 。</p><p>只关心：<strong>正样本的得分高于负样本的得分</strong>， <strong>其它的不关注</strong>。即要<span class="math inline">\(s - s_c &gt; 0\)</span>： <span class="math display">\[\mathrm{maxmize}(s -s_c) \leftrightarrow \mathrm{minmize}(s_c - s)\]</span> <code>优化目标函数</code>如下： <span class="math display">\[\rm{minimize} \; J =\max(s_c - s, 0) \; = \begin{cases}&amp; s_c - s, &amp; s &lt; s_c \\&amp; 0, &amp; s \ge s_c\end{cases}\]</span> 上式其实有风险，更需要<span class="math inline">\(s - s_c &gt; \Delta\)</span>， 即<span class="math inline">\(s\)</span>比<span class="math inline">\(s_c\)</span>得分大于<span class="math inline">\(\Delta\)</span>，来保证一个安全的间距。 <span class="math display">\[\rm{minimize} \; J = \max(\Delta + s_c - s, 0)\]</span> 给具体间距<span class="math inline">\(\Delta=1\)</span>， 所以<code>优化目标函数</code>：详情见SVM。 <span class="math display">\[\rm{minimize} \; J = \max(1 + s_c - s, 0)\]</span> 其中<span class="math inline">\(s_c = U^T f(Wx_c + b), \; s = U^T f(Wx+b)\)</span> 。</p><h1 id="反向传播训练">反向传播训练</h1><p><a href="https://plmsmile.github.io/2017/08/20/ml-ng-notes/#梯度下降">梯度下降</a> ，或者SGD： <span class="math display">\[\theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \Delta_{\theta^{(t)}} J\]</span> <code>反向传播</code> 使用<code>链式法则</code> 来计算<code>前向计算</code>中用到的参数的<code>梯度</code>。</p><h2 id="符号定义">符号定义</h2><p>如下图，一个简单的网络：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/421nnet.png" style="display:block; margin:auto" width="40%"></p><p>网络在输入层和输出层是<strong>等价输入和等价输出</strong>，只有<strong>中间层会使用激活函数</strong>进行非线性变换。</p><table><thead><tr class="header"><th>符号</th><th>意义</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(x\)</span></td><td>网络输入，这里是4维</td></tr><tr class="even"><td><span class="math inline">\(s\)</span></td><td>网络输出，这里是1维，即一个数字</td></tr><tr class="odd"><td><span class="math inline">\(W^{(k)}\)</span></td><td>第<span class="math inline">\(k \to k+1\)</span>层的转移矩阵。<span class="math inline">\(W \in \mathbb R^{n \times m}\)</span>。 k层m个神经元，k+1层n个神经元</td></tr><tr class="even"><td><span class="math inline">\(W_{ij}^{(k)}\)</span></td><td>k+1层的<span class="math inline">\(i\)</span> 神经元 到 到<span class="math inline">\(k\)</span>层<span class="math inline">\(j\)</span>神经元的 的权值</td></tr><tr class="odd"><td><span class="math inline">\(b_i^{(k)}\)</span></td><td><span class="math inline">\(k \to k+1\)</span> 转移， k+1层的<span class="math inline">\(i\)</span> 神经元的接收偏置</td></tr><tr class="even"><td><span class="math inline">\(z^{(k)}_j\)</span></td><td>第<span class="math inline">\(k\)</span>层的第<span class="math inline">\(j\)</span>个神经元的输入</td></tr><tr class="odd"><td>计算输入</td><td><span class="math inline">\(z_j^{(k+1)} = \sum_i W_{ji}^{(k)} \cdot a^{(k)}_i + b^{(k)}_j\)</span></td></tr><tr class="even"><td><span class="math inline">\(a_j^{(k)}\)</span></td><td>第<span class="math inline">\(k\)</span>层的第<span class="math inline">\(j\)</span>个神经元的输入。<span class="math inline">\(a = f(z)\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\delta_j^{(k)}\)</span></td><td>BP时，<strong>在<span class="math inline">\(z_j^{(k)}\)</span>处的梯度。即<span class="math inline">\(f^\prime(z_j^{(k)}) \cdot g\)</span> ，<span class="math inline">\(g\)</span>是传递来的梯度</strong></td></tr></tbody></table><h2 id="w梯度推导">W梯度推导</h2><p><code>误差函数</code><span class="math inline">\(J = \max (1 + s_c - s, 0)\)</span> ，<strong>当<span class="math inline">\(J &gt; 0\)</span>的时候</strong>，<span class="math inline">\(J = 1 + s_c - s\)</span>要去更新参数W和b。 <span class="math display">\[\frac{\partial J} {\partial s} = - \frac{\partial J} {\partial s_c} = -1\]</span> 反向传播时，必须<strong>知道参数在前向时所贡献所关联的对象</strong>，即知道<code>路径</code>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/421nnet.png" style="display:block; margin:auto" width="40%"></p><p>这里是等价输出： <span class="math display">\[s = a_1^{(3)} = z_1^{(3)}  = W_1^{(2)}a_1^{(2) } + W_2^{(2)}a_2^{(2) }\]</span> 这里对<span class="math inline">\(W_{ij}^{(1)}\)</span>的偏导进行反向传播<strong>推导</strong>： <span class="math display">\[\begin{align}\frac{\partial s}{\partial W_{ij}^{(1)}}&amp;= \frac{\partial W^{(2)} a^{(2)}}{\partial W_{ij}^{(1)}} \\&amp;= \frac{ \color{blue} {\partial W_i^{(2)} a_i^{(2)}}} {\partial W_{ij}^{(1)}}= \color{blue}{W_i^{(2)}} \cdot \frac{\partial  a_i^{(2)}}{\partial W_{ij}^{(1)}} \\&amp; = W_i^{(2)} \cdot \color{blue} {\frac{\partial  a_i^{(2)}}{\partial z_i^{(2)}} \cdot  \frac{\partial  z_i^{(2)}}{\partial W_{ij}^{(1)}}} \\&amp; = W_i^{(2)} \cdot \color{blue}{f^\prime(z_i^{(2)})} \cdot \frac{\partial }{\partial W_{ij}^{(1)}} \left(\color{blue}{b_i^{(2)} + \sum_k^4 a_k^{(1)}W_{ik}^{(1)}}\right)\\&amp; =  W_i^{(2)}f^\prime(z_i^{(2)}) \color{blue}{a_j^{(1)}} \\&amp; = \color{blue}{\delta^{(2)}_i} \cdot a_j^{(1)}\end{align}\]</span> <strong>结果分析</strong></p><p>我们知道<span class="math inline">\(z_i^{(2)} = \sum_k^4 a_k^{(1)}W_{ik}^{(1)} + b_i^{(2)}\)</span>。 <strong>单纯</strong><span class="math inline">\(z_i^{(2)}\)</span>对<span class="math inline">\(W_{ij}^{(2)}\)</span>的导数是<span class="math inline">\(a_j^{(1)}\)</span>。<strong>反向时</strong>，在<span class="math inline">\(z_i^{(2)}\)</span>处的梯度是<span class="math inline">\(\delta_i^{(2)}\)</span>。</p><p>反向时，<span class="math inline">\(\frac{\partial s}{\partial W_{ij}^{(1)}} = \delta^{(2)}_i \cdot a_j^{(1)}\)</span>，是<strong>传来的梯度和当前梯度的乘积</strong>。这正好应证了<code>反向传播</code>。 传来的梯度也作<code>error signal</code>。 反向过程也是<code>error sharing/distribution</code>。</p><h2 id="w元素实例">W元素实例</h2><p><span class="math inline">\(W_{14}^{(1)}\)</span> 只直接贡献于<span class="math inline">\(z_1^{(2)}\)</span>和<span class="math inline">\(a_1^{(2)}\)</span></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ErrorSignal.png" style="display:block; margin:auto" width="30%"></p><table style="width:97%;"><colgroup><col width="40%"><col width="56%"></colgroup><thead><tr class="header"><th>步骤</th><th>梯度</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(s \to a_1^{(3)}\)</span></td><td>梯度<span class="math inline">\(g=1\)</span>。开始为1。</td></tr><tr class="even"><td><span class="math inline">\(a_1^{(3)} \to z_1^{(3)}\)</span></td><td>在<span class="math inline">\(z_1^{(3)}\)</span>处的梯度<span class="math inline">\(g = 1 \cdot 1 = \delta_1^{(3)}\)</span> 。<span class="math inline">\(local \; g= 1\)</span> ，等价变换</td></tr><tr class="odd"><td><span class="math inline">\(z_1^{(3)} \to a_1^{(2)}\)</span></td><td><span class="math inline">\(g = \delta_1^{(3)} \cdot W_1^{(2)} = W_1^{(2)}\)</span> 。<span class="math inline">\(lg = w\)</span>, <span class="math inline">\(z=wa+b\)</span></td></tr><tr class="even"><td><span class="math inline">\(a_1^{(2)} \to z_1^{(2)}\)</span></td><td><span class="math inline">\(g = W_1^{(2)} \cdot f^\prime(z_1^{(2)}) = \delta_1^{(2)}\)</span>。 <span class="math inline">\(lg=f^\prime(z_1^{(2)})\)</span></td></tr><tr class="odd"><td><span class="math inline">\(z_1^{(2)} \to W_{14}^{(1)}\)</span></td><td><span class="math inline">\(g =W_1^{(2)} \cdot f^\prime(z_1^{(2)}) \cdot a_4^{(1)} = \delta_1^{(2)} \cdot a_4^{(1)}\)</span>。 <span class="math inline">\(lg = a_4^{(1)}\)</span> ， 因为<span class="math inline">\(z =wa+b\)</span></td></tr><tr class="even"><td><span class="math inline">\(z_1^{(2)} \to b_1^{(1)}\)</span></td><td><span class="math inline">\(g = W_1^{(2)} \cdot f^\prime(z_1^{(2)}) \cdot 1 = \delta_1^{(2)} \cdot a_4^{(1)}\)</span>。 <span class="math inline">\(lg = 1\)</span> ， 因为<span class="math inline">\(z =wa+b\)</span></td></tr></tbody></table><p>对于上式的梯度计算，有两种理解方法，通过这<strong>两种思路去思考</strong>能更深入了解。</p><ul><li><code>链式法则</code></li><li><code>error sharing and distributed flow approach</code></li></ul><h2 id="梯度反向传播">梯度反向传播</h2><p><span class="math inline">\(\delta_i^{(k)} \to \delta_j^{(k-1)}\)</span> 传播图如下：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ErrorSignal2.png" style="display:block; margin:auto" width="30%"></p><p>但是更多时候，当前层的某个神经元的信息会传播到下一层的多个节点上，如下图：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ErrorSignal3.png" style="display:block; margin:auto" width="30%"></p><p><strong>梯度推导公式</strong>如下： <span class="math display">\[\begin{align}&amp; g_w = \delta_i^{(k)} \cdot a_j^{(k-1)} &amp; W_{ij}^{(k-1)}的梯度\\\\&amp; g_a = \sum_i \delta_i^{(k)}W_{ij}^{(k-1)} &amp; a_j^{(k-1)}的梯度 \\\\&amp; g_z  = \delta_j^{(k-1)} = f^\prime(z_j^{(k-1)}) \cdot \sum_i \delta_i^{(k)}W_{ij}^{(k-1)}  &amp; z_j^{(k-1)}的梯度 \\\\\end{align}\]</span></p><h2 id="bp向量化">BP向量化</h2><p>很明显，不能一个一个参数地去更新<code>element-wise</code>。所以需要用矩阵和向量去表达，去一次性全部更新<code>matrix-vector level</code>。</p><p><strong>梯度计算</strong>， <span class="math inline">\(W_{ij}^{(k)}\)</span>的梯度是<span class="math inline">\(\delta_i^{(k+1)} \cdot a_j^{(k)}\)</span> 。向量表达如下： <span class="math display">\[\Delta _{W^{(k)}} = \begin{bmatrix}\delta_1^{(k+1)} \cdot a_1^{(k)} &amp; \delta_1^{(k+1)} \cdot a_2^{(k)}  &amp; \cdots\\\delta_2^{(k+1)} \cdot a_1^{(k)} &amp; \delta_2^{(k+1)} \cdot a_2^{(k)} &amp; \cdots \\\vdots &amp; \vdots &amp; \ddots\end{bmatrix}= \delta^{(k+1)}  a^{(k)T}\]</span> <strong>梯度传播</strong>，<span class="math inline">\(\delta_j^{(k)} = f^\prime(z_j^{(k)}) \cdot \sum_i \delta_i^{(k+1)}W_{ij}^{(k)}\)</span>。向量表达如下： <span class="math display">\[\delta^{(k)} = f^\prime(z^{(k)}) \circ (\delta^{(k+1)}W^{(k)})\]</span> 其中<span class="math inline">\(\circ\)</span>是叉积向量积<code>element-wise</code>，是各个位置相乘， 即<span class="math inline">\(\mathbb R^N \times \mathbb R^N \to \mathbb R^N\)</span>。 点积和数量积是各个位置相乘求和。</p><p><strong>计算效率</strong></p><p>很明显，在计算的时候要把上一层的<span class="math inline">\(\delta^{(k+1)}\)</span>存起来，去计算<span class="math inline">\(\delta^{(k)}\)</span> ，这样可以减少大量的多余的计算。</p><h1 id="神经网络常识">神经网络常识</h1><h2 id="梯度检查">梯度检查</h2><p>使用导数的定义来估计导数，去和BP算出来的梯度做对比。<br><span class="math display">\[f ^\prime (\theta) \approx \frac{J(\theta^{(i+)}) - J(\theta^{(i-)})} {2 \epsilon }\]</span> 由于这样计算非常，效率特别低，所以只用这种办法来检查梯度。具体实现代码见原notes。</p><h1 id="激活函数">激活函数</h1><p>激活函数有很多，现在<strong>主要用</strong><code>ReLu</code>，<strong>不要用</strong><code>sigmoid</code>。</p><p>用ReLU学习率一定不要设置太大！同一个网络中都使用同一种类型的激活函数。</p><h2 id="sigmoid">Sigmoid</h2><p>数学形式和导数如下： <span class="math display">\[\begin{align}&amp; \sigma (z) = \frac {1} {1 + \exp(-z)}, \; \sigma(z) \in (0,1) \\ \\&amp; \sigma^\prime (z) = \sigma(z) (1 - \sigma(z)) \\\end{align}\]</span> 图像</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/sigmoid_2.gif" style="display:block; margin:auto" width="40%"></p><p>优点是具有好的解释性，将实数挤压到<span class="math inline">\((0,1)\)</span>中，很大的负数变成0，很大的正数变成1 。但现在用的已经越来越少了。有下面2个缺点。</p><p>Sigmoid会<strong>造成梯度消失</strong></p><ul><li><strong>靠近0和1两端时</strong>，梯度会变成0。 BP链式法则，<span class="math inline">\(0 \times g_{from} = 0\)</span> ，后面的<strong>梯度接近0</strong>， <strong>将没有信息去更新参数</strong>。</li><li><strong>初始化权重过大</strong>，<strong>大部分神经元会饱和，无法更新参数</strong>。因为输入值很大，靠近1了。<span class="math inline">\(f^\prime(z) = 0\)</span>， 没法传播了。</li></ul><p>Sigmoid输出<strong>不是以0为均值</strong></p><ul><li>如果输出<span class="math inline">\(x\)</span>全是正的，<span class="math inline">\(z=wx+b\)</span>， 那么<span class="math inline">\(\frac{\partial z}{\partial w} = x\)</span> 梯度就全是正的</li><li>不过一般是batch训练，其实问题也还好</li></ul><p>Sigmoid梯度消失的问题最严重。</p><h2 id="tanh">Tanh</h2><p>数学公式和导数如下： <span class="math display">\[\begin{align}&amp; \tanh (z) = \frac{\exp(z) - \exp(-z)}{\exp(z) + \exp(-z)} = 2 \sigma(2z) - 1, \; \tanh(z) \in(-1, 1) \\ \\&amp;  \tanh^\prime (z) = 1 -  \tanh^2 (z)  \end{align}\]</span> 图像：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/tanh-2.png" style="display:block; margin:auto" width="40%"></p><p>Tanh是Sigmoid的代替，它是0均值的，但是依然存在梯度消失的问题。</p><h2 id="relu">ReLU</h2><p>ReLU<code>Rectified Linear Unit</code> 最近越来越流行，不会对于大值<span class="math inline">\(z\)</span>就导致神经元饱和的问题。在CV取得了很大的成功。 <span class="math display">\[\begin{align}&amp; \rm{rect}(z) = \max(z, 0) \\ \\&amp;  \rm{rect}^\prime (z) = \begin{cases} &amp;1, &amp;z &gt; 0 \\&amp; 0, &amp; z \le 0 \end{cases} \\\end{align}\]</span> 其实ReLU是一个关于0的阈值，<strong>现在一般都用ReLU</strong>：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/relu-3.png" style="display:block; margin:auto" width="40%"></p><p>ReLU的<strong>优点</strong></p><ul><li>加速收敛（6倍）。线性的，不存在梯度消失的问题。一直是1。</li><li>计算简单</li></ul><p>ReLU的<strong>缺点</strong></p><p>训练的时候很脆弱</p><ul><li>BP时，如果有大梯度经过ReLU，当前在z处的梯度<span class="math inline">\(\delta^{(k+1)} = 1 \times g_m\)</span> 就很大</li><li>对参数<span class="math inline">\(w\)</span>的梯度 <span class="math inline">\(\Delta_{W^{(k)}} =\delta^{(k)} a^{(k)T}\)</span> 也就很大</li><li>参数<span class="math inline">\(w\)</span>会更新的特别小 <span class="math inline">\(W^{(k)} = W^{(k)} - \alpha \cdot \Delta_{W^{(k)}}\)</span></li><li>前向时，<span class="math inline">\(z =wx+b \le 0\)</span> 也就特别小，激活函数就不会激活</li><li>不激活，梯度就为0。</li><li>再BP的时候，就无法更新参数了</li></ul><p>总结也就是：大梯度<span class="math inline">\(\to\)</span>小参数<span class="math inline">\(w\)</span> ，新小$z = wx+b $ ReLU不激活， 不激活梯度为0 <span class="math inline">\(\to\)</span> 不更新参数w了。</p><p>当然可以使用比<strong>较小的学习率</strong>来解决这个问题。</p><h2 id="maxout">Maxout</h2><p><code>maxout</code> 有ReLU的优点，同时避免了它的缺点。但是maxout加倍了模型的参数，导致了模型的存储变大。 <span class="math display">\[\begin{align}&amp; \rm{mo}(x) = \max(w_1x+b_1, w_2x+b_2) \\ \\&amp;  \rm{mo}^\prime (x) = \begin{cases} &amp;w_1, &amp;w_1x+b_1 大 \\&amp; w_2, &amp; 其它 \\\end{cases} \\\end{align}\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;神经网络基础&quot;&gt;神经网络基础&lt;/h1&gt;
&lt;p&gt;很多数据都是非线性分割的，所以需要一种&lt;code&gt;非线性non-linear决策
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="神经网络" scheme="http://plmsmile.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="反向传播" scheme="http://plmsmile.github.io/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
      <category term="激活函数" scheme="http://plmsmile.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>Word2vec详细介绍</title>
    <link href="http://plmsmile.github.io/2017/11/12/cs224n-notes1-word2vec/"/>
    <id>http://plmsmile.github.io/2017/11/12/cs224n-notes1-word2vec/</id>
    <published>2017-11-12T12:54:37.000Z</published>
    <updated>2018-03-20T08:19:18.329Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="word2vec">Word2vec</h1><h2 id="简介">简介</h2><p>把词汇变成词向量。</p><table><thead><tr class="header"><th></th><th>类别1</th><th>类别2</th></tr></thead><tbody><tr class="odd"><td>算法</td><td>CBOW，上下文预测中心词汇</td><td>Skip-gram，中心词汇预测上下文</td></tr><tr class="even"><td>训练方法</td><td>负采样</td><td>哈夫曼树</td></tr></tbody></table><h2 id="语言模型">语言模型</h2><p>两种句子：</p><ul><li>正常的句子：<code>The cat jumped over the puddle</code>。 概率高，有意义。</li><li>没意义的句子：<code>stock boil fish is toy</code> 。概率低，没意义。</li></ul><p><strong>二元模型</strong></p><p>一个句子，有<span class="math inline">\(n\)</span>个单词。每个词出现的概率由上一个词语来决定。则整体句子的概率如下表示： <span class="math display">\[P(w_1, w_2, \cdots, w_n) = \prod_{i=2}^n P(w_i \mid w_{i-1})\]</span> <strong>缺点</strong></p><ul><li>只考虑单词相邻传递概率，而<strong>忽略句子整体的可能性</strong>。</li><li>context size=1，只学了相邻单词对的概率</li><li>会计算整个大数据集的全局信息</li></ul><h1 id="cbow">CBOW</h1><p>给上下文<code>The cat _ over the puddle</code>，预测<code>jump</code> 。对于每个单词，学习两个向量：</p><ul><li><span class="math inline">\(v\)</span> ：<code>输入向量</code> ，（上下文单词）</li><li><span class="math inline">\(u\)</span>： <code>输出向量</code> ， （中心单词）</li></ul><h2 id="符号说明">符号说明</h2><ul><li><span class="math inline">\(V\)</span> ：词汇表，后面用<span class="math inline">\(V\)</span>代替词汇表单词个数</li><li><span class="math inline">\(w_i\)</span> ：词汇表中第<span class="math inline">\(i\)</span>个单词</li><li><span class="math inline">\(d\)</span> ：向量的维数</li><li><span class="math inline">\(\mathcal V_{d \times |V|}\)</span>：输入矩阵，也可以用<span class="math inline">\(W\)</span>来表达</li><li><span class="math inline">\(v_i\)</span> ：<span class="math inline">\(\mathcal{V}\)</span>的第<span class="math inline">\(i\)</span>列，<span class="math inline">\(w_i\)</span>的输入向量表达</li><li><span class="math inline">\(\mathcal {U}_{|V| \times d}\)</span> ：输出矩阵，可以用<span class="math inline">\(W ^ \prime\)</span>来表达</li><li><span class="math inline">\(u_i\)</span> ：<span class="math inline">\(\mathcal U\)</span>的第i行， <span class="math inline">\(w_i\)</span>的输出向量表达</li></ul><p>输入与输出</p><ul><li><span class="math inline">\(x^{(c)}\)</span>， 输入<span class="math inline">\(2m\)</span>个上下文单词，上下文词汇的one-hot向量</li><li><span class="math inline">\(y_c\)</span>： 真实标签</li><li><span class="math inline">\(\hat y^{(c)}\)</span>， 输出一个中心单词，中心词汇的one-hot向量</li></ul><h2 id="步骤">步骤</h2><p><strong>1 上下文单词onehot向量</strong></p><p>one-hot向量的表达：<span class="math inline">\((x^{(c-m)}, \cdots, x^{(c-1)}, x^{(c+1)}, x^{(c+m)} \in \mathbb R^V)\)</span></p><p><strong>2 上下文单词向量</strong></p><p><span class="math inline">\((v_{c-m}, v_{c-m+1}, \cdots. v_{c+m} \in \mathbb{R}^d)\)</span>， 其中，<span class="math inline">\(v_{c-m}=\mathcal V x^{(c-m)}\)</span>， 即输入矩阵乘以one-hot向量就找到所在的列</p><p><strong>3 平均上下文词向量</strong></p><p><span class="math inline">\(\hat v = \frac {v_{c-m} + \cdots + v_{c+m}}{2m} \in \mathbb R^d\)</span></p><p><strong>4 输出单词与上下文计算得分向量</strong></p><p><span class="math inline">\(z = \mathcal U \hat v \in \mathbb R ^V\)</span> 。点积，单词越相似，得分越高</p><p><strong>5 得分向量转为概率</strong></p><p>$y = (z) R^V $</p><p><strong>6 真实预测概率对比</strong></p><p>预测的概率向量<span class="math inline">\(\hat y\)</span>与唯一真实中心单词one-hot向量<span class="math inline">\(y\)</span>，进行<code>交叉熵</code>比较算出loss。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/14-cbow.png" style="display:block; margin:auto" width="50%"></p><h2 id="目标函数">目标函数</h2><p>使用<code>交叉熵</code>计算loss，损失函数如下： <span class="math display">\[H(\hat y, y) = - \sum_{j=1}^{|V|} y_j \log (\hat y_j)\]</span> 由于中心单词<span class="math inline">\(y\)</span>是one-hot编码，<strong>只有正确位置才为1，其余均为0</strong>，所以<strong>只需计算中心单词对应的位置概率的loss</strong>即可： <span class="math display">\[H(\hat y, y) = - y_c \log (\hat y_c) = -  \log (\hat y_c) \]</span> <strong>交叉熵很好</strong>是因为</p><ul><li><span class="math inline">\(-1 \cdot \log (1) = 0\)</span>，预测得好</li><li><span class="math inline">\(-1 \cdot \log (0.01) = 4.605\)</span>， 预测得不好</li></ul><p>最终<code>损失函数</code>： <span class="math display">\[\begin{align}\rm{minimize} \;  J &amp; = - \log P(w_c \mid w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}) \\&amp; = - \log P(u_c \mid \hat v) \\&amp; = - \log \frac {\exp(u_c^T \hat v)}{\sum_{j=1}^{|V|} \exp(u_j^T \hat v)}   \\&amp; = -u_c^T \hat v + \log \sum_{j=1}^{|V|} \exp(u_j^T \hat v)\end{align}\]</span> 再使用<code>SGD</code>方法<strong>去更新相关的两种向量<span class="math inline">\(u_c, v_j\)</span></strong> 。</p><h1 id="skip-gram">Skip-gram</h1><p>给中心单词<code>jump</code>，预测上下文<code>The cat _ over the puddle</code> 。</p><p>输入中心单词<span class="math inline">\(x\)</span>， 输出上下文单词<span class="math inline">\(y\)</span> 。与CBOW正好输入输出相反，但同样有两个矩阵<span class="math inline">\(\mathcal {U, V}\)</span> 。符号说明同CBOW。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/15-skipgram.png" style="display:block; margin:auto" width="50%"></p><h2 id="步骤-1">步骤</h2><p><strong>1 中心单词onehot向量</strong></p><p><span class="math inline">\(x \in \mathbb {R}^{|V|}\)</span></p><p><strong>2 中心单词词向量</strong></p><p><span class="math inline">\(v_c = \mathcal V x \in \mathbb R^d\)</span></p><p><strong>3 中心词与其他词的得分向量</strong></p><p><span class="math inline">\(z = \mathcal U v_c \in \mathbb R ^{|V|}\)</span></p><p><strong>4 得分向量转为概率</strong></p><p>概率 <span class="math inline">\(\hat y = \rm {softmax} (z)\)</span>， <strong><span class="math inline">\(\hat y_{c-m}, \ldots, \hat y_{c+m}\)</span> 是目标上下文单词是中心单词的上下文的预测概率</strong>。</p><p><strong>5 预测真实概率对比</strong></p><p>预测概率<span class="math inline">\(\hat y\)</span> 与<span class="math inline">\(2m\)</span> 个真实上下文onehot向量<span class="math inline">\(y_{c-m}, \ldots, y_{c+m}\)</span>进行交叉熵对比，算出loss</p><h2 id="目标函数-1">目标函数</h2><p>与CBOW不同的是，Skip-gram做了一个朴素贝叶斯条件假设，<strong>所有的输出上下文单词都是独立的</strong>。 <span class="math display">\[\begin {align}\rm{minimize} \;  J &amp; = - \log P(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} \mid w_c) \\&amp; = - \log \prod_{j=0, j \neq m}^{2m} P(w_{c-m+j} \mid  w_c) \\&amp; = -\log \prod_{j=0, j \neq m}^{2m} \frac {\exp (u_{c-m+j}^T \cdot v_c)} {\sum_{k=1}^{|V|} \exp (u_k^T \cdot v_c)} \\&amp; = - \sum_{j=0, j \neq m}^{2m} \left ( \log \exp (u_{c-m+j}^T \cdot v_c) -  \log \sum_{k=1}^{|V|} \exp (u_k^T \cdot v_c) \right) \\&amp; = - \sum_{j=0, j \neq m}^{2m} u_{c-m+j}^T v_c + 2m  \cdot \log \sum_{k=1}^{|V|} \exp (u_k^T \cdot v_c)\end {align}\]</span> 一样，使用SGD去优化U和V。</p><p><strong>损失函数实际上是<span class="math inline">\(2m\)</span>个交叉熵求和</strong>，求出的向量<span class="math inline">\(\hat y\)</span>与<span class="math inline">\(2m\)</span>个onehot向量<span class="math inline">\(y_{c-m+j}\)</span> 计算<code>交叉熵</code>： <span class="math display">\[\begin {align}J &amp; = - \sum_{j=0, j \neq m}^{2m} \log P(u_{c-m+j} \mid v_c) \\&amp; =  \sum_{j=0, j \neq m}^{2m} H(\hat y, y_{c-m+j}) \\ \end{align}\]</span></p><h1 id="负采样训练">负采样训练</h1><p>每次计算都会算整个<span class="math inline">\(|V|\)</span>词表，太耗时了。 可以从噪声分布<span class="math inline">\(P_n(w)\)</span>中进行<strong>负采样</strong>，来代替整个词表。当然单词采样概率与其<code>词频</code>相关。只需关心：<code>目标函数</code>、<code>梯度</code>、<code>更新规则</code>。</p><h2 id="标签函数">标签函数</h2><p>对于一对中心词和上下文单词<span class="math inline">\((w, c)\)</span> ，设<code>标签</code>如下：</p><ul><li><span class="math inline">\(P(l = 1 \mid w, c)\)</span>， <span class="math inline">\((w, c)\)</span> 来自于<strong>真实语料</strong></li><li><span class="math inline">\(P(l = 0 \mid w, c)\)</span> ，<span class="math inline">\((w, c)\)</span>来自于<strong>负样本</strong>，即不在语料中</li></ul><p>用<code>sigmoid</code>表示标签函数： <span class="math display">\[\begin {align}&amp; P(l = 1 \mid w, c; \theta) = \sigma (u^T_w v_c) = \frac {1}{ 1 + e^{-u^T_w v_c}} \\&amp;  P(l = 0 \mid w, c; \theta) = 1 -  \sigma (u^T_w v_c) = \frac {1}{ 1 + e^{u^T_w v_c} } \\\end {align}\]</span></p><h2 id="目标函数-2">目标函数</h2><p>选取合适的<span class="math inline">\(\theta= \mathcal {U, V}\)</span> ，<strong>去增大正样本的概率，减小负样本的概率</strong>。设<span class="math inline">\(D\)</span>为正样本集合，<span class="math inline">\(\bar D\)</span>为负样本集合。 <span class="math display">\[\begin {align}\theta &amp; = \mathop{\rm{argmax}}_\limits{\theta} \prod_{(w, c) \in D} P(l=1 \mid w, c, \theta) \prod_{(w, c) \in \bar D} P(l=0 \mid w, c, \theta) \\&amp; = \mathop{\rm{argmax}}_\limits{\theta}  \prod_{(w, c) \in D} P(l=1 \mid w, c, \theta) \prod_{(w, c) \in \bar D} (1 - P(l=1 \mid w, c, \theta) )\\&amp; = \mathop{\rm{argmax}}_\limits{\theta}  \sum_{(w, c) \in D} \log P(l=1 \mid w, c, \theta) + \sum_{(w, c) \in \bar D} \log (1 - P(l=1 \mid w, c, \theta) )\\&amp; = \mathop{\rm{argmax}}_\limits{\theta}  \sum_{(w, c) \in D} \log  \frac {1}{ 1 + \exp (-u^T_w v_c)}+\sum_{(w, c) \in \bar D} \log \frac {1}{ 1 + \exp (u^T_w v_c) } \\&amp; =  \mathop{\rm{argmax}}_\limits{\theta}  \sum_{(w, c) \in D} \log \sigma(u^T_w v_c) + \sum_{(w, c) \in \bar D} \log \sigma (-u^T_w v_c)  \end {align}\]</span> 最大化概率也就是最小化负对数似然 <span class="math display">\[J = - \sum_{(w, c) \in D} \log \sigma(u^T_w v_c) - \sum_{(w, c) \in \bar D} \log \sigma (-u^T_w v_c)\]</span></p><h2 id="负采样集合选择">负采样集合选择</h2><p>为中心单词<span class="math inline">\(w_c\)</span> 从<span class="math inline">\(P_n(w)\)</span> 采样<span class="math inline">\(K\)</span>个假的上下文单词。表示为<span class="math inline">\(\{ \bar u_k \mid k=1\ldots K\}\)</span></p><h2 id="cbow-1">CBOW</h2><p>给上下文向量<span class="math inline">\(\hat{v}=\frac {v_{c-m} + \cdots + v_{c+m}}{2m}\)</span> 和真实中心词<span class="math inline">\(u_c\)</span></p><p><strong>原始loss</strong> <span class="math display">\[J  = -u_c^T \hat v + \log \sum_{j=1}^{|V|} \exp(u_j^T \hat v)\]</span> <strong>负采样loss</strong> <span class="math display">\[J = - \log \sigma (u_c^T \cdot \hat v) - \sum_{k=1}^K \log \sigma (- \bar u_k^T \cdot \hat v)\]</span></p><h2 id="skip-gram-1">Skip-gram</h2><p>给中心单词<span class="math inline">\(v_c\)</span>， 和<span class="math inline">\(2m\)</span>个真实上下文单词<span class="math inline">\(u_{c-m+j}\)</span></p><p><strong>原始loss</strong> <span class="math display">\[J  = - \sum_{j=0, j \neq m}^{2m} u_{c-m+j}^T v_c + 2m  \cdot \log \sum_{k=1}^{|V|} \exp (u_k^T \cdot v_c)\]</span> <strong>负采样loss</strong> <span class="math display">\[J = - \sum_{j=0, j \neq m}^{2m} \log \sigma (u_{c-m+j}^T \cdot v_c) - \sum_{k=1}^K  \log \sigma (-\bar u_{k}^T \cdot v_c)\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;word2vec&quot;&gt;Word2vec&lt;/h1&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;把词汇变成词向量。&lt;/p&gt;

      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="word2vec" scheme="http://plmsmile.github.io/tags/word2vec/"/>
    
      <category term="cbow" scheme="http://plmsmile.github.io/tags/cbow/"/>
    
      <category term="skip-gram" scheme="http://plmsmile.github.io/tags/skip-gram/"/>
    
      <category term="负采样" scheme="http://plmsmile.github.io/tags/%E8%B4%9F%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>word2vec中的数学模型</title>
    <link href="http://plmsmile.github.io/2017/11/02/word2vec-math/"/>
    <id>http://plmsmile.github.io/2017/11/02/word2vec-math/</id>
    <published>2017-11-02T13:53:49.000Z</published>
    <updated>2017-11-13T05:02:50.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>word2vec有两种模型：CBOW和Skip-gram，有两种训练方法：Hierarchical Softmax和Negative Sampling</p></blockquote><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="背景介绍">背景介绍</h1><h2 id="符号">符号</h2><ul><li><span class="math inline">\(C\)</span> ：语料Corpus，所有的文本内容，包含重复的词。</li><li>D：词典，D是从C中取出来的，不重复。</li><li><span class="math inline">\(w\)</span>：一个词语</li><li><span class="math inline">\(m\)</span>：窗口大小，词语<span class="math inline">\(w\)</span>的前后<span class="math inline">\(m\)</span>个词语<br></li><li><span class="math inline">\(\rm{Context(w)} = C_w\)</span>： 词<span class="math inline">\(w\)</span>的上下文词汇，取决于<span class="math inline">\(m\)</span></li><li><span class="math inline">\(v(w)\)</span>： 词典<span class="math inline">\(D\)</span>中单词<span class="math inline">\(w\)</span>的词向量</li><li><span class="math inline">\(k\)</span>：词向量的长度<br></li><li><span class="math inline">\(i_w\)</span>：词语<span class="math inline">\(w\)</span>在词典<span class="math inline">\(D\)</span>中的下标</li><li><span class="math inline">\(NEG(w)\)</span> ： 词<span class="math inline">\(w\)</span>的负样本子集</li></ul><p>常用公式： <span class="math display">\[\begin {align}&amp; \log (a^n b^m) = \log a^n + \log b^m = n \log a + m \log b \\\end{align}\]</span></p><p><span class="math display">\[\log \prod_{i=1}a^i b^{1-i} =  \sum_{i=1} \log a^i + \log b^{1-i} = \sum_{i=1} i \cdot \log  a + (i-1) \cdot \log b\]</span></p><h2 id="目标函数">目标函数</h2><p><a href="https://plmsmile.github.io/2017/07/31/nlp-notes/">n-gram模型</a>。当然，我们使用<strong>神经概率语言模型</strong>。</p><p><span class="math inline">\(P(w \mid C_w)\)</span> 表示上下文词汇推出中心单词<span class="math inline">\(w\)</span>的概率。</p><p>对于统计语言模型来说，一般利用<strong>最大似然</strong>，把目标函数设为： <span class="math display">\[\prod_{w \in C} p(w \mid C_w)\]</span> 一般使用<strong>最大对数似然</strong>，则目标函数为： <span class="math display">\[L = \sum_{w \in C} \log p(w \mid C_w)\]</span> 其实概率<span class="math inline">\(P(w \mid C_w)\)</span>是关于<span class="math inline">\(w\)</span>和<span class="math inline">\(C_w\)</span>的函数，其中<span class="math inline">\(\theta\)</span>是待定参数集，就是要<strong>求最优</strong> <span class="math inline">\(\theta^*\)</span>，来确定函数<span class="math inline">\(F\)</span>： <span class="math display">\[p(w \mid C_w) = F(w, C_w; \; \theta)\]</span> 有了函数<span class="math inline">\(F\)</span>以后，就能够直接算出所需要的概率。 而F的构造，就是通过神经网络去实现的。</p><h2 id="神经概率语言模型">神经概率语言模型</h2><p>一个二元对<span class="math inline">\((C_w, w)\)</span>就是一个训练样本。神经网络结构如下，<span class="math inline">\(W, U\)</span>是权值矩阵，<span class="math inline">\(p, q\)</span>是对应的偏置。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/05-nn-structure.png" style="display:block; margin:auto" width="60%"></p><p>但是一般会减少一层，如下图：（其实是去掉了隐藏层，保留了投影层，是一样的）</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/12-simple-nn.png" style="display:block; margin:auto" width="50%"></p><p>窗口大小是<span class="math inline">\(m\)</span>，<span class="math inline">\(\rm{Context}(w)\)</span>包含<span class="math inline">\(2m\)</span>个词汇，<strong>词向量</strong>长度是<span class="math inline">\(k\)</span>。可以做拼接或者求和（下文是）。拼接得到长向量<span class="math inline">\(2mk\)</span>， 在投影层得到<span class="math inline">\(\mathbf{x_w}\)</span>，然后给到隐藏层和输出层进行计算。 <span class="math display">\[\mathbf{z}_w = \rm{tanh}(W\mathbf{z}_w + \mathbf{p}) \;\to \;\mathbf{y}_w = U \mathbf{z}_w + \mathbf{q}\]</span> 再对<span class="math inline">\(\mathbf{y}_w = (y_1, y_2, \cdots, y_K)\)</span> 向量进行<strong>softmax</strong>即可得到所求得中心词汇的概率： <span class="math display">\[p(w \mid C_w) = \frac{e^{y_{i_w}}}{\sum_{i=1}^K e^{y_i}}\]</span> <strong>优点</strong></p><ul><li>词语的<strong>相似性</strong>可以通过<strong>词向量</strong>来体现</li><li>自带平滑功能。N-Gram需要自己进行平滑。</li></ul><h2 id="词向量的理解">词向量的理解</h2><p>有两种词向量，一种是one-hot representation，另一种是<strong>Distributed Representation</strong>。one-hot太长了，所以DR中把词映射成为<strong>相对短</strong>的向量。不再是只有1个1（<strong>孤注一掷</strong>），而是向量分布于每一维中（<strong>风险平摊</strong>）。再利用<strong>欧式距离</strong>就可以算出词向量之间的<strong>相似度</strong>。</p><p>传统可以通过LSA（Latent Semantic Analysis）和LDA（Latent Dirichlet Allocation）来获得词向量，现在也可以用神经网络算法来获得。</p><p>可以把一个词向量空间向另一个词向量空间进行映射，就可以实现翻译。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/13-vectorspace-trans.png" style="display:block; margin:auto" width="70%"></p><h1 id="hierarchical-softmax">Hierarchical Softmax</h1><p>两种模型都是基于下面三层模式（<strong>无隐藏层</strong>），输入层、投影层和输出层。没有hidden的原因是据说是因为计算太多了。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/05-nn-structure.png" style="display:block; margin:auto" width="60%"></p><p>CBOW和Skip-gram模型：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/06-skipgram-cbow.png" style="display:block; margin:auto" width="70%"></p><h2 id="cbow模型">CBOW模型</h2><p>一共有<span class="math inline">\(\left| C \right|\)</span>个单词。CBOW是基于上下文<span class="math inline">\(context(w) = c_w\)</span>去预测目标单词<span class="math inline">\(w\)</span>，求条件概率<span class="math inline">\(p(w \mid c_w)\)</span>，语言模型一般取目标函数为对数似然函数： <span class="math display">\[L = \sum_{w \in C} \log p(w \mid c_w)\]</span> 窗口大小设为<span class="math inline">\(m\)</span>，则<span class="math inline">\(c_w\)</span>是<span class="math inline">\(w\)</span>的前后m个单词。</p><p><strong>输入层</strong> 是上下文单词的词向量。（初始随机，训练过程中逐渐更新）</p><p><strong>投影层</strong> 就是对上下文词向量进行求和，向量加法。得到单词<span class="math inline">\(w\)</span>的所有上下文词<span class="math inline">\(c_w\)</span>的词向量的和<span class="math inline">\(\mathbf{x}_w\)</span>，待会儿参数更新的时候再依次更新回来。</p><p><strong>输出层</strong> 从<span class="math inline">\(C\)</span>中选择一个词语，实际上是多分类。这里是<strong>哈夫曼树</strong>层次softmax。</p><p>因为词语太多，用softmax太慢了。多分类实际上是多个二分类组成的，比如SVM二叉树分类：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/08-svm-tree-classfier.gif" style="display:block; margin:auto" width="50%"></p><p>这是一种二叉树结构，应用到word2vec中，被称为<strong>Hierarchical Softmax</strong>。CBOW完整结构如下：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/07-cbow-info.png" style="display:block; margin:auto" width="50%"></p><p>每个叶子节点代表一个词语<span class="math inline">\(w\)</span>，每个词语被01唯一编码。</p><h2 id="哈夫曼编码">哈夫曼编码</h2><p>哈夫曼树很简单。每次从许多节点中，选择权值最小的两个合并，根节点为合并值；依次循环，直到只剩一棵树。</p><p>比如“我 喜欢 看 巴西 足球 世界杯”，这6个词语，出现的次数依次是15, 8, 6, 5, 3, 1。建立得到哈夫曼树，并且得到哈夫曼编码，如下：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/09-huffman-code.png" style="display:block; margin:auto" width="60%"></p><h2 id="cbow足球例子">CBOW足球例子</h2><p>引入一些符号：</p><ul><li><span class="math inline">\(p^w\)</span> ：从根节点到达<span class="math inline">\(w\)</span>叶子节点的路径</li><li><span class="math inline">\(l^w\)</span> ： 路径<span class="math inline">\(p^w\)</span>中节点的个数</li><li><span class="math inline">\(p^w_1, \cdots, p^w_{l_w}\)</span> ：依次代表路径中的节点，根节点-中间节点-叶子节点</li><li><span class="math inline">\(d^w_2, \cdots, d^w_{l^w} \in \{0, 1\}\)</span>：词<span class="math inline">\(w\)</span>的哈夫曼编码，由<span class="math inline">\(l^w-1\)</span>位构成， 根节点无需编码</li><li><span class="math inline">\(\theta_1^w, \cdots, \theta^w_{l^w -1}\)</span>：路径中<strong>非叶子节点对应的向量</strong>， 用于辅助计算。</li><li>单词<span class="math inline">\(w\)</span>是足球，对应的所有上下文词汇是<span class="math inline">\(c_w\)</span>， 上下文词向量的和是<span class="math inline">\(\mathbf{x}_w\)</span></li></ul><p>看一个例子：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/10-huffman-cbow.png" style="display:block; margin:auto" width="70%"></p><p>约定编码为1是负类，为0是正类。<strong>即左边是负类，右边是正类</strong>。</p><p><strong>每一个节点就是一个二分类器，是逻辑回归</strong>(sigmoid)。其中<span class="math inline">\(\theta\)</span>是对应的非叶子节点的向量，一个节点被分为正类和负类的概率分别如下： <span class="math display">\[\sigma(\mathbf{x}_w^T \theta) = \frac {1}{ 1 + e^{-\mathbf{x}_w^T \theta}},\quad 1 - \sigma(\mathbf{x}_w^T \theta)\]</span> 那么从根节点到达足球的概率是： <span class="math display">\[p (足球 \mid c_{足球}) = \prod_{j=2}^5 p(d_j^w \mid \mathbf{x}_w, \theta_{j-1}^w)\]</span></p><h2 id="cbow总结">CBOW总结</h2><p><strong>目标函数</strong></p><p>从根节点到每一个单词<span class="math inline">\(w\)</span>都存在一条路径<span class="math inline">\(p^w\)</span>，路径上有<span class="math inline">\(l^w-1\)</span>个分支节点，每个节点就是一个二分类，每次产生一个概率 <span class="math inline">\(p(d_j^w \mid \mathbf{x}_w, \theta^w_{j-1})\)</span>， 把这些概率乘起来就得到了<span class="math inline">\(p(w \mid c_w)\)</span>。</p><p>其中每个节点的概率是，与各个节点的参数和传入的上下文向量和<span class="math inline">\(\mathbf{x}_w\)</span>相关。 <span class="math display">\[p(d_j^w \mid \mathbf{x}_w, \theta^w_{j-1}) = \begin{cases}&amp; \sigma(\mathbf{x}^T_w \theta^w_{j-1}), &amp; d_j^w = 0 \\&amp; 1 - \sigma(\mathbf{x}^T_w \theta^w_{j-1}), &amp; d_j^w = 1\\\end{cases}\]</span> 写成指数形式是 <span class="math display">\[p(d_j^w \mid \mathbf{x}_w, \theta^w_{j-1}) = [\sigma(\mathbf{x}^T_w \theta^w_{j-1})]^{1-d_j^w}  \cdot [1 - \sigma(\mathbf{x}^T_w \theta^w_{j-1})]^{d_j^w}\]</span> 则上下文推中间单词的概率，即<strong>目标函数</strong>： <span class="math display">\[p(w \mid c_w) = \prod_{j=2}^{l^w} p(d_j^w \mid \mathbf{x}_w, \theta^w_{j-1})\]</span> <strong>对数似然函数</strong></p><p>对目标函数取对数似然函数是： <span class="math display">\[\begin{align}L &amp; = \sum_{w \in C} \log p(w  \mid c_w) = \sum_{w \in C} \log  \prod_{j=2}^{l^w} [\sigma(\mathbf{x}^T_w \theta^w_{j-1})]^{1-d_j^w} \cdot[1 - \sigma(\mathbf{x}^T_w \theta^w_{j-1})]^{d_j^w} \\&amp; = \sum_{w \in C} \sum_{j=2}^{l^w} \left( (1-d_j^w) \cdot \log \sigma(\mathbf{x}^T_w \theta^w_{j-1})+ d_j^w \cdot \log (1 - \sigma(\mathbf{x}^T_w \theta^w_{j-1}))\right) \\&amp; = \sum_{w \in C} \sum_{j=2}^{l^w} \left( (1-d_j^w) \cdot \log A+ d_j^w \cdot \log (1 -A))\right)\end{align}\]</span> 简写： <span class="math display">\[\begin{align}&amp; L(w, j) =  (1-d_j^w) \cdot \log \sigma(\mathbf{x}^T_w \theta^w_{j-1})+ d_j^w \cdot \log (1 - \sigma(\mathbf{x}^T_w \theta^w_{j-1})) \\&amp; L = \sum_{w,j} L(w, j)\end{align}\]</span> 怎样最大化对数似然函数呢，可以最大化每一项，或者使整体最大化。尽管最大化每一项不一定使整体最大化，但是这里还是使用最大化每一项<span class="math inline">\(L(w, j)\)</span>。</p><p>sigmoid函数的求导： <span class="math display">\[\sigma ^{\prime}(x) = \sigma(x)(1 - \sigma(x))\]</span> <span class="math inline">\(L(w, j)\)</span>有两个参数：输入层的<span class="math inline">\(\mathbf{x}_w\)</span> 和 每个节点的参数向量<span class="math inline">\(\theta_{j-1}^w\)</span> 。 分别求偏导并且进行更新参数： <span class="math display">\[\begin{align}&amp; \frac{\partial}{\theta_{j-1}^w} L(w, j) = [1 - d_j^w - \sigma(\mathbf{x}^T_w \theta^w_{j-1})] \cdot \mathbf{x}_w   \quad \to \quad  \theta_{j-1}^w = \theta_{j-1}^w + \alpha \cdot  \frac{\partial}{\theta_{j-1}^w} L(w, j)\\&amp;  \frac{\partial}{\mathbf{x}_w} L(w, j) = [1 - d_j^w - \sigma(\mathbf{x}^T_w \theta^w_{j-1})] \cdot \theta_{j-1}^w  \quad \to \quad v(\hat w)+=  v(\hat w) + \alpha \cdot \sum_{j=2}^{l^w} \frac{\partial}{\mathbf{x}_w} L(w, j), \hat w \in c_w\\\end{align}\]</span> 注意：<span class="math inline">\(\mathbf{x}_w\)</span>是所有上下文词向量的和，应该把它的更新平均更新到每个上下文词汇中去。<span class="math inline">\(\hat w\)</span> 代表<span class="math inline">\(c_w\)</span>中的一个词汇。</p><h2 id="skip-gram模型">Skip-Gram模型</h2><p>Skip-gram模型是根据当前词语，预测上下文。网络结构依然是输入层、投影层(其实无用)、输出层。如下：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/11-huffman-skipgram.png" style="display:block; margin:auto" width="60%"></p><p>输入一个中心单词的词向量<span class="math inline">\(v(w)\)</span>，简记为<span class="math inline">\(v_w\)</span>，输出是一个哈夫曼树。单词<span class="math inline">\(u\)</span>是<span class="math inline">\(w\)</span>的上下文单词<span class="math inline">\(c_w\)</span>中的一个。这是一个词袋模型，每个<span class="math inline">\(u\)</span>是互相独立的。</p><p><strong>目标函数</strong></p><p>所以<span class="math inline">\(c_w\)</span>是<span class="math inline">\(w\)</span>的上下文词汇的概率是： <span class="math display">\[p(c_w \mid w) = \prod_{u \in c_w} p(u \mid w)\]</span> 与上面同理，<span class="math inline">\(p(u \mid w)\)</span> 与传入的中心单词向量<span class="math inline">\(v(w)\)</span>和路径上的各个节点相关： <span class="math display">\[\begin{align}&amp; p(u \mid w) =  \prod_{j=2}^{l^w} p(d_j^u \mid v_w,\; \theta^u_{j-1}) \\&amp; p(d_j^u \mid v_w ,\; \theta^u_{j-1} ) = [\sigma(v_w^T \theta^u_{j-1})]^{1-d_j^u}  \cdot [1 - \sigma(v_w^T \theta^u_{j-1})]^{d_j^u}\\\end{align}\]</span> 下文<span class="math inline">\(v_w^T \theta^w_{j-1}\)</span>简记为<span class="math inline">\(v_w \theta_{j-1}^w\)</span>，要记得转置向量相乘就可以了。</p><p><strong>对数似然函数</strong> <span class="math display">\[\begin{align}L&amp;  = \sum_{w \in C} \log p(c_w \mid w) \\&amp; = \sum_{w \in C} \log \prod_{u \in c_w} \prod _{j=2}^{l^w}    [\sigma(v_w^T \theta^u_{j-1})]^{1-d_j^u}  \cdot [1 - \sigma(v_w^T \theta^u_{j-1})]^{d_j^u} \\ &amp; = \sum_{w \in C} \sum_{u \in c_w} \sum_{j=2}^{l^w}  \left( (1-d_j^u) \cdot \log \sigma(v_w^T \theta^u_{j-1})+ d_j^u \cdot \log (1 - \sigma(v_w^T  \theta^u_{j-1}))\right) \\\end{align}\]</span> 同样，简写每一项为<span class="math inline">\(L(w, u, j)\)</span> <span class="math display">\[L(w, u, j) = (1-d_j^u) \cdot \log \sigma(v_w^T \theta^u_{j-1})+ d_j^u \cdot \log (1 - \sigma(v_w^T  \theta^u_{j-1}))\]</span> 然后就是，分别对<span class="math inline">\(v_w\)</span>和<span class="math inline">\(\theta_{j-1}^u\)</span>求梯度更新即可，同上面的类似。得到下面的更新公式 <span class="math display">\[\begin{align}&amp; \theta_{j-1}^u = \theta_{j-1}^u + \alpha \cdot [1 - d_j^u - \sigma(v_w^t \cdot \theta_{j-1}^u)] \cdot v(w) \\&amp; v_w = v_w + \alpha \cdot \sum_{u \in c_w} \sum_{j=2}^{l^w} \frac{\partial L(w, u, j)}{\partial v_w} \\\end{align}\]</span></p><h1 id="negative-sampling">Negative Sampling</h1><h2 id="背景知识介绍">背景知识介绍</h2><p>Negative Sampling简称NEG，是Noise Contrastive Estimation(NCE)的一个简化版本，目的是用来提高训练速度和改善所得词向量的质量。</p><p>NEG不使用复杂的哈夫曼树，而是使用<strong>随机负采样</strong>，大幅度提高性能，是Hierarchical Softmax的一个替代。</p><p>NCE 细节有点复杂，本质上是利用已知的概率密度函数来估计未知的概率密度函数。简单来说，如果已知概率密度X，未知Y，如果知道X和Y的关系，Y也就求出来了。</p><p>在训练的时候，需要给正例和负例。Hierarchical Softmax是把负例放在二叉树的根节点上，而NEG，是随机挑选一些负例。</p><h2 id="cbow">CBOW</h2><p>对于一个单词<span class="math inline">\(w\)</span>，<strong>输入上下文<span class="math inline">\(\rm{Context}(w) = C_w\)</span>，输出单词<span class="math inline">\(w\)</span></strong>。那么词<span class="math inline">\(w\)</span>是正样本<strong>，</strong>其他词都是负样本。 负样本很多，该怎么选择呢？后面再说。</p><p>定义<span class="math inline">\(\rm{Context}(w)\)</span>的负样本子集<span class="math inline">\(\rm{NEG}(w)\)</span>。对于样本<span class="math inline">\((C_w, w)\)</span>，<span class="math inline">\(\mathbf{x}_w\)</span>依然是<span class="math inline">\(C_w\)</span>的词向量之和。<span class="math inline">\(\theta_u\)</span>为词<span class="math inline">\(u\)</span>的一个（辅助）向量，待训练参数。</p><p>设集合<span class="math inline">\(S_w = w \bigcup NEG(w)\)</span> ，对所有的单词<span class="math inline">\(u \in S_w\)</span>，有<strong>标签函数</strong>： <span class="math display">\[b^w(u) = \begin{cases}&amp; 1, &amp; u = w \\&amp; 0, &amp; u \neq w \\\end{cases}\]</span> <strong>单词<span class="math inline">\(u\)</span>是<span class="math inline">\(C_w\)</span> 的中心词的概率</strong>是： <span class="math display">\[p(u \mid C_w) = \begin{cases}&amp; \sigma(\mathbf x_w^T \theta^u), &amp; u=w \; \text{正样本} \\&amp; 1 -  \sigma(\mathbf x_w^T \theta^u), &amp; u \neq w \; \text{负样本} \\\end{cases}\]</span> 简写为： <span class="math display">\[\color{blue} {p(u \mid C_w)} = [ \sigma(\mathbf x_w^T \theta^u)]^{b^w(u)} \cdot [1 -  \sigma(\mathbf x_w^T \theta^u)]^{1 - b^w(u)}\]</span> <strong>要最大化目标函数</strong><span class="math inline">\(g(w) = \sum_{u \in S_w} p(u \mid C_w)\)</span>： <span class="math display">\[\begin{align}\color{blue}{g(w) }&amp; = \prod_{u \in S_w} [ \sigma(\mathbf x_w^T \theta^u)]^{b^w(u)} \cdot [1 -  \sigma(\mathbf x_w^T \theta^u)]^{1 - b^w(u)} \\&amp;= \color{blue} {\sigma(\mathbf x_w^T \theta^u)\prod_{u \in NEG(w)} (1 -  \sigma(\mathbf x_w^T \theta^u)) } \\\end{align}\]</span> 观察<span class="math inline">\(g(w)\)</span>可知，最大化就是要：<strong>增大正样本概率和减小化负样本概率</strong>。</p><p>每个词都是这样，对于整个<strong>语料库</strong>的所有词汇，将<span class="math inline">\(g\)</span>累计得到优化目标，目标函数如下： <span class="math display">\[\begin {align}L &amp; =  \log \prod_{w \in C}g(w) = \sum_{w \in C} \log g(w) \\&amp; = \sum_{w \in C} \log \left(  \prod_{u \in S_w} [ \sigma(\mathbf x_w^T \theta^u)]^{b^w(u)} \cdot [1 -  \sigma(\mathbf x_w^T \theta^u)]^{1 - b^w(u)} \right) \\ &amp;=  \sum_{w \in C} \sum_{u \in S_w}  \left[ b^w_u \cdot \sigma(\mathbf x_w^T \theta^u) + (1-b^w_u) \cdot (1 - \sigma(\mathbf x_w^T \theta^u)) \right]\end {align}\]</span> 简写每一步<span class="math inline">\(L(w, u)\)</span>： <span class="math display">\[L(w, u) = b^w_u \cdot \sigma(\mathbf x_w^T \theta^u) + (1-b^w_u) \cdot (1 - \sigma(\mathbf x_w^T \theta^u))\]</span> 计算<span class="math inline">\(L(w, u)\)</span>对<span class="math inline">\(\theta^u\)</span>和<span class="math inline">\(\mathbf{x}_w\)</span>的<strong>梯度</strong>进行更新，得到梯度(<strong>对称性</strong>)： <span class="math display">\[\frac{\partial L(w, u) }{ \partial \theta^u} = [b^w(u) - \sigma(\mathbf x_w^T \theta^u)] \cdot \mathbf{x}_w, \quad \frac{\partial L(w, u) }{ \partial \mathbf{x}_w} = [b^w(u) - \sigma(\mathbf x_w^T \theta^u)] \cdot \theta^u\]</span> <strong>更新</strong>每个单词的<strong>训练参数<span class="math inline">\(\theta^u\)</span></strong> ： <span class="math display">\[\theta^u = \theta^u + \alpha \cdot \frac{\partial L(w, u) }{ \partial \theta^u}\]</span> 对每个单词<strong>更新词向量<span class="math inline">\(v(u)\)</span></strong> ： <span class="math display">\[v(u) = v(u) + \alpha \cdot \sum_{u \in S_w} \frac{\partial L(w, u) }{ \partial \mathbf{x}_w}\]</span></p><h2 id="skip-gram">Skip-gram</h2><p>H给单词<span class="math inline">\(w\)</span>，预测上下文向量<span class="math inline">\(\rm{Context}(w) = C_w\)</span>。 输入样本<span class="math inline">\((w, C_w)\)</span>。</p><p>中心单词是<span class="math inline">\(w\)</span>，遍历样本中的上下文单词<span class="math inline">\(w_o \in C_w\)</span>，为每个上下文单词<span class="math inline">\(w_o\)</span>生成一个包含负采样的集合<span class="math inline">\(S_o = w \bigcup \rm{NEG}(o)\)</span> 。即<span class="math inline">\(S_o\)</span>里面只有<span class="math inline">\(w\)</span>才是<span class="math inline">\(o\)</span>的中心单词。</p><p>下面<span class="math inline">\(w_o\)</span>简写为<span class="math inline">\(o\)</span>，要注意实际上是当前中心单词<span class="math inline">\(w\)</span>的上下文单词。</p><p><span class="math inline">\(S_o\)</span>中的<span class="math inline">\(u\)</span>是实际的w就为1，否则为0。标签函数如下：<br><span class="math display">\[b^w(u) = \begin{cases}&amp; 1, &amp; u = w \\&amp; 0, &amp; u \neq w \\\end{cases}\]</span> <span class="math inline">\(S_o​\)</span>中的<span class="math inline">\(u​\)</span>是<span class="math inline">\(o​\)</span>的中心词的概率是 <span class="math display">\[p(u \mid o)   = \begin{cases}&amp; \sigma (v_o^T \theta^u ), &amp; u=w \; \leftrightarrow \; b^w(u) = 1  \\&amp; 1 - \sigma (v_o^T \theta^u ), &amp;u \neq w \; \leftrightarrow \; b^w(u) = 0 \\\end{cases}\]</span> 简写为 <span class="math display">\[\color{blue} {p(u \mid o)} = [ \sigma(v_o^T \theta^u)]^{b^w(u)} \cdot[1 -  \sigma(v_o^T \theta^u)]^{1 - b^w(u)}\]</span> 对于<span class="math inline">\(w\)</span>的一个上下文单词<span class="math inline">\(o\)</span>来说，要最大化这个概率： <span class="math display">\[\prod_{u \in S_o} p(u \mid o )\]</span> 对于<span class="math inline">\(w\)</span>的所有上下文单词<span class="math inline">\(C_w\)</span>来说，要最大化： <span class="math display">\[g(w) = \prod_{o \in C_w} \prod_{u \in S_o} p(u \mid o )\]</span> 那么，对于整个预料，要最大化： <span class="math display">\[G = \prod_{w \in C} g(w) =\prod_{w \in C}   \prod_{o \in C_w} \prod_{u \in S_o} p(u \mid o )\]</span> 对G取对数，<strong>最终的目标函数</strong>就是： <span class="math display">\[\begin {align}L &amp; = \log G = \sum_{w \in C}   \sum_{o \in C_w} \log \prod_{u \in S_o}  p(u \mid o ) \\&amp;=\sum_{w \in C}   \sum_{o \in C_w}  \log  \prod_{u \in S_o}[ \sigma(v_o^T \theta^u)]^{b^w(u)}  \cdot [1 -  \sigma(v_o^T \theta^u)]^{1 - b^w(u)} \\&amp; =  \sum_{w \in C}   \sum_{o \in C_w} \sum_{u \in S_o} \left (b^w_u \cdot \sigma(v_o^T \theta^u) + (1 - b^w_u) \cdot (1-\sigma(v_o^T \theta^u))\right)\end{align}\]</span> 取<span class="math inline">\(w, o, u\)</span><strong>简写</strong>L(w, o, u)： <span class="math display">\[L(w, o, u) = b^w_u \cdot \sigma(v_o^T \theta^u) + (1 - b^w_u) \cdot (1-\sigma(v_o^T \theta^u))\]</span> <strong>分别对<span class="math inline">\(\theta^u、v_o\)</span>求梯度</strong> <span class="math display">\[\frac{\partial L(w, o, u) }{ \partial \theta^u} = [b^w_u - \sigma(v_o^T \theta^u)] \cdot v_o, \quad \frac{\partial L(w,o, u) }{ \partial v_o}= [b^w_u - \sigma(v_o^T \theta^u)] \cdot \theta^u\]</span> <strong>更新</strong>每个单词的<strong>训练参数<span class="math inline">\(\theta^u\)</span></strong> ： <span class="math display">\[\theta^u = \theta^u + \alpha \cdot \frac{\partial L(w, o,u) }{ \partial \theta^u}\]</span> 对每个单词<strong>更新词向量<span class="math inline">\(v(o)\)</span></strong> ： <span class="math display">\[v(o) = v(o) + \alpha \cdot \sum_{u \in S_o} \frac{\partial L(w, u) }{ \partial v_o}\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;word2vec有两种模型：CBOW和Skip-gram，有两种训练方法：Hierarchical Softmax和Negative Sampling&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;&quot; style=&quot;displ
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="word2vec" scheme="http://plmsmile.github.io/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>Word2vec推导图文笔记</title>
    <link href="http://plmsmile.github.io/2017/11/02/cs224n-lecture2-word2vec/"/>
    <id>http://plmsmile.github.io/2017/11/02/cs224n-lecture2-word2vec/</id>
    <published>2017-11-02T01:33:45.000Z</published>
    <updated>2018-03-20T08:18:18.697Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="word-meaning">Word meaning</h1><h2 id="词意">词意</h2><p>词的意思就是<code>idea</code>，如下：</p><ul><li>词汇本身表达的意义</li><li>人通过词汇传达的想法</li><li>在写作、艺术中表达的意思</li><li>signifier - signified(idea or thing) - denotation</li></ul><h2 id="传统离散表达">传统离散表达</h2><p>传统使用分类学去建立一个WordNet，其中包含许多上位词<code>is-a</code>和同义词集等。如下：</p><table><colgroup><col width="50%"><col width="50%"></colgroup><thead><tr class="header"><th align="center">上义词</th><th align="center">同义词</th></tr></thead><tbody><tr class="odd"><td align="center">entity, physical_entity,object, organism, animal</td><td align="center">full, good; estimable, good, honorable, respectable</td></tr></tbody></table><p>离散表达的问题：</p><ul><li>丢失了细微差别，比如同义词：adept, expert, good, practiced, proficient, skillful</li><li>不能处理新词汇</li><li>分类太主观</li><li>需要人力去构建和修改</li><li><strong>很难去计算词汇相似度</strong></li></ul><p>每个单词使用<code>one-hot</code>编码，比如hotel=<span class="math inline">\([0, 1, 0, 0, 0]\)</span>，motel=<span class="math inline">\([0, 0, 1, 0, 0]\)</span>。 当我搜索<code>settle hotel</code>的时候也应该去匹配包含<code>settle motel</code>的文章。 但是我们的查询hotel向量和文章里面的motel向量却是<strong>正交的</strong>，<strong>算不出相似度</strong>。</p><h2 id="分布相似表达">分布相似表达</h2><p>通过一个单词的上下文去表达这个单词。</p><blockquote><p>You shall know a word by the company it keeps. --- JR. Firth</p></blockquote><p>例如，下面用周围的单词去表达<strong>banking</strong> ：</p><blockquote><p>government debt problems turning into <strong>banking</strong> crises as has happened in ​ saying that Europe needs unified <strong>banking</strong> regulation to replace the hodgepodge</p></blockquote><p><strong>稠密词向量</strong></p><p>一个单词的意义应该是由它本身的词向量来决定的。这个词向量可以预测出的上下文单词。</p><p>比如lingustics的词向量是<span class="math inline">\([0.286, 0.792, -0.177, -0.107, 0.109, -0.542, 0.349]\)</span></p><h2 id="词嵌入思想">词嵌入思想</h2><p>构建一个模型，根据中心单词<span class="math inline">\(w_t\)</span>，通过自身词向量，去预测出它的上下文单词。 <span class="math display">\[p (context \mid w_t) = \cdots\]</span> 损失函数如下，<span class="math inline">\(w_{-t}\)</span>表示<span class="math inline">\(w_t\)</span>的上下文（负号通常表示除了某某之外），如果完美预测，损失函数为0。 <span class="math display">\[J = 1 - p(w_{-t} \mid w_t)\]</span></p><h1 id="word2vec">Word2Vec</h1><p>在每个单词和其上下文之间进行预测。</p><p>有两种算法：</p><ul><li><strong>Skip-grams(SG)</strong>： 给目标单词，预测上下文</li><li>Continuous Bag of Words(CBOW)：给上下文，预测目标单词</li></ul><p>两个稍微高效的训练方法：</p><ul><li>分层softmax</li><li>负采样</li></ul><p>课上只是Naive softmax。两个模型，两种方法，一共有4种实现。<a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="noopener">这里是word2vec详细信息</a>。</p><h2 id="skip-gram">Skip-gram</h2><p>对于每个单词<span class="math inline">\(w_t\)</span>，会选择一个上下文窗口<span class="math inline">\(m\)</span>。 然后要预测出范围内的上下文单词，使概率<span class="math inline">\(P(w_{t+i} \mid w_t)\)</span>最大。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/01_skip-gram.png" style="display:block; margin:auto" width="60%"></p><p><strong>目标函数</strong></p><p><span class="math inline">\(\theta\)</span>是我们要训练的参数，目标函数就是所有位置预测结果的乘积，最大化目标函数： <span class="math display">\[J^\prime (\theta) = \prod_{t=1}^T \prod_{-m\le j \le m} p(w_{t+j} \mid w_t ;\; \theta), \quad t \neq j\]</span> 一般使用<code>negative log likelihood</code> ：<a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="noopener">负采样教程</a>。</p><p>要<strong>最大化目标函数</strong>，就得得到损失函数。对于对数似然函数，<strong>取其负对数就可以得到损失函数</strong>，再最小化损失函数，其中<span class="math inline">\(T\)</span>是文本长度，<span class="math inline">\(m\)</span>是窗口大小： <span class="math display">\[J(\theta) = - \frac{1}{T} \sum_{t=1}^T \sum_{-m\le j \le m} \log P(w_{t+j} \mid w_t)\]</span></p><ul><li>Loss 函数 = Cost 函数 = Objective 函数</li><li>对于softmax概率分布，一般使用交叉熵作为损失函数</li><li>单词<span class="math inline">\(w_{t+j}\)</span>是one-hot编码</li><li>negative log probability</li></ul><h2 id="word2vec细节">Word2vec细节</h2><p>词汇和词向量符号说明：</p><ul><li><span class="math inline">\(u\)</span> 上下文词向量，向量是<span class="math inline">\(d\)</span>维的</li><li><span class="math inline">\(v\)</span> 词向量</li><li>中心词汇<span class="math inline">\(t\)</span>，对应的向量是<span class="math inline">\(v_t\)</span></li><li>上下文词汇<span class="math inline">\(j\)</span> ，对应的词向量是<span class="math inline">\(u_j\)</span></li><li>一共有<span class="math inline">\(V\)</span>个词汇</li></ul><p>计算<span class="math inline">\(p(w_{t+j} \mid w_t)\)</span>， 即： <span class="math display">\[p(w_{j} \mid w_t)= \mathrm{softmax} (u_j^T \cdot v_t) = \frac{\exp(u_j^T \cdot v_t)} {\sum_{i=1}^V \exp(u_i^T \cdot v_t)}\]</span> 两个单词越相似，点积越大，向量点积如下： <span class="math display">\[u^T \cdot v = \sum_{i=1}^M u_i \times v_i\]</span> softmax之所以叫softmax，是因为指数会让大的数越大，小的数越小。类似于max函数。下面是计算的详细信息：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/02-skip-gram-detail.png" style="display:block; margin:auto" width="80%"></p><p>一些理解和解释：</p><ul><li><span class="math inline">\(w_t\)</span>是one-hot编码的中心词汇，维数是<span class="math inline">\((V, 1)\)</span></li><li><span class="math inline">\(W\)</span>是词汇表达矩阵，维数是<span class="math inline">\((d, V)\)</span>，一列就是一个单词</li><li><span class="math inline">\(Ww_t = v_t\)</span> 相乘得到词向量<span class="math inline">\(v_t\)</span> ，<span class="math inline">\((d, V) \cdot (V, 1) \to (d, 1)\)</span>， <strong>用<span class="math inline">\(d\)</span>维向量去表达了词汇t</strong></li><li><span class="math inline">\(W^\prime\)</span>， <span class="math inline">\(W^{\prime}\cdot v_t = s\)</span>，<span class="math inline">\((V, d) \cdot (d, 1) \to (V, 1)\)</span> ， 得到 语义相似度向量<span class="math inline">\(s\)</span></li><li>再对<span class="math inline">\(s\)</span>进行softmax即可求得上下文词汇</li><li>每个单词有两个向量，作为center单词向量和context单词向量</li></ul><h2 id="偏导计算">偏导计算</h2><p>设<span class="math inline">\(o\)</span>是上下文单词，<span class="math inline">\(c\)</span>是中心单词，条件概率如下： <span class="math display">\[P(o \mid c) = \frac{\exp(u_o^T \cdot v_c)} {\sum_{i=1}^V \exp(u_i^T \cdot v_c)}\]</span> 这里只计算<span class="math inline">\(\log P\)</span>对<span class="math inline">\(v_c\)</span>向量的偏导。</p><p>用<span class="math inline">\(\mathbf{\theta}\)</span>向量表示所有的参数，有<span class="math inline">\(V\)</span>个单词，<span class="math inline">\(d\)</span>维向量。每个单词有2个向量。参数个数一共是<span class="math inline">\(2dV\)</span>个。</p><p>向量偏导计算公式，<span class="math inline">\(\mathbf{x, a}\)</span> 均是向量 <span class="math display">\[\frac {\partial \mathbf{x}^T \mathbf{a}} { \partial \mathbf{x}}= \frac {\partial  \mathbf{a}^T  \mathbf{x}} { \partial \mathbf{x}} = \mathbf{a}\]</span> 函数偏导计算，<strong>链式法则</strong>，<span class="math inline">\(y=f(u), u=g(x)\)</span> <span class="math display">\[\frac{\mathrm{d}y}{\mathrm{d} x}= \frac{\mathrm{d}y}{\mathrm{d} u} \frac{\mathrm{d}u}{\mathrm{d} x}\]</span> 最小化<code>损失函数</code>： <span class="math display">\[J(\theta) = - \frac{1}{T} \sum_{t=1}^T \sum_{-m\le j \le m} \log P(w_{t+j} \mid w_t), \quad j \neq m\]</span> 这里<strong>只计算<span class="math inline">\(v_c\)</span>的偏导</strong>，先进行<strong>分解原式为2个部分</strong>： <span class="math display">\[ \frac { \partial} {\partial v_c} \log P(o \mid c)= \frac { \partial} {\partial v_c} \log \frac{\exp(u_o^T \cdot v_c)} {\sum_{i=1}^V \exp(u_i^T \cdot v_c)} =  \underbrace { \frac { \partial} {\partial v_c} \log \exp (u_o^T \cdot v_c) }_{1} -   \underbrace { \frac { \partial} {\partial v_c} \log \sum_{i=1}^V \exp(u_i^T \cdot v_c) }_{2}\]</span> <strong>部分1推导</strong> <span class="math display">\[\begin{align}\frac { \partial} {\partial v_c}  \color{red}{\log \exp (u_o^T \cdot v_c) }&amp; = \frac { \partial} {\partial v_c}  \color{red}{u_o^T \cdot v_c} = \mathbf{u_o}\end{align}\]</span> <strong>部分2推导</strong> <span class="math display">\[\begin{align} \frac { \partial} {\partial v_c} \log \sum_{i=1}^V \exp(u_i^T \cdot v_c) &amp; = \frac{1}{\sum_{i=1}^V \exp(u_i^T \cdot v_c)}  \cdot  \color{red}{ \frac { \partial} {\partial v_c} \sum_{x=1}^V \exp(u_x^T \cdot v_c)} \\ &amp; = \frac{1}{A} \cdot \sum_{x=1}^V \color{red} {\frac { \partial} {\partial v_c} \exp(u_x^T \cdot v_c)} \\ &amp; =  \frac{1}{A} \cdot \sum_{x=1}^V \exp (u_x^T \cdot v_c) \color{red} {\frac { \partial} {\partial v_c} u_x^T \cdot v_c} \\ &amp; = \frac{1}{\sum_{i=1}^V \exp(u_i^T \cdot v_c)}  \cdot \sum_{x=1}^V \exp (u_x^T \cdot v_c)  \color{red} {u_x} \\ &amp; = \sum_{x=1}^V \color{red} { \frac{\exp (u_x^T \cdot v_c)} {\sum_{i=1}^V \exp(u_i^T \cdot v_c)}} \cdot u_x \\ &amp; = \sum_{x=1}^V \color{red} {P(x \mid c) }\cdot u_x \end{align}\]</span> 所以，综合起来可以求得，单词o是单词c的上下文概率<span class="math inline">\(\log P(o \mid c)\)</span> 对center向量<span class="math inline">\(v_c\)</span>的偏导： <span class="math display">\[\frac { \partial} {\partial v_c} \log P(o \mid c)= u_o -\sum_{x=1}^V P(x \mid c) \cdot u_x = \color{blue} {\text{观察到的} - \text{期望的}}\]</span> 实际上偏导是，单词<span class="math inline">\(o\)</span>的上下文词向量，减去，所有单词<span class="math inline">\(x\)</span>的上下文向量乘以x作为<span class="math inline">\(c\)</span>的上下文向量的概率。</p><h2 id="总体梯度计算">总体梯度计算</h2><p>在一个window里面，对中间词汇<span class="math inline">\(v_c\)</span>求了梯度， 然后再对各个上下文词汇<span class="math inline">\(u_o\)</span>求梯度。 然后更新这个window里面用到的参数。</p><p>比如句子<code>We like learning NLP</code>。设<span class="math inline">\(m=1\)</span>：</p><ul><li>中间词汇求梯度 <span class="math inline">\(v_{like}\)</span></li><li>上下文词汇求梯度 <span class="math inline">\(u_{we}\)</span> 和 <span class="math inline">\(u_{learning}\)</span></li><li>更新参数</li></ul><h2 id="梯度下降">梯度下降</h2><p>有了梯度之后，参数减去梯度，就可以朝着最小的方向走了。<a href="https://plmsmile.github.io/2017/08/20/ml-ng-notes/#梯度下降">机器学习梯度下降</a> <span class="math display">\[\theta^{new} = \theta^{old} - \alpha \frac{\partial}{\partial \theta^{old}} J(\theta),\quad \quad\theta^{new} = \theta^{old} - \alpha \Delta_{\theta} J(\theta)\]</span> <strong>随机梯度下降</strong></p><p>预料会有很多个window，因此每次不能更新所有的。只更新每个window的，对于window t： <span class="math display">\[\theta^{new} = \theta^{old} - \alpha \Delta_{\theta} J_t(\theta)\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/03-gradient-down.jpg" style="display:block; margin:auto" width="50%"></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/stanf/02word2vec/04-gradient-down-2.jpg" style="display:block; margin:auto" width="50%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;word-meaning&quot;&gt;Word meaning&lt;/h1&gt;
&lt;h2 id=&quot;词意&quot;&gt;词意&lt;/h2&gt;
&lt;p&gt;词的意思就是&lt;
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="深度学习" scheme="http://plmsmile.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="word2vec" scheme="http://plmsmile.github.io/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>subword-units</title>
    <link href="http://plmsmile.github.io/2017/10/19/subword-units/"/>
    <id>http://plmsmile.github.io/2017/10/19/subword-units/</id>
    <published>2017-10-19T14:16:07.000Z</published>
    <updated>2017-10-21T10:13:16.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a></p></blockquote><h1 id="背景">背景</h1><h2 id="摘要">摘要</h2><p>NMT处理的词汇表是定长的，但是实际翻译却是OOV(out of vocabulary)的。以前是把 新词汇加到词典里去。本文是提出一种subword单元的策略，会把稀有和未知词汇以subword units序列来进行编码，更简单更有效。</p><p>会介绍不同分词技术的适用性，包括简单的字符n元模型和基于字节对编码压缩算法的分词技术。也会以经验说明subword模型比传统back-off词典的方法好。</p><h2 id="简介">简介</h2><p>NMT模型的词汇一般是30000-5000，但是翻译却是open-vocabulary的问题。很多语言富有信息创造力，比如凝聚组合等等，翻译系统就需要一种低于word-level的机制。</p><p><strong>Word-level NMT的缺点</strong></p><p>对于word-level的NMT模型，翻译out-of-vocabulary的单词会回退到dictionary里面去查找。有下面几个缺点</p><ul><li>种技术在实际上使这种假设并不成立。比如源单词和目标单词并不是一对一的，你怎么找呢</li><li>不能够翻译或者产生未见单词</li><li>把unknown单词直接copy到目标句子中，对于人名有时候可以。但是有时候却需要改变形态或者直译。</li></ul><p><strong>Subword-level NMT</strong></p><p>我们的目标是建立open-vocabulary的翻译模型，不用针对稀有词汇去查字典。事实证明，subword模型效果比传统大词汇表方法更好、更精确。Subword神经网络模型可以从subword表达中学习到组合和直译等能力，也可以有效的产生不在训练数据集中的词汇。本文主要有下面两个贡献</p><ul><li>open-vocabulary的问题可以通过对稀有词汇使用subword units单元来编码解决</li><li>采用<strong>Byte pair encoding (BPE)</strong> 算法来进行分割。BPE通过一个固定大小的词汇表来表示开放词汇，这个词汇表里面的是变长的字符串序列。这是一种对于神经网络模型非常合适的词分割策略。</li></ul><h1 id="神经机器翻译">神经机器翻译</h1><p>NMT是使用的Bahdanau的Attention模型。Encoder是双向RNN，输入<span class="math inline">\(X=(x_1, x_2, \cdots, x_m)\)</span>，会把两个方向的隐状态串联起来得到annotation向量<span class="math inline">\(\mathbf x\)</span>。实际上是一个矩阵，对于单个<span class="math inline">\(x_j\)</span>来说，对应的注释向量是<span class="math inline">\(\mathbf{x}_j\)</span>。</p><p>Decoder是一个单向的RNN，预测<span class="math inline">\(Y=(y_1, y_2, \cdots, y_n)\)</span>。 预测<span class="math inline">\(y_i\)</span> 时，需要：</p><ul><li>当前的隐状态<span class="math inline">\(s_i\)</span></li><li>上一时刻的输出<span class="math inline">\(y_{i-1}\)</span>作为当前的输入<br></li><li>语义向量<span class="math inline">\(\mathbf c_i\)</span> 。语义向量是由所有的注释向量<span class="math inline">\(x_j\)</span> 加权求和得到的。权就是对齐概率<span class="math inline">\(\alpha_{ij}\)</span>。 即<span class="math inline">\(\mathbf c_i = \sum_{j=1} ^ m \alpha_{ij} \mathbf x_j\)</span></li></ul><p>详情请看<a href="https://plmsmile.github.io/2017/10/17/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/#系统总览">谷歌论文里面的介绍</a>或者Bahdanau的论文。</p><h1 id="subword-翻译">Subword 翻译</h1><p>下面词汇的翻译是透明的(transparent，明显的)</p><ul><li>命名实体。如果两个语言的字母表相同，可以直接copy到目标句子中去，也可以抄写音译直译等。</li><li>同源词和外来词。有着共同的起源，但是不同的语言表达形式不同，所以character-level翻译规则就可以了。</li><li>形态复杂的词语。包含多个语素的单词，可以通过单独翻译语素来翻译。</li></ul><p>总之，通过subword单元表示稀有词汇对于NMT来说可以学到transparent翻译，并且可以翻译和产生未见词汇。</p><h2 id="相关工作">相关工作</h2><p>对于SMT(Statistical Machine Translation)来说，翻译未见单词一直是研究的主题。</p><p>很多未见单词都是人名，如果两种语言的字母表一样，那么可以直接复制过去。如果不一样，那么就得音译过去。基于字符（character-based）的翻译是比较成功的。</p><p>形态上很复杂的单词往往需要分割，这里有很多的分割算法。基于短语的SMT的分割算法是比较保守的。而我们需要<strong>积极的细分</strong>，让网络可以处理open-vocabulary，而不是去求助于背字典。 怎么选择subword units要看具体的任务。</p><p>提出了很多这样的技术：生成基于字符或者基于语素的定长的连续的词向量。于此同时，word-based的方法并没有重大发现。现在的注意力机制还是基于word-level的。我们希望，<strong>注意力机制</strong>能从我们<strong>变长表达</strong>中收益：<strong>网络可以把注意力放在不同的subword units中。</strong> 这可以突破定长表达的信息传达瓶颈。</p><p>NMT减少词汇表可以大大节省时间和增加空间效率。我们也想要对一个句子<strong>更紧凑的表达</strong>。因为文本长度增加了，会减少效率，也会增加模型传递信息的距离。(hidden size？)</p><p>权衡词汇表大小和文本长度，可以用未分割单词列表，subword 单元表达的稀有词汇。作为一个代替，Byte pair encoding就是这样的一种分割算法，可以学到一个词汇表，同时对文本有很好的压缩率。</p><h2 id="byte-pair-encoding">Byte Pair Encoding</h2><p>Byte pair encoding是一种简单的数据压缩技术，它把句子中经常出现的字节pairs用一个没有出现的字节去替代。我们使用这种算法去分割单词，但我们合并字符或者字符序列。</p><p><strong>算法步骤</strong></p><p>算法步骤如下：</p><ul><li>初始化符号词表。用所有的字符加入到符号词表中。对所有单词的末尾加入特殊标记，如<code>-</code>。翻译后恢复原始的标记。</li><li>迭代对所有符号进行计数，找出次数最多的(A, B)，用AB代替。</li><li>每次合并，会产生一个新的符号，代表着n-gram字符</li><li>常见的n-grams字符(或者whole words)，最终会被合并到一个符号</li><li>最终符号词表大小=初始大小+合并操作次数。操作次数是算法唯一的超参数。</li></ul><p>不用考虑不在训练集里面的pair，为每个word根据出现频率设置权重。</p><p>和传统的压缩算法(哈夫曼编码)相比，我们的以subword 单元堆积的符号序列依然是<strong>可以解释的</strong>，<strong>网络也可以翻译和产生新的词汇</strong>（训练集没有见过的）。</p><p>下面是代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_raw_words</span><span class="params">(words, endtag=<span class="string">'-'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''把单词分割成最小的符号，并且加上结尾符号'''</span></span><br><span class="line">    vocabs = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word, count <span class="keyword">in</span> words.items():</span><br><span class="line">        <span class="comment"># 加上空格</span></span><br><span class="line">        word = re.sub(<span class="string">r'([a-zA-Z])'</span>, <span class="string">r' \1'</span>, word)</span><br><span class="line">        word += <span class="string">' '</span> + endtag</span><br><span class="line">        vocabs[word] = count</span><br><span class="line">    <span class="keyword">return</span> vocabs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_symbol_pairs</span><span class="params">(vocabs)</span>:</span></span><br><span class="line">    <span class="string">''' 获得词汇中所有的字符pair，连续长度为2，并统计出现次数</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocabs: 单词dict，(word, count)单词的出现次数。单词已经分割为最小的字符</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        pairs: ((符号1, 符号2), count)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#pairs = collections.defaultdict(int)</span></span><br><span class="line">    pairs = dict()</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocabs.items():</span><br><span class="line">        <span class="comment"># 单词里的符号</span></span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols) - <span class="number">1</span>):</span><br><span class="line">            p = (symbols[i], symbols[i + <span class="number">1</span>])</span><br><span class="line">            pairs[p] = pairs.get(p, <span class="number">0</span>) + freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_symbols</span><span class="params">(symbol_pair, vocabs)</span>:</span></span><br><span class="line">    <span class="string">'''把vocabs中的所有单词中的'a b'字符串用'ab'替换</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        symbol_pair: (a, b) 两个符号</span></span><br><span class="line"><span class="string">        vocabs: 用subword(symbol)表示的单词，(word, count)。其中word使用subword空格分割</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        vocabs_new: 替换'a b'为'ab'的新词汇表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    vocabs_new = &#123;&#125;</span><br><span class="line">    raw = <span class="string">' '</span>.join(symbol_pair)</span><br><span class="line">    merged = <span class="string">''</span>.join(symbol_pair)</span><br><span class="line">    <span class="comment"># 非字母和数字字符做转义</span></span><br><span class="line">    bigram =  re.escape(raw)</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    <span class="keyword">for</span> word, count <span class="keyword">in</span> vocabs.items():</span><br><span class="line">        word_new = p.sub(merged, word)</span><br><span class="line">        vocabs_new[word_new] = count</span><br><span class="line">    <span class="keyword">return</span> vocabs_new</span><br><span class="line"></span><br><span class="line">raw_words = &#123;<span class="string">"low"</span>:<span class="number">5</span>, <span class="string">"lower"</span>:<span class="number">2</span>, <span class="string">"newest"</span>:<span class="number">6</span>, <span class="string">"widest"</span>:<span class="number">3</span>&#125;</span><br><span class="line">vocabs = process_raw_words(raw_words)</span><br><span class="line"></span><br><span class="line">num_merges = <span class="number">10</span></span><br><span class="line"><span class="keyword">print</span> (vocabs)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line">    pairs = get_symbol_pairs(vocabs)</span><br><span class="line">    <span class="comment"># 选择出现频率最高的pair</span></span><br><span class="line">    symbol_pair = max(pairs, key=pairs.get)</span><br><span class="line">    vocabs = merge_symbols(symbol_pair, vocabs)</span><br><span class="line"><span class="keyword">print</span> (vocabs)</span><br></pre></td></tr></table></figure><p><strong>单独BPE</strong></p><p>为目标语言和原语言分别使用BPE去计算词典。从文本和词汇表大小来说更加紧凑，能保证每个subword单元在各自的训练数据上都有。同样的名字在不同的语言中可能切割的不一样，神经网络很难去学习subword units之间的映射。</p><p><strong>Joint BPE</strong></p><p>为目标语言和原语言一起使用BPE，即联合两种语言的词典去做BPE。提高了源语言和目标语言的分割一致性。训练中一般concat两种语言。</p><h1 id="评估">评估</h1><p>有两个重要问题</p><ul><li>subword units表达稀有词汇，是否真的对翻译有效果？</li><li>根据词汇表大小，文本长度，翻译质量，怎样分割才是最好的？</li></ul><p>我们的使用WMT2015的数据，使用BLEU来评判结果。英语-德语：</p><ul><li>420万句子对</li><li>1亿个token</li></ul><p>英语-俄罗斯语：</p><ul><li>260万句子对</li><li>5000万个token</li></ul><p>minibatch-size是80，每个epoch都会reshuffle训练数据。训练了7天，每12个小时存一次模型，取最后4个模型再单独训练。分别选择clip 梯度是5.0和1.0，1.0效果好一些。最终是融合了8个模型。</p><p>Beam search的大小是12，使用双语词典进行快速对齐，类似于对稀有词汇查找词典，也会用词典去加速训练。</p><h2 id="subword统计">Subword统计</h2><p>我们的目标是通过一个紧凑的固定大小的subword词典去代表一个open-vocabulary，并且能够有效的训练和解码。</p><p>一个简单的基准就是把单词分割成字符n-grams 。n的选择很重要，可以在序列长度(tokens)和词汇表大小(size)之间做一个权衡。序列的长度会增加许多，一个比较好得减少长度的方法就是使用k个最常见的未被分割的词列表。只有unigram(n=1，一元模型)表达才能真正实现open-vocabulary，但是实际上效果却并不好。Bigram效果好，但是不能产生测试集中的tokens。</p><p>BPE符合open-vocabulary的目标，并且合并操作可以应用于测试集，去发现未知符号的分割。与字符集模型的主要区别在于，BPE更紧凑的表示较短序列，注意力模型可以应对变长的单元。</p><table><thead><tr class="header"><th>分割方法</th><th>tokens</th><th>types</th><th>unk</th><th>merge次数</th></tr></thead><tbody><tr class="odd"><td>BPE</td><td>112 m</td><td>63000</td><td>0</td><td>59500</td></tr><tr class="even"><td>BPE(joint)</td><td>111 m</td><td>82000</td><td>32</td><td>89500</td></tr></tbody></table><p>实际上，NMT词汇表中，并不会包含不常见的subword单元，因为里面有很多噪声。</p><table><thead><tr class="header"><th>name</th><th>seg</th><th>shotlist</th><th>s-v</th><th>t-v</th><th>S-BLEU</th><th>BLEU</th><th>CHAR-F3</th><th>CHAR-F3</th><th>F1</th><th>F1</th><th>F1</th></tr></thead><tbody><tr class="odd"><td>BPE-60k</td><td>BPE</td><td>无</td><td>60000</td><td>60000</td><td>21.5</td><td>24.5</td><td>52.0</td><td>53.9</td><td>58.4</td><td>40.9</td><td>29.3</td></tr><tr class="even"><td>BPE-J60k</td><td>BPE(joint)</td><td>无</td><td>90000</td><td>90000</td><td>22.8</td><td>24.7</td><td>51.7</td><td>54.1</td><td>58.5</td><td>41.8</td><td>33.6</td></tr></tbody></table><h2 id="翻译评估">翻译评估</h2><p>所有的subword系统都不会去查字典。使用UNK表示模型词典以外的单词，OOV表示训练集里面没有的单词。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.07909.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Neural Machine Translation of Rare Words with
      
    
    </summary>
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NMT" scheme="http://plmsmile.github.io/tags/NMT/"/>
    
      <category term="subword" scheme="http://plmsmile.github.io/tags/subword/"/>
    
  </entry>
  
  <entry>
    <title>Wordpiece模型</title>
    <link href="http://plmsmile.github.io/2017/10/19/26_wordpieacemodel/"/>
    <id>http://plmsmile.github.io/2017/10/19/26_wordpieacemodel/</id>
    <published>2017-10-19T06:41:00.000Z</published>
    <updated>2017-10-19T14:15:36.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/37842.pdf" target="_blank" rel="noopener">Japanese and Korean Voice Search</a> 看了半天才发现不稳啊。</p></blockquote><h1 id="背景知识">背景知识</h1><h2 id="摘要">摘要</h2><p>这篇文章主要讲了构建基于日语和法语的语音搜索系统遇到的困难，并且提出了一些解决的方法。主要是下面几个方面：</p><ul><li>处理无限词汇表的技术</li><li>在语言模型和词典的书面语中，完全建模并且避免系统复杂度</li><li>如何去构建词典、语言和声学模型</li></ul><p>展示了由于模糊不清，多个script语言的打分结果的困难性。这些语言语音搜索的发展，大大简化了构建一门新的语言的语音搜索系统的最初的处理过程，这些很多都成为了语言搜索国际化的默认过程。</p><h2 id="简介">简介</h2><p>语音搜索通过手机就可以访问到互联网，这对于一些不好输入字符的语言来说，非常有用。尽管从基础技术来讲，语音识别的技术是在不同的语言之间是非常相似的，但是许多亚洲语言面临的问题，如果只是用传统的英语的方法去对待，这根本很难解决嘛。许多亚洲语言都有非常大的字符库。这让发音词典就很复杂。在解码的时候，由于很多同音异义词汇，解码也会很复杂。基本字符集里面的很多字符都会以多种形式存在，还要数字也会有多种形式，在某些情况下，这都需要适当的标准化。</p><p>很多亚洲语言句子中没有空格去分割单词。需要使用<code>segmenters</code>去产生一些<code>词单元</code>。 这些词单元会在词典和语言模型中使用，词单元之间可能需要添加或者删除空白字符。我们开发了一个纯数据驱动的sementers，可以使用任何语言，不需要修改。</p><p>还有就是如何去处理英文中的许多词汇，比如URL、数字、日期、姓名、邮件、缩写词汇、标点符号和其它特殊词汇等等。</p><h2 id="语音数据收集">语音数据收集</h2><p>公告开放的数据集很难用作商用，有很多限制，所以自己收集数据集。通过手机，从不同的地区、年龄、方言等等，收集数据。一般是尽可能使用这些原始的数据并且建模，而不是转化为书面的数据或者有利于英语的数据。</p><h1 id="分词和词库">分词和词库</h1><p>提出一种<code>WordPieceModel</code>去解决OOV(out-of-vocabulary)的问题。WordPieaceModel通过一种贪心算法，自动地、增量地从大量文本中学得单词单元（word units），一般数量是200k。算法可以，不关注语义，而去最大化训练数据语言模型的可能性，这也是解码过程中的度量标准。该算法可以有效地自动学习词库。</p><h2 id="wordpiecemodel算法步骤">WordPieceModel算法步骤</h2><p><strong>1 初始化词库</strong></p><p>给词库添加基本的所有的unicode字符和ascii字符。日语是22000，韩语是11000。</p><p><strong>2 建立模型</strong></p><p>基于训练数据，建立模型，使用初始化好的词库。</p><p><strong>3 生成新单元</strong></p><p>从词库中选择两个词单元组成新的词单元，加入到词库中。组成的新词要使模型的似然函数likelyhood最大。</p><p><strong>4 继续加或者停止</strong></p><p>如果达到词库数量的上限，或者似然函数增加很小，那么就停止，否则就继续2步，继续合并添加。</p><h2 id="算法优化">算法优化</h2><p>你也发现了，计算所有可能的Pair这样会非常非常耗费时间。如果当前词库数量是<span class="math inline">\(K\)</span>，那么每次迭代计算的复杂度是<span class="math inline">\(O(K^2)\)</span> 。有下面3个步骤可以进行优化</p><ul><li>选择组合新的单元时，只测试训练数据中有的单元。</li><li>只测试有很大机会成为最好的Pair，例如high priors</li><li>把一些不会影响到彼此的group pairs组合到一起，作为一个单一的迭代过程</li><li>only modify the language model counts for the affected entries （不懂什么意思）</li></ul><p>使用这些加速算法，我们可以在一个机器上，几个小时以内，从频率加权查询列表中，构建一个200k的词库。</p><p>得到wordpiece词库之后，可以用来语言建模，做词典和解码。分割算法，构建了以基础字符开始的Pairs的逆二叉树。本身已经不需要动态规划或者其他的搜索方法。因此在计算上非常有效。分开基本的字符，基于树从上到下，会在线性时间给出一个确定的分割信息，线性时间取决于句子的长度。大约只有4%的单词具有多个发音。如果添加太多的发音会影响性能，可能是因为在训练和解码时对齐过程期间的可能数太多了</p><h2 id="继续说明">继续说明</h2><p>一般是句子没有空格的，但是有的时候却有空格，比如韩文，搜索关键字。线上系统没有办法去把这些有空格的word pieces组合在一起。这对于常见的词汇和短查询是没有影响的，因为它们已经组合成一个完整的word unit。但是对于一些例如空格出现在不该出现的地方等不常见的查询，就很烦恼了。</p><p>在解码的时候，加空格效率更高，采用下面的技术：</p><p>1 原始语言模型数据被用来&quot;as written&quot;，表示一些有空格一些没有空格。</p><p>2 WPM模型分割LM数据时，每个单元在前面或者后面遇到一个空格，那么就添加一个空格标记。单元有4种情况：两边都有空格，左边有，右边有，两边都没有。使用下划线标记</p><p>3 基于这个新词库构建LM和词典</p><p>4 解码时，根据模型会选择一个最佳路径，之前在哪些地方放了空格或者没有。为了输出显示，需要把空格全部移除。有3种情况，移除所有空格；移除两个空格用一个空格表示；移除一个空格。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/37842.pdf&quot; target=&quot;_blank&quot; rel=
      
    
    </summary>
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="WPM" scheme="http://plmsmile.github.io/tags/WPM/"/>
    
      <category term="语音搜索" scheme="http://plmsmile.github.io/tags/%E8%AF%AD%E9%9F%B3%E6%90%9C%E7%B4%A2/"/>
    
      <category term="语音识别" scheme="http://plmsmile.github.io/tags/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络</title>
    <link href="http://plmsmile.github.io/2017/10/18/rnn/"/>
    <id>http://plmsmile.github.io/2017/10/18/rnn/</id>
    <published>2017-10-18T12:35:29.000Z</published>
    <updated>2018-03-28T09:24:15.050Z</updated>
    
    <content type="html"><![CDATA[<p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="lstm经典描述">LSTM经典描述</h1><h2 id="经典rnn模型">经典RNN模型</h2><p><strong>模型</strong></p><p>人类在思考的时候，会从上下文、从过去推断出现在的结果。传统的神经网络无法记住过去的历史信息。</p><p>循环神经网络是指随着时间推移，重复发生的结构。它可以记住之前发生的事情，并且推断出后面发生的事情。用于处理时间序列很好。所有的神经元<strong>共享权值</strong>。如下图所示。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/10_RNN-unrolled.png" style="display:block; margin:auto" width="60%"></p><p><strong>记住短期信息</strong></p><p>比如预测“天空中有__”，如果过去的信息“鸟”离当前位置比较近，则RNN可以利用这个信息预测出下一个词为“鸟”</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/11_RNN-shorttermdepdencies.png" style="display:block; margin:auto" width="50%"></p><p><strong>不能长期依赖</strong></p><p>如果需要的历史信息距离当前位置很远，则RNN无法学习到过去的信息。这就是<strong>不能长期依赖</strong>的问题。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/12_RNN-longtermdependencies.png" style="display:block; margin:auto" width="50%"></p><h2 id="lstm总览与核心结构">LSTM总览与核心结构</h2><ul><li>LSTM可以记住一些记忆，捕获长依赖问题</li><li>也可以让ERROR根据输入，依照不同强度流动</li></ul><p>见后面GRU解决梯度消失</p><p><strong>总览</strong></p><p>所有的RNN有着重复的结构，如下图，比如内部是一个简单的<code>tanh</code> 层。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/08_LSTM3-SimpleRNN.png" style="display:block; margin:auto" width="50%"></p><p>LSTM也是一样的，只不过内部复杂一些。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/09_LSTM3-chain.png" style="display:block; margin:auto" width="60%"></p><p><strong>单元状态</strong></p><p><strong>单元状态</strong>像一个传送带，通过整个链向下运行，只有一些小的<strong>线性作用</strong>。<strong>信息就沿着箭头方向流动</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/13_LSTM3-C-line.png" style="display:block; margin:auto" width="50%"></p><p><strong>LSTM的门结构</strong></p><p>LSTM的<strong>门结构</strong> 可以添加或者删除单元状态的信息，<strong>去有选择地让信息通过</strong>。它由<strong>sigmoid网络层</strong> 和 <strong>点乘操作</strong>组成。输出属于<span class="math inline">\([0, 1]\)</span>之间，代表着信息通过的比例。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/14_LSTM3-gate.png" style="display:block; margin:auto" width="15%"></p><h2 id="lstm细节解剖">LSTM细节解剖</h2><p>一些符号说明，都是<span class="math inline">\(t\)</span>时刻的信息 ：</p><ul><li><span class="math inline">\(C_{t-1}\)</span> : 的单元状态</li><li><span class="math inline">\(h_{t}\)</span> : 隐状态信息 （也作单个神经元的输出信息）</li><li><span class="math inline">\(x_t\)</span> : 输入信息</li><li><span class="math inline">\(o_t\)</span> ：输出信息 （输出特别的信息）</li></ul><p><strong>1 遗忘旧信息</strong></p><p>对于<span class="math inline">\(C_{t-1}\)</span>中的每一个数字， <span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(x_t\)</span>会输出0-1之间的数来决定遗忘<span class="math inline">\(C_{t-1}\)</span>中的多少信息。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/15_LSTM3-focus-f.png" style="display:block; margin:auto" width="60%"></p><p><strong>2 生成候选状态和它的更新比例</strong></p><p>生成新的状态：<strong>tanh</strong>层创建<strong>新的候选状态<span class="math inline">\(\hat{C}_t\)</span></strong></p><p>输入门：决定新的状态<strong>哪些信息会被更新<span class="math inline">\(i_t\)</span></strong>，即候选状态<span class="math inline">\(\hat{C}_t\)</span>的保留比例。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/16_LSTM3-focus-i.png" style="display:block; margin:auto" width="60%"></p><p><strong>3 新旧状态合并更新</strong></p><p>生成新状态<span class="math inline">\(C_t\)</span>：<strong>旧状态<span class="math inline">\(C_{t-1}\)</span> + 候选状态<span class="math inline">\(\hat{C}_t\)</span></strong>。</p><p>旧状态<span class="math inline">\(C_{t-1}\)</span><strong>遗忘不需要的</strong>， 候选状态<span class="math inline">\(\hat{C}_{t-1}\)</span><strong>保留需要更新的</strong>，都是以乘积比例形式去遗忘或者更新。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/17_LSTM3-focus-C.png" style="display:block; margin:auto" width="60%"></p><p><strong>4 输出特别的值</strong></p><p>sigmoid：决定单元状态<span class="math inline">\(C_t\)</span>的哪些信息要输出。</p><p>tanh: 把单元状态<span class="math inline">\(C_t\)</span>的值变到<span class="math inline">\([-1, 1]\)</span>之间。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/18_LSTM3-focus-o.png" style="display:block; margin:auto" width="60%"></p><h2 id="lstm总结">LSTM总结</h2><p>核心结构如下图所示</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/23_LSTM_raw.png" style="display:block; margin:auto" width="50%"></p><p>要忘掉部分旧信息，旧信息<span class="math inline">\(C_{t-1}\)</span>的遗忘比例<span class="math inline">\(f_t\)</span><br><span class="math display">\[f_t = \sigma (W_f \cdot [h_{t-1}, x_t] + b_f)\]</span> 新的信息来了，生成一个新的候选<span class="math inline">\(\hat{C}_t\)</span> <span class="math display">\[\hat{C}_t =  \tanh (W_C \cdot [h_{t-1}, x_t] + b_C)\]</span> 新信息留多少呢，新候选<span class="math inline">\(\hat C_t\)</span>的保留比例<span class="math inline">\(i_t\)</span> <span class="math display">\[i_t = \sigma (W_i \cdot [h_{t-1}, x_t] + b_i)\]</span> 合并旧信息和新信息，生成新的状态信息<span class="math inline">\(C_t\)</span> <span class="math display">\[C_t = f_t * C_{t-1} + i_t * \hat C_t\]</span> 输出多少呢，单元状态<span class="math inline">\(C_t\)</span>的输出比例<span class="math inline">\(o_t\)</span> <span class="math display">\[o_t = \sigma (W_o \cdot [h_{t-1}, x_t] + b_o)\]</span> 把<span class="math inline">\(C_t\)</span>化到<span class="math inline">\([-1, 1]\)</span>再根据比例输出 <span class="math display">\[h_t  = o_t * \tanh(C_t)\]</span></p><h1 id="图文简介描述lstm">图文简介描述LSTM</h1><h2 id="总体架构">总体架构</h2><p>单元架构</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/07_LSTM.png" style="display:block; margin:auto" width="70%"></p><p>流水线架构</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/24_LSTM_colored_line.png" style="display:block; margin:auto" width="100%"></p><h2 id="数据流动">数据流动</h2><p>圆圈叉叉代表着遗忘<span class="math inline">\(C_{t-1}\)</span>的信息。乘以向量来实现，向量各个值在<span class="math inline">\([0, 1]\)</span>之间。 靠近0就代表着遗忘很多，靠近1就代表着保留很多。</p><p>框框加号代表着数据的合并。旧信息<span class="math inline">\(C_{t-1}\)</span>和新候选信息<span class="math inline">\(\hat C_t\)</span>的合并。 合并之后就得到新信息<span class="math inline">\(C_t\)</span>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/25_LSTM_colored_cline.png" style="display:block; margin:auto" width="60%"></p><h2 id="遗忘门">遗忘门</h2><p>上一个LSTM的输出<span class="math inline">\(h_{t-1}\)</span> 和 当前的输入<span class="math inline">\(x_t\)</span>，一起作为遗忘门的输入。 0是偏置<span class="math inline">\(b_0\)</span>， 一起做个合并，再经过sigmoid生成遗忘权值<span class="math inline">\(f_t\)</span>信息， 去遗忘<span class="math inline">\(C_{t-1}\)</span>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/26_LSTM_colored_f.png" style="display:block; margin:auto" width="60%"></p><p><strong>新信息门</strong></p><p>新信息门决定着新信息对旧信息的影响力。和遗忘门一样<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(x_t\)</span>作为输入。</p><p>sigmoid：生成新信息的保留比例。tanh：生成新的信息。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/27_LSTM_colored_i.png" style="display:block; margin:auto" width="60%"></p><h2 id="新旧信息合并">新旧信息合并</h2><p>旧信息<span class="math inline">\(C_{t-1}\)</span>和新信息<span class="math inline">\(\hat{C}_t\)</span>合并，当然分别先过遗忘阀门和更新阀门。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/28_LSTM_colored_merge.png" style="display:block; margin:auto" width="60%"></p><h2 id="输出特别的值">输出特别的值</h2><p>把新生成的状态信息<span class="math inline">\(C_t\)</span>使用tanh变成<span class="math inline">\((-1, 1)\)</span>之间，然后经过输出阀门进行输出。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/29_LSTM_colored_output.png" style="display:block; margin:auto" width="60%"></p><h1 id="lstm变体">LSTM变体</h1><h2 id="观察口连接">观察口连接</h2><p>传统LSTM阀门值比例的计算，即更新、遗忘、输出的比例只和<span class="math inline">\(h_{t-1}, x_t\)</span>有关。</p><p>观察口连接，把观察到的单元状态也连接sigmoid上，来计算。即遗忘、更新比例和<span class="math inline">\(C_{t-1}, h_{t-1}, x_t\)</span>有关，输出的比例和<span class="math inline">\(C_t, h_{t-1}, x_t\)</span>有关。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/30_LSTM3-var-peepholes.png" style="display:block; margin:auto" width="70%"></p><h2 id="组队遗忘">组队遗忘</h2><p>如下图所示，计算好<span class="math inline">\(C_{t-1}\)</span>的遗忘概率<span class="math inline">\(i_t\)</span>后，就不再单独计算新候选<span class="math inline">\(\hat C_t\)</span>的保留概率<span class="math inline">\(i_t\)</span>。而是直接由1减去遗忘概率得到更新概率。即<span class="math inline">\(i_t = 1 - f_t\)</span>，再去更新。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/31_LSTM3-var-tied.png" style="display:block; margin:auto" width="70%"></p><h2 id="gru">GRU</h2><p>LSTM有隐状态<span class="math inline">\(h_t\)</span>和输出状态<span class="math inline">\(o_t\)</span>，而GRU只有<span class="math inline">\(h_t\)</span>，即GRU的隐状态和输出状态是一样的，都用<span class="math inline">\(h_t\)</span>表示。</p><p><code>更新门</code><span class="math inline">\(z_t\)</span>负责<strong>候选隐层<span class="math inline">\(\hat h_t\)</span>保留的比例</strong>， <span class="math inline">\(1-z_t\)</span>负责遗忘旧状态信息<span class="math inline">\(h_{t-1}\)</span>的比例<br><span class="math display">\[z_t = \sigma (W_z \cdot [h_{t-1}, x_t])\]</span> 候选隐藏层<span class="math inline">\(\hat h_t\)</span>的计算由<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(x_t\)</span>一起计算得到。所以计算<span class="math inline">\(\hat h_t\)</span>之前，要先计算<span class="math inline">\(h_{t-1}\)</span>的重置比例。</p><p><code>重置门</code><span class="math inline">\(r_t\)</span>负责<strong><span class="math inline">\(h_{t-1}\)</span>对</strong>于生成<strong>新的候选<span class="math inline">\(\hat h_t\)</span>的作用比例</strong> <span class="math display">\[r_t = \sigma (W_r \cdot [h_{t-1}, x_t])\]</span> <code>新记忆</code><span class="math inline">\(\hat h_t\)</span>的计算 <span class="math display">\[\hat h_t = \tanh (W \cdot [r_t * h_{t-1}, x_t])\]</span> <code>最终记忆</code><span class="math inline">\(h_t\)</span>由<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(\hat h_t\)</span>计算得到，分别的保留比例是<span class="math inline">\(1-z_t\)</span>和<span class="math inline">\(z_t\)</span> <span class="math display">\[h_t = (1 - z_t) * h_{t-1} + z_t * \hat h_t\]</span></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/32_LSTM3-var-GRU.png" style="display:block; margin:auto" width="70%"></p><p><code>更新门</code> <span class="math inline">\(z_t\)</span>：过去的信息有多重要。 <span class="math inline">\(z=1\)</span>， 则过去信息非常重要，完全保留下来</p><p><code>重置门</code><span class="math inline">\(r_t\)</span>： 旧记忆对新记忆的贡献程度。<span class="math inline">\(r=0\)</span>， 则当前新记忆和旧记忆不想关。</p><h1 id="rnn梯度问题">RNN梯度问题</h1><h2 id="rnn梯度推导">RNN梯度推导</h2><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/rnn/rnn-.png" style="display:block; margin:auto" width="70%"></p><p>简单点 <span class="math display">\[\begin {align}&amp; h_t = Wh_{t-1} + W^{(hx)} x_t \\ \\&amp; \hat y_t =W^{(s)} f(h_t)  \\ \\\end{align}\]</span> 总的误差是之前每个时刻的误差之和 <span class="math display">\[\frac{\partial E}{\partial W} = \sum_{t=1}^T \frac{\partial E_t}{\partial W}\]</span> 每一时刻的误差又是之前每个时刻的误差之和，应用链式法则 <span class="math display">\[\frac{\partial E_t}{\partial W} = \frac{\partial E_t}{\partial y_t} \frac{\partial y_t}{\partial h_t}\sum_{k=1}^t \frac{\partial h_t}{\partial h_k} \frac{\partial h_k}{\partial W}\]</span></p><p><span class="math display">\[\frac{\partial E_t}{\partial W} = \sum_{k=1}^t \frac{\partial E_t}{\partial y_t} \frac{\partial y_t}{\partial h_t}\color{blue} {\frac{\partial h_t}{\partial h_k}}\frac{\partial h_k}{\partial W}\]</span></p><p><span class="math display">\[\frac{\partial E_t}{\partial W} = \sum_{k=1}^t \frac{\partial E_k}{\partial y_k} \frac{\partial y_k}{\partial h_k}\frac{\partial h_k}{\partial h_{k-1}} \frac{\partial h_{k-1}}{\partial W}\]</span></p><p>而<span class="math inline">\(\frac{\partial h_t}{\partial h_k}\)</span>会变得非常大或者非常小！！ <span class="math display">\[\frac{\partial h_t}{\partial h_k} = \prod_{j=k+1}^t \frac{\partial h_j}{\partial h_{j-1}} =  \prod_{j=k+1}^t W^T \times \rm{diag}[f^{\prime}(j_{j-1})]\]</span> 而导数矩阵<code>雅克比矩阵</code> <span class="math display">\[\frac{\partial h_j}{\partial h_{j-1}} = [ \frac{\partial h_{j}}{\partial h_{j-1,1}}, \cdots , \frac{\partial h_{j}}{\partial h_{j-1,d_h}}]=\begin{bmatrix} \frac{\partial h_{j,1}}{\partial h_{j-1,1}} &amp; \cdots &amp; \frac{\partial h_{j,1}}{\partial h_{j-1,d_h}} \\ \vdots &amp; \ddots &amp; \vdots \\  \frac{\partial h_{j,d_h}}{\partial h_{j-1,1}} &amp; \cdots &amp; \frac{\partial h_{j,d_h}}{\partial h_{j-1,d_h}} \\\end{bmatrix}\]</span> 合并起来，得到最终的 <span class="math display">\[\frac{\partial E}{\partial W} = \sum_{t=1}^T\sum_{k=1}^t \frac{\partial E_t}{\partial y_t} \frac{\partial y_t}{\partial h_t}(\prod_{j=k+1}^t \frac{\partial h_j}{\partial h_{j-1}})\frac{\partial h_k}{\partial W}\]</span> 两个不等式 <span class="math display">\[\| \frac{\partial h_j}{\partial h_{j-1}}\| \le \| W^T\| \cdot \|\rm{diag}[f^{\prime}(h_{j-1})] \| \le \beta_W \beta_h\]</span> 所以有，会变得非常大或者非常小。会产生梯度消失或者梯度爆炸问题。 <span class="math display">\[\| \frac{\partial h_t}{\partial h_k} \| =\| \prod_{j=k+1}^t \frac{\partial h_j}{\partial h_{j-1}}  \| \le \color{blue}{ (\beta_W \beta_h)^{t-k}}\]</span> <code>梯度</code> 是过去对未来影响力的一个度量方法。如果梯度消失了不确定<span class="math inline">\(t\)</span>和<span class="math inline">\(k\)</span>之间是否有关系，或者是因为参数错误 。</p><h2 id="解决梯度爆炸">解决梯度爆炸</h2><p>原始梯度 <span class="math display">\[\mathbf {\hat g} =\frac{\partial E}{\partial W} \\ \\ \\\]</span> 如果<span class="math inline">\(\mathbf {\hat g} &gt; 阈值\)</span>， 则更新 <span class="math display">\[\mathbf {\hat g} = \frac{\text{threshold}}{\|\mathbf {\hat g}\|} \mathbf {\hat g}\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/rnn/clip-grads.png" style="display:block; margin:auto" width="60%"></p><h2 id="gru解决梯度消失">GRU解决梯度消失</h2><ul><li>LSTM可以记住一些记忆，捕获长依赖问题</li><li>也可以让ERROR根据输入，依照不同强度流动</li></ul><p>RNN的前向和反向传播，都会经过每一个节点</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/rnn/rnn-direct.png" style="display:block; margin:auto" width="60%"></p><p><strong>GRU可以自动地去创建一些短连接，也可以自动地删除一些不必要的连接</strong>。（门的功能）</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/rnn/gru-shortcut-connection.png" style="display:block; margin:auto" width="60%"></p><p>RNN会读取之前所有信息，并且更新所有信息。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/rnn/rnn-whole-read-update.png" style="display:block; margin:auto" width="60%"></p><p>GRU</p><ul><li>选择可读部分，读取</li><li>选择可写部分，更新</li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/rnn/gru-select-read-update.png" style="display:block; margin:auto" width="60%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;lstm经典描述&quot;&gt;LSTM经典描述&lt;/h1&gt;
&lt;h2 id=&quot;经典rnn模型&quot;&gt;经典RNN模型&lt;/h2&gt;
&lt;p&gt;&lt;stro
      
    
    </summary>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="RNN" scheme="http://plmsmile.github.io/tags/RNN/"/>
    
      <category term="LSTM" scheme="http://plmsmile.github.io/tags/LSTM/"/>
    
      <category term="GRU" scheme="http://plmsmile.github.io/tags/GRU/"/>
    
  </entry>
  
  <entry>
    <title>谷歌翻译论文笔记</title>
    <link href="http://plmsmile.github.io/2017/10/17/25_%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://plmsmile.github.io/2017/10/17/25_谷歌翻译论文笔记/</id>
    <published>2017-10-17T05:25:38.000Z</published>
    <updated>2017-10-19T06:03:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">谷歌神经机器翻译系统</a></p></blockquote><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="简介">简介</h1><p><code>神经机器翻译</code>是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点</p><ul><li>训练和翻译都太慢了，花费代价很大</li><li>缺乏鲁棒性，特别是输入句子包含生僻词汇</li><li>精确度和速度也不行</li></ul><h2 id="传统nmt缺点">传统NMT缺点</h2><p>神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。</p><p>NMT避开了传统基于短语的翻译模型的很多缺点。但是，在实际中，NMT的准确度要比基于短语的翻译模型更差些。</p><p>NMT有3个主要的缺点：训练和推理速度太慢，不能有效处理稀有词汇，有时不能完全翻译原句子。</p><p><strong>训练和推理速度太慢</strong></p><p>训练大数据集，需要大量时间和资源；反馈太慢周期太长。加了一个小技巧，看结果要等很长时间。推理翻译的时候，要使用大量的参数去计算，也很慢。</p><p><strong>不能有效处理稀有词汇</strong></p><p>有两个方法去复制稀有单词：</p><ul><li>模仿传统对齐模型去训练1个<code>copy model</code></li><li>使用注意力机制去复制</li></ul><p>但是效果都不是很好，都不可靠，不同语言的对齐效果差；在网络很深的时候，注意力机制的对齐向量也不稳定。而且，简单的复制过去也不是最好的办法，比如需要直译的时候。</p><p><strong>不能完整翻译整个句子</strong></p><p>不能覆盖整个输入句子的内容，然后会导致一些奇怪的翻译结果。</p><h2 id="gnmt的模型优点">GNMT的模型优点</h2><p>采用的模型：<strong>深层LSTM</strong> 、<strong>Encoder8层</strong>、<strong>Decoder8层</strong> 。<a href="https://plmsmile.github.io/2017/10/18/rnn/">我的LSTM笔记</a>。</p><p>各层之间使用<strong>残差连接</strong>促进梯度流，顶层Enocder到底层Decoder使用<strong>注意力连接</strong>，提高并行性。</p><p>进行翻译推断的时候，使用低精度算法，去加速翻译。</p><p>处理<strong>稀有词汇</strong>：使用<strong>sub-word单元</strong>，也称作wordpieces方法。把单词划分到有限的<strong>sub-word</strong> (wordpieces)单元集合，输入输出都这样。sub-word结合了字符分割模型的弹性和单词分割模型的效率。</p><p><code>Beam Search</code> 使用<strong>长度规范化</strong>和<strong>覆盖惩罚</strong>。覆盖惩罚就是说，希望，翻译的结果句子，尽量多地包含输入句子中的所有词汇。</p><p>使用强化学习去优化模型， 优化翻译的<code>BLEU</code> 分数。</p><h2 id="先进技术">先进技术</h2><p>有很多先进的技术来提高NMT，下面这些都有论文的。</p><ul><li>利用attention去处理稀有词汇</li><li>建立翻译覆盖的机制</li><li>多任务和半监督训练，去合并使用更多数据</li><li>字符分割的encoder和decoder</li><li>使用subword单元处理稀疏的输出</li></ul><h1 id="系统架构">系统架构</h1><h2 id="系统总览">系统总览</h2><p><strong>架构</strong></p><p>有3个模块：Encoder，Decoder，Attention。</p><p>Encdoer：把句子转换成一系列的向量，每一个向量代表一个输入词汇（符号）。</p><p>Decoder：根据这些向量，每一时刻会生成一个目标词汇，直到EOS。</p><p>Attention：连接Encoder和Decoder，在解码的过程中，可以让Decoder有权重的有选择的关注输入句子的部分区域。</p><p><strong>符号说明</strong></p><ul><li>加粗小写代表<strong>向量</strong>，如<span class="math inline">\(\boldsymbol {v, o_i}\)</span></li><li>加粗大写，<strong>矩阵</strong>，如<span class="math inline">\(\boldsymbol {U, W}\)</span> 和<span class="math inline">\(\mathbf{U, W}\)</span></li><li>手写体，<strong>集合</strong>，如<span class="math inline">\(\mathcal{V, F}\)</span></li><li>大写字母，<strong>句子</strong>，如<span class="math inline">\(X, Y\)</span></li><li>小写字母，<strong>单个符号</strong>，如<span class="math inline">\(x_1, x_2\)</span></li></ul><p><strong>Encoder</strong></p><p>输入句子和目标句子组成一个Pair <span class="math inline">\((X, Y)\)</span>，其中输入句子<span class="math inline">\(X = x_1, x_2, \cdots, x_M\)</span> ，<span class="math inline">\(M\)</span> 个单词，翻译的输出目标句子<span class="math inline">\(Y = y_1, y_2, \cdots, y_N\)</span> ，有<span class="math inline">\(N\)</span>个单词。</p><p>Encoder其实就是一个转换函数，得到<span class="math inline">\(M\)</span>个长度固定的向量，也就是其中Encoder对各个<span class="math inline">\(x_i\)</span>的<strong>编码向量 <span class="math inline">\(\mathbf{x_i}\)</span> </strong>： <span class="math display">\[\mathbf {x_1, x_2, \cdots, x_M} = \mathit{EncoderRNN} (x_1, x_2, \cdots, x_n)\]</span> 使用链式条件概率可得到翻译概率<span class="math inline">\(\color{blue} {P (Y \mid X)}\)</span> ，其中<span class="math inline">\(y_0\)</span>是起始符号<span class="math inline">\(SOS\)</span> 。 <span class="math display">\[\begin{align}P(Y \mid X) &amp; = P(Y \mid \mathbf{x_1, x_2, \cdots, x_M}) \\&amp; =\prod_{i=1}^N P(y_i \mid y_0, \cdots, y_{i-1}; \mathbf{x_1, x_2, \cdots, x_M}) \\\end{align}\]</span> <strong>Decoder</strong></p><p>在翻译<span class="math inline">\(y_i\)</span>的时候， 利用Encoder得到的编码向量<span class="math inline">\(\mathbf{x_i}\)</span> 和 <span class="math inline">\(y_0 \sim y_{i-1}\)</span> 来进行计算概率翻译 <span class="math display">\[P(y_i \mid y_0, \cdots, y_{i-1}; \mathbf{x_1, x_2, \cdots, x_M})\]</span> Decoder是由<strong>RNN+Softmax</strong>构成的。会得到一个<strong>隐状态<span class="math inline">\(\mathbf{y_i}\)</span> 向量</strong>，有2个作用：</p><ul><li>作为下一个RNN的输入</li><li><span class="math inline">\(\mathbf{y_i}\)</span>经过softmax得到概率分布， 选出<span class="math inline">\(y_i\)</span> 输出符号</li></ul><p><strong>Attention</strong></p><p>在之前的文章里有介绍<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/">论文</a> 和 <a href="https://plmsmile.github.io/2017/10/10/attention-model/">通俗理解</a>，其实就是<strong>影响力模型</strong>。原句子的各个单词对翻译当前单词分别有多少的影响力，也叫作<strong>对齐概率</strong>吧。使用decoder-RNN的输出<span class="math inline">\(\mathbf{y_{t-1}}\)</span> 向量作为时刻<span class="math inline">\(t\)</span>的输入。</p><p>时刻<span class="math inline">\(t\)</span>，给定<span class="math inline">\(\mathbf{y_{t-1}}\)</span></p><p>有3个符号定义：</p><ul><li><span class="math inline">\(s_i\)</span> ： <strong><span class="math inline">\(y_t\)</span>与<span class="math inline">\(x_i\)</span>的得分</strong>，在luong论文里面有3种计算方式，分别是dot, general和concat。</li><li><span class="math inline">\(p_i\)</span> ：<strong><span class="math inline">\(y_t\)</span>与<span class="math inline">\(x_i\)</span>的对齐概率</strong>，<span class="math inline">\((p_1, p_2, \cdots, p_M)\)</span> 联合起来就是<span class="math inline">\(y_t\)</span>与<span class="math inline">\(X\)</span>的<strong>对齐向量</strong>。其实就是对得分softmax。</li><li><span class="math inline">\(\mathbf{a_t}\)</span> ：<strong>带注意力的语义向量</strong>。对于所有的<span class="math inline">\(x_i\)</span>，使用<span class="math inline">\(y_t\)</span>与它的对齐概率<span class="math inline">\(p_i\)</span>乘以本身的编码向量<span class="math inline">\(\mathbf{x_i}\)</span>，得到<span class="math inline">\(x_i\)</span>传达的语义，再对所有的语义求和，即得到总体的带有注意力的语义。</li></ul><p>整体详细计算的流程，如下面的公式： <span class="math display">\[\begin {align}&amp;  s_i = \mathit{AttentionFunction} (\mathbf{y_{t-1}}, \mathbf{x_i}), \quad i \in [1, M] \\&amp; p_i = \frac {\exp (s_i)}{\sum_{j=1}^M \exp(s_j)}  \quad i \in [1, M] \\&amp;  \mathbf{a_t} = \sum_{i=1}^M p_i \cdot \mathbf{x_i} \quad \color{blue}{对所有带注意力的x_i的语义求和得总体的语义}\end{align}\]</span> 计算打分的函数即<span class="math inline">\(\mathit{AttentionFunction}\)</span>是一个有隐藏层的前馈网络！实现是Badh这个人的，不是Luong的。</p><p><strong>系统架构图说明</strong></p><p>架构图如下</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/05_gnmt.png" style="display:block; margin:auto" width="80%"></p><p>Encoder是8层的LSTM：最底层是双向的LSTM，得到两个方向的信息；上面7层都是单向的。Encoder和Decoder的残差连接都是从第3层开始的。</p><p>训练时，会让Encoder最底层的双向的LSTM开始训练，完成之后，再训练别的层，每层都用单独的GPU。</p><p>为了提高并行性，Decoder最底层，只是为了用来计算Attention Context。带注意力的语义计算好之后，会单独发给其它的各个层。</p><p><strong>经验说明</strong></p><p>实验结果得到，要想NMT有好效果，<strong>Encoder和Decoder的网络层数一定要够深</strong>，才能发现2种语言之间的细微异常规则。和这个同理，深层LSTM比浅层LSTM明显效果好。每加一层，会大约减少10%的perplexity。所以使用deep stacked LSTM。</p><h2 id="残差连接">残差连接</h2><p><a href="http://www.zhuanzhi.ai/#/document/79d7c640238540eba497a7267a07d805" target="_blank" rel="noopener">残差网络讲解</a> 。</p><p>虽然深层LSTM比浅层LSTM效果好，但是如果只是简单堆积的话，只在几个少数层效果才可以。经过试验，4层的话估计效果还可以，6层大部分都不好，8层的话，效果就相当差了。这是因为网络会变得很慢和很难训练，很大程度是因为梯度爆炸和梯度消失的问题。</p><p>根据在中间层和目标之间建立差别的思想，引入<strong>残差连接</strong>，如下图右边所示。其实就是<strong>把之前层的输入和当前的输出合并起来，作为下一层的输入</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/06_residual_LSTM.png" style="display:block; margin:auto" width="75%"></p><p>一些参数和符号说明，一下均是时刻<span class="math inline">\(t\)</span></p><ul><li><span class="math inline">\(\mathbf{x^i_t}\)</span> : 第<span class="math inline">\(i+1\)</span>层 <span class="math inline">\(\mathit{LSTM}_{i+1}\)</span>的输入。 即<strong>上标代表LSTM的层数，下标代表时间</strong>。</li><li><span class="math inline">\(\mathbf{W} ^i\)</span> : 第<span class="math inline">\(i\)</span>层LSTM的参数</li><li><span class="math inline">\(\mathbf {h} _t^i\)</span> : 第<span class="math inline">\(i\)</span>层输出隐状态</li><li><span class="math inline">\(\mathbf {c} ^i_t\)</span> : 第<span class="math inline">\(i\)</span>层输出单元状态</li></ul><p>那么<span class="math inline">\(LSTM_i\)</span>和<span class="math inline">\(LSTM_{i+1}\)</span>是这样交互的。即<strong>层层纵向传递输入，时间横向传递隐状态和单元状态</strong>。 <span class="math display">\[\begin{align}&amp;  \mathbf{c}_t^i, \mathbf{h}_t^i = LSTM_i(\mathbf{c}_{t-1}^i, \mathbf{h}_{t-1}^i, \mathbf x_{t}^{i-1} ; \; \mathbf W^i ) \\&amp;  \mathbf x_t^i = \mathbf h_t^i \quad\quad\quad\quad  \color{blue}{普通连接：i+1层输入=i层隐层输出} \\&amp; \mathbf{x}_t^i = \mathbf h_t^i + \mathbf{x}_t^{i-1} \quad \color{blue} {残差连接：第i+1层的输入=i层输入+i层隐层输出} \\&amp; \mathbf{c}_t^{i+1}, \mathbf{h}_t^{i+1} = LSTM_{i+1} (\mathbf c_{t-1}^{i+1}, \mathbf {h} _{t-1}^{i+1}, x_{t}^i ; \; \mathbf W ^{i+1}) \\\end{align}\]</span> 残差连接可以在反向传播的时候大幅度提升梯度流，这样就可以训练很深的网络。</p><h2 id="双向encoder">双向Encoder</h2><p>一般输入的句子是从左到右，输出也是。但是由于语言的复杂性，有助于翻译的关键信息可能在原句子的不同地方。为了在Encoder中的每一个点都有最好的上下文语义，所以需要使用双向LSTM。</p><p>这里只在Encoder的最底层使用双向LSTM，其余各层均使用单向的LSTM。双向LSTM训练完成之后，再训练别的层。</p><p><span class="math inline">\(LSTM_f\)</span>从左到右处理句子，<span class="math inline">\(LSTM_b\)</span>从右到左处理句子。<strong>把两个方向的信息<span class="math inline">\(\mathbf{x^f_i}\)</span>和<span class="math inline">\(\mathbf{x}^b_i\)</span>concat起来，传递给下一层。</strong></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/33_bidir_lstm.png" style="display:block; margin:auto" width="50%"></p><h2 id="模型并行性">模型并行性</h2><p>模型很复杂，所以使用模型并行和数据并行，来加速。</p><p><strong>数据并行</strong></p><p>数据并行很简单，使用<a href="https://wlypku.github.io/2016/10/06/Downpour-SGD/" target="_blank" rel="noopener">大规模分布式深度网络(Downpour SGD)</a> 同时训练<span class="math inline">\(n\)</span>个模型副本，它们都使用相同的模型参数，但是每个副本会使用Adam和SGD去异步地更新参数。每个模型副本一次处理m个句子。一般实验中，<span class="math inline">\(n=10, m=128\)</span>。</p><p><strong>模型并行</strong></p><p>除了数据并行以外，模型并行也会加速每个副本的梯度计算。Encoder和Decoder会进行深度去划分，一般每一层会放在一个单独的GPU上。除了第一层的Encoder之外，所有的层都是单向的，所以第<span class="math inline">\(i+1\)</span>层可以提前运行，不必等到第<span class="math inline">\(i\)</span>层完全训练好了才进行训练。Softmax也会进行划分，每个处理一部分的单词。</p><p><strong>并行带来的约束</strong></p><p>由于要并行计算，所以我们不能够在Encoder的所有层上使用双向LSTM。因为如果使用了双向的，上面层必须等到下面层前向后向完全训练好之后才能开始训练，就不能并行计算。在Attention上，我们也只能使用最顶层的Encoder和最底层的Decoder进行对齐计算。如果使用顶层Encoder和顶层Decoder，那么整个Decoder将没有任何并行性，也就享受不到多个GPU的快乐了。</p><h1 id="分割技巧">分割技巧</h1><p>一般NMT都是的词汇表都是定长的，但是实际上词汇表却是开放的。比如人名、地名和日期等等。一般有两种方法去处理OOV(out-of-vocabulary)单词，复制策略和sub-word单元策略。GNMT是使用sub-word单元策略，也称为wordpiece模型。</p><p><strong>复制策略</strong></p><p>有下面几种复制策略</p><ul><li>把稀有词汇直接复制到目标句子中，因为大部分都是人名和地名</li><li>使用注意力模型，添加特别的注意力</li><li>使用一个外部的对齐模型，去处理稀有词汇</li><li>使用一个复杂的带有特殊目的的指出网络，去指出稀有词汇</li></ul><p><strong>sub-word单元</strong></p><p>比如字符，混合单词和字符，更加智能的sub-words。</p><h2 id="wordpiece-模型">Wordpiece 模型</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1609.08144.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;谷歌神经机器翻译系统&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img sr
      
    
    </summary>
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="神经机器翻译" scheme="http://plmsmile.github.io/tags/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>谷歌翻译论文笔记</title>
    <link href="http://plmsmile.github.io/2017/10/17/%E8%B0%B7%E6%AD%8C%E7%BF%BB%E8%AF%91%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://plmsmile.github.io/2017/10/17/谷歌翻译论文笔记/</id>
    <published>2017-10-17T05:25:38.000Z</published>
    <updated>2017-10-20T03:32:10.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">谷歌神经机器翻译系统</a></p></blockquote><p><img src="" style="display:block; margin:auto" width="60%"></p><h1 id="简介">简介</h1><p><code>神经机器翻译</code>是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点</p><ul><li>训练和翻译都太慢了，花费代价很大</li><li>缺乏鲁棒性，特别是输入句子包含生僻词汇</li><li>精确度和速度也不行</li></ul><h2 id="传统nmt缺点">传统NMT缺点</h2><p>神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。</p><p>NMT避开了传统基于短语的翻译模型的很多缺点。但是，在实际中，NMT的准确度要比基于短语的翻译模型更差些。</p><p>NMT有3个主要的缺点：训练和推理速度太慢，不能有效处理稀有词汇，有时不能完全翻译原句子。</p><p><strong>训练和推理速度太慢</strong></p><p>训练大数据集，需要大量时间和资源；反馈太慢周期太长。加了一个小技巧，看结果要等很长时间。推理翻译的时候，要使用大量的参数去计算，也很慢。</p><p><strong>不能有效处理稀有词汇</strong></p><p>有两个方法去复制稀有单词：</p><ul><li>模仿传统对齐模型去训练1个<code>copy model</code></li><li>使用注意力机制去复制</li></ul><p>但是效果都不是很好，都不可靠，不同语言的对齐效果差；在网络很深的时候，注意力机制的对齐向量也不稳定。而且，简单的复制过去也不是最好的办法，比如需要直译的时候。</p><p><strong>不能完整翻译整个句子</strong></p><p>不能覆盖整个输入句子的内容，然后会导致一些奇怪的翻译结果。</p><h2 id="gnmt的模型优点">GNMT的模型优点</h2><p>采用的模型：<strong>深层LSTM</strong> 、<strong>Encoder8层</strong>、<strong>Decoder8层</strong> 。<a href="https://plmsmile.github.io/2017/10/18/rnn/">我的LSTM笔记</a>。</p><p>各层之间使用<strong>残差连接</strong>促进梯度流，顶层Enocder到底层Decoder使用<strong>注意力连接</strong>，提高并行性。</p><p>进行翻译推断的时候，使用低精度算法，去加速翻译。</p><p>处理<strong>稀有词汇</strong>：使用<strong>sub-word单元</strong>，也称作wordpieces方法。把单词划分到有限的<strong>sub-word</strong> (wordpieces)单元集合，输入输出都这样。sub-word结合了字符分割模型的弹性和单词分割模型的效率。</p><p><code>Beam Search</code> 使用<strong>长度规范化</strong>和<strong>覆盖惩罚</strong>。覆盖惩罚就是说，希望，翻译的结果句子，尽量多地包含输入句子中的所有词汇。</p><p>使用强化学习去优化模型， 优化翻译的<code>BLEU</code> 分数。</p><h2 id="先进技术">先进技术</h2><p>有很多先进的技术来提高NMT，下面这些都有论文的。</p><ul><li>利用attention去处理稀有词汇</li><li>建立翻译覆盖的机制</li><li>多任务和半监督训练，去合并使用更多数据</li><li>字符分割的encoder和decoder</li><li>使用subword单元处理稀疏的输出</li></ul><h1 id="系统架构">系统架构</h1><h2 id="系统总览">系统总览</h2><p><strong>架构</strong></p><p>有3个模块：Encoder，Decoder，Attention。</p><p>Encdoer：把句子转换成一系列的向量，每一个向量代表一个输入词汇（符号）。</p><p>Decoder：根据这些向量，每一时刻会生成一个目标词汇，直到EOS。</p><p>Attention：连接Encoder和Decoder，在解码的过程中，可以让Decoder有权重的有选择的关注输入句子的部分区域。</p><p><strong>符号说明</strong></p><ul><li>加粗小写代表<strong>向量</strong>，如<span class="math inline">\(\boldsymbol {v, o_i}\)</span></li><li>加粗大写，<strong>矩阵</strong>，如<span class="math inline">\(\boldsymbol {U, W}\)</span> 和<span class="math inline">\(\mathbf{U, W}\)</span></li><li>手写体，<strong>集合</strong>，如<span class="math inline">\(\mathcal{V, F}\)</span></li><li>大写字母，<strong>句子</strong>，如<span class="math inline">\(X, Y\)</span></li><li>小写字母，<strong>单个符号</strong>，如<span class="math inline">\(x_1, x_2\)</span></li></ul><p><strong>Encoder</strong></p><p>输入句子和目标句子组成一个Pair <span class="math inline">\((X, Y)\)</span>，其中输入句子<span class="math inline">\(X = x_1, x_2, \cdots, x_M\)</span> ，<span class="math inline">\(M\)</span> 个单词，翻译的输出目标句子<span class="math inline">\(Y = y_1, y_2, \cdots, y_N\)</span> ，有<span class="math inline">\(N\)</span>个单词。</p><p>Encoder其实就是一个转换函数，得到<span class="math inline">\(M\)</span>个长度固定的向量，也就是其中Encoder对各个<span class="math inline">\(x_i\)</span>的<strong>编码向量 <span class="math inline">\(\mathbf{x_i}\)</span> </strong>： <span class="math display">\[\mathbf {x_1, x_2, \cdots, x_M} = \mathit{EncoderRNN} (x_1, x_2, \cdots, x_n)\]</span> 使用链式条件概率可得到翻译概率<span class="math inline">\(\color{blue} {P (Y \mid X)}\)</span> ，其中<span class="math inline">\(y_0\)</span>是起始符号<span class="math inline">\(SOS\)</span> 。 <span class="math display">\[\begin{align}P(Y \mid X) &amp; = P(Y \mid \mathbf{x_1, x_2, \cdots, x_M}) \\&amp; =\prod_{i=1}^N P(y_i \mid y_0, \cdots, y_{i-1}; \mathbf{x_1, x_2, \cdots, x_M}) \\\end{align}\]</span> <strong>Decoder</strong></p><p>在翻译<span class="math inline">\(y_i\)</span>的时候， 利用Encoder得到的编码向量<span class="math inline">\(\mathbf{x_i}\)</span> 和 <span class="math inline">\(y_0 \sim y_{i-1}\)</span> 来进行计算概率翻译 <span class="math display">\[P(y_i \mid y_0, \cdots, y_{i-1}; \mathbf{x_1, x_2, \cdots, x_M})\]</span> Decoder是由<strong>RNN+Softmax</strong>构成的。会得到一个<strong>隐状态<span class="math inline">\(\mathbf{y_i}\)</span> 向量</strong>，有2个作用：</p><ul><li>作为下一个RNN的输入</li><li><span class="math inline">\(\mathbf{y_i}\)</span>经过softmax得到概率分布， 选出<span class="math inline">\(y_i\)</span> 输出符号</li></ul><p><strong>Attention</strong></p><p>在之前的文章里有介绍<a href="https://plmsmile.github.io/2017/10/12/Attention-based-NMT/">论文</a> 和 <a href="https://plmsmile.github.io/2017/10/10/attention-model/">通俗理解</a>，其实就是<strong>影响力模型</strong>。原句子的各个单词对翻译当前单词分别有多少的影响力，也叫作<strong>对齐概率</strong>吧。使用decoder-RNN的输出<span class="math inline">\(\mathbf{y_{t-1}}\)</span> 向量作为时刻<span class="math inline">\(t\)</span>的输入。</p><p>时刻<span class="math inline">\(t\)</span>，给定<span class="math inline">\(\mathbf{y_{t-1}}\)</span></p><p>有3个符号定义：</p><ul><li><span class="math inline">\(s_i\)</span> ： <strong><span class="math inline">\(y_t\)</span>与<span class="math inline">\(x_i\)</span>的得分</strong>，在luong论文里面有3种计算方式，分别是dot, general和concat。</li><li><span class="math inline">\(p_i\)</span> ：<strong><span class="math inline">\(y_t\)</span>与<span class="math inline">\(x_i\)</span>的对齐概率</strong>，<span class="math inline">\((p_1, p_2, \cdots, p_M)\)</span> 联合起来就是<span class="math inline">\(y_t\)</span>与<span class="math inline">\(X\)</span>的<strong>对齐向量</strong>。其实就是对得分softmax。</li><li><span class="math inline">\(\mathbf{a_t}\)</span> ：<strong>带注意力的语义向量</strong>。对于所有的<span class="math inline">\(x_i\)</span>，使用<span class="math inline">\(y_t\)</span>与它的对齐概率<span class="math inline">\(p_i\)</span>乘以本身的编码向量<span class="math inline">\(\mathbf{x_i}\)</span>，得到<span class="math inline">\(x_i\)</span>传达的语义，再对所有的语义求和，即得到总体的带有注意力的语义。</li></ul><p>整体详细计算的流程，如下面的公式： <span class="math display">\[\begin {align}&amp;  s_i = \mathit{AttentionFunction} (\mathbf{y_{t-1}}, \mathbf{x_i}), \quad i \in [1, M] \\&amp; p_i = \frac {\exp (s_i)}{\sum_{j=1}^M \exp(s_j)}  \quad i \in [1, M] \\&amp;  \mathbf{a_t} = \sum_{i=1}^M p_i \cdot \mathbf{x_i} \quad \color{blue}{对所有带注意力的x_i的语义求和得总体的语义}\end{align}\]</span> 计算打分的函数即<span class="math inline">\(\mathit{AttentionFunction}\)</span>是一个有隐藏层的前馈网络！实现是Badh这个人的，不是Luong的。</p><p><strong>系统架构图说明</strong></p><p>架构图如下</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/05_gnmt.png" style="display:block; margin:auto" width="80%"></p><p>Encoder是8层的LSTM：最底层是双向的LSTM，得到两个方向的信息；上面7层都是单向的。Encoder和Decoder的残差连接都是从第3层开始的。</p><p>训练时，会让Encoder最底层的双向的LSTM开始训练，完成之后，再训练别的层，每层都用单独的GPU。</p><p>为了提高并行性，Decoder最底层，只是为了用来计算Attention Context。带注意力的语义计算好之后，会单独发给其它的各个层。</p><p><strong>经验说明</strong></p><p>实验结果得到，要想NMT有好效果，<strong>Encoder和Decoder的网络层数一定要够深</strong>，才能发现2种语言之间的细微异常规则。和这个同理，深层LSTM比浅层LSTM明显效果好。每加一层，会大约减少10%的perplexity。所以使用deep stacked LSTM。</p><h2 id="残差连接">残差连接</h2><p><a href="http://www.zhuanzhi.ai/#/document/79d7c640238540eba497a7267a07d805" target="_blank" rel="noopener">残差网络讲解</a> 。</p><p>虽然深层LSTM比浅层LSTM效果好，但是如果只是简单堆积的话，只在几个少数层效果才可以。经过试验，4层的话估计效果还可以，6层大部分都不好，8层的话，效果就相当差了。这是因为网络会变得很慢和很难训练，很大程度是因为梯度爆炸和梯度消失的问题。</p><p>根据在中间层和目标之间建立差别的思想，引入<strong>残差连接</strong>，如下图右边所示。其实就是<strong>把之前层的输入和当前的输出合并起来，作为下一层的输入</strong>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/06_residual_LSTM.png" style="display:block; margin:auto" width="75%"></p><p>一些参数和符号说明，一下均是时刻<span class="math inline">\(t\)</span></p><ul><li><span class="math inline">\(\mathbf{x^i_t}\)</span> : 第<span class="math inline">\(i+1\)</span>层 <span class="math inline">\(\mathit{LSTM}_{i+1}\)</span>的输入。 即<strong>上标代表LSTM的层数，下标代表时间</strong>。</li><li><span class="math inline">\(\mathbf{W} ^i\)</span> : 第<span class="math inline">\(i\)</span>层LSTM的参数</li><li><span class="math inline">\(\mathbf {h} _t^i\)</span> : 第<span class="math inline">\(i\)</span>层输出隐状态</li><li><span class="math inline">\(\mathbf {c} ^i_t\)</span> : 第<span class="math inline">\(i\)</span>层输出单元状态</li></ul><p>那么<span class="math inline">\(LSTM_i\)</span>和<span class="math inline">\(LSTM_{i+1}\)</span>是这样交互的。即<strong>层层纵向传递输入，时间横向传递隐状态和单元状态</strong>。 <span class="math display">\[\begin{align}&amp;  \mathbf{c}_t^i, \mathbf{h}_t^i = LSTM_i(\mathbf{c}_{t-1}^i, \mathbf{h}_{t-1}^i, \mathbf x_{t}^{i-1} ; \; \mathbf W^i ) \\&amp;  \mathbf x_t^i = \mathbf h_t^i \quad\quad\quad\quad  \color{blue}{普通连接：i+1层输入=i层隐层输出} \\&amp; \mathbf{x}_t^i = \mathbf h_t^i + \mathbf{x}_t^{i-1} \quad \color{blue} {残差连接：第i+1层的输入=i层输入+i层隐层输出} \\&amp; \mathbf{c}_t^{i+1}, \mathbf{h}_t^{i+1} = LSTM_{i+1} (\mathbf c_{t-1}^{i+1}, \mathbf {h} _{t-1}^{i+1}, x_{t}^i ; \; \mathbf W ^{i+1}) \\\end{align}\]</span> 残差连接可以在反向传播的时候大幅度提升梯度流，这样就可以训练很深的网络。</p><h2 id="双向encoder">双向Encoder</h2><p>一般输入的句子是从左到右，输出也是。但是由于语言的复杂性，有助于翻译的关键信息可能在原句子的不同地方。为了在Encoder中的每一个点都有最好的上下文语义，所以需要使用双向LSTM。</p><p>这里只在Encoder的最底层使用双向LSTM，其余各层均使用单向的LSTM。双向LSTM训练完成之后，再训练别的层。</p><p><span class="math inline">\(LSTM_f\)</span>从左到右处理句子，<span class="math inline">\(LSTM_b\)</span>从右到左处理句子。<strong>把两个方向的信息<span class="math inline">\(\mathbf{x^f_i}\)</span>和<span class="math inline">\(\mathbf{x}^b_i\)</span>concat起来，传递给下一层。</strong></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/33_bidir_lstm.png" style="display:block; margin:auto" width="50%"></p><h2 id="模型并行性">模型并行性</h2><p>模型很复杂，所以使用模型并行和数据并行，来加速。</p><p><strong>数据并行</strong></p><p>数据并行很简单，使用<a href="https://wlypku.github.io/2016/10/06/Downpour-SGD/" target="_blank" rel="noopener">大规模分布式深度网络(Downpour SGD)</a> 同时训练<span class="math inline">\(n\)</span>个模型副本，它们都使用相同的模型参数，但是每个副本会使用Adam和SGD去异步地更新参数。每个模型副本一次处理m个句子。一般实验中，<span class="math inline">\(n=10, m=128\)</span>。</p><p><strong>模型并行</strong></p><p>除了数据并行以外，模型并行也会加速每个副本的梯度计算。Encoder和Decoder会进行深度去划分，一般每一层会放在一个单独的GPU上。除了第一层的Encoder之外，所有的层都是单向的，所以第<span class="math inline">\(i+1\)</span>层可以提前运行，不必等到第<span class="math inline">\(i\)</span>层完全训练好了才进行训练。Softmax也会进行划分，每个处理一部分的单词。</p><p><strong>并行带来的约束</strong></p><p>由于要并行计算，所以我们不能够在Encoder的所有层上使用双向LSTM。因为如果使用了双向的，上面层必须等到下面层前向后向完全训练好之后才能开始训练，就不能并行计算。在Attention上，我们也只能使用最顶层的Encoder和最底层的Decoder进行对齐计算。如果使用顶层Encoder和顶层Decoder，那么整个Decoder将没有任何并行性，也就享受不到多个GPU的快乐了。</p><h1 id="分割技巧">分割技巧</h1><p>一般NMT都是的词汇表都是定长的，但是实际上词汇表却是开放的。比如人名、地名和日期等等。一般有两种方法去处理OOV(out-of-vocabulary)单词，复制策略和sub-word单元策略。GNMT是使用sub-word单元策略，也称为wordpiece模型。</p><p><strong>复制策略</strong></p><p>有下面几种复制策略</p><ul><li>把稀有词汇直接复制到目标句子中，因为大部分都是人名和地名</li><li>使用注意力模型，添加特别的注意力</li><li>使用一个外部的对齐模型，去处理稀有词汇</li><li>使用一个复杂的带有特殊目的的指出网络，去指出稀有词汇</li></ul><p><strong>sub-word单元</strong></p><p>比如字符，混合单词和字符，更加智能的sub-words。</p><h2 id="wordpiece-模型">Wordpiece 模型</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1609.08144.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;谷歌神经机器翻译系统&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img sr
      
    
    </summary>
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="神经机器翻译" scheme="http://plmsmile.github.io/tags/%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>那些年折磨过的问题</title>
    <link href="http://plmsmile.github.io/2017/10/16/tips/"/>
    <id>http://plmsmile.github.io/2017/10/16/tips/</id>
    <published>2017-10-16T14:47:46.000Z</published>
    <updated>2017-11-26T08:20:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="搭建博客">搭建博客</h1><h2 id="搭建博客-1">搭建博客</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">mkdir PLMBlogs</span><br><span class="line">cd PLMBlogs</span><br><span class="line"><span class="meta">#</span> install hexo</span><br><span class="line">npm install hexo-cli -g</span><br><span class="line"><span class="meta">#</span> init</span><br><span class="line">hexo init </span><br><span class="line">npm install</span><br><span class="line">hexo server</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> install plugins</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line">npm install hexo-renderer-scss --save</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 在默认_config.yml中添加需要的插件</span><br><span class="line">plugins:</span><br><span class="line">  hexo-generator-feed #RSS订阅插件</span><br><span class="line">  hexo-generator-sitemap  #sitemap插件</span><br><span class="line"></span><br><span class="line">git clone https://github.com/ahonn/hexo-theme-even themes/even</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 替换配置文件 or 一步一步地去配置</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 生成，再替换文件</span><br><span class="line">hexo new page tags</span><br><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure><h2 id="indigo主题">indigo主题</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br><span class="line"><span class="meta">#</span> 配置.yml文件，复制旧的过来即可</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line">git clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigo</span><br><span class="line">git checkout -b card origin/card</span><br><span class="line"></span><br><span class="line">npm install hexo-renderer-less --save</span><br><span class="line">npm install hexo-generator-feed --save</span><br><span class="line">npm install hexo-generator-json-content --save</span><br><span class="line">npm install hexo-helper-qrcode --save</span><br><span class="line"></span><br><span class="line">hexo new page tags</span><br><span class="line">hexo new page categories</span><br><span class="line">hexo new page about</span><br><span class="line"><span class="meta">#</span> 再去配置各个目录下的index文件，也可以直接copy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 配置主题中的yml，直接copy</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 修改图片等</span><br><span class="line"><span class="meta">#</span> 后续 修改宽度</span><br><span class="line"><span class="meta">#</span> source/css/_partial/variable.css 中第28行，修改为80%的宽度</span><br><span class="line">contentWidth: 80%</span><br><span class="line"><span class="meta">#</span> 主题配置文件中 cdn改为false</span><br><span class="line">cdn: false</span><br></pre></td></tr></table></figure><h1 id="搭建indigo博客">搭建indigo博客</h1><h2 id="安装博客">安装博客</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line">''' 安装 '''</span><br><span class="line">git clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigo</span><br><span class="line">npm install hexo-renderer-less --save</span><br><span class="line">npm install hexo-generator-feed --save</span><br><span class="line">npm install hexo-generator-json-content --save</span><br><span class="line">npm install hexo-helper-qrcode --save</span><br><span class="line">''' 配置标签和类别页面，去配置index.md中的数据 '''</span><br><span class="line">hexo new page tags</span><br><span class="line">hexo new page categories</span><br><span class="line"><span class="meta">#</span> 主要是配置 layout: tags layout: categories comment: false</span><br><span class="line"><span class="meta">#</span> 新建关于我的页面，并填上相应的信息 layout: about</span><br><span class="line">hexo new page about</span><br></pre></td></tr></table></figure><p>配置<code>hexo/_config.yml</code>中的主题是<code>indigo</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置indigo主题</span></span><br><span class="line">theme: indigo</span><br></pre></td></tr></table></figure><h2 id="配置数学公式">配置数学公式</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 在主题_config.yml 中配置 </span><br><span class="line">mathjax: true</span><br><span class="line"><span class="meta">#</span> 要先卸载已有的渲染器</span><br><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line"><span class="meta">#</span> 潜在的</span><br><span class="line">npm uninstall hexo-renderer-kramed --save</span><br><span class="line">npm uninstall hexo-math --save</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 只需要安装pandoc就可以了</span><br><span class="line"><span class="meta">#</span> 先在本地下载pandoc，安装好，再执行如下命令</span><br><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure><p>做到这里，要先启动，放两篇带数学公式的文档，去测试一下。</p><h2 id="博客配置">博客配置</h2><p>编辑<code>hexo/_config.yml</code> ，添加如下项目</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site</span></span><br><span class="line">title: PLM<span class="string">'s Blog</span></span><br><span class="line"><span class="string">subtitle: 好好学习，天天向上</span></span><br><span class="line"><span class="string">description: 菜鸟程序员</span></span><br><span class="line"><span class="string">author: 蒲黎明</span></span><br><span class="line"><span class="string">language: zh-CN</span></span><br><span class="line"><span class="string"># url    </span></span><br><span class="line"><span class="string">url: https://plmsmile.github.io/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># Deployment</span></span><br><span class="line"><span class="string">deploy:</span></span><br><span class="line"><span class="string">  type: git</span></span><br><span class="line"><span class="string">  repo: git@github.com:plmsmile/plmsmile.github.io.git</span></span><br><span class="line"><span class="string">  branch: master</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># indigo的配置项</span></span><br><span class="line"><span class="string">feed:</span></span><br><span class="line"><span class="string">  type: atom</span></span><br><span class="line"><span class="string">  path: atom.xml</span></span><br><span class="line"><span class="string">  limit: 0</span></span><br><span class="line"><span class="string">jsonContent:</span></span><br><span class="line"><span class="string">  meta: false</span></span><br><span class="line"><span class="string">  pages: false</span></span><br><span class="line"><span class="string">  posts:</span></span><br><span class="line"><span class="string">    title: true</span></span><br><span class="line"><span class="string">    date: true</span></span><br><span class="line"><span class="string">    path: true</span></span><br><span class="line"><span class="string">    text: true</span></span><br><span class="line"><span class="string">    raw: false</span></span><br><span class="line"><span class="string">    content: false</span></span><br><span class="line"><span class="string">    slug: false</span></span><br><span class="line"><span class="string">    updated: false</span></span><br><span class="line"><span class="string">    comments: false</span></span><br><span class="line"><span class="string">    link: false</span></span><br><span class="line"><span class="string">    permalink: false</span></span><br><span class="line"><span class="string">    excerpt: false</span></span><br><span class="line"><span class="string">    categories: false</span></span><br><span class="line"><span class="string">    tags: true</span></span><br></pre></td></tr></table></figure><h2 id="indigo主题配置">indigo主题配置</h2><p>参考<a href="https://github.com/yscoder/hexo-theme-indigo/wiki/%E9%85%8D%E7%BD%AE" target="_blank" rel="noopener">官方配置说明</a></p><p>编辑<code>themes/indigo/_config.yml</code></p><p>配置左侧菜单</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home:</span><br><span class="line">    text: 主页</span><br><span class="line">    url: /</span><br><span class="line">  archives:</span><br><span class="line">    text: 归档</span><br><span class="line">    url: /archives</span><br><span class="line">  tags:</span><br><span class="line">    text: 标签</span><br><span class="line">    url: /tags</span><br><span class="line">  th-list:</span><br><span class="line">    text: 类别</span><br><span class="line">    url: /categories</span><br><span class="line">  user:</span><br><span class="line">    url: /about</span><br><span class="line">    text: 关于我</span><br><span class="line">  github:</span><br><span class="line">    url: https://github.com/plmsmile</span><br><span class="line">    target: _blank</span><br></pre></td></tr></table></figure><p>配置自我介绍</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">about: 自然语言处理，机器学习，深度学习，Spark，Leetcode，Java，C++，数据结构。都不会呢，赶紧快学吧！</span><br></pre></td></tr></table></figure><p>设置图片，需要配置站点图片、头像图片，也可以换头像背景图片。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 你的头像url</span><br><span class="line">avatar: /img/avatar.jpg</span><br><span class="line"># avatar link</span><br><span class="line">avatar_link: https://plmsmile.github.io/about</span><br><span class="line"># 头像背景图</span><br><span class="line">brand: /img/brand.jpg</span><br><span class="line"># favicon</span><br><span class="line">favicon: /img/favicon.png</span><br><span class="line"></span><br><span class="line"># email</span><br><span class="line">email: plmsmile@126.com</span><br></pre></td></tr></table></figure><p>配置页面宽度</p><p>修改<code>source/css/_partial/variable.css</code> 中的28行，设置为80%。设置主题配置文件中，<code>cdn: false</code>。 、</p><p>配置支付宝和微信图片，默认就有打赏功能。可以关掉。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">''' 是否开启打赏，关闭 reward: false'''</span><br><span class="line">reward:</span><br><span class="line">  title: 谢谢大爷~</span><br><span class="line">  wechat: /img/wechat.png     '微信，关闭设为 false</span><br><span class="line">  alipay: /img/alipay.png     '支付宝，关闭设为 false</span><br></pre></td></tr></table></figure><p>每张文章最后的备注</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">postMessage: &lt;br&gt;原始链接：&lt;a href="&lt;%- url_for(page.path).replace(/index\.html$/, '') %&gt;" target="_blank" rel="external"&gt;&lt;%- page.permalink.replace(/index\.html$/, '') %&gt;&lt;/a&gt;</span><br></pre></td></tr></table></figure><p>关闭动态title</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动态定义title</span></span><br><span class="line"><span class="comment"># title_change:</span></span><br><span class="line"><span class="comment">#   normal: (つェ⊂)咦!又好了!</span></span><br><span class="line"><span class="comment">#   leave: 死鬼去哪里了！</span></span><br></pre></td></tr></table></figure><p>配置几个页面的标题</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 页面标题</span></span><br><span class="line">tags_title: 标签</span><br><span class="line">archives_title: 归档</span><br><span class="line">categories_title: 类别</span><br></pre></td></tr></table></figure><h2 id="总结">总结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br><span class="line"><span class="comment"># 1. 运行下面的脚本</span></span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line">git clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigo</span><br><span class="line">npm install hexo-renderer-less --save</span><br><span class="line">npm install hexo-generator-feed --save</span><br><span class="line">npm install hexo-generator-json-content --save</span><br><span class="line">npm install hexo-helper-qrcode --save</span><br><span class="line">hexo new page tags</span><br><span class="line">hexo new page categories</span><br><span class="line">hexo new page about</span><br><span class="line"><span class="comment"># 2. 替换source/中3个文件夹，about, categories, tags</span></span><br><span class="line"><span class="comment"># 3. 替换hexo和主题中的配置文件 _config.yml</span></span><br><span class="line"><span class="comment"># 4. 替换\indigo\source\img 文件夹</span></span><br><span class="line"><span class="comment"># 5. 修改source/css/_partial/variable.css的28行 宽度为80%</span></span><br><span class="line"><span class="comment"># 6. 把博客文件复制过来，运行查看。</span></span><br></pre></td></tr></table></figure><h1 id="pyplot使用中文">PyPlot使用中文</h1><p><a href="http://www.jianshu.com/p/b76481530472" target="_blank" rel="noopener">参考文档</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 下载字体放到下面的目录</span><br><span class="line"><span class="meta">#</span> 下载simhei.tff</span><br><span class="line">/home/plm/app/anaconda2/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/ttf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 编辑文件</span><br><span class="line">/home/plm/app/anaconda2/lib/python2.7/site-packages/matplotlib/mpl-data/matplotlibrc</span><br><span class="line"><span class="meta">#</span> 打开下面的注释</span><br><span class="line"><span class="meta">#</span> font.family         : sans-serif</span><br><span class="line"><span class="meta">#</span> 打开注释，加上SimHei</span><br><span class="line"><span class="meta">#</span> font.sans-serif     : SimHei,Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 删除缓存</span><br><span class="line">rm -rf ~/.cache/matplotlib</span><br></pre></td></tr></table></figure><p>简单测试例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"></span><br><span class="line">matplotlib.matplotlib_fname()</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'simhei'</span>] <span class="comment">#用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="keyword">False</span> <span class="comment">#用来正常显示负号</span></span><br><span class="line">matplotlib.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="keyword">False</span>   <span class="comment">#-号为方块问题</span></span><br><span class="line">plt.plot((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>),(<span class="number">4</span>,<span class="number">3</span>,<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">s = <span class="string">"横坐标"</span></span><br><span class="line"></span><br><span class="line">plt.xlabel(unicode(s))</span><br><span class="line">plt.ylabel(<span class="string">u'纵坐标'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">print</span> (s)</span><br></pre></td></tr></table></figure><p>有时候依然不好使，那么就</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys  </span><br><span class="line">reload(sys)  </span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf8'</span>)</span><br></pre></td></tr></table></figure><p>应该就可以了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;搭建博客&quot;&gt;搭建博客&lt;/h1&gt;
&lt;h2 id=&quot;搭建博客-1&quot;&gt;搭建博客&lt;/h2&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;
      
    
    </summary>
    
      <category term="心得体会" scheme="http://plmsmile.github.io/categories/%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A/"/>
    
    
      <category term="心得" scheme="http://plmsmile.github.io/tags/%E5%BF%83%E5%BE%97/"/>
    
      <category term="hexo" scheme="http://plmsmile.github.io/tags/hexo/"/>
    
      <category term="latex" scheme="http://plmsmile.github.io/tags/latex/"/>
    
      <category term="中文" scheme="http://plmsmile.github.io/tags/%E4%B8%AD%E6%96%87/"/>
    
      <category term="pyplot" scheme="http://plmsmile.github.io/tags/pyplot/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制和PyTorch实现机器翻译</title>
    <link href="http://plmsmile.github.io/2017/10/12/Attention-based-NMT/"/>
    <id>http://plmsmile.github.io/2017/10/12/Attention-based-NMT/</id>
    <published>2017-10-12T08:12:39.000Z</published>
    <updated>2018-03-18T11:23:34.563Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></p><p>前面阐述注意力理论知识，后面简单描述PyTorch利用注意力实现机器翻译</p></blockquote><h1 id="简介">简介</h1><p><img src="" style="display:block; margin:auto" width="70%"></p><h2 id="attention介绍">Attention介绍</h2><p>在翻译的时候，选择性的选择一些重要信息。<a href="https://plmsmile.github.io/2017/10/10/attention-model/">详情看这篇文章</a> 。</p><p>本着简单和有效的原则，本论文提出了两种<code>注意力机制</code>。</p><p><strong>Global</strong></p><p>每次翻译时，都选择关注<strong>所有的单词</strong>。和<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">Bahdanau的方式</a> 有点相似，但是更简单些。<a href="https://plmsmile.github.io/2017/10/10/attention-model/#思想">简单原理介绍</a>。</p><p><strong>Local</strong></p><p>每次翻译时，只选择<strong>关注一部分的单词</strong>。介于soft和hard注意力之间。(soft和hard见别的论文)。</p><p>优点有下面几个</p><ul><li>比Global和Soft<strong>更好计算</strong></li><li>局部注意力 随处可见、可微，<strong>更好实现和训练</strong>。</li></ul><h2 id="应用范围">应用范围</h2><p>在训练神经网络的时候，注意力机制应用十分广泛。让模型在不同的形式之间，学习<strong>对齐</strong>等等。有下面一些领域：</p><ul><li>机器翻译</li><li>语音识别</li><li>图片描述</li><li>between image objects and agent actions in the dynamic control problem (不懂，以后再说吧)</li></ul><h1 id="神经机器翻译">神经机器翻译</h1><h2 id="思想">思想</h2><p>输入句子<span class="math inline">\(x = (x_1, x_2, \cdots, x_n)\)</span> ，输出目标句子<span class="math inline">\(y = (y_1, y_2, \cdots, y_m)\)</span> 。</p><p>神经机器翻译(Neural machine translation, NMT)，利用神经网络，直接对<span class="math inline">\(\color{blue} {p(y \mid x)}\)</span> 进行建模。一般由Encoder和Decoder构成。<a href="https://plmsmile.github.io/2017/10/10/attention-model/#encoder-decoder">Encoder-Decoder介绍文章链接</a> 。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/01-encoder-decoder-n" style="display:block; margin:auto" width="70%"></p><p>Encoder把输入句子<span class="math inline">\(x\)</span> 编码成一个<strong>语义向量<span class="math inline">\(s\)</span></strong> (c表示也可以)，然后由Decoder 一个一个产生目标单词 <span class="math inline">\(y_i\)</span> <span class="math display">\[\log p(y \mid x) = \sum_{j=1}^m \log \color{red} {p(y_j \mid y _{&lt;j}, s) }=  \sum_{j=1}^m \log p(y_j \mid y_1, \cdots, y_{j-1}, s)\]</span> 但是怎么选择Encoder和Decoder（RNN, CNN, GRU, LSTM），怎么去生成语义<span class="math inline">\(s\)</span>却有很多选择。</p><h2 id="概率计算">概率计算</h2><p>结合Decoder上一时刻的隐状态<span class="math inline">\(\color{blue}{h_{j-1}}\)</span>和encoder给的语义向量<span class="math inline">\(\color{blue}{s}\)</span>，通过函数<span class="math inline">\(\color{blue}{f}\)</span> ，就可以计算出<strong>当前的隐状态</strong><span class="math inline">\(\color{blue}{h_j}\)</span> ： <span class="math display">\[h_j = f(h_{j-1}, s)\]</span> 通过<strong>函数</strong><span class="math inline">\(\color{blue}{g}\)</span>对当前隐状态<strong><span class="math inline">\(h_j\)</span>进行转换</strong>，再<code>softmax</code>，就可以得到翻译的目标单词<span class="math inline">\(y_i\)</span>了（选概率最大的那个）。</p><p><span class="math inline">\(g\)</span>一般是<strong>线性变换</strong>，维数变化是<span class="math inline">\([1, h] \to [1, vocab\_size]\)</span>。 <span class="math display">\[p(y_j \mid y _{&lt;j}, s) =  \mathrm{softmax}  \; g(h_j)\]</span> <strong>语义向量<span class="math inline">\(s​\)</span> 会贯穿整个翻译的过程，每一步翻译都会使用到语义向量的内容</strong>，这就是<code>注意力机制</code>。</p><h2 id="本论文的模型">本论文的模型</h2><p>本论文采用stack LSTM的构建NMT系统。如下所示：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/01_nmt.png" style="display:block; margin:auto" width="60%"></p><p>训练目标是 <span class="math display">\[J_t = \sum_{(x, y)} - \log p(y \mid x)\]</span></p><h1 id="注意力模型">注意力模型</h1><p>注意力模型广义上分为<code>global</code>和<code>local</code>。Global的attention来自于整个序列，而local的只来自于序列的一部分。</p><p><strong>解码总体流程</strong></p><p>Decoder时，在时刻<span class="math inline">\(t\)</span>，要翻译出单词<span class="math inline">\(y_t\)</span> ，如下步骤：</p><ul><li>最顶层<strong>LSTM的隐状态 <span class="math inline">\(h_t\)</span></strong></li><li>计算带有原句子信息<strong>语义向量<span class="math inline">\(c_t\)</span></strong>。Global和Local的区别在于<span class="math inline">\(c_t\)</span>的计算方式不同</li><li><strong>串联<span class="math inline">\(h_t, c_t\)</span></strong>，计算得到带有<strong>注意力的隐状态</strong> <span class="math inline">\(\hat {h}_t = \tanh (W_c [c_t; h_t])\)</span></li><li>通过注意力隐状态得到预测概率 <span class="math inline">\(p(y_t \mid y_{&lt;t}, x) = \rm {softmax} (W_s \hat h _t)\)</span></li></ul><h2 id="global-attention">Global Attention</h2><p><strong>总体思路</strong></p><p>在计算<span class="math inline">\(c_t\)</span> 的时候，<strong>会考虑整个encoder的隐状态</strong>。Decoder当前隐状态<span class="math inline">\(h_t\)</span>， Encoder时刻s的隐状态<span class="math inline">\(\bar h _s\)</span>。</p><p><strong>对齐向量</strong><span class="math inline">\(\color{blue}{\alpha_t}\)</span>代表时刻<span class="math inline">\(t\)</span> <strong>输入序列中的单词对当前单词<span class="math inline">\(y_t\)</span> 的对齐概率</strong>，长度是<span class="math inline">\(T_x\)</span>， 随着输入句子的长度而改变 。<span class="math inline">\(x_s\)</span>与<span class="math inline">\(y_t\)</span> 的对齐概率如下： <span class="math display">\[\alpha_t(s) = \mathrm {align} (h_t, \bar h_s) = \frac {score(h_t, \bar h_s)}{ \sum_{i=1}^{T_x} score(h_t, \bar h_i)}, \quad 实际上\mathrm{softmax}\]</span> 结合上面的解码总体流程，有下面的流程 <span class="math display">\[all (\bar h_s) ,  h_t \to \alpha_t \to c_t . \quad c_t , h_t \to \hat h_t .\quad \hat h_t \to y_t \quad\]</span> 简单来说是<span class="math inline">\(h_t \to \alpha_t \to c_t \to \hat h_t \to y_t\)</span> 。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/02_global_attention.png" style="display:block; margin:auto" width="50%"></p><p><strong>score计算</strong></p><p><span class="math inline">\(score(h_t, \bar h_s)\)</span> 是一种基于内容<code>content-based</code>的函数，有3种实现方式 <span class="math display">\[\color{blue}{score(h_t, \bar h_s)} = \begin{cases}h_t^T \bar h_s &amp; dot \\h_t^T W_a \bar h_s  &amp; general \\v_a^T \tanh (W_a [h_t; \bar h_s]) &amp; concat \\\end{cases}\]</span> <strong>缺点</strong></p><p>生成每个目标单词的时候，都<strong>必须注意所有的原单词</strong>， 这样<strong>计算量很大</strong>，<strong>翻译长序列可能很难</strong>，比如段落或者文章。</p><h2 id="local-attention">Local Attention</h2><p>在生成目标单词的时候，Local会选择性地关注一小部分原单词去计算<span class="math inline">\(\alpha_t, c_t\)</span>，这样就解决了Global的问题。如下图</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/03_local_attention.png" style="display:block; margin:auto" width="50%"></p><p><strong>Soft和Hard注意</strong></p><p><code>Soft 注意</code> ：类似global注意，权值会放在图片的所有patches中。计算复杂。</p><p><code>Hard 注意</code>： 不同时刻，会选择不同的patch。虽然计算少，但是<strong>non-differentiable</strong>，并且需要复杂的技术去训练模型，比如方差减少和强化学习。</p><p><strong>Local注意</strong></p><p>类似于滑动窗口，计算一个<strong>对齐位置</strong><span class="math inline">\(\color{blue}{p_t}\)</span>，<strong>根据经验</strong>设置<strong>窗口大小</strong><span class="math inline">\(D\)</span>，那么需要注意的源单词序列是 ： <span class="math display">\[[p_t -D, p_t + D]\]</span> <span class="math inline">\(\alpha_t\)</span> 的长度就是<span class="math inline">\(2D\)</span>，只需要选择这<span class="math inline">\(2D\)</span>个单词进行注意力计算，而不是Global的整个序列。</p><p><strong>对齐位置选择</strong></p><p>对齐位置的选择就很重要，主要有两种办法。</p><p>local-m (monotonic) 设置位置， 即以当前单词位置作为对齐位置 <span class="math display">\[p_t = t\]</span> local-p (predictive) 预测位置</p><p><span class="math inline">\(S\)</span> 是输入句子的长度，预测对齐位置如下 <span class="math display">\[p_t = S \cdot \mathrm{sigmoid} \left(v_p^T \tanh (W_p h_t) \right),\quad p_t \in [0, S]\]</span> <strong>对齐向量计算</strong></p><p><strong><span class="math inline">\(\alpha_t\)</span>的长度就是<span class="math inline">\(2D\)</span></strong>，对于每一个<span class="math inline">\(s \in [p_t -D, p_t + D]\)</span>， 为了更好地对齐，添加一个正态分布<span class="math inline">\(N(\mu, \sigma ^2)\)</span>，其中 <span class="math inline">\(\mu = p_t, \sigma = \frac{D}{2}\)</span>。</p><p>计算对齐概率： <span class="math display">\[\alpha_t(s) = \mathrm{align} (h_t, \bar h_s)\exp \left( - \frac{(s - \mu)^2}{2\sigma^2}\right)= \mathrm{align} (h_t, \bar h_s)\exp \left( - \frac{2(s - p_t)^2}{D^2}\right)\]</span></p><h2 id="input-feeding">Input-feeding</h2><p>前面的Global和Local两种方式中，在每一步的时候，计算每一个attention (实际上是指 <span class="math inline">\(\hat h_t\)</span>)，都是独立的，这样只是<strong>次最优的</strong>。</p><p>在每一步的计算中，<strong>这些attention应该有所关联，当前知道之前的attention才对</strong>。实际是应该有个<code>coverage set</code>去追踪之前的信息。</p><p>我们会把当前的注意<span class="math inline">\(\hat h_t\)</span> 作为下一次的输入，并且做一个<strong>串联</strong>，来计算新的attention，如下图所示</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/paper/04_input_feed.png" style="display:block; margin:auto" width="50%"></p><p>这样有两重意义：</p><ul><li>模型会知道之前的对齐选择</li><li>会建立一个水平和垂直都很深的网络</li></ul><h1 id="pytorch实现机器翻译">PyTorch实现机器翻译</h1><p><a href="https://github.com/plmsmile/NLP-Demos/blob/master/en-zh-translation/model.py" target="_blank" rel="noopener">机器翻译github源代码</a></p><h2 id="计算输入语义">计算输入语义</h2><p>比较简单，使用GRU进行编码，使用<code>outputs</code>作为哥哥句子的编码语义。<a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247487237&amp;idx=2&amp;sn=17e366215d434030b9f5ab76dffa4e0e&amp;chksm=ebb437d1dcc3bec703ec0d53267eb58911432f57ab5cd6c44154eafd0aba5093853a0e482188&amp;mpshare=1&amp;scene=1&amp;srcid=0317dkRhljV3FU7H6p2YHyqb#rd" target="_blank" rel="noopener">PyTorch RNN处理变长序列</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_seqs, input_lengths, hidden=None)</span>:</span></span><br><span class="line"><span class="string">''' 对输入的多个句子经过GRU计算出语义信息</span></span><br><span class="line"><span class="string">   1. input_seqs &gt; embeded</span></span><br><span class="line"><span class="string">   2. embeded - packed &gt; GRU &gt; outputs - pad -output</span></span><br><span class="line"><span class="string">   Args:</span></span><br><span class="line"><span class="string">       input_seqs: [s, b]</span></span><br><span class="line"><span class="string">       input_lengths: list[int]，每个batch句子的真实长度</span></span><br><span class="line"><span class="string">   Returns:</span></span><br><span class="line"><span class="string">       outputs: [s, b, h]</span></span><br><span class="line"><span class="string">       hidden: [n_layer, b, h]</span></span><br><span class="line"><span class="string">   '''</span></span><br><span class="line">   <span class="comment"># 一次运行，多个batch，多个序列</span></span><br><span class="line">   embedded = self.embedding(input_seqs)</span><br><span class="line">   packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)</span><br><span class="line">   outputs, hidden = self.gru(packed, hidden)</span><br><span class="line">   outputs, output_length = nn.utils.rnn.pad_packed_sequence(outputs)  </span><br><span class="line">   <span class="comment"># 双向，两个outputs求和</span></span><br><span class="line">   <span class="keyword">if</span> self.bidir <span class="keyword">is</span> <span class="keyword">True</span>:</span><br><span class="line">       outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]</span><br><span class="line">   <span class="keyword">return</span> outputs, hidden</span><br></pre></td></tr></table></figure><h2 id="计算对齐向量">计算对齐向量</h2><p>实际上就是<code>attn_weights</code>， 也就是输入序列对当前要预测的单词的一个<a href="https://plmsmile.github.io/2017/10/10/attention-model/#%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E9%85%8D%E6%A6%82%E7%8E%87%E8%AE%A1%E7%AE%97">注意力分配</a>。</p><p><strong>输入输出定义</strong></p><p>Encoder的输出，所有语义<span class="math inline">\(c\)</span>，<code>encoder_outputs</code>， <code>[is, b, h]</code>。 <code>is=input_seq_len</code>是输入句子的长度</p><p>当前时刻Decoder的<span class="math inline">\(h_t\)</span>， <code>decoder_rnn_output</code>， <code>[ts, b, h]</code> 。实际上<code>ts=1</code>， 因为每次解码一个单词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, rnn_outputs, encoder_outputs)</span>:</span></span><br><span class="line">    <span class="string">'''ts个时刻，计算ts个与is的对齐向量，也是注意力权值</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    rnn_outputs -- Decoder中GRU的输出[ts, b, h]</span></span><br><span class="line"><span class="string">        encoder_outputs -- Encoder的最后的输出, [is, b, h]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        attn_weights -- Yt与所有Xs的注意力权值，[b, ts, is]</span></span><br><span class="line"><span class="string">    '''</span></span><br></pre></td></tr></table></figure><p><strong>计算得分</strong></p><p>使用<code>gerneral</code>的方式，先过神经网络(线性层)，再<strong>乘法计算得分</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 过Linear层 (b, h, is)</span></span><br><span class="line">encoder_outputs = self.attn(encoder_outputs).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># [b,ts,is] &lt; [b,ts,h] * [b,h,is]</span></span><br><span class="line">attn_energies = rnn_outputs.bmm(encoder_outputs)</span><br></pre></td></tr></table></figure><p><strong>softmax计算对齐向量</strong></p><p><strong>每一行都是原语义对于某个单词的注意力分配权值向量</strong>。<a href="https://plmsmile.github.io/2017/10/10/attention-model/#%E8%AF%AD%E4%B9%89%E5%90%91%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97">对齐向量实际例子</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [b,ts,is]</span></span><br><span class="line">attn_weights = my_log_softmax(attn_energies)</span><br><span class="line"><span class="keyword">return</span> attn_weights</span><br></pre></td></tr></table></figure><h2 id="计算新的语义">计算新的语义</h2><p>新的语义也就是，对于翻译单词<span class="math inline">\(w_t\)</span>所需要的<strong>带注意力的语义</strong>。</p><p><strong>输入输出</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_seqs, last_hidden, encoder_outputs)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    一次输入(ts, b)，b个句子, ts=target_seq_len</span></span><br><span class="line"><span class="string">    1. input &gt; embedded </span></span><br><span class="line"><span class="string">    2. embedded, last_hidden --GRU-- rnn_output, hidden</span></span><br><span class="line"><span class="string">    3. rnn_output, encoder_outpus --Attn-- attn_weights</span></span><br><span class="line"><span class="string">    4. attn_weights, encoder_outputs --相乘-- context</span></span><br><span class="line"><span class="string">    5. rnn_output, context --变换,tanh,变换-- output </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">    input_seqs: [ts, b] batch个上一时刻的输出的单词，id表示。每个batch1个单词</span></span><br><span class="line"><span class="string">        last_hidden: [n_layers, b, h]</span></span><br><span class="line"><span class="string">        encoder_outputs: [is, b, h]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    output: 最终的输出，[ts, b, o]</span></span><br><span class="line"><span class="string">        hidden: GRU的隐状态，[nl, b, h]</span></span><br><span class="line"><span class="string">        attn_weights: 对齐向量，[b, ts, is]</span></span><br><span class="line"><span class="string">    '''</span></span><br></pre></td></tr></table></figure><p><strong>当前时刻Decoder的隐状态</strong></p><p>输入上一时刻的单词和隐状态，通过GRU，计算当前的隐状态。实际上<code>ts=1</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (ts, b, h), (nl, b, h)</span></span><br><span class="line">rnn_output, hidden = self.gru(embedded, last_hidden)</span><br></pre></td></tr></table></figure><p><strong>计算对齐向量</strong></p><p>当前时刻的隐状态 <code>rnn_output</code> 和源句子的语义<code>encoder_outputs</code>，计算对齐向量。<a href="https://plmsmile.github.io/2017/10/10/attention-model/#%E8%AF%AD%E4%B9%89%E5%90%91%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97">对齐向量</a></p><blockquote><p>每一行都是原句子对当前单词(只有一行)的注意力分配。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对齐向量 [b,ts,is]</span></span><br><span class="line">attn_weights = self.attn(rnn_output, encoder_outputs)</span><br><span class="line"><span class="comment"># 如</span></span><br><span class="line">[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.7</span>]</span><br></pre></td></tr></table></figure><p><strong>计算新的语义</strong></p><p>原语义和原语义对当前单词分配的注意力，计算当前需要的新语义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新的语义 </span></span><br><span class="line"><span class="comment"># [b,ts,h] &lt; [b,ts,is] * [b,is,h]</span></span><br><span class="line">context = attn_weights.bmm(encoder_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="预测当前单词">预测当前单词</h2><p>结合新语义和当前隐状态预测新单词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 语义和当前隐状态结合 [ts, b, 2h] &lt; [ts, b, h], [ts, b, h]</span></span><br><span class="line">output_context = torch.cat((rnn_output, context), <span class="number">2</span>)</span><br><span class="line"><span class="comment"># [ts, b, h] 线性层2h-h</span></span><br><span class="line">output_context = self.concat(output_context)</span><br><span class="line">concat_output = F.tanh(output_context)</span><br><span class="line"><span class="comment"># [ts, b, o] 线性层h-o</span></span><br><span class="line">output = self.out(concat_output)</span><br><span class="line">output = my_log_softmax(output)</span><br><span class="line"><span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h2 id="总结">总结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 对齐向量</span></span><br><span class="line"><span class="comment"># 过Linear层 (b, h, is)</span></span><br><span class="line">encoder_outputs = self.attn(encoder_outputs).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关联矩阵 [b,ts,is] &lt; [b,ts,h] * [b,h,is]</span></span><br><span class="line">attn_energies = rnn_outputs.bmm(encoder_outputs)</span><br><span class="line"><span class="comment"># 每一行求softmax [b,ts,is] </span></span><br><span class="line"><span class="string">'''每一行都是原语义对当前单词的注意力分配向量'''</span></span><br><span class="line">attn_weights = my_log_softmax(attn_energies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 新语义</span></span><br><span class="line"><span class="comment"># 新的语义 [b,ts,h] &lt; [b,ts,is] * [b,is,h]</span></span><br><span class="line">context = attn_weights.bmm(encoder_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 新语义和当前隐状态结合，输出</span></span><br><span class="line"><span class="comment"># 语义和输出 [ts, b, 2h] &lt; [ts, b, h], [ts, b, h]</span></span><br><span class="line">output_context = torch.cat((rnn_output, context), <span class="number">2</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.04025.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Effective Approaches to Attention-based Neura
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="论文笔记" scheme="http://plmsmile.github.io/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
      <category term="注意力" scheme="http://plmsmile.github.io/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
      <category term="Attention" scheme="http://plmsmile.github.io/tags/Attention/"/>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器翻译" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>图文介绍注意力机制</title>
    <link href="http://plmsmile.github.io/2017/10/10/attention-model/"/>
    <id>http://plmsmile.github.io/2017/10/10/attention-model/</id>
    <published>2017-10-10T03:40:01.000Z</published>
    <updated>2018-03-18T10:30:58.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="encoder-decoder">Encoder-Decoder</h1><h2 id="基本介绍">基本介绍</h2><p>举个翻译的例子，原始句子<span class="math inline">\(X = (x_1, x_2, \cdots, x_m)\)</span> ，翻译成目标句子<span class="math inline">\(Y = (y_1, y_2, \cdots, y_n)\)</span> 。</p><p>现在采用<code>Encoder-Decoder</code>架构模型，如下图</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/01-encoder-decoder-n" style="display:block; margin:auto" width="70%"></p><p>Encoder会利用整个原始句子生成一个<code>语义向量</code>，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句子的意思、句法结构、性别信息等等。具体框架可以参考<a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247484196&amp;idx=1&amp;sn=efa6b79d24b138ff79f33ec3426c79e2&amp;chksm=ebb43bf0dcc3b2e6e69ff3b7d7cabf28744e276ee08ccfcfc1dc2c3f0bf52a122c9bd77ff319&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Encoder-Decoder框架</a>。</p><p>Encoder对<span class="math inline">\(X\)</span> 进行非线性变换得到<code>中间语义向量c</code> ： <span class="math display">\[c = G(x_1, x_2, \cdots, x_n)\]</span> Decoder根据<strong>语义<span class="math inline">\(c\)</span> 和生成的历史单词</strong><span class="math inline">\((y_1, y_2, \cdots, y_{i-1})\)</span> 来<strong>生成第<span class="math inline">\(i\)</span> 个单词 <span class="math inline">\(y_i\)</span>：</strong> <span class="math display">\[y_i = f(c, y_1, y_2, \cdots, y_{i-1})\]</span> Encoder-Decoder是个<strong>创新大杀器</strong>，是个通用的计算框架。Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN，RNN，BiRNN，GRU，LSTM， Deep LSTM。上面的内容<strong>任意组合</strong>，只要得到的<strong>效果好</strong>，就是一个<strong>创新</strong>，就可以毕业了。（当然别人没有提出过）</p><h2 id="缺点">缺点</h2><p>在生成目标句子<span class="math inline">\(Y\)</span>的单词时，<strong>所有的单词<span class="math inline">\(y_i\)</span>使用的语义编码<span class="math inline">\(c\)</span> 都是一样的</strong>。而语义编码<span class="math inline">\(c\)</span>是由句子<span class="math inline">\(X\)</span> 的每个单词经过Encoder编码产生，也就是说<strong>每个<span class="math inline">\(x_i\)</span>对所有<span class="math inline">\(y_j\)</span>的影响力都是相同的</strong>，没有任何区别的。所以上面的是<strong>注意力不集中的分心模型</strong>。</p><p>句子较短时问题不大，但是较长时，所有语义完全通过一个中间语义向量来表示，<strong>单词自身的信息已经消失，会丢失更多的细节信息</strong>。</p><p><strong>例子</strong></p><p>比如输入<span class="math inline">\(X\)</span>是<code>Tom chase Jerry</code>，模型翻译出 <code>汤姆 追逐 杰瑞</code>。在翻译“杰瑞”的时候，“Jerry”对“杰瑞”的贡献<strong>更重要</strong>。但是显然普通的Encoder-Decoder模型中，三个单词对于翻译“Jerry-杰瑞”的贡献是一样的。</p><p>解决方案应该是，每个单词对于翻译“杰瑞”的<strong>贡献应该不一样</strong>，如翻译“杰瑞”时： <span class="math display">\[(Tom, 0.3), \;  (Chase, 0.2), \;  (Jerry, 0.5)\]</span></p><h1 id="attention-model">Attention Model</h1><h2 id="基本架构">基本架构</h2><p>Attention Model的架构如下：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/02-encoder-decoder-am-n" style="display:block; margin:auto" width="60%"></p><p>如图所示，生成每个单词<span class="math inline">\(y_i\)</span>时，都有<strong>各自的语义向量<span class="math inline">\(C_i\)</span></strong>，不再是统一的<span class="math inline">\(C\)</span> 。 <span class="math display">\[y_i = f(C_i, y_1, \cdots, y_{i-1})\]</span> 例如，前3个单词的生成： <span class="math display">\[\begin{align}&amp; y_1 = f(C_1) \\&amp; y_2 = f(C_2, y_1) \\&amp; y_3 = f(C_3, y_1, y_2) \\\end{align}\]</span></p><h2 id="语义向量的计算">语义向量的计算</h2><p><code>注意力分配概率</code> <strong><span class="math inline">\(a_{ij}\)</span> 表示 <span class="math inline">\(y_i\)</span>收到<span class="math inline">\(x_j\)</span> 的注意力概率</strong>。</p><p>例如<span class="math inline">\(X=(Tom, Chase, Jerry)\)</span>，<span class="math inline">\(Y = (汤姆, 追逐, 杰瑞)\)</span> 。<span class="math inline">\(a_{12}=0.2\)</span>表示<code>汤姆</code> 收到来自<code>Chase</code>的注意力概率是0.2。</p><p>有下面的注意力分配矩阵： <span class="math display">\[A = [a_{ij}] = \begin {bmatrix}0.6 &amp; 0.2 &amp; 0.2  \\0.2 &amp; 0.7 &amp; 0.1 \\0.3 &amp; 0.1 &amp; 0.5 \\\end {bmatrix}\]</span> 第<span class="math inline">\(i\)</span>行表示<strong><span class="math inline">\(y_i\)</span> 收到的所有来自输入单词的注意力分配概率</strong>。<span class="math inline">\(y_i\)</span> 的语义向量<span class="math inline">\(C_i\)</span> 由这些<strong>注意力分配概率</strong>和Encoder对<strong>单词<span class="math inline">\(x_j\)</span>的转换函数</strong>相乘，计算而成，例如： <span class="math display">\[\begin {align}&amp; C_1 = C_{汤姆} = g(0.6 \cdot h(Tom),\; 0.2 \cdot h(Chase),\; 0.2 \cdot h(Jerry)) \\&amp; C_2 = C_{追逐} = g(0.2 \cdot h(Tom) ,\;0.7 \cdot h(Chase) ,\;0.1 \cdot h(Jerry)) \\&amp; C_3 = C_{汤姆} = g(0.3 \cdot h(Tom),\; 0.2 \cdot h(Chase) ,\;0.5 \cdot h(Jerry)) \\\end {align}\]</span> <span class="math inline">\(\color{blue}{h(x_j)}\)</span> 就表示Encoder<strong>对输入英文单词的某种变换函数</strong>。比如Encoder使用RNN的话，<span class="math inline">\(h(x_j)\)</span>往往都是某个时刻输入<span class="math inline">\(x_j\)</span> 后<strong>隐层节点的状态值</strong>。</p><p><code>g函数</code> 表示注意力分配后的整个句子的语义转换信息，一般都是<strong>加权求和</strong>，则有<strong>语义向量</strong>计算公式： <span class="math display">\[C_i  = \sum_{j=1}^{T_x} a_{ij} \cdot h_j, \quad h_j = h(x_j)\]</span> 其中<span class="math inline">\(\color{blue}{T_x}\)</span> 代表<strong>输入句子的长度</strong>。形象来看计算过程如下图：</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/03-ci-compute-n" style="display:block; margin:auto" width="50%"></p><h2 id="注意力分配概率计算">注意力分配概率计算</h2><p>语义向量需要注意力分配概率和Encoder输入单词变换函数来共同计算得到。</p><p>但是比如<code>汤姆</code>收到的分配概率<span class="math inline">\(a_1 = (0.6, 0.2, 0.2)\)</span>是怎么计算得到的呢？</p><p>这里采用RNN作为Encoder和Decoder来说明。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/04-rnn-encoder-decoder-n" style="display:block; margin:auto" width="60%"></p><p>注意力分配概率如下图计算</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/05-aij-compute-n" style="display:block; margin:auto" width="60%"></p><p>对于<span class="math inline">\(a_{ij}\)</span> 其实是通过一个<code>对齐函数F</code>来进行计算的，两个参数：输入节点<span class="math inline">\(j\)</span>，和输出节点<span class="math inline">\(i\)</span>，当然一般是取<strong>隐层状态</strong>。 <span class="math display">\[a_i = F(i, j), \quad j \in [1, T_x], \quad h(j)\,Encoder, \; H(i)\,Decoder\]</span> <span class="math inline">\(\color{blue}{F(i, j)}\)</span>代表<span class="math inline">\(y_i\)</span>和<span class="math inline">\(x_j\)</span>的<strong>对齐可能性</strong>。一般F输出后，再经过<code>softmax</code>就得到了注意力分配概率。</p><h2 id="am模型的意义">AM模型的意义</h2><p>一般地，会把AM模型看成单词对齐模型，输入句子单词和这个目标生句子成单词的对齐概率。</p><p>其实，理解为<code>影响力模型</code>也是合理的。就是在生成目标单词的时候，<strong>输入句子中的每个单词，对于生成当前目标单词有多大的影响程度。</strong></p><p>AM模型有很多的应用，思想大都如此。</p><h2 id="文本摘要例子">文本摘要例子</h2><p>比如<code>文本摘要</code>的例子，输入一个长句，提取出重要的信息。</p><p>输入&quot;russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism&quot;。</p><p>输出&quot;russia calls for joint front against terrorism&quot;。</p><p>下图代表着输入单词对输出单词的影响力，颜色越深，影响力越大，注意力分配概率也越大。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/06-summary-am-n" style="display:block; margin:auto" width="60%"></p><h1 id="pytorch翻译am实现">PyTorch翻译AM实现</h1><h2 id="思想">思想</h2><p>参考<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">这篇论文</a> 。</p><p>生成目标单词<span class="math inline">\(y_i\)</span> 的计算概率是 <span class="math display">\[p(y_i \mid (y_1,\cdots, y_{i-1}), x) = g(y_{i-1}, s_i, c_i)\]</span> 符号意义说明</p><ul><li><span class="math inline">\(y_i\)</span> 当前应该生成的目标单词，<span class="math inline">\(y_{i-1}\)</span> 上一个节点的输出单词</li><li><span class="math inline">\(s_i\)</span> 当前节点的<strong>隐藏状态</strong></li><li><span class="math inline">\(c_i\)</span> 生成当前单词应该有的<strong>语义向量</strong></li><li><span class="math inline">\(g\)</span> 全连接层的函数</li></ul><p><strong>隐层状态<span class="math inline">\(s_i\)</span></strong></p><p>求当前Decoder隐层状态<span class="math inline">\(s_i\)</span>：由上一层的隐状态<span class="math inline">\(s_{i-1}\)</span>，输出单词<span class="math inline">\(y_{i-1}\)</span> ，语义向量<span class="math inline">\(c_i\)</span> <span class="math display">\[s_i = f(s_{i-1}, y_{i-1}, c_i)\]</span> <strong>语义向量<span class="math inline">\(c_i\)</span></strong></p><p>语义向量：分配权值<span class="math inline">\(a_{ij}\)</span>，Encoder的输出 <span class="math display">\[c_i  = \sum_{j=1}^{T_x} a_{ij} \cdot h_j, \quad h_j = h(x_j)\]</span> <strong>分配概率<span class="math inline">\(a_{ij}\)</span></strong></p><p>注意力分配概率<span class="math inline">\(a_{ij} ，\)</span> <span class="math inline">\(y_i\)</span> 收到<span class="math inline">\(x_j\)</span> 的注意力：分配能量<span class="math inline">\(e_{ij}\)</span> <span class="math display">\[a_{ij} = \frac{\exp(e_{ij})} {\sum_{k=1}^{T_x} \exp (e_{ik})}\]</span> <strong>分配能量<span class="math inline">\(e_{ij}\)</span></strong></p><p><span class="math inline">\(x_j\)</span> 注意<span class="math inline">\(y_i\)</span> 的能量，由encoder的隐状态<span class="math inline">\(h_j\)</span> 和 decoder的上一层的隐状态<span class="math inline">\(s_{i-1}\)</span> 计算而成。a函数就是一个线性层。也就是上面的F函数。 <span class="math display">\[e_{ij} = a(s_{i-1}, h_j)\]</span></p><h2 id="实现">实现</h2><p>Decoder由4层组成</p><ul><li>embedding : word2vec</li><li>attention layer: 为每个encoder的output计算Attention</li><li>RNN layer:</li><li>output layer:</li></ul><p>Decoder输入 <span class="math inline">\(s_{i-1}\)</span> , <span class="math inline">\(y_{i-1}\)</span> 和encoder的所有outputs <span class="math inline">\(h_*\)</span></p><p><strong>Embedding Layer</strong></p><p>输入<span class="math inline">\(y_{i-1}\)</span>，对其进行编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y(i-1)</span></span><br><span class="line">embedded = embedding(last_rnn_output)</span><br></pre></td></tr></table></figure><p><strong>Attention Layer</strong></p><p>输入<span class="math inline">\(s_{i-1}, h_j\)</span>，输出分配能量<span class="math inline">\(e_{ij}\)</span>， 计算出<span class="math inline">\(a_{ij}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attn_weights[j] = attn_layer(last_hidden, encoder_outputs[j])</span><br><span class="line">attn_weights = normalize(attn_weights)</span><br></pre></td></tr></table></figure><p><strong>计算语义向量</strong></p><p>求语义向量<span class="math inline">\(c_i\)</span>， 一般是加权求和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context = sum(attn_weights * encoder_outputs)</span><br></pre></td></tr></table></figure><p><strong>RNN Layer</strong></p><p>输入<span class="math inline">\(s_{i-1}, y_{i-1}, c_i\)</span> ，内部隐层状态，输出<span class="math inline">\(s_i\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rnn_input = concat(embeded, context)</span><br><span class="line">rnn_output, rnn_hidden = rnn(rnn_input, last_hidden)</span><br></pre></td></tr></table></figure><p><strong>输出层</strong></p><p>输入<span class="math inline">\(y_{i-1}, s_i, c_i\)</span> ，输出<span class="math inline">\(y_i\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = out(embedded, rnn_output, context)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;encoder-decoder&quot;&gt;Encoder-Decoder&lt;/h1&gt;
&lt;h2 id=&quot;基本介绍&quot;&gt;基本介绍&lt;/h2&gt;
&lt;p&gt;举个翻译的例子，原始句子&lt;span class=&quot;math inline&quot;&gt;\(X = (x_1, x_2, \cdots, x_
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="注意力" scheme="http://plmsmile.github.io/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="机器翻译" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
      <category term="Attention Model" scheme="http://plmsmile.github.io/tags/Attention-Model/"/>
    
      <category term="Encoder-Decoder" scheme="http://plmsmile.github.io/tags/Encoder-Decoder/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch快速上手</title>
    <link href="http://plmsmile.github.io/2017/10/05/pytorch-start/"/>
    <id>http://plmsmile.github.io/2017/10/05/pytorch-start/</id>
    <published>2017-10-05T05:30:54.000Z</published>
    <updated>2017-10-06T04:05:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pytorch介绍">PyTorch介绍</h1><p><img src="" style="display:block; margin:auto" width="60%"></p><h2 id="pytorch">PyTorch</h2><p><code>Torch</code> 是一个使用<code>Lua</code> 语言的神经网络库，而<code>PyTorch</code>是Torch在Python的衍生。</p><p>PyTorch是一个基于python的科学计算包。本质上是Numpy的代替者，支持GPU、带有高级功能，可以用来搭建和训练深度神经网络；是一个深度学习框架，速度更快，弹性更好。</p><h2 id="pytorch和tensorflow">PyTorch和Tensorflow</h2><p><code>Tensorflow</code> 类似一个嵌入Python的编程语言。写的Tensorflow代码会被Python编译成一张计算图，然后由TensorFlow执行引擎运行。Tensorflow有一些额外的概念需要学习，上手时间慢。</p><p>对比参考这篇文章<a href="http://www.zhuanzhi.ai/#/document/c690b0f77db65e79cb719eb90e576059" target="_blank" rel="noopener">PyTorch还是Tensorflow</a> 。下面是结论。后续再补充详细内容。</p><blockquote><p>PyTorch更有利于研究人员、爱好者、小规模项目等快速搞出原型，易于理解。而TensorFlow更适合大规模部署，特别是需要跨平台和嵌入式部署时。</p></blockquote><p>PyTorch和Tensorflow对比如下</p><table><thead><tr class="header"><th></th><th>PyTORCH</th><th>Tensorflow</th></tr></thead><tbody><tr class="odd"><td>动静态</td><td>建立的神经网络是动态的</td><td>建立静态计算图</td></tr><tr class="even"><td>代码难度</td><td>易于理解，好看一些。有弹性</td><td>底层代码难以看懂</td></tr><tr class="odd"><td>工业化</td><td>好上手</td><td>高度工业化</td></tr></tbody></table><h1 id="数据操作">数据操作</h1><h2 id="tensor">Tensor</h2><p><strong>创建Tensor</strong></p><p>Tensor实际上是一个数据矩阵，<strong>PyTorch处理的单位就是一个一个的Tensor</strong>。下面是一些创建方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 未初始化，都是0</span></span><br><span class="line">x = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 2. 随机初始化</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 3. 传递参数初始化</span></span><br><span class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 4. 通过Numpy初始化</span></span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line"><span class="comment"># 5. 获取size，返回一个tuple [5, 3]</span></span><br><span class="line"><span class="keyword">print</span> x.size()</span><br></pre></td></tr></table></figure><p>Tensor也可以通过Numpy来进行创建，或者从Tensor得到一个Numpy。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">c = b.numpy()</span><br></pre></td></tr></table></figure><p><strong>Tensor操作</strong></p><p>Tensor的运算也很简单，一般的四则运算都是支持的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 直接相加</span></span><br><span class="line">z = x + y</span><br><span class="line"><span class="comment"># 2. torch相加</span></span><br><span class="line">z = torch.add(x, y)</span><br><span class="line"><span class="comment"># 3. 传递参数返回结果</span></span><br><span class="line">result = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out = result)</span><br><span class="line"><span class="comment"># 4. 加到自身去，自身y会改变</span></span><br><span class="line">y.add_(x)</span><br></pre></td></tr></table></figure><p>其中所有类似于<code>x.add_(y)</code>的操作<strong>都会改变自己</strong>x，如<code>x.copy_(y)</code> 、<code>x.t_()</code> 。</p><p>对于Tensor可以像Numpy那样<strong>索引和切片</strong>。</p><p><strong>改变Tensor和Numpy</strong></p><p>改变Tensor后，对应的Numpy也会发生改变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line"><span class="comment"># 改变a</span></span><br><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># b也会改变</span></span><br><span class="line"><span class="keyword">print</span> a<span class="comment"># 22222</span></span><br><span class="line"><span class="keyword">print</span> b<span class="comment"># 22222</span></span><br></pre></td></tr></table></figure><p><strong>CUDA Tensors</strong></p><p>使用GPU很简单，只需使用<code>.cuda</code>就可以了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    x = x.cuda()</span><br><span class="line">    y = y.cuda()</span><br><span class="line">    x + y</span><br></pre></td></tr></table></figure><h2 id="variable">Variable</h2><p>在神经网络中，最重要的是<code>torch.autograd</code>这个包，而其中最重要的一个类就是<code>Variable</code>。</p><p>本质上Variable和Tensor没有区别，不过<strong>Variable会放入一个计算图</strong>，然后进行<strong>前向传播，反向传播和自动求导</strong>。这也是PyTorch和Numpy不同的地方。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/pytorch/Variable.png" style="display:block; margin:auto" width="30%"></p><p>Variable由<code>data</code>, <code>grad</code>, <code>creator</code> 三部分组成。</p><ul><li>data: 包装的Tensor，即数据</li><li>grad: 方向传播的梯度缓冲区</li><li>creator: 得到这个Variable的操作，如乘法加法等等。</li></ul><p>用一个Variable进行计算，返回的也是一个同类型的Variable。</p><p><strong>梯度计算例子</strong></p><p>线性计算<span class="math inline">\(z= 2 \cdot x + 3 \cdot y + 4\)</span> ，求<span class="math inline">\(\frac{ \partial z}{\partial x}\)</span> 和<span class="math inline">\(\frac{ \partial z}{\partial y}\)</span> 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="comment"># 1. 准备式子</span></span><br><span class="line"><span class="comment"># 默认求导是false</span></span><br><span class="line">x = Variable(torch.Tensor([<span class="number">2</span>]), requires_grad = <span class="keyword">True</span>)</span><br><span class="line">y = Variable(torch.Tensor([<span class="number">3</span>]), requires_grad = <span class="keyword">True</span>)</span><br><span class="line">z = <span class="number">2</span> * x + <span class="number">3</span> * y + <span class="number">4</span></span><br><span class="line"><span class="comment"># 2. z对x和y进行求导</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="comment"># 3. 获得z对x和y的导数</span></span><br><span class="line"><span class="keyword">print</span> x.grad.data<span class="comment"># 2</span></span><br><span class="line"><span class="keyword">print</span> y.grad.data       <span class="comment"># 3</span></span><br></pre></td></tr></table></figure><p>复杂计算$ y = x + 2$， <span class="math inline">\(z = y * y * 3\)</span>， <span class="math inline">\(o = avg(z)\)</span> ，求<span class="math inline">\(\frac{dz}{dx}\)</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.ones(<span class="number">2</span>, <span class="number">2</span>), requires_grad = <span class="keyword">True</span>)</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment"># d(out)/dx</span></span><br><span class="line"><span class="keyword">print</span> x.grad</span><br></pre></td></tr></table></figure><p>传递梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">2</span>]), requires_grad = <span class="keyword">True</span>)</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">gradients = torch.FloatTensor([<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>])</span><br><span class="line">y.backward(gradients)</span><br><span class="line"><span class="keyword">print</span> x.grad.data</span><br></pre></td></tr></table></figure><h2 id="一些常用的api总结">一些常用的API总结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 随机创建数字</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>)</span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 求平均值</span></span><br><span class="line">x.mean()</span><br><span class="line"><span class="comment"># 范数</span></span><br><span class="line">x.norm()</span><br><span class="line"><span class="comment"># torch view。数据相同，改变形状。得到一个Tensor</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">z = x.view(<span class="number">2</span>, <span class="number">2</span>, <span class="number">-1</span>)<span class="comment"># 最后-1，会自己适配</span></span><br></pre></td></tr></table></figure><h1 id="神经网络">神经网络</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;pytorch介绍&quot;&gt;PyTorch介绍&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;&quot; style=&quot;display:block; margin:auto&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;pytorch&quot;&gt;PyTorch&lt;/h2&gt;
&lt;p&gt;&lt;co
      
    
    </summary>
    
      <category term="PyTorch" scheme="http://plmsmile.github.io/categories/PyTorch/"/>
    
    
      <category term="神经网络" scheme="http://plmsmile.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="PyTorch" scheme="http://plmsmile.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>神经网络机器翻译</title>
    <link href="http://plmsmile.github.io/2017/10/02/NMT/"/>
    <id>http://plmsmile.github.io/2017/10/02/NMT/</id>
    <published>2017-10-02T02:03:31.000Z</published>
    <updated>2017-10-04T13:21:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在机器翻译、语音识别、文本摘要等领域中，<code>Sequence-to-sequence</code> 模型都取得了了非常好的效果。神经机器翻译(Neural Machine Translation, NMT) 使用seq2seq模型取得了巨大的成功。</p></blockquote><p>本文参考<a href="https://github.com/tensorflow/nmt" target="_blank" rel="noopener">谷歌NMT教程</a>。</p><h1 id="basic">Basic</h1><p><img src="" style="display:block; margin:auto" width="60%"></p><h2 id="背景知识">背景知识</h2><p>传统翻译是以词为核心一词一词翻译的，这样会切断句子本身的意思，翻译出来也很死板，不像我们人类说的话。</p><p><strong>Encoder-Decoder</strong></p><p>现在采用<code>Encoder-Decoder</code>架构模型。如下图</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/nmt/01_encdec.jpg" style="display:block; margin:auto" width="60%"></p><p>Encoder会利用整个原始句子生成一个<code>语义向量</code>，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句子的意思、句法结构、性别信息等等。具体框架可以参考<a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247484196&amp;idx=1&amp;sn=efa6b79d24b138ff79f33ec3426c79e2&amp;chksm=ebb43bf0dcc3b2e6e69ff3b7d7cabf28744e276ee08ccfcfc1dc2c3f0bf52a122c9bd77ff319&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Encoder-Decoder框架</a>。</p><p>举个翻译的例子，原始句子<span class="math inline">\(X = (x_1, x_2, \cdots, x_m)\)</span> ，翻译成目标句子<span class="math inline">\(Y = (y_1, y_2, \cdots, y_m)\)</span> 。</p><p>Encoder对<span class="math inline">\(X\)</span> 进行<strong>非线性变换</strong>得到中间语义<span class="math inline">\(C\)</span> <span class="math display">\[C = \Gamma(x_1, x_2, \cdots, x_n)\]</span> Decoder根据<strong>语义<span class="math inline">\(C\)</span> 和生成的历史信息</strong><span class="math inline">\(y_1, y_2, \cdots, y_{i-1}\)</span> 来生成第<span class="math inline">\(i\)</span> 个单词 <span class="math inline">\(y_i\)</span> <span class="math display">\[y_i = \Psi(C, y_1, y_2, \cdots, y_{i-1})\]</span> 当然，在<code>Attention Model</code> 中，Decoder生成Y的时候每个单词对应的<span class="math inline">\(C\)</span>不一样，记作<span class="math inline">\(C_j, j \in [1, n]\)</span> 。<span class="math inline">\(C_j\)</span> 就是体现了源语句子中不同的单词对目标句子中不同的单词的注意力概率分布。即各个单词的对齐的概率，也就是student对&quot;学生&quot;更重要，而对&quot;我&quot;不那么重要。这个在后续会用到。</p><p>Encoder-Decoder是个<strong>创新大杀器</strong>，是个通用的计算框架。Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN,RNN,BiRNN,GRU,LSTM, Deep LSTM。比如编码CNN-解码RNN, 编码BiRNN-解码Deep LSTM等等。</p><p>上面的内容<strong>任意组合</strong>，只要得到的<strong>效果好</strong>，就是一个<strong>创新</strong>，就可以毕业了。（当然别人没有提出过）</p><h2 id="nmt模型选择">NMT模型选择</h2><p>有3个维度需要选择。</p><ul><li>方向性。是单向还是双向</li><li>深度：是一层还是多层</li><li>网络选择：encoder和decoder具体分别选什么</li></ul><p>在本文的实现中，我们选择<strong>单向的、多层的、LSTM</strong>。基于<a href="https://github.com/lmthang/thesis" target="_blank" rel="noopener">这篇论文</a>。如下图。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/nmt/02_seq2seq.jpg" style="display:block; margin:auto" width="50%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;在机器翻译、语音识别、文本摘要等领域中，&lt;code&gt;Sequence-to-sequence&lt;/code&gt; 模型都取得了了非常好的效果。神经机器翻译(Neural Machine Translation, NMT) 使用seq2seq模型取得了巨
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="机器翻译" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>条件随机场</title>
    <link href="http://plmsmile.github.io/2017/09/28/crf/"/>
    <id>http://plmsmile.github.io/2017/09/28/crf/</id>
    <published>2017-09-28T03:17:59.000Z</published>
    <updated>2018-03-23T02:16:09.522Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>条件随机场(Conditional Random Field, CRF)是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型。常常用于<code>标注问题</code>。<code>隐马尔科夫模型</code>和<code>条件随机场</code>是自然语言处理中最重要的算法。CRF最重要的就是根据观测序列，把标记序列给推测出来。</p></blockquote><h1 id="概率无向图模型">概率无向图模型</h1><p>概率无向图模型又称为<code>马尔科夫随机场</code>，是一个可以<strong>由无向图表示的联合概率分布</strong>。<a href="https://plmsmile.github.io/2017/08/04/pgm-01/#概率图模型">一些类似内容</a>。</p><p>有一组随机变量<span class="math inline">\(Y \in \Gamma\)</span>，联合概率分布为<span class="math inline">\(P(Y)\)</span>，由图<span class="math inline">\(G=(V,E)\)</span>表示。节点v代表<strong>变量</strong><span class="math inline">\(Y_v\)</span>，节点之间的边代表两个变量的<strong>概率依赖关系</strong>。</p><p><img src="" style="display:block; margin:auto" width="60%"></p><h2 id="定义">定义</h2><p>马尔可夫性就是说，给定一些条件下，没有连接的节点之间是条件独立的。</p><p><strong>成对马尔可夫性</strong></p><p>设<span class="math inline">\(u\)</span>和<span class="math inline">\(v\)</span>是两个没有边连接的节点，其它所有节点为<span class="math inline">\(O\)</span>。<code>成对马尔可夫性</code>是说，给定随机变量组<span class="math inline">\(Y_O\)</span>的条件下，<strong>随机变量<span class="math inline">\(Y_u\)</span>和<span class="math inline">\(Y_v\)</span>是独立的</strong>。即有如下： <span class="math display">\[P(Y_u, Y_v \mid Y_O) = P(Y_u \mid Y_O)P(Y_v \mid Y_O)\]</span> <strong>局部马尔可夫性</strong></p><p>节点<span class="math inline">\(v\)</span>，<span class="math inline">\(W\)</span>是与<span class="math inline">\(v\)</span>连接的所有节点，<span class="math inline">\(O\)</span>是与<span class="math inline">\(v\)</span>没有连接的节点。<code>局部马尔可夫性</code>认为，给定<span class="math inline">\(Y_w\)</span>的条件下，<span class="math inline">\(Y_v\)</span>和<span class="math inline">\(Y_O\)</span>独立。即有： <span class="math display">\[P(Y_v, Y_O \mid Y_W) = P(Y_v \mid Y_W) P(Y_O \mid Y_W)\]</span> <strong>全局马尔可夫性</strong></p><p>节点集合<span class="math inline">\(A\)</span>，<span class="math inline">\(B\)</span>被中间节点集合<span class="math inline">\(C\)</span>分隔开，即不相连。<code>全局马尔可夫性</code>认为，给定<span class="math inline">\(Y_C\)</span>的条件下，<span class="math inline">\(Y_A\)</span>和<span class="math inline">\(Y_B\)</span>是独立的。即有： <span class="math display">\[P(Y_A, Y_B \mid Y_C) = P(Y_A \mid Y_C) P(Y_B \mid Y_C)\]</span> 上面的3个马尔可夫性的定义是等价的。</p><p><strong>概率无向图模型</strong></p><p>设有联合概率密度<span class="math inline">\(P(Y)\)</span>，由无向图<span class="math inline">\(G=(V,E)\)</span>表示。节点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率密度<span class="math inline">\(P(Y)\)</span>满足马尔可夫性，那么就称此联合概率分布为<code>概率图模型</code>，或<code>马尔可夫随机场</code>。</p><p>实际上我们更关心怎么求联合概率密度，<strong>一般是把整体的联合概率写成若干个子联合概率的乘积</strong>，即进行<code>因子分解</code>。概率无向图模型最大的优点就是易于因子分解。</p><h2 id="概率无向图因子分解">概率无向图因子分解</h2><p><strong>团与最大团</strong></p><p><code>团</code>：无向图中的一个子集，任何两个节点<strong>均有边连接</strong>。</p><p><code>最大团</code>：无向图中的一个子集，任何两个节点均有边连接。不能再加入一个节点组成更大的团了。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/crf/01_%E6%97%A0%E5%90%91%E5%9B%BE%E5%9B%A2.png" style="display:block; margin:auto" width="20%"></p><p>如<span class="math inline">\(\{Y_1, Y_2\}\)</span>，<span class="math inline">\(\{Y_1, Y_2, Y_3\}\)</span> 都是团，其中后者是最大团。而<span class="math inline">\(\{Y_1, Y_2, Y_3, Y_4\}\)</span> 不是团，因为<span class="math inline">\(Y_1\)</span>和<span class="math inline">\(Y_4\)</span>没有边连接。</p><p><strong>因子分解</strong></p><p>有无向图模型<span class="math inline">\(G\)</span>, <span class="math inline">\(C\)</span> 是 <span class="math inline">\(G\)</span> 上的最大团，有很多个。<span class="math inline">\(Y_C\)</span> 是<span class="math inline">\(C\)</span> 对应的随机变量。则<strong>联合概率分布<span class="math inline">\(P(Y)\)</span> 可以写成多个最大团<span class="math inline">\(C\)</span> 上的势函数的乘积</strong>。 <span class="math display">\[\color{blue}{P(Y)} = \frac {1} {Z} \prod_C \Psi_C(Y_C), \quad Z = \sum_Y \prod_C \Psi_C(Y_C)\]</span> 其中<span class="math inline">\(Z\)</span>是<code>规范化因子</code>。<span class="math inline">\(\Psi_C(Y_C)\)</span>是 <code>势函数</code>，是一个严格正函数。等式左右两端都取条件概率也是可以的。下文就是。 <span class="math display">\[\color{blue}{\Psi_C(Y_C)} = \exp \left(-E(Y_C) \right)\]</span> 其中<span class="math inline">\(\color{blue}{E(Y_C) }\)</span> 是<code>能量函数</code>。</p><h1 id="条件随机场的定义与形式">条件随机场的定义与形式</h1><h2 id="hmm的问题">HMM的问题</h2><p><a href="https://plmsmile.github.io/2017/08/04/pgm-01/#隐马尔可夫模型">这里是HMM的讲解</a> 。HMM有下面几个问题</p><ul><li>需要给出<strong>隐状态和观察符号的联合概率分布</strong>，即<code>发射概率</code> <span class="math inline">\(b_j(k)\)</span>，是生成式模型，也是它们的通病。</li><li>观察符号需要是<strong>离散的</strong>，可以枚举的，要<strong>遍历所有</strong>观察符号。如果是一个连续序列，则不行。</li><li>观察符号是独立的，<strong>没有观察相互之间的依赖关系</strong>。如一个句子的前后，都有关联才是。即<code>输出独立性假设问题</code>。</li><li>无法考虑除了字词顺序以外的<strong>其它特征</strong>。比如字母为大小写，包含数字等。</li><li><code>标注偏置问题</code>。</li></ul><p>标注偏置问题，举例，是说有两个单词&quot;rib-123&quot;和&quot;rob-456&quot;，&quot;ri&quot;应该标记为&quot;12&quot;，&quot;ro&quot;应该标记为&quot;45&quot;。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/crf/04_%E6%A0%87%E6%B3%A8%E5%81%8F%E7%BD%AE.png" style="display:block; margin:auto" width="50%"> <span class="math display">\[\begin {align}&amp; P(12 \mid ri) = P(1 \mid r)P(2 \mid i, r=1) = P(1 \mid r) \cdot 1 = P(1 \mid r)  \\ &amp; P(45 \mid ro) = P(4 \mid r)P(5 \mid o, r=4) = P(4 \mid r) \cdot 1 = P(4 \mid r) \\ \end {align}\]</span> 由上面计算概率可知，ri标为12和 ro标为45的概率最终变成r标为1和4的概率。但是由于语料库中&quot;rob&quot;的出现次数很多，所以<span class="math inline">\(P(4 \mid r) &gt; P(1 \mid r)\)</span> ，所以可能会一直把&quot;rib&quot;中的&quot;i&quot;标记为1，会导致标记出错。这就是标记偏置问题。</p><h2 id="定义-1">定义</h2><p><code>条件随机场</code>是给定随机变量<span class="math inline">\(X\)</span>条件下，随机变量<span class="math inline">\(Y\)</span> 的马尔可夫随机场。我们主要关心<code>线性链随机场</code>，它可以用于标注问题。</p><p><strong>条件随机场</strong></p><p><span class="math inline">\(X\)</span>与<span class="math inline">\(Y\)</span>是随机变量，条件概率分布<span class="math inline">\(P(Y \mid X)\)</span>。随机变量<span class="math inline">\(Y\)</span>可以构成一个无向图表示的马尔可夫随机场。任意一节点<span class="math inline">\(Y_v\)</span>，<span class="math inline">\(Y_A\)</span>是与<span class="math inline">\(v\)</span>相连接的节点，<span class="math inline">\(Y_B\)</span>是除了<span class="math inline">\(v\)</span>以外的所有节点。若都有 <span class="math display">\[P(Y_v \mid X, Y_B) = P(Y_v \mid X, Y_A)\]</span> 则<strong>称<span class="math inline">\(P(Y \mid X)\)</span> 为条件随机场</strong>。并不要求<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span> 具有相同的结构。</p><p><strong>线性链条件随机场</strong></p><p><span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span> 有相同的线性结构。设<span class="math inline">\(X = (X_1, X_2, \cdots, X_n)\)</span>，<span class="math inline">\(Y = (Y_1, Y_2, \cdots, Y_n)\)</span>均为线性链表示的随机变量序列。每个最大团包含2个节点。</p><p><span class="math inline">\(P(Y \mid X)\)</span> 构成条件随机场，即满足马尔可夫性 <span class="math display">\[P(Y_i \mid X, Y_1, \cdots, Y_{i-1}, Y_{i+1}, \cdots, Y_n) = P(Y_i \mid X, Y_{i-1}, Y_{i+1}), \quad i=1,\cdots, n。 \; (1和n时只考虑单边)\]</span> 则称<span class="math inline">\(P(Y \mid X)\)</span>为<code>线性链条件随机场</code>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/crf/02_%E7%BA%BF%E6%80%A7%E9%93%BE%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA.png" style="display:block; margin:auto" width="60%"></p><p>HMM，每个观察状态只与当前的隐状态有关系，分离了关系。就像1个字1个字地向后讲。输出观察符号还需要条件独立。</p><p>线性链条件随机场， 每个状态都与整个序列有关系。即先想好了整句话，再依照相应的次序去说出来。更加直击语言模型的核心。<span class="math inline">\(X_1, X_2\)</span> 不需要条件独立。</p><h2 id="基本形式">基本形式</h2><p><strong>两种特征函数</strong></p><p><code>状态转移特征函数t</code>，只依赖与当前和前一个位置，即<span class="math inline">\(y_i\)</span>和<span class="math inline">\(y_{i-1}\)</span>。一般是01函数。 <span class="math display">\[t(y_{i-1}, y_i, x, i) = \begin {cases}1, \quad &amp; 满足某种条件, i \in [2, n].  \;例如y_{i-1}+y_{i}=3 \\0, \quad &amp; 其他\end {cases}\]</span> <code>状态特征函数s</code>，只依赖与当前位置<span class="math inline">\(y_i\)</span> <span class="math display">\[s(y_i, x, i) = \begin {cases}1, \quad &amp; 满足某种条件, i \in [1, n].  \;例如y_{i}是偶数 \\0, \quad &amp; 其他\end {cases}\]</span> <strong>基础形式</strong></p><p>设有<span class="math inline">\(K_1\)</span>个状态特征转移函数，<span class="math inline">\(K_2\)</span>个状态特征函数。分别对应的权值是<span class="math inline">\(\lambda_{k_1}\)</span>和<span class="math inline">\(\mu_{k_2}\)</span>。则<code>线性链条件随机场参数化形式</code><span class="math inline">\(P(y \mid x)\)</span> 如下： <span class="math display">\[P(y \mid x) = \frac {1}{Z(x)}     \exp \left(    \sum_k^{K_1}\lambda_k \sum_{i=2}^n t_k(y_{i-1}, y_i, x, i)     +  \sum_k^{K_2}\mu_k \sum_{i=1}^n s_k(y_i, x, i)     \right)\]</span> 其中<span class="math inline">\(Z(x)\)</span>是<code>规范化因子</code>，如下 <span class="math display">\[Z(x) = \sum_x\exp \left(    \sum_k^{K_1}\lambda_k \sum_{i=2}^nt_k(y_{i-1}, y_i, x, i)     +  \sum_k^{K_2}\mu_k \sum_{i=1}^n s_k(y_i, x, i) \right)\]</span> <strong>条件随机场完全由特征函数<span class="math inline">\(t_{k_1}\)</span> 、<span class="math inline">\(s_{k_2}\)</span>，和对应的权值<span class="math inline">\(\lambda_{k_1}\)</span> 和<span class="math inline">\(\mu_{k_2}\)</span> 决定的。</strong> 特征函数实际上也是势函数。</p><h2 id="简化形式">简化形式</h2><p>有<span class="math inline">\(K=K_1 + K_2\)</span>个特征，特征函数如下： <span class="math display">\[f_k(y_{i-1}, y_i, i) = \begin {cases}t_k(y_{i-1}, y_i, x, i)  \quad &amp; k = 1,  \cdots, K_1 \\ s_k(y_i, x, i)  \quad &amp; k = K_1 + l; \; l = 1, \cdots, K_2   \\\end {cases}\]</span> 同一个特征函数，要在整个<span class="math inline">\(Y​\)</span>序列的各个位置进行计算，可以进行求和，即转化为<code>全局特征函数</code>， <strong>新的特征函数<span class="math inline">\(f_k (y, x)​\)</span></strong>如下： <span class="math display">\[\color{blue} {f_k (y, x)} = \sum _{i=1} ^n f_k(y_{i-1}, y_i, i), \quad k = 1, \cdots, K\]</span> <span class="math inline">\(f_k (y, x)\)</span> 对应的<strong>新的权值<span class="math inline">\(w_k\)</span></strong>如下 <span class="math display">\[\color{blue} {w_k} = \begin {cases}\lambda_{k}, \quad &amp; k = 1, \cdots, K_1 \\\mu_{k - K_1}, \quad &amp; k = K_1 + 1,  K_1 + 2, \cdots, K \\\end {cases}\]</span> 所以新的<strong>条件随机场形式</strong>如下： <span class="math display">\[P(y \mid x) =  \frac {1} {Z(x)} \exp \sum_{k=1} ^K w_k f_k(y, x), \quadZ(x) = \sum_y  \exp \sum_{k=1} ^K w_k f_k(y, x)\]</span> 可以看出，格式和<a href="https://plmsmile.github.io/2017/09/20/maxentmodel/#最大熵模型-1">最大熵模型</a>很像。条件随机场最重要的就是，<strong>根据观察序列，把标记序列给推测出来</strong>。</p><h2 id="向量形式">向量形式</h2><p>向量化特征函数和权值 <span class="math display">\[F(y, x) = (f_1(y, x), \cdots, f_K(y, x))^T, \quad w = (w_1, \cdots, w_K)^T\]</span> 可以写成向量内积的形式 <span class="math display">\[P_w (y \mid x) = \frac{1}{Z(x)}  \exp (w \cdot F(y, x)), \quad Z(x) = \sum_y \exp (w \cdot F(y, x))\]</span></p><h2 id="矩阵形式">矩阵形式</h2><p>为状态序列<span class="math inline">\(Y\)</span>设置起点和终点标记，<span class="math inline">\(y_0 = start\)</span> 和<span class="math inline">\(y_{n+1} = stop\)</span>。从<span class="math inline">\(0 \to n+1\)</span>中，<strong>有<span class="math inline">\(n+1\)</span>次的状态转移</strong>。我们可以用<span class="math inline">\(n+1\)</span> 个<code>状态转移矩阵</code>来表示状态转移的概率。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/crf/03_%E7%8A%B6%E6%80%81%E8%B7%AF%E5%BE%84.png" style="display:block; margin:auto" width="60%"></p><p>设<span class="math inline">\(\color{blue}{M_i(x)}\)</span> 是<span class="math inline">\(i-1 \to i\)</span>的转移矩阵，是<span class="math inline">\(m\)</span>阶，<span class="math inline">\(m\)</span> 是<span class="math inline">\(y_i\)</span> 取值的个数。表示了<strong>各种取值情况互相转化的概率</strong>。 <span class="math display">\[M_1(x) =   \begin{bmatrix}  a_{01} &amp; a_{02} \\  0 &amp; 0 \\  \end{bmatrix}  , \;  M_2(x) =   \begin{bmatrix}  b_{11} &amp; b_{12} \\  b_{21} &amp; b_{22} \\  \end{bmatrix}    , \;   M_3(x) =   \begin{bmatrix}  c_{11} &amp; c_{12} \\  c_{21} &amp; c_{22} \\  \end{bmatrix}    , \;  M_4(x) =   \begin{bmatrix}  1 &amp; 0 \\  1 &amp; 0 \\  \end{bmatrix}\]</span> 要求什么样的路径概率，则相乘相应的数值概率即可。那么这些矩阵里面的数值是怎么来的呢。有下面的<strong>矩阵定义</strong> ： <span class="math display">\[\color{blue} {g(y_{i-1}, y_i, x, i)} = M_i(y_{i-1}, y_i \mid x) =\exp \sum_{k=1}^K w_kf_k(y_{i-1}, y_i, i, x),\quad\color{blue} {M_i(x)}= [g(y_{i-1}, y_i, x, i) ]\]</span> 每一步转移的时候，由于<span class="math inline">\((y_{i-1}, y_i)\)</span> <strong>有<span class="math inline">\(m^2\)</span> 种情况，计算<span class="math inline">\(g\)</span>时会得到多个值，即可得到一个矩阵</strong>。</p><p>其中<span class="math inline">\(g(y_{i-1}, y_i, x, i)\)</span> 在计算的时候，<strong>会根据<span class="math inline">\(i\)</span> 的不同，而选择不同的特征函数进行计算</strong>。不要忘记了<span class="math inline">\(f_k\)</span>函数的定义。 <span class="math display">\[\color{blue} {f_k(y_{i-1}, y_i, i)} = \begin {cases}t_k(y_{i-1}, y_i, x, i)  \quad &amp; k = 1,  \cdots, K_1 \\ s_k(y_i, x, i)  \quad &amp; k = K_1 + l; \; l = 1, \cdots, K_2   \\\end {cases}\]</span> 所以非规范化条件概率可以通过矩阵的某些元素的乘积表示，有 <span class="math display">\[P_w( y \mid x) = \frac {1}{Z_w(x)} \prod_{i=1}^{n+1} M_i(y_{i-1}, y_i \mid x)\]</span> 其中<code>规范化因子</code> 是<strong><span class="math inline">\(n+1\)</span>的矩阵相乘</strong>的结果矩阵中，第<span class="math inline">\((start, stop)\)</span> 元素。例如第<span class="math inline">\((0, 0)\)</span>。其中是<span class="math inline">\((start, end)\)</span> 是<strong>矩阵下标对应</strong>。</p><h1 id="条件随机场的概率计算问题">条件随机场的概率计算问题</h1><p>主要问题是给定条件随机场<span class="math inline">\(P(Y \mid X)\)</span> ，给定输入序列<span class="math inline">\(x\)</span> 和输出序列<span class="math inline">\(y\)</span>，求<span class="math inline">\(P(Y_i = y_i \mid x)\)</span> 和<span class="math inline">\(P(Y_{i-1} = y_{i-1}, Y_i = y_{i} \mid X)\)</span>， 以及相应的期望问题。</p><p>关键是求这些特征函数期望值，当模型训练好之后，去验证我们的模型。</p><h2 id="前向后向算法">前向后向算法</h2><p><span class="math inline">\(y_i\)</span> 确定后， <span class="math inline">\(\color{blue} {\alpha_i(y_i \mid x) }\)</span> 表示，从<span class="math inline">\(start \to i\)</span>，就是$y = (start, y_1, , y_i) $ 的<strong>概率</strong>，也就是从前面到位置<span class="math inline">\(y_i\)</span> 的概率。特别地 <span class="math display">\[\alpha_0(y \mid x) = \begin{cases}1, \quad &amp; y=start  \\0, \quad &amp; 其他 \\\end{cases}\]</span> 而<span class="math inline">\(y_i​\)</span> 的取值有<span class="math inline">\(m​\)</span> 种， 所以<code>前向变量</code> <span class="math inline">\(\color{blue}{\alpha_i(x)}​\)</span> 到<span class="math inline">\(y​\)</span> 到第 <span class="math inline">\(i​\)</span> 个位置 的所有<strong>概率取值向量</strong>。 <span class="math display">\[\color{blue} {\alpha_i^T(x)}  = \alpha_{i-1}^T(x)  \cdot M_i(x),\quadi = 1,2,\cdots, n+1\]</span> <span class="math inline">\(y_i\)</span> 确定后， <span class="math inline">\(\color{blue} {\beta_i (y_i \mid x) }\)</span> 表示，位置<span class="math inline">\(i\)</span>的标记为<span class="math inline">\(y_i\)</span> ，并且后面为<span class="math inline">\(y_{i+1}, \cdots, y_n, stop\)</span> 的概率。同理一个是概率值。特别地 <span class="math display">\[\beta_{n+1}(y \mid x) = \begin{cases}1, \quad &amp; y=stop  \\0, \quad &amp; 其他 \\\end{cases}\]</span> 后向变量 <span class="math inline">\(\color{blue} {\beta_i (y \mid x) }​\)</span>，是一个m维向量 <span class="math display">\[\color{blue} {\beta_i (x) } = M_{i+1}(x) \cdot \beta_{i+1}(x)\]</span></p><p>可以得到<span class="math inline">\(Z(x)\)</span>： <span class="math display">\[Z(x) = \alpha_n^T(x) \cdot 1 = 1^T \cdot \beta_{i+1}(x)\]</span></p><h2 id="条件概率计算">条件概率计算</h2><p>位置是<span class="math inline">\(i\)</span> 标记<span class="math inline">\(y_i\)</span> 的条件概率<span class="math inline">\(P(Y_i = y_i \mid x)\)</span>是 <span class="math display">\[P(Y_i = y_i \mid x) = \frac {1}{Z(x)} \cdot  \alpha_i(y_i \mid x) \beta_i(y_i \mid x)\]</span> 位置<span class="math inline">\(i-1, i\)</span> 分别标记为<span class="math inline">\(y_{i-1}, y_i\)</span> 的概率是 <span class="math display">\[P(Y_{i-1} = y_{i-1}, Y_i = y_i \mid x) = \frac{1}{Z(x)} \cdot  \alpha_{i-1}(y_{i-1} \mid x)  M_i(y_{i-1}, y_i \mid x) \beta_i (y_i \mid x)\]</span></p><h2 id="特征期望值计算">特征期望值计算</h2><p>两个期望值和<a href="https://plmsmile.github.io/2017/09/20/maxentmodel/#约束条件等式">最大熵模型的约束条件等式</a> 有点像。</p><p>特征函数<span class="math inline">\(f_k\)</span> 关于条件概率分布<span class="math inline">\(P(Y \mid X)\)</span> 的概率 <span class="math display">\[E_{P(Y \mid X)}(f_k) = \sum_y P(y \mid x) f_k(y, x)= \sum_{i=1}^{n+1}\sum_{y_{i-1}y_i} f_k(y_{i-1}, y_i, x, i) P(y_{i-1}, y_i \mid x)\]</span> 特征函数<span class="math inline">\(f_k\)</span> 关于条件概率分布<span class="math inline">\(P(X, Y)\)</span> 的概率，<span class="math inline">\(\hat P(x)\)</span> 是经验分布 <span class="math display">\[E_{P(X, Y)}(f_k) = \sum_{x, y} P(x, y) \sum_{i=1}^{n+1}  f_k(y_{i-1}, y_i, x, i)= \sum_{x} \hat P(x) \sum_y P(y \mid x) \sum_{i=1}^{n+1}  f_k(y_{i-1}, y_i, x, i)\]</span> 通过前向和后向向量可以计算出两个概率，然后可以计算出相应的期望值。就可以与我们训练出的模型进行比较。</p><h1 id="学习算法">学习算法</h1><p>条件随机场模型实际上是定义在时序数据上的对数线性模型，学习方法有极大似然估计和正则化的极大似然估计。具体的优化实现算法有：改进的迭代尺度法IIS、梯度下降法和拟牛顿法。</p><h2 id="改进的迭代尺度法">改进的迭代尺度法</h2><p>这里是最大熵模型中的<a href="https://plmsmile.github.io/2017/09/20/maxentmodel/#改进的迭代尺度法">改进的迭代尺度算法</a>。每次更新一个<span class="math inline">\(\delta_i\)</span> 使得似然函数的该变量的下界增大，即似然函数增大。</p><p>已知经验分布<span class="math inline">\(\hat P(X, Y)\)</span>，和模型如下 <span class="math display">\[P(y \mid x) =  \frac {1} {Z(x)} \exp \sum_{k=1} ^K w_k f_k(y, x), \quadZ(x) = \sum_y  \exp \sum_{k=1} ^K w_k f_k(y, x)\]</span> 对数似然函数和<a href="https://plmsmile.github.io/2017/09/20/maxentmodel/#极大似然估计">最大熵算法的极大似然函数</a>很相似，如下：<br><span class="math display">\[L(w) = L_{\hat P}(P_w)= \log \prod_{x,y} P_w(y \mid x) ^ {\widetilde P(x, y)}= \sum_{x,y} \widetilde P(x, y) \log P_w(y \mid x)\]</span> 对数似然函数<span class="math inline">\(L(w)\)</span> <span class="math display">\[L(w) = \sum_{j=1}^{N} \sum_{k=1}^K w_k f_k(y_j, x_j) - \sum_{j=1}^N \log Z_w(x_j)\]</span> 数据<span class="math inline">\((x, y)\)</span> 中出现的<strong>特征总数<span class="math inline">\(T(x, y)\)</span></strong> ： <span class="math display">\[T(x, y) = \sum_k f_k(y, x) = \sum_{k=1}^K \sum_{i=1}^{n+1}f_k(y_{i-1}, y_i, x)\]</span> 输入：特征函数<span class="math inline">\(t_1, t_2, \cdots, t_{K_1}\)</span>和<span class="math inline">\(s_1, s_2, \cdots, s_{K_2}\)</span> ；经验分布<span class="math inline">\(\hat P(x, y)\)</span></p><p>输出：模型参数<span class="math inline">\(\hat w\)</span>，模型<span class="math inline">\(P_{\hat w}\)</span></p><p>步骤：</p><p>1 赋初值 <span class="math inline">\(w_k = 0\)</span></p><p>2 对所有<span class="math inline">\(k\)</span>，求解方程，解为<span class="math inline">\(\delta_k\)</span> <span class="math display">\[\begin{align}&amp; \sum_{x, y} \hat P(x) P(y \mid x) \sum_{i=1}^{n+1} t_k(y_{i-1}, y_i, x, i) \exp (\delta_k T(x, y)) = E_{\hat p}[t_k], \quad k = 1, 2, \cdots, K_1 时\\&amp; \sum_{x, y} \hat P(x) P(y \mid x) \sum_{i=1}^{n+1} s_l(y_{i-1}, y_i, x, i) \exp (\delta_k T(x, y)) = E_{\hat p}[s_l] , \quad k = K_1 + l,  l = 1, 2, \cdots, K_2 时\\\end{align}\]</span> 3 更新<span class="math inline">\(w_k + \delta_k \to w_k\)</span> ，如果还有<span class="math inline">\(w_k\)</span>未收敛，则继续2</p><p><strong>算法S</strong></p><p>对于不同的数据<span class="math inline">\((x, y)\)</span>的特征出现次数<span class="math inline">\(T(x, y)\)</span> 可能不同，可以<strong>选取一个尽量大的数<span class="math inline">\(S\)</span>作为特征总数</strong>，使得所有松弛特征<span class="math inline">\(s(x, y) \ge 0\)</span> ： <span class="math display">\[s(x, y) = S - \sum_{i=1}^{n+1}\sum_{k=1}^K f_k(y_{i-1}, y_i, x, i)\]</span> 所以可以直接解得<span class="math inline">\(\delta_k\)</span> ，当然<span class="math inline">\(f_k\)</span> 要分为<span class="math inline">\(t_k\)</span>和<span class="math inline">\(s_k\)</span>，对应的期望值计算也不一样。具体见书上。<br><span class="math display">\[\delta_k = \frac{1}{S} \log \frac{E_{ \hat P}[f_k] } {E_P[f_k]}\]</span> <strong>算法T</strong></p><p>算法S中<span class="math inline">\(S\)</span>会选择很大，导致每一步的迭代增量会加大，算法收敛会变慢，算法T重新选择一个<strong>特征总数 T(x)</strong> <span class="math display">\[T(x) = \max \limits_y T(x, y)\]</span> 使用前后向递推公式，可以算得<span class="math inline">\(T(x)=t\)</span> 。</p><p>对于<span class="math inline">\(k \in [1, K_1]\)</span>的<span class="math inline">\(t_k\)</span>关于经验分布的期望： <span class="math display">\[E_{\hat P}[t_k] = \sum_{t=0}^{T_{max}} a_{k,t} \beta_{k}^t\]</span> 其中，<span class="math inline">\(a_{k,t}\)</span>是<span class="math inline">\(t_k\)</span>的期待值， <span class="math inline">\(\delta_k = \log \beta_k\)</span></p><p>对于<span class="math inline">\(k \in [1+K_1, K]\)</span>的<span class="math inline">\(s_k\)</span>关于经验分布的期望： <span class="math display">\[E_{\hat P}[s_k] = \sum_{t=0}^{T_{max}} b_{k,t} \gamma_{k}^t\]</span> 其中<span class="math inline">\(\gamma_k^t\)</span>是特征<span class="math inline">\(s_k\)</span>的期望值，<span class="math inline">\(\delta_k = \log \gamma_k\)</span>。当然，求根也可以使用牛顿法去求解。</p><h2 id="拟牛顿法">拟牛顿法</h2><h1 id="预测算法">预测算法</h1><h2 id="预测问题">预测问题</h2><p>给定条件随机场<span class="math inline">\(P(Y \mid X)\)</span>和输入序列<span class="math inline">\(x\)</span>，求条件概率最大的输出序列（标记序列）<span class="math inline">\(y^*\)</span>，即对观测序列进行标注。 <span class="math display">\[\begin{align}y^* &amp; = \arg \max \limits_y P_w(y \mid x)   = \arg \max \limits_y \frac{\exp (w \cdot F(y, x))}{Z_w(x)}  \\&amp; =   \arg \max \limits_y ( w \cdot F(y, x)) \\\end {align}\]</span> 其中路径<span class="math inline">\(y\)</span>表示标记序列，下面是参数说明 <span class="math display">\[\begin {align}&amp; w = (w_1, w_2, \cdots, w_k)^T \\&amp; F(y, x) =  (f_1(y, x), \cdots, f_K(y, x))^T, \quad w = (w_1, \cdots, w_K)^T \\&amp; f_k (y, x) = \sum _{i=1} ^n f_k(y_{i-1}, y_i, x, i) \\&amp; F_i(y_{i-1}, y_i, x) = \left(f_1(y_{i-1}, y_i, x, i), f_2(y_{i-1}, y_i, x, i),\cdots, f_k(y_{i-1}, y_i, x, i) \right)^T\end {align}\]</span> 所以，为了求解最优路径，只需计算非规范化概率，即转换为下面的问题： <span class="math display">\[\max \limits_y \quad  \sum_{i=1}^n w \cdot F_i(y_{i-1}, y_i, x)\]</span></p><h2 id="维特比算法">维特比算法</h2><p><a href="https://plmsmile.github.io/2017/08/04/pgm-01/#维特比算法">HMM的维特比算法</a>。</p><p>维特比变量<span class="math inline">\(\delta_i(l)\)</span>，到达位置<span class="math inline">\(i\)</span>， 标记为<span class="math inline">\(l \in [1, m]\)</span> 的概率 <span class="math display">\[\delta_i(l) = \max \limits_{1 \le j \le m} \{  \delta_{i-1}(j) + w \cdot F_i(y_{i-1} = j, y_i = l, x) \}, \quad j = 1, 2, \cdots, m\]</span> 记忆路径<span class="math inline">\(\psi_i(l) = a\)</span> 当前时刻<span class="math inline">\(t\)</span>标记为l, <span class="math inline">\(t-1\)</span>时刻标记为a <span class="math display">\[\psi_i(l) = = \arg \max \limits_{1 \le j \le m} \{  \delta_{i-1}(j) + w \cdot F_i(y_{i-1} = j, y_i = l, x) \}\]</span> 算法主体</p><p>输入：特征向量<span class="math inline">\(F(y, x)\)</span>和权值向量<span class="math inline">\(\mathbf{w}\)</span>，观测向量<span class="math inline">\(x = (x_1, x_2, \cdots. x_n)\)</span></p><p>输出：最优路径<span class="math inline">\(y^* = (y_1^*, y_2^*, \cdots, y_n^*)\)</span></p><p>步骤如下</p><p>初始化 <span class="math display">\[\delta_1(j) = w \cdot F_1(y_0 = start, y_1 = j, x), \quad j = 1, \cdots, m\]</span> 递推 <span class="math display">\[\begin{align}&amp; \delta_i(l) = \max \limits_{1 \le j \le m} \{  \delta_{i-1}(j) + w \cdot F_i(y_{i-1} = j, y_i = l, x) \}, \quad j = 1, 2, \cdots, m \\&amp; \psi_i(l) =  \arg \max \limits_{1 \le j \le m} \delta_i(j), \quad \text{即上式的参数j} \\\end{align}\]</span> 终止 <span class="math display">\[\begin{align}&amp; \max \limits_y (w \cdot F(y, x)) = \max \limits_{1 \le j \le m} \delta_n(j) \\&amp;  y_n^* = \arg \max \limits_{1 \le j \le m} \delta_n(j) \\\end{align}\]</span> 返回路径 <span class="math display">\[y_i^* = \psi_{i+1} (y_{i+1}^*), \quad i = n-1, n-2, \cdots, 1\]</span> 求得最优路径<span class="math inline">\(y^* = (y_1^*, y_2^*, \cdots, y_n^*)\)</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;条件随机场(Conditional Random Field, CRF)是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型。常常用于&lt;code&gt;标注问题&lt;/code&gt;。&lt;code&gt;隐马尔科夫模型&lt;/code&gt;和&lt;code&gt;条件随机场
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="条件随机场" scheme="http://plmsmile.github.io/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>最大熵模型</title>
    <link href="http://plmsmile.github.io/2017/09/20/maxentmodel/"/>
    <id>http://plmsmile.github.io/2017/09/20/maxentmodel/</id>
    <published>2017-09-20T09:39:12.000Z</published>
    <updated>2017-10-30T03:20:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="最大熵原理">最大熵原理</h1><h2 id="预备知识">预备知识</h2><p>离散型变量<span class="math inline">\(X\)</span>的概率分布是<span class="math inline">\(\color{blue}{P(X)}\)</span>。它的<strong>熵</strong><span class="math inline">\(\color{blue}{H(X) \; or \; H(P)}\)</span>越大，代表<strong>越均匀、越混乱、越不确定</strong>。<a href="https://plmsmile.github.io/2017/07/31/nlp-notes/#信息量和信息熵">各种熵点这里</a> <span class="math display">\[\color{blue}{H(P)} =  \color{red} {- \sum_{x \in X}P(x) \log P(x)}\]</span> 熵满足下面不等式 <span class="math display">\[0 \le H(P) \le \log |X|, \quad 其中|X|是X的取值个数\]</span> 当前仅当<span class="math inline">\(X\)</span>的分布是均匀分布的时候等号成立。当<span class="math inline">\(X\)</span>服从均匀分布时，熵最大。</p><h2 id="最大熵的思想">最大熵的思想</h2><p>最大熵原理认为，学习概率模型时，在所有可能的概率模型中，<strong>熵最大的模型是最好的模型</strong>。</p><p>事情分为两个部分：确定的部分（约束条件）和不确定的部分。选择模型时要</p><ul><li>要满足所有的<strong>约束条件</strong>，即满足已有的确定的事实</li><li>要<strong>均分</strong>不确定的部分</li></ul><p><span class="math inline">\(X\)</span>有5个取值<span class="math inline">\(\{A, B,C,D,E\}\)</span>，取值概率分别为<span class="math inline">\(P(A), P(B), P(C), P(D), P(E)\)</span>。满足以下约束条件 <span class="math display">\[P(A)+ P(B)+ P(C)+ P(D)+ P(E) = 1\]</span> 满足这个条件的模型有很多。再加一个约束条件 <span class="math display">\[P(A) + P(B) = \frac{3}{10}\]</span> 则，<strong>满足约束条件</strong>，<strong>不确定的平分（熵最大）</strong>：这样的模型是最好的模型 <span class="math display">\[P(A) = P(B) = \frac{3}{20}, \quad P(C)=P(D)=P(E) = \frac{7}{30}\]</span> 即：<strong>约束条件，熵最大</strong></p><h1 id="最大熵模型">最大熵模型</h1><p>假设分类模型是一个条件概率分布<span class="math inline">\(\color{blue}{P(Y \mid X)}\)</span>。(有的不是选择条件模型，如论文里面)。训练数据集<span class="math inline">\(N\)</span>个样本 <span class="math inline">\(T = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}\)</span></p><h2 id="基本概念">基本概念</h2><ul><li>联合分布：<span class="math inline">\(\color{blue}{P(X, Y)}\)</span></li><li>边缘分布：<span class="math inline">\(\color{blue}{P(X)}\)</span></li><li>联合经验分布：<span class="math inline">\(\color{blue}{\widetilde{P}(X, Y)} = \color{red}{\frac {v(X=x, Y=y)}{N}}\)</span>，其中<span class="math inline">\(v(x, y)\)</span>为频数</li><li>联合边缘分布：<span class="math inline">\(\color{blue}{\widetilde P(X) = \color{red} {\frac{v(X=x)}{N}}}\)</span></li></ul><p><strong>特征函数</strong><span class="math inline">\(\color{blue}{f(x, y)}\)</span>用来描述<span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>满足的一个事实<strong>约束条件</strong>： <span class="math display">\[f(x, y) = \begin{cases}1, \quad &amp; x与y满足一个事实，即约束条件 \\0, \quad &amp; 否则\end{cases}\]</span> 如果有<span class="math inline">\(n\)</span>个特征函数<span class="math inline">\(\color{blue}{f_i(x, y), i = 1, 2, \cdots, n}\)</span>, 就有<span class="math inline">\(n\)</span>个约束条件。</p><h2 id="概率期望的计算">概率期望的计算</h2><p><strong><span class="math inline">\(X\)</span>的期望</strong></p><p><span class="math inline">\(X\)</span> 是随机变量，概率分布是<span class="math inline">\(P(X)\)</span> ，或概率密度函数是<span class="math inline">\(f(x)\)</span> <span class="math display">\[E(X) = \begin{cases}&amp;\sum_{i} x_i P(x_i), \quad &amp; \text{离散} \\&amp; \int_{-\infty}^{+\infty} {x \cdot f(x)} \, {\rm d}x  , \quad &amp; \text{连续} \\\end{cases}\]</span> 下面只考虑离散型的期望，连续型同理，求积分即可。</p><p><strong>一元函数的期望</strong></p><p><span class="math inline">\(Y = g(X)\)</span>，期望是 <span class="math display">\[E[Y] = E[g(X)] = \sum_{i}^{\infty} g(x_i) \cdot P(x_i)\]</span> <strong>二元函数的期望</strong></p><p><span class="math inline">\(Z = g(X, Y)\)</span> ，期望是 <span class="math display">\[E(Z) = \sum_{x, y} g(x, y) \cdot p(x, y) = \sum_{i=1} \sum_{j=1} g(x_i, y_j) p(x_i, y_j)\]</span> 期望其实就是<span class="math inline">\(E 狗 = \sum 狗 \cdot 老概率\)</span> 。可离散，可连续。</p><h2 id="约束条件等式">约束条件等式</h2><p><strong>实际分布期望</strong></p><p>特征函数<span class="math inline">\(f(x, y)\)</span>关于经验分布<span class="math inline">\(\color{blue}{\widetilde P(x, y)}\)</span>的期望<span class="math inline">\(\color{blue}{E_{\widetilde P}(f)}\)</span>，即<strong>实际应该有的特征</strong> ，也就是一个给模型加的<strong>约束条件</strong> ： <span class="math display">\[\color{blue}{E_{\widetilde P}(f)} = \sum_{x, y} \color{red} {\widetilde P(x,y)} f(x, y)\]</span> <strong>理论模型期望</strong></p><p>特征函数<span class="math inline">\(f(x, y)\)</span> 关于模型<span class="math inline">\(\color{blue}{P(Y\mid X)}\)</span>和经验分布<span class="math inline">\(\color{blue}{\widetilde P(X)}\)</span>的期望<span class="math inline">\(\color{blue}{E_{P}(f)}\)</span> ，即理论上模型学得后的期望： <span class="math display">\[\color{blue}{E_{ P}(f)}  =\sum_{x, y}  \color{red} { \widetilde{P}(x) P(y \mid x)}  f(x, y)\]</span> 要从训练数据中获取信息，特征函数关于实际经验分布和理论模型的两个期望就得相等，即<strong>理论模型要满足实际约束条件</strong> <span class="math display">\[E_{\widetilde P}(f) = E_{ P}(f)\]</span></p><h2 id="最大熵模型思想">最大熵模型思想</h2><p>条件概率分布<span class="math inline">\(P(Y \mid X)\)</span>的<strong>条件熵</strong>为<span class="math inline">\(\color{blue}{H(P)}\)</span>如下，<a href="https://plmsmile.github.io/2017/07/31/nlp-notes/#联合熵和条件熵">条件熵</a>： <span class="math display">\[\color{blue}{H(P)} = \color{red} {- \sum_{x, y} \widetilde P(x) P(y \mid x) \log P(y \mid x)}\]</span> 则满足约束条件<span class="math inline">\(\color{blue}{E_{\widetilde P}(f) = E_{ P}(f)}\)</span>的模型中，<strong>条件熵</strong><span class="math inline">\(\color{blue}{H(P)}\)</span><strong>最大的模型</strong>就是<code>最大熵模型</code>。</p><h1 id="最大熵模型的学习">最大熵模型的学习</h1><h2 id="学习问题">学习问题</h2><p>最大熵模型的学习等价于<strong>约束最优化问题</strong>，这类问题可以用<code>拉格朗日对偶性</code>去求解。</p><p>给定数据集<span class="math inline">\(T = \{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}\)</span>和<strong>特征函数</strong><span class="math inline">\(\color{blue}{f_i(x, y), i = 1, 2, \cdots, n}\)</span>。</p><p>要满足2个<strong>约束条件</strong> <span class="math display">\[\color{red} {E_{\widetilde P}(f) = E_{ P}(f), \quad \sum_{x, y}P(y \mid x) = 1 }\]</span> 要得到<strong>最大化熵</strong> <span class="math display">\[\max \limits_{P \in C} \; \color{blue}{H(P)} = \color{red} {- \sum_{x, y} \widetilde P(x) P(y \mid x) \log P(y \mid x)}\]</span> 按照最优化问题的习惯，<strong>将求最大值问题改写为等价的求最小值问题</strong> ，如下 <span class="math display">\[\min \limits_{P \in C} \; \color{blue}{ - H(P)} = \color{red} { \sum_{x, y} \widetilde P(x) P(y \mid x) \log P(y \mid x)}\]</span></p><h2 id="推导最大熵模型">推导最大熵模型</h2><p>一般使用<code>拉格朗日对偶性</code>去求解，可以见李航书附录。 引入拉格朗日乘子<span class="math inline">\(\color{blue}{w = (w_0, w_1, \cdots, w_n)}\)</span>，即参数向量，构造拉格朗日函数<span class="math inline">\(\color{blue}{L(P, w)}\)</span> ： <span class="math display">\[\color{blue}{L(P, w)} = \color{red}{-H(P) +w_0 \cdot \left( 1 - \sum_{x, y} P(y \mid x)\right) + \sum_{i=1}^n w_i \cdot \left(E_{\widetilde P}(f) - E_{ P}(f) \right)}\]</span> 由于是凸函数，根据相关性质，所以原始问题和对偶问题同解，<code>原始问题</code>如下： <span class="math display">\[\min \limits_{P \in C} \max \limits_{w} L(P, w)\]</span> 对应的<code>对偶问题</code>如下： <span class="math display">\[\max \limits_{w} \min \limits_{P \in C} L(P, w)\]</span> 主要思路是：先固定<span class="math inline">\(\color{blue}{w}\)</span>，去计算<span class="math inline">\(\color{blue} {\min \limits_{P \in C} L(P, w)}\)</span>，即去找到一个合适的<span class="math inline">\(\color{blue}{P(Y \mid X)}\)</span>。再去找到一个合适的<span class="math inline">\(\color{blue}{w}\)</span>。</p><p>第一步：求解<span class="math inline">\(\color{blue}{P}\)</span> 。设<code>对偶函数</code><span class="math inline">\(\color{blue} { \Psi (w)}\)</span>如下： <span class="math display">\[\color{blue} { \Psi (w)} = \color{red} {\min \limits_{P \in C} L(P, w) = L(P_w, w)}\]</span> 对偶函数的解，即我们找到的<span class="math inline">\(P(Y \mid X)\)</span>，记作<span class="math inline">\(\color{blue}{P_w}\)</span>，如下： <span class="math display">\[\color{blue}{P_w} = \arg \min \limits_{P \in C} L(P, w) =\color{red} {P_w(y \mid x)}\]</span> 用<span class="math inline">\({L(P, w)}\)</span>对<span class="math inline">\(P\)</span>进行求偏导，令偏导为0，可以解得<span class="math inline">\({P_w}\)</span>，即<code>最大熵模型</code> 如下： <span class="math display">\[\color{blue}{P_w(y \mid x)} = \color{red} {\frac{1}{Z_w(x)} \cdot \exp \left({\sum_{i=1}^nw_if_i(x, y)}\right) }, \;\color{blue}{Z_w(x)} = \color{red} {\sum_{y} \exp \left( \sum_{i=1}^{n} w_i f_i(x, y)\right) }\]</span> 其中<span class="math inline">\(\color{blue}{Z_w(x)}\)</span>是<code>归一化因子</code>，<span class="math inline">\(\color{blue} {f_i(x, y)}\)</span>是<code>特征函数</code>，<span class="math inline">\(\color{blue}{w_i}\)</span>是<code>特征的权值</code>，<span class="math inline">\(\color{blue}{P_w(y \mid x)}\)</span> 就是<code>最大熵模型</code>， <span class="math inline">\(\color{blue}{w}\)</span>是最大熵模型中的<code>参数向量</code>。</p><p>第二步：求解<span class="math inline">\(\color{blue}{w}\)</span>。即求<span class="math inline">\(w\)</span>去最大化对偶函数，设解为<span class="math inline">\(\color{blue} {w^*}\)</span> 。可以使用最优化算法去求极大化。 <span class="math display">\[\color{blue}{w^*} = \color{red} {\arg \max \limits_{w} \Psi(w)}\]</span> 最终，求到的<span class="math inline">\(\color{blue} {P^* = P_{w^*} = P_{w^*}(y \mid x)}\)</span>就是学习得到的<strong>最大熵模型</strong>。</p><h2 id="最大熵模型-1">最大熵模型</h2><p>最大熵模型如下，其中<span class="math inline">\(\color{blue}{Z_w(x)}\)</span>是<code>归一化因子</code>，<span class="math inline">\(\color{blue} {f_i(x, y)}\)</span>是<code>特征函数</code>，<span class="math inline">\(\color{blue}{w_i}\)</span>是<code>特征的权值</code> 。 <span class="math display">\[\color{blue}{P_w(y \mid x)} = \color{red} {\frac{1}{Z_w(x)} \cdot \exp \left({\sum_{i=1}^nw_if_i(x, y)}\right) }, \;\color{blue}{Z_w(x)} = \color{red} {\sum_{y} \exp \left( \sum_{i=1}^{n} w_i f_i(x, y)\right) }\]</span></p><h2 id="极大似然估计">极大似然估计</h2><p>其实对偶函数<span class="math inline">\(\color{blue}{\Psi(w)}\)</span>的极大化等价于最大熵模型的极大似然估计。</p><p>已知训练数据的经验分布<span class="math inline">\(\widetilde{P}(X, Y)\)</span>，条件概率分布的<span class="math inline">\(P(Y \mid X)\)</span>的<code>对数似然函数</code>是： <span class="math display">\[\begin{align*}\color{blue} {L_{\widetilde P}(P_w)} &amp; = \log \prod_{x, y} P(y \mid x) ^ {\widetilde{P}(X, Y)} = \sum_{x, y} \widetilde P(x, y) \log P(y \mid x)  \\&amp; = \color{red}{\sum_{x, y}\widetilde{P}(X, Y)  \sum_{i=1}^{n} w_i f_i(x, y) - \sum_{x} \widetilde{P}(X)\log Z_w(x) }\end{align*}\]</span> 可以证明得到，$L_{P}(P_w) = (w) $，极大似然函数等于对偶函数。</p><h1 id="模型学习的最优化算法">模型学习的最优化算法</h1><p><code>逻辑回归</code>、<code>最大熵模型</code>的学习都是<strong>以似然函数为目标函数的最优化问题</strong>，可以通过迭代算法求解。这个目标函数是个<strong>光滑的凸函数</strong>。通过很多方法都可以保证找到全局最优解，常用的有<code>改进的迭代尺度法</code>、<code>梯度下降法</code>、<code>牛顿法</code>或<code>拟牛顿法</code>，其中牛顿法和拟牛顿法一般收敛速度更快。</p><h2 id="改进的迭代尺度法">改进的迭代尺度法</h2><p>改进的迭代尺度法(improved iterative scaling, IIS)是一种最大熵模型学习的最优化算法。</p><p>已知最大熵模型如下： <span class="math display">\[\color{blue}{P_w(y \mid x)} = \color{red} {\frac{1}{Z_w(x)} \cdot \exp \left({\sum_{i=1}^nw_if_i(x, y)}\right) }, \;\color{blue}{Z_w(x)} = \color{red} {\sum_{y} \exp \left( \sum_{i=1}^{n} w_i f_i(x, y)\right) }\]</span> 对数似然函数如下： <span class="math display">\[\color{blue} {L_{\widetilde P}(P_w)} =  \color{red}{\sum_{x, y}\widetilde{P}(X, Y)  \sum_{i=1}^{n} w_i f_i(x, y) - \sum_{x} \widetilde{P}(X)\log Z_w(x) }\]</span> 目标是：通过极大似然估计学习模型参数，即求对数似然函数的<strong>极大值</strong><span class="math inline">\(\color{blue} {\hat w}\)</span>。</p><p><strong>基本思想</strong></p><p>当前参数向量<span class="math inline">\(w = (w_1, w_2, \cdots, w_n)^T\)</span>，找到一个新的参数向量<span class="math inline">\(w+\delta = (w_1 + \delta_1, w_2 + \delta_2, \cdots, w_n + \delta_n)\)</span>，使得每次更新都使似然函数值增大。</p><p>由于<span class="math inline">\(\delta\)</span>是一个向量，含有多个变量，不易同时优化。所以<code>IIS</code> <strong>每次只优化其中一个变量</strong><span class="math inline">\(\delta_i\)</span>，而固定其他变量<span class="math inline">\(\delta_j\)</span>。</p><p>设所有特征在<span class="math inline">\((x, y)\)</span>中的出现次数<span class="math inline">\(f^\#(x, y) = M\)</span> ： <span class="math display">\[\color{blue}{f^\# (x, y)} = \sum_i f_i(x, y)\]</span> 计算每次的改变量： <span class="math display">\[L(w+\delta) - L(w) \ge \color{blue}{B(\delta \mid w)}, \; 改变量的下界限\]</span> 如果找到适当的<span class="math inline">\(\delta\)</span>使得改变量的下界<span class="math inline">\(B(\delta \mid w)\)</span>提高，则对数似然函数也能提高。</p><p>计算<span class="math inline">\(B(\delta \mid w)\)</span>对<span class="math inline">\(\delta_i\)</span>求偏导，<strong>令偏导等于0，得如下方程</strong>： <span class="math display">\[\color{red} {\sum_{x, y} \widetilde P(x) P_w(y \mid x) f_i(x, y) \exp \left(\delta_i f^\#(x, y)\right) = E_{\widetilde p}(f_i) }, \quad 其中\, E_{\widetilde p}(f_i) = \sum_{x, y} \widetilde P(x, y)f_i(x, y)\]</span> 然后，<strong>依次对<span class="math inline">\(\delta_i​\)</span>求解该方程</strong>，就可以求得<span class="math inline">\(\delta​\)</span>，也就能够更新<span class="math inline">\(w​\)</span>，即<span class="math inline">\(w \to w+\delta​\)</span></p><p><strong>算法步骤</strong></p><p>输入：特征函数<span class="math inline">\(f_1, \cdots, f_n\)</span>；经验分布<span class="math inline">\(\widetilde P(x, y)\)</span>，模型<span class="math inline">\(P_w(y \mid x)\)</span></p><p>输出：最优参数值<span class="math inline">\(w_i^*\)</span>；最优模型<span class="math inline">\(P_{w^*}\)</span></p><ol style="list-style-type: decimal"><li><p>初始化参数，取初值 <span class="math inline">\(w_i = 0\)</span></p></li><li>求解方程 <span class="math inline">\(\delta_i\)</span> <span class="math display">\[\sum_{x, y} \widetilde P(x) P_w(y \mid x) f_i(x, y) \exp \left(\delta_i f^\#(x, y)\right) = E_{\widetilde p}(f_i),\]</span></li><li><p>更新参数 <span class="math inline">\(w_i + \delta_i \to w_i\)</span></p></li></ol><p>其中解方程的时候，如果特征出现次数<span class="math inline">\(f^\#(x, y)\)</span> 是常数<span class="math inline">\(M\)</span>，则可以<strong>直接计算</strong><span class="math inline">\(\delta_i\)</span> ： <span class="math display">\[\color{blue}{\delta_i } = \color{red} {\frac{1}{M} \log \frac{E_{\widetilde p}(f_i)}{E_{p}(f_i)} }\]</span> 如果<span class="math inline">\(f^\#(x, y)\)</span>不是常数，则必须通过数值计算<span class="math inline">\(\delta_i\)</span>。最简单就是通过<code>牛顿迭代法</code>去<strong>迭代求解</strong><span class="math inline">\(\delta_i^*\)</span>。以<span class="math inline">\(g(\delta_i) = 0\)</span> 表示该方程，进行如下迭代： <span class="math display">\[\color{blue} {\delta_i^{(k+1)}} = \color{red} {\delta_i^{(k)} - \frac{g(\delta_i^{(k)})}{g^\prime (\delta_i^{(k)})} }\]</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;最大熵原理&quot;&gt;最大熵原理&lt;/h1&gt;
&lt;h2 id=&quot;预备知识&quot;&gt;预备知识&lt;/h2&gt;
&lt;p&gt;离散型变量&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;的概率分布是&lt;span class=&quot;math inline&quot;&gt;\(\color{b
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="机器学习" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="各种熵" scheme="http://plmsmile.github.io/tags/%E5%90%84%E7%A7%8D%E7%86%B5/"/>
    
      <category term="最大熵模型" scheme="http://plmsmile.github.io/tags/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="IIS" scheme="http://plmsmile.github.io/tags/IIS/"/>
    
      <category term="期望" scheme="http://plmsmile.github.io/tags/%E6%9C%9F%E6%9C%9B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记</title>
    <link href="http://plmsmile.github.io/2017/08/20/ml-ng-notes/"/>
    <id>http://plmsmile.github.io/2017/08/20/ml-ng-notes/</id>
    <published>2017-08-20T13:38:54.000Z</published>
    <updated>2018-03-01T12:49:17.715Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性回归">线性回归</h1><p>有<span class="math inline">\(m\)</span>个样本<span class="math inline">\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})\)</span>，假设函数有2个参数<span class="math inline">\(\theta_0, \theta_1\)</span>，形式如下： <span class="math display">\[h_\theta(x) = \theta_0 + \theta_1x\]</span></p><h2 id="代价函数">代价函数</h2><p>代价函数 <span class="math display">\[\color{red} {J(\theta_0, \theta_1) = \frac {1}{2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2}\]</span> 目标是要找到合适的参数，去最小化代价函数<span class="math inline">\(min\, J(\theta_0, \theta_1)\)</span>。</p><p>假设<span class="math inline">\(\theta_0 = 0\)</span>，去描绘出<span class="math inline">\(J(\theta_1)\)</span>和<span class="math inline">\(\theta_1\)</span>的关系，如下面右图所示。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/costfunction1.png" style="display:block; margin:auto;" width="60%"></p><p>假设有3个样本<span class="math inline">\((1,1), (2,2), (3,3)\)</span>，图中选取了3个<span class="math inline">\(\theta_1 = 1, 0.5, 0\)</span>，其中<span class="math inline">\(J(\theta_1)\)</span>在<span class="math inline">\(\theta_1=1\)</span>时最小。</p><p>那么回到最初的两个参数<span class="math inline">\(h_\theta(x) = \theta_0 + \theta_1x\)</span>，如何去找<span class="math inline">\(min\, J(\theta_0, \theta_1)\)</span>呢？这里绘制一个等高图去表示代价函数，如下面右图所示，其中中间点是代价最小的。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/costfunction2.png" style="display:block; margin:auto" width="60%"></p><h2 id="梯度下降">梯度下降</h2><p><strong>基础说明</strong></p><p>上文已经定义了代价函数<span class="math inline">\(J(\theta_0, \theta_1)\)</span>，这里要使用<code>梯度下降算法</code>去最小化<span class="math inline">\(J(\theta_0, \theta_1)\)</span>，自动寻找出最合适的<span class="math inline">\(\theta\)</span>。梯度下降算法应用很广泛，很重要。大体步骤如下：</p><ul><li>设置初始值<span class="math inline">\(\theta_0, \theta_1\)</span></li><li>不停改变<span class="math inline">\(\theta_0, \theta_1\)</span>去减少<span class="math inline">\(J(\theta_0, \theta_1)\)</span></li></ul><p>当然选择不同的初始值，可能会得到不同的结果，得到局部最优解。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/gradientdescent3.png" style="display:block; margin:auto;" width="60%"></p><p>对于所有的参数<span class="math inline">\(\theta_j\)</span>进行<strong>同步</strong>更新，式子如下 <span class="math display">\[\color{red}{\theta_j = \theta_j - \underbrace{\alpha \cdot  \frac{\partial}{\partial_{\theta_j}} J(\theta_0, \theta_1)}_{学习率 \times 偏导}}\]</span> 上面公式中<span class="math inline">\(\color{blue}{\alpha}\)</span>是<code>学习率</code>(learning rate)，是指一次迈多大的步子，一次更新的幅度大小。</p><p>例如上面的两个参数，对于一次同步更新(梯度下降) <span class="math display">\[t_0 = \theta_0 - \alpha \frac{\partial}{\partial_{\theta_0}} J(\theta_0, \theta_1), t_1 = \theta_1 - \alpha \frac{\partial}{\partial_{\theta_1}} J(\theta_0, \theta_1) \quad \to\quad \theta_0 = t_0, \theta_1 = t_1\]</span> 也有异步更新(一般指别的算法) <span class="math display">\[t_0 = \theta_0 - \alpha \frac{\partial}{\partial_{\theta_0}} J(\theta_0, \theta_1),\theta_0 = t_0 \quad \to\quad t_1 = \theta_1 - \alpha \frac{\partial}{\partial_{\theta_1}} J(\theta_0, \theta_1),  \theta_1 = t_1\]</span> <strong>偏导和学习率</strong></p><p>这里先看一个参数的例子，即<span class="math inline">\(J(\theta_1)\)</span>。<span class="math inline">\(\theta_1 = \theta_1 - \alpha \frac{d}{dx}J(\theta_1)\)</span>。当<span class="math inline">\(\theta\)</span>从左右靠近中间值，导数值(偏导/斜率)分别是负、正，所以从左右两端都会靠近中间值。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/gradientdescent2.png" style="display:block; margin:auto" width="60%"></p><p>当学习率<span class="math inline">\(\alpha\)</span><strong>太小</strong>，梯度下降会<strong>很缓慢</strong>；<span class="math inline">\(\alpha\)</span><strong>太大</strong>，可能会<strong>错过最低点</strong>，导致无法收敛。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/costfunction3.png" style="display:block; margin:auto" width="60%"></p><p>当已经处于局部最优的时候，导数为0，并不会改变参数的值，如下图</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/costfunction4.png" style="display:block; margin:auto" width="60%"></p><p>当逐渐靠近局部最优的时候，梯度下降会<strong>自动采取小步子</strong>到达局部最优点。是因为越接近，导数会越来越小。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/costfunction5.png" style="display:block; margin:auto" width="40%"></p><p><strong>在线性回归上使用梯度下降</strong></p><p>代价函数 <span class="math display">\[\color{blue} {J(\theta_0, \theta_1)} = \frac {1}{2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2 = \color{red}{\frac{1}{2m} \sum_{i=1}^m (\theta_0 + \theta_1x^{(i)} -y^{(i)})^2}\]</span> 分别对<span class="math inline">\(\theta_0\)</span>和<span class="math inline">\(\theta_1\)</span>求偏导有 <span class="math display">\[\color{blue}{\frac{\partial}{\partial_{\theta_0}}J(\theta_0, \theta_1) }= \frac{1}{m} \sum_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}\right), \quad\color{blue}{\frac{\partial}{\partial_{\theta_1}} J(\theta_0, \theta_1)}= \frac{1}{m} \sum_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}\right)\cdot x^{(i)}\]</span> 那么使用梯度下降对<span class="math inline">\(\theta_0 和 \theta_1\)</span>进行更新，如下 <span class="math display">\[\theta_0 = \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}\right), \quad\theta_1 = \theta_1 - \alpha \frac{1}{m}  \sum_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}\right)\cdot x^{(i)}\]</span> 当前代价函数实际上是一个<a href="https://plmsmile.github.io/2017/08/13/em/#em算法证明">凸函数</a>，如下图所示。它只有全局最优，没有局部最优。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/costfucntion6.png" style="display:block; margin:auto" width="50%"></p><p>通过不断地改变参数减小代价函数<span class="math inline">\(\color{blue} {J(\theta_0, \theta_1)}\)</span>，逼近最优解，最终会得到<strong>一组比较好的参数</strong>，正好拟合了我们的训练数据，就可以进行新的值预测。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9C%80%E7%BB%88.png" style="display:block; margin:auto" width="60%"></p><h2 id="梯度下降技巧">梯度下降技巧</h2><p><strong>特征缩放</strong></p><p>不同的特征的单位的数值变化范围不一样，比如<span class="math inline">\(x_1 \in (0,2000), x_2 \in (1,5)\)</span>，这样会导致代价函数<span class="math inline">\(J(\theta)\)</span>特别的偏，椭圆。这样来进行<strong>梯度下降会特别的慢，会来回震荡</strong>。</p><p>所以<code>特征缩放</code>是<strong>把所有的特征缩放到相同的规模上</strong>。得到的<span class="math inline">\(J(\theta)\)</span>就会比较圆，梯度下降能<strong>很快地</strong>找到一条通往全局最小的捷径。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE.png" style="display:block; margin:auto" width="60%"></p><p>特征缩放的数据规模不能太小或者太大，如下面可以的规模是 <span class="math display">\[[-1, 1], [0, 3], [-2, 0.5], [-3, 3], [-\frac{1}{3}, \frac{1}{3}] 都是可以的。而[-100, 100], [-0.0001, 0.0001]是不可以的\]</span> 有一些常见的缩放方法</p><ul><li><span class="math inline">\(x_i = \frac{x_i - \mu}{max - min}\)</span>, <span class="math inline">\(x_i = \frac{x_i - \mu}{s}\)</span>，其中<span class="math inline">\(\mu\)</span>是均值，<span class="math inline">\(s\)</span>是标准差</li><li><span class="math inline">\(x_i = \frac{x_i - min} {max - min}\)</span></li><li><span class="math inline">\(x_i = \frac{x_i}{max}\)</span></li></ul><p><strong>学习率的选择</strong></p><p>当梯度下降正确运行的时候，每一次迭代<span class="math inline">\(J(\theta)\)</span>都会减少，但是减少到什么时候合适呢？当然最好的办法就是<strong>画图去观察</strong>，当然也可以设定减小的最小值来判断。下图中， 迭代次数到达400的时候就已经收敛。不同的算法，收敛次数不一样。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/%E5%AD%A6%E4%B9%A0%E7%8E%87%E6%94%B6%E6%95%9B.png" style="display:block; margin:auto" width="50%"></p><p>当图像呈现如下的形状，就需要使用更小的学习率。理论上讲，只要使用足够小的学习率，<span class="math inline">\(J(\theta)\)</span>每次都会减少。但是太小的话，梯度下降会太慢，难以收敛。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/%E9%80%89%E6%8B%A9%E5%B0%8F%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87.png" style="display:block; margin:auto" width="60%"></p><p>学习率总结</p><ul><li>学习率太小，慢收敛</li><li>学习率太大，<span class="math inline">\(J(\theta)\)</span>可能不会每次迭代都减小，甚至不会收敛</li><li>这样去选择学习率调试： <span class="math inline">\(\ldots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \ldots\)</span></li></ul><h2 id="多变量线性回归">多变量线性回归</h2><p>数据有<span class="math inline">\(n\)</span>个特征，如<span class="math inline">\(x^{(i)} = (1, x_1, x_2, \cdots, x_n)\)</span>，其中<span class="math inline">\(x_0 = 1\)</span>。则假设函数有<span class="math inline">\(n+1\)</span>个参数，形式如下 <span class="math display">\[\color{blue}{h_\theta(x)} = \theta^Tx = \theta_0  + \theta_1 x_1 +  \theta_2 x_2 + \cdots + \theta_n x_n\]</span> 代价函数 <span class="math display">\[\color{blue}{J(\theta)} = \frac{1}{2m} \sum_{i=1}^m\left( h_\theta(x^{(i)}) - y^{(i)}\right) ^ 2\]</span> 梯度下降，更新每个参数<span class="math inline">\(\theta_j\)</span> <span class="math display">\[\theta_j = \theta_j - \alpha \cdot \frac{\partial J(\theta)}{\partial_{\theta_j}} =\color{red}{ \theta_j -\alpha \cdot  \frac{1}{m} \sum_{i=1}^m \left(h_\theta(x^{(i)})-y^{(i)} \right) \cdot x_j^{(i)}}\]</span></p><h2 id="多项式回归">多项式回归</h2><p>有时候，线性回归并不能很好地拟合数据，所以我们需要<strong>曲线</strong>来适应我们的数据。比如一个二次方模型 <span class="math display">\[h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x^2_2\]</span></p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%E6%96%B0.png" style="display:block; margin:auto" width="60%"></p><p>当然可以用<span class="math inline">\(x_2 = x^2_2, x_3 = x_3^3\)</span>来转化为多变量线性回归。如果使用多项式回归，那么在梯度下降之前，就必须要使用特征缩放。</p><h2 id="正规方程">正规方程</h2><p>对于一些线性回归问题，使用正规方程方法求解参数<span class="math inline">\(\theta\)</span>，比用梯度下降更好些。代价函数如下 <span class="math display">\[\color{red} {J(\theta_0, \theta_1) = \frac {1}{2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2}\]</span> 正规方程的思想是函数<span class="math inline">\(J(\theta)\)</span>对每个<span class="math inline">\(\theta_j\)</span>求偏导令其等于0，就能得到所有的参数。即<span class="math inline">\(\frac{\partial J}{\partial \theta_j} = 0\)</span>。</p><p>那么设<span class="math inline">\(X_{m\times(n+1)}\)</span>为数据矩阵（其中包括<span class="math inline">\(x_0=1\)</span>），<span class="math inline">\(y\)</span>为标签向量。则通过如下方程可以求得<span class="math inline">\(\theta\)</span> <span class="math display">\[\theta = (X^TX)^{-1}X^Ty\]</span> 正规方程和梯度下降的比较</p><table><thead><tr class="header"><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr class="odd"><td>需要特征缩放</td><td>不需要特征缩放</td></tr><tr class="even"><td>需要选择学习率<span class="math inline">\(\alpha\)</span></td><td>不虚选择学习率</td></tr><tr class="odd"><td>需要多次迭代计算</td><td>一次运算出结果</td></tr><tr class="even"><td>特征数量<span class="math inline">\(n\)</span>很大时，依然适用</td><td><span class="math inline">\(n\)</span>太大，求矩阵逆运算代价太大，复杂度为<span class="math inline">\(O(n^3)\)</span>。<span class="math inline">\(n\leq10000\)</span>可以接受</td></tr><tr class="odd"><td>适用于各种模型</td><td>只适用于线性模型，不适合逻辑回归和其他模型</td></tr></tbody></table><h1 id="逻辑回归">逻辑回归</h1><p>线性回归有2个不好的问题：直线难以拟合很多数据；数据标签一般是<span class="math inline">\(0, 1\)</span>，但是<span class="math inline">\(h_\theta(x)\)</span>却可能远大于1或者远小于0。如下图。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/lr/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E4%B8%8D%E6%8B%9F%E5%90%88.png" style="display:block; margin:auto" width="60%"></p><h2 id="基本模型">基本模型</h2><p><code>逻辑回归</code>是一种分类算法，使得输出预测值永远在0和1之间，是使用最广泛的分类算法。模型如下 <span class="math display">\[h_\theta(x) = g(\theta^Tx), \quad g(z) = \frac{1}{1+e^{-z}}\]</span> <span class="math inline">\(g(z)\)</span>的图像如下，也称作<code>Sigmoid函数</code>或者<code>Logistic函数</code>，是S形函数。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/lr/sigmoid%E5%87%BD%E6%95%B0.png" style="display:block; margin:auto" width="50%"></p><p>将上面的公式整理后得到逻辑回归的模型 <span class="math display">\[\color{red}{h_\theta(x) = \frac {1}{1+e^{-\theta^Tx}}}, \quad 其中\;  \color{red}{0 \le h_\theta(x) \le 1}\]</span> 模型的意义是<strong>给出分类为1的概率</strong>，即<span class="math inline">\(h_\theta(x) = P(y=1\mid x; \theta)\)</span>。例如<span class="math inline">\(h_\theta(x)=0.7\)</span>，则分类为1的概率是0.7，分类为0的概率是<span class="math inline">\(1-0.7=0.3\)</span>。 <span class="math display">\[x的分类预测, y = \begin{cases}    1, \;  &amp; h_\theta(x) \ge 0.5, \;即\; \theta^Tx \ge 0\\    0, \; &amp; h_\theta(x) &lt; 0.5,  \; 即 \; \theta^Tx &lt; 0 \\\end{cases}\]</span></p><p><code>逻辑回归</code>就是要学到合适的<span class="math inline">\(\theta\)</span>，使得<strong>正例的特征远大于0</strong>，<strong>负例的特征远小于0</strong>。</p><h2 id="决策边界">决策边界</h2><p><strong>线性边界</strong></p><p>假设我们有一个模型<span class="math inline">\(h_\theta(x) = g(\theta_0 + \theta_1x_1 + \theta_2x_2)\)</span>，已经确定参数<span class="math inline">\(\theta = (-3, 1, 1)\)</span>，即模型<span class="math inline">\(h_\theta(x) = g(-3+x_1+x_2)\)</span>，数据和模型如下图所示</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/lr/01_%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C.png" style="display:block; margin:auto" width="60%"></p><p>由上可知，分类结果如下 <span class="math display">\[y =\begin{cases}1, \, &amp; x_1+ x_2 \ge 3 \\0, \, &amp; x_1 + x_2 &lt; 3 \\\end{cases}\]</span> 那么直线<span class="math inline">\(x_1+x_2=3\)</span>就称作模型的<code>决策边界</code>，将预测为1的区域和预测为0的区域分隔开来。gg</p><p><strong>非线性边界</strong></p><p>先看下面的数据</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/lr/02_%E5%9C%86%E5%BD%A2%E6%95%B0%E6%8D%AE.png" style="display:block; margin:auto" width="30%"></p><p>使用这样的模型去拟合数据 <span class="math display">\[h_\theta(x)  = g(\theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_1^2 + \theta_4x_2^2) , \; \theta = (-1, 0, 0, 1, 1), \; 即\, \color{red}{h_\theta(x) = g(-1+x_1^2 + x_2^2)}\]</span> 对于更复杂的情况，可以用更复杂的模型去拟合，如<span class="math inline">\(x_1x_2, x_1x_2^2\)</span>等</p><h2 id="代价函数和梯度下降">代价函数和梯度下降</h2><p>我们知道线性回归中的代价函数是<span class="math inline">\(J(\theta_0, \theta_1) = \frac {1}{2m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)}\right)^2\)</span>，但是由于逻辑回归的模型是<span class="math inline">\(\color{red}{h_\theta(x) = \frac {1}{1+e^{-\theta^Tx}}}\)</span>，所以代价函数关于<span class="math inline">\(\theta\)</span>的图像就是一个<code>非凸函数</code>，容易达到局部收敛，如下图左边所示。而右边，则是一个凸函数，有全局最小值。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/lr/03_%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png" style="display:block; margin:auto" width="50%"></p><p><strong>代价函数</strong></p><p>逻辑回归的<strong>代价函数</strong> <span class="math display">\[\rm{Cost}(h_\theta(x), y) = \color{red}{\begin{cases}-\log(h_\theta(x)),\; &amp; y=1 \\-\log(1-h_\theta(x)), \; &amp; y=0 \\\end{cases}}\]</span> 当实际上<span class="math inline">\(y=1\)</span>时，若预测为0，则代价会无穷大。当实际上<span class="math inline">\(y=0\)</span>时，若预测为1，则代价会无穷大。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/ml/lr/06_y01%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png" style="display:block; margin:auto" width="50%"></p><p>整理代价函数如下 <span class="math display">\[\rm{Cost}(h_\theta(x), y) =\color{red}{ -y \cdot \log(h_\theta(x)) - (1-y) \cdot \log(1- h_\theta(x))}\]</span> 得到所有的<span class="math inline">\(\color{red}{J(\theta)}\)</span> <span class="math display">\[J(\theta) = -\frac{1}{m} \sum_{i=1}^m\left(y^{(i)}\log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log(1-h_\theta(x^{(i)}))\right)\]</span> <strong>梯度下降</strong></p><p>逻辑回归的假设函数估计<span class="math inline">\(y=1\)</span>的概率 <span class="math inline">\(\color{red}{h_\theta(x) = \frac {1}{1+e^{-\theta^Tx}}}\)</span>。</p><p>代价函数<span class="math inline">\(\color{red}{J(\theta)}\)</span>，求参数<span class="math inline">\(\theta\)</span>去<span class="math inline">\(\color{red}{\min \limits_{\theta} J(\theta)}\)</span></p><p>对每个参数<span class="math inline">\(\theta_j\)</span>，依次更新参数 <span class="math display">\[\color{red} {\theta_j = \theta_j -\alpha \cdot  \frac{1}{m} \sum_{i=1}^m \left(h_\theta(x^{(i)})-y^{(i)} \right) \cdot x_j^{(i)}}\]</span> 逻辑回归虽然梯度下降的式子和线性回归看起来一样，但是实际上<span class="math inline">\(h_\theta(x)\)</span>和<span class="math inline">\(J(\theta)\)</span>都不一样，所以是不一样的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性回归&quot;&gt;线性回归&lt;/h1&gt;
&lt;p&gt;有&lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt;个样本&lt;span class=&quot;math inline&quot;&gt;\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \c
      
    
    </summary>
    
      <category term="机器学习" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://plmsmile.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="梯度下降" scheme="http://plmsmile.github.io/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>最大期望算法</title>
    <link href="http://plmsmile.github.io/2017/08/13/em/"/>
    <id>http://plmsmile.github.io/2017/08/13/em/</id>
    <published>2017-08-13T10:37:48.000Z</published>
    <updated>2017-09-22T03:23:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="em算法定义">EM算法定义</h1><h2 id="背景">背景</h2><p>如果概率模型的变量都是观测变量，那么可以直接使用<code>极大似然估计法</code>或<code>贝叶斯估计法</code>去估计模型参数。</p><p>如果模型既有观测变量又有隐变量，就不能简单使用上述方法。</p><p><code>EM</code>算法，期望极大算法，就是含有隐变量的概率模型参数的极大似然估计法或极大后验概率估计法，是一种迭代算法。每次迭代分为如下两步</p><ul><li>E步：求期望(expectation)</li><li>M步：求极大(maximization)</li></ul><h2 id="三硬币模型">三硬币模型</h2><p>有3枚硬币，记做ABC，每次出现正面的概率分别是<span class="math inline">\(\pi, p, q\)</span>。先掷A，正面选B，反面选C。再掷B/C，得到正面1或反面0作为一次的结果。问：去估计参数<span class="math inline">\(\theta = (\pi, p, q)\)</span></p><ul><li>观察变量：一次 实验得到的结果1或0，记做<span class="math inline">\(y\)</span></li><li>隐变量：A的结果，即掷的是B还是C，记做<span class="math inline">\(z\)</span></li></ul><p>对于一次实验，求出<span class="math inline">\(y\)</span>的概率分布 <span class="math display">\[\begin{align*}\color{blue}{P(y \mid \theta)} &amp;= \underbrace{\sum_zP(y, z \mid \theta)}_{\color{red}{把所有z的y加起来}}   = \sum_z \underbrace{P(z \mid \theta)P(y \mid z, \theta)}_{\color{red}{贝叶斯公式}}= \underbrace{\pi p^y(1-p)^{1-y}}_{\color{red}{z=1时}}+\underbrace{(1-\pi)q^y(1-q)^{1-y}}_{\color{red}{z=0时}}\end{align*}\]</span> 设观察序列<span class="math inline">\(Y=(y_1, y_2, \cdots, y_n)^T\)</span>，隐藏数据<span class="math inline">\(Z=(z_1, z_2, \cdots, z_n)^T\)</span>，则观测数据的似然函数为 <span class="math display">\[\begin{align*}\color{blue}{P(Y \mid \theta)} &amp;= \sum_Z \color{red}{P(Z \mid \theta)P(Y \mid Z, \theta)} \\&amp;= \prod_{i=1}^n[\pi p^{y_i}(1-p)^{1-y_i} + (1-\pi)q^{y_i}(1-q)^{1-y_i}]\end{align*}\]</span> 求模型参数<span class="math inline">\(\theta = (\pi, p, q)\)</span>的最大似然估计，即 <span class="math display">\[\hat\theta = arg max_\theta \log P(Y \mid \theta)\]</span> 这个问题不能直接求解，只有通过迭代的方法求解。EM算法就是解决这种问题的一种迭代算法。先给<span class="math inline">\(\theta^{(0)}\)</span> 选择初始值，然后去迭代。每次迭代分为E步和M步。</p><h2 id="em算法">EM算法</h2><p><strong>基本概念</strong></p><p>一些概念如下</p><ul><li><span class="math inline">\(Y\)</span> 观测变量，<span class="math inline">\(Z\)</span> 隐变量</li><li>不完全数据：<span class="math inline">\(Y\)</span>；概率分布：<span class="math inline">\(P(Y \mid \theta)\)</span>；<strong>对数似然函数</strong>：<span class="math inline">\(\color{red} {L(\theta) = \log P(Y \mid \theta)}\)</span></li><li>完全数据：<span class="math inline">\(Y\)</span>和<span class="math inline">\(Z\)</span>合在一起；概率分布：<span class="math inline">\(P(Y, Z \mid \theta)\)</span>；对数似然函数：<span class="math inline">\(\log P(Y, Z \mid \theta)\)</span></li></ul><p>EM算法通过迭代求<span class="math inline">\(L(\theta) = \log P(Y \mid \theta)\)</span>的极大似然估计。</p><p><strong>概率论函数的期望</strong></p><p>设<span class="math inline">\(Y\)</span>是随机变量<span class="math inline">\(X\)</span>的函数，<span class="math inline">\(Y = g(X)\)</span>，<span class="math inline">\(g\)</span>是连续函数，那么</p><p><span class="math inline">\(X\)</span>是离散型变量，<span class="math inline">\(X\)</span>的分布律为<span class="math inline">\(P(X = x_i) = p_i, \; i=1,2,3\cdots\)</span>，则有 <span class="math display">\[E(Y) = E(g(X)) = \sum_{i=1}^{\infty}g(x_i)p_i, \quad 左式收敛时成立\]</span> <span class="math inline">\(X\)</span>是 连续型变量，<span class="math inline">\(X\)</span>的概率密度为<span class="math inline">\(f(x)\)</span>，则有 <span class="math display">\[E(Y) = E(g(X)) = \int_{-\infty}^{+\infty} {g(x)f(x)} \, {\rm d}x, \quad 左式绝对收敛成立\]</span> <strong>Q函数</strong></p><p><span class="math inline">\(\color{blue}{Q(\theta, \theta^{(i)})}\)</span>是EM算法的核心。<strong>它是完全数据的对数似然函数<span class="math inline">\(\log P(Y,Z \mid \theta)\)</span>的期望</strong>，是关于未观测数据<span class="math inline">\(Z\)</span>的条件概率分布<span class="math inline">\(P(Z \mid Y, \theta^{(i)})\)</span>，而<span class="math inline">\(Z\)</span>的条件是在给定观测数据<span class="math inline">\(Y\)</span>和当前参数<span class="math inline">\(\theta^{(i)}\)</span>。（都是后置定语，不通顺） <span class="math display">\[\begin {align*}\color{blue}{Q(\theta, \theta^{(i)})} &amp;= E_Z[\log P(Y, Z \mid \theta) \mid Y, \theta^{(i)}] = \sum_Z \color{red}{\log P(Y, Z \mid \theta)P(Z \mid Y, \theta^{(i)})}\end {align*}\]</span> 下面是我具体的理解</p><ul><li><p><span class="math inline">\(\color{blue}{\theta^{(i)}}\)</span>是第<span class="math inline">\(i\)</span>次迭代参数<span class="math inline">\(\theta\)</span>的估计值</p></li><li><p><span class="math inline">\(P(Z \mid Y, \theta^{(i)})\)</span>是以<span class="math inline">\(Y\)</span>和当前参数<span class="math inline">\(\theta^{(i)}\)</span>的条件下的分布律，简写为<span class="math inline">\(P(Z)\)</span>。类似于上面的<span class="math inline">\(X\)</span></p></li><li><p><span class="math inline">\(P(Y, Z \mid \theta)\)</span> 是在以<span class="math inline">\(\theta\)</span>为参数的分布的联合概率密度，简写为<span class="math inline">\(P(Y, Z)\)</span>。类似于上面的<span class="math inline">\(Y=g(X)\)</span></p></li><li>求对数似然函数<span class="math inline">\(\log P(Y, Z)\)</span>的期望，转移到隐变量<span class="math inline">\(Z\)</span>上</li><li><strong>把目标函数映射到<span class="math inline">\(Z\)</span>上</strong>，<span class="math inline">\(g(z) = \log P(Y, Z)\)</span>，<span class="math inline">\(E(g(z)) = \sum_z \log P(Y, Z) P(Z)\)</span></li><li><p><span class="math inline">\(\color{blue}{Q(\theta, \theta^{(i)})}\)</span> 是因为要找到一个新的<span class="math inline">\(\theta\)</span>优于之前的<span class="math inline">\(\theta^{(i)}\)</span>，是代表的分布优于</p></li></ul><p><strong>EM算法步骤</strong></p><p>输入：<span class="math inline">\(\color{blue}{Y}\)</span> 观测变量，<span class="math inline">\(\color{blue}{Z}\)</span> 隐藏变量，<span class="math inline">\(\color{blue}{P(Y, Z \mid \theta)}\)</span> 联合分布，条件分布 <span class="math inline">\(\color{blue}{P(Z \mid Y, \theta)}\)</span></p><p>输出：模型参数<span class="math inline">\(\color{blue}{\theta}\)</span></p><p>步骤</p><ul><li>选择参数初始值<span class="math inline">\(\color{blue}{\theta^{(0)}}\)</span>，开始迭代</li><li>E步：第<span class="math inline">\(i+1\)</span>次迭代， 求 <span class="math inline">\(\color{blue}{Q(\theta, \theta^{(i)})} = \sum_Z \color{red}{\log P(Y, Z \mid \theta)P(Z \mid Y, \theta^{(i)})}\)</span></li><li>M步：<strong>求使<span class="math inline">\(Q(\theta, \theta^{(i)})\)</span>极大化的<span class="math inline">\(\theta\)</span></strong>，得到<span class="math inline">\(i+1\)</span>次迭代新的估计值<span class="math inline">\(\color{blue}{\theta^{(i+1)}} = \color{red}{arg \, max_\theta (Q(\theta, \theta^{(i)}))}\)</span></li><li>重复E和M步，直到收敛</li></ul><h2 id="jensen不等式">Jensen不等式</h2><p><strong>凸函数与凹函数</strong></p><p>从图像上讲，在函数上两点连接一条直线。直线完全在图像上面，就是<code>凸函数 convex</code>；完全在下面，就是<code>凹函数 concave</code>。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/pgm1/Convex_b.png" style="display:block; margin:auto" width="50%"></p><p><span class="math inline">\(f^{\prime\prime}(x) \ge 0 \implies f(x) 是凸函数; \quad f^{\prime\prime}(x) \le 0 \implies f(x)是凹函数\)</span></p><p><strong>Jensen不等式</strong></p><p>函数<span class="math inline">\(f(x)​\)</span>，上有两点<span class="math inline">\(x_1, x_2​\)</span>，对于任意<span class="math inline">\(\lambda \in [0,1]​\)</span></p><ul><li>如果<span class="math inline">\(f(x)\)</span>是凸函数，<span class="math inline">\(f(\lambda x_1+(1-\lambda)x_2) \le \lambda f(x_1) + (1-\lambda)f(x_2)\)</span></li><li>如果<span class="math inline">\(f(x)\)</span>是凹函数，<span class="math inline">\(f(\lambda x_1+(1-\lambda)x_2) \ge \lambda f(x_1) + (1-\lambda)f(x_2)\)</span></li></ul><p>一般地，<span class="math inline">\(n\)</span>个点<span class="math inline">\(x_1, x_2, \cdots, x_n\)</span>和参数<span class="math inline">\(\lambda_1 + \lambda_2 + \cdots + \lambda_n = 1\)</span>，对于凸函数，则有 <span class="math display">\[f(\underbrace{\lambda_1x_1 + \lambda_2x_2 + \cdots + \lambda_nx_n}_{\color{red}{E[X], 总体是f(E[X])}}) \le \underbrace{f(\lambda_1x_1) + f(\lambda_2x_2) + \cdots + f(\lambda_nx_n)}_{\color{red}{E[f(X)]}}, \;即\color{red}{f(\sum_{i=1}^n\lambda_ix_i) \leq \sum_{i=1}^nf(\lambda_ix_i)}\]</span> 琴声不等式</p><ul><li>凸函数 <span class="math inline">\(f(E[X]) \le E[f(X)]\)</span>，即<span class="math inline">\(\color{red}{f(\sum_{i=1}^n\lambda_ix_i) \leq \sum_{i=1}^nf(\lambda_ix_i)}\)</span></li><li>凹函数 <span class="math inline">\(f(E[X]) \ge E[f(X)]\)</span>，即<span class="math inline">\(\color{red}{f(\sum_{i=1}^n\lambda_ix_i) \geq \sum_{i=1}^nf(\lambda_ix_i)}\)</span></li></ul><p><img src="http://otafnwsmg.bkt.clouddn.com/image/dl/em/jensen.png" style="display:block; margin:auto" width="50%"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;em算法定义&quot;&gt;EM算法定义&lt;/h1&gt;
&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;如果概率模型的变量都是观测变量，那么可以直接使用&lt;code&gt;极大似然估计法&lt;/code&gt;或&lt;code&gt;贝叶斯估计法&lt;/code&gt;去估计模型参数。&lt;/p&gt;
&lt;p&gt;如果模型既有
      
    
    </summary>
    
      <category term="机器学习" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="最大期望算法" scheme="http://plmsmile.github.io/tags/%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95/"/>
    
      <category term="Jesen不等式" scheme="http://plmsmile.github.io/tags/Jesen%E4%B8%8D%E7%AD%89%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>马尔可夫模型</title>
    <link href="http://plmsmile.github.io/2017/08/04/pgm-01/"/>
    <id>http://plmsmile.github.io/2017/08/04/pgm-01/</id>
    <published>2017-08-04T03:06:41.000Z</published>
    <updated>2018-03-21T01:30:51.723Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述">概述</h1><h2 id="产生式和判别式">产生式和判别式</h2><blockquote><p><strong>判别方法</strong> 由数据直接去学习决策函数<span class="math inline">\(Y=f(X)\)</span> 或者<span class="math inline">\(P(Y \mid X)\)</span>作为预测模型 ，即<code>判别模型</code></p></blockquote><blockquote><p><strong>生成方法</strong> 先求出联合概率密度<span class="math inline">\(P(X, Y)\)</span>，然后求出条件概率密度<span class="math inline">\(P(Y \mid X)\)</span>。即<code>生成模型</code><span class="math inline">\(P(Y \mid X) = \frac {P(X, Y)} {P(X)}\)</span></p></blockquote><table><thead><tr class="header"><th></th><th>判别式</th><th>生成式</th></tr></thead><tbody><tr class="odd"><td>原理</td><td>直接求<span class="math inline">\(Y=f(X)\)</span> 或<span class="math inline">\(P(Y \mid X)\)</span></td><td>先求<span class="math inline">\(P(X,Y)\)</span>，然后 <span class="math inline">\(P(Y \mid X) = \frac {P(X, Y)} {P(X)}\)</span></td></tr><tr class="even"><td>差别</td><td>只关心差别，根据差别分类</td><td>关心数据怎么生成的，然后进行分类</td></tr><tr class="odd"><td>应用</td><td>k近邻、感知机、决策树、LR、SVM</td><td>朴素贝叶斯、隐马尔可夫模型</td></tr></tbody></table><h2 id="概率图模型">概率图模型</h2><blockquote><p><strong>概率图模型(probabilistic graphical models)</strong> 在概率模型的基础上，使用了基于图的方法来表示概率分布。节点表示变量，边表示变量之间的概率关系</p></blockquote><p>概率图模型便于理解、降低参数、简化计算，在下文的贝叶斯网络中会进行说明。</p><h1 id="贝叶斯网络">贝叶斯网络</h1><blockquote><p><strong>贝叶斯网络</strong> 又称为信度网络或者信念网络（belief networks），实际上就是一个有向无环图。</p></blockquote><p>节点表示随机变量；边表示条件依存关系。没有边说明两个变量在某些情况下条件独立或者说是计算独立，有边说明任何条件下都不条件独立。</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/pgm/pgm1.png" style="display:block; margin:auto" width="60%"></p><p>如上图所示，要表示上述情况的概率只需要求出<span class="math inline">\(4*2*2*2*2-1=63\)</span>个参数的联合概率密度就行了，实际上这个太难以求得。我们可以考虑一下独立关系<span class="math inline">\((F \perp H \mid S) \,\,\, 表示在S确定的情况下，F和H独立\)</span>，所以有以下独立关系： <span class="math display">\[(F \perp H \mid S)、\, (C \perp S \mid F,H)、\, (M \perp H, C \mid F)、 \, (M \perp C | F)\]</span> 所以我们得到如下的<strong>计算独立假设</strong>： <span class="math display">\[P(C \mid FHS) = P(C \mid FH)，即假设C只与FH有关，而与S无关\]</span> 又由<span class="math inline">\(P(AB)=P(A|B)P(B)\)</span>，所以得到联合概率分布： <span class="math display">\[\begin{align*}P(SFHMC) &amp;= P(M \mid SHFC) \cdot P(SHFC) = \underbrace {P(M \mid F)}_{\color {red}{计算独立性}} \cdot \underbrace {P(C \mid SHF) \cdot P(SHF)}_{\color{red}{继续分解}} \\&amp;=  P(M \mid F) \cdot P(C \mid FH) \cdot P(F \mid S) \cdot P(H \mid S) \cdot  P(S)\end{align*}\]</span> <span class="math inline">\(P(S)\)</span> 4个季节，需要3个参数；<span class="math inline">\(P(H \mid S)\)</span>时，<span class="math inline">\(P(Y \mid Spring)\)</span> 和 <span class="math inline">\(P(N \mid Spring)\)</span>只需要一个参数，所以<span class="math inline">\(P(H \mid S)\)</span>只需要4个参数即可，其他同理。</p><p>所以联合概率密度就转化成了上述公式中的5个乘积项，其中每一项需要的参数个数分别是2、4、4、4、3，所以一共只需要17个参数，这就大大降低了参数的个数。</p><h1 id="马尔可夫模型">马尔可夫模型</h1><h2 id="简介">简介</h2><p>马尔可夫模型(Markov Model) 描述了一类重要的随机过程，未来只依赖于现在，不依赖于过去。这样的特性的称为<code>马尔可夫性</code>，具有这样特性的过程称为<code>马尔可夫过程</code>。</p><p>时间和状态都是<strong>离散的</strong>马尔可夫过程称为<code>马尔可夫链</code>，简称马氏链，关键定义如下</p><ul><li>系统有<span class="math inline">\(N\)</span>个状态<span class="math inline">\(S = \{ s_1, s_2, \cdots, s_N\}\)</span>，随着时间的推移，系统将从某一状态转移到另一状态</li><li>设<span class="math inline">\(q_t \in S\)</span>是系统在<span class="math inline">\(t\)</span>时刻的状态，<span class="math inline">\(Q = \{q_q, q_2, \cdots, q_T \}\)</span>系统时间的随机变量序列</li></ul><p>一般地，系统在时间<span class="math inline">\(t\)</span>时的状态<span class="math inline">\(s_j\)</span>取决于<span class="math inline">\([1, t-1]\)</span>的所有状态<span class="math inline">\(\{q_1, q_2, \cdots, q_{t-1}\}\)</span>，则当前时间的概率是 <span class="math display">\[P(q_t = s_j \mid q_{t-1} = s_i, q_{t-2} = s_k, \cdots)\]</span></p><p>在时刻<span class="math inline">\(m\)</span>处于<span class="math inline">\(s_i\)</span>状态，那么在时刻<span class="math inline">\(m+n\)</span>转移到状态<span class="math inline">\(s_j\)</span>的概率称为<code>转移概率</code>，即从时刻<span class="math inline">\(m \to m+n\)</span>： <span class="math display">\[\color{blue} {P_{ij}(m, m+n)} = P(q_{m+n} = s_j \mid q_m = s_i)\]</span></p><p>如果<span class="math inline">\(P_{ij}(m, m+n)\)</span>只与状态<span class="math inline">\(i, j\)</span>和步长<span class="math inline">\(n\)</span>有关，而与起始时间<span class="math inline">\(m\)</span>无关，则记为<span class="math inline">\(\color {blue} {P_{ij}(n)}\)</span>,称为<code>n步转移概率</code>。 并且称此转移概率具有<code>平稳性</code>，且称此链是<code>齐次</code>的，称为齐次马氏链，我们重点研究齐次马氏链。<span class="math inline">\(P(n) = [P_{ij}(n)]\)</span>称为<code>n步转移矩阵</code>。 <span class="math display">\[P_{ij}(m, m+n) =\color {blue} {P_{ij}(n)} = P(q_{m+n} = s_j \mid q_m = s_i)\]</span> 特别地，<span class="math inline">\(n = 1\)</span>时，有<code>一步转移概率</code>如下 <span class="math display">\[p_{ij}  = P_{ij}(1)  = P(q_{m+1} \mid q_{m}) = a_{ij}\]</span></p><h2 id="一阶马尔可夫">一阶马尔可夫</h2><p>特别地，如果<strong><span class="math inline">\(t\)</span>时刻状态只与<span class="math inline">\(t-1\)</span>时刻状态有关</strong>，那么下有<code>离散的一阶马尔可夫链</code>如下： <span class="math display">\[P(q_t = s_j \mid q_{t-1} = s_i, q_{t-2} = s_k, \cdots) = P(q_t = s_j\mid q_{t-1} = s_i)\]</span> 其中<span class="math inline">\(t-1​\)</span>的状态<span class="math inline">\(s_i​\)</span>转移到<span class="math inline">\(t​\)</span>的状态<span class="math inline">\(s_j​\)</span>的概率定义如下： <span class="math display">\[P(q_t = s_j\mid q_{t-1} = s_i) = \color{blue} {a_{ij}}，其中i, j \in [1, N]，a_{ij} \ge 0，\sum_{j=1}^Na_{ij} = 1\]</span> 显然，<span class="math inline">\(N​\)</span>个状态的一阶马尔可夫链有<span class="math inline">\(N^2​\)</span>次状态转移，这些概率<span class="math inline">\(a_{ij}​\)</span>构成了<code>状态转移矩阵</code>。 <span class="math display">\[A = [a_{ij}] = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn} \\\end{bmatrix}\]</span> 设系统在<strong>初始状态的概率向量</strong>是 <span class="math inline">\(\color{blue} {\pi_i} \ge 0\)</span> ，其中，<span class="math inline">\(\sum_{i=1}^{N}\pi_i = 1\)</span></p><p>那么<strong>时间序列</strong><span class="math inline">\(Q = \{q_1, q_2, \cdots, q_T \}\)</span>出现的概率是 <span class="math display">\[\color{blue} {P(q_1, q_2, \cdots, q_T) }= P(q_1) P(q2 \mid q_1) P(q_3 \mid q_2) \cdots P(q_T \mid q_{T-1})= \color{red} {\underbrace {\pi_{q_1}}_{初态概率} \prod_{t=1}^{T-1} a_{q_tq_{t+1}}}\]</span> 下图是一个例子</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/pgm/markov01.gif" style="display:block; margin:auto" width="50%"></p><h2 id="多步转移概率">多步转移概率</h2><p>对于齐次马氏链，多步转移概率就是<span class="math inline">\(u+v\)</span>时间段的状态转移，可以分解为先转移<span class="math inline">\(u\)</span>步，再转移<span class="math inline">\(v\)</span>步。则有<code>CK方程</code>的矩阵形式 <span class="math display">\[P(u+v) = P(u)P(v)\]</span> 由此得到<span class="math inline">\(n\)</span>步转移概率矩阵是一次转移概率矩阵的<span class="math inline">\(n\)</span>次方 <span class="math display">\[P(n) = P(1) P(n-1) = PP(n-1) \implies P(n) = P^n\]</span> 对于求矩阵的幂<span class="math inline">\(A^n\)</span>，则最好使用<code>相似对角化</code>来进行矩阵连乘。</p><p>存在一个可逆矩阵P，使得<span class="math inline">\(P^{-1}AP = \Lambda，A = P \Lambda P^{-1}\)</span>，其中<span class="math inline">\(\Lambda\)</span>是矩阵<span class="math inline">\(A\)</span>的特征值矩阵 <span class="math display">\[\Lambda = \begin{bmatrix} \lambda_1 &amp;  &amp; &amp;  \\ &amp;\lambda_2 &amp;  &amp;  \\ &amp;&amp; \ddots &amp;  \\ &amp;  &amp;  &amp; \lambda_n \\\end{bmatrix}，其中\lambda是矩阵A的特征值\]</span> 则有<span class="math inline">\(A^n = P\Lambda ^ {n}P^{-1}\)</span></p><h2 id="遍历性">遍历性</h2><p>齐次马氏链，状态<span class="math inline">\(i\)</span>向状态<span class="math inline">\(j\)</span>转移，经过无穷步，<strong>任何状态<span class="math inline">\(s_i\)</span>经过无穷步转移到状态<span class="math inline">\(s_j\)</span>的概率收敛于一个定值<span class="math inline">\(\pi_j\)</span></strong>，即<span class="math inline">\(\lim_{n \to \infty} P_{ij}(n) = \pi_j \; (与i无关)\)</span> 则称此链具有<code>遍历性</code>。若<span class="math inline">\(\sum_{j=1}^N \pi_j = 1\)</span>，则称<span class="math inline">\(\vec{\pi} = (\pi_1, \pi_2, \cdots)\)</span>为链的<code>极限分布</code>。</p><p>遍历性的充分条件：如果存在正整数<span class="math inline">\(m\)</span>(步数)，使得对于任意的，都有如下（<strong>转移概率大于0</strong>），则该马氏链<strong>具有遍历性</strong> <span class="math display">\[P_{ij}(m) &gt; 0, \quad i, j =1, 2, \cdots, N, \quad s_i,s_j \in S  \;\]</span> 那么它的极限分布<span class="math inline">\(\vec{\pi} = (\pi_1, \pi_2, \cdots, \pi_N)​\)</span>，它是下面方程组的唯一解 <span class="math display">\[\pi = \pi P, \quad 即\pi_j = \sum_{i=1}^{N} \pi_i p_{ij}, \quad 其中\pi_j &gt; 0, \sum_{j=1}^N \pi_j = 1\]</span></p><h2 id="pagerank应用">PageRank应用</h2><p>有很多应用，压缩算法、排队论等统计建模、语音识别、基因预测、搜索引擎鉴别网页质量-PR值。</p><p><strong>Page Rank算法</strong></p><p>这是Google最核心的算法，用于给每个网页价值评分，是Google“在垃圾中找黄金”的关键算法。</p><p>大致思想是要为搜索引擎返回最相关的页面。<code>页面相关度</code>是由和当前网页相关的一些页面决定的。</p><ul><li><p>当前页面会<strong>把自己的<code>importance</code>平均传递给它所指向的页面</strong>，若有<span class="math inline">\(k\)</span>个，则为每个传递<span class="math inline">\(\frac 1 k\)</span></p></li><li>如果有很多页面都指向当前页面，则当前页面很重要，相关度高</li><li><p>当前页面有一些来自官方页面的<code>backlink</code>，当前页面很重要</p></li></ul><p>例如有4个页面，分别如下</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/pgm/pagerank-graf2.PNG" style="display:block; margin:auto" width="30%"></p><p>矩阵<span class="math inline">\(\color {blue }A\)</span>是页面跳转的一次转移矩阵，<span class="math inline">\(\color {blue }q\)</span>是当前时间每个页面的相关度向量，即<code>PageRank vector</code>。 <span class="math display">\[A = \begin{bmatrix} 0 &amp; 0 &amp;1 &amp; \frac {1}{2} \\  \frac {1}{3}&amp; 0 &amp; 0 &amp; 0 \\\frac {1}{3}&amp;  \frac {1}{2} &amp; 0 &amp;  \frac {1}{2} \\ \frac {1}{3} &amp;  \frac {1}{2} &amp; 0 &amp; 0 \\\end{bmatrix}\quad初始时刻，q = \begin {bmatrix}\frac1 4 \\\frac1 4 \\\frac1 4 \\\frac1 4 \\\end {bmatrix}\]</span> <span class="math inline">\(A\)</span>的<strong>一列是当前页面出去的所有页面</strong>，<strong>一行是进入当前页面的所有页面</strong>。设<span class="math inline">\(u\)</span>表示第<span class="math inline">\(A\)</span>的第<span class="math inline">\(i\)</span>行，那么<span class="math inline">\(u*q\)</span>就表示当页面<span class="math inline">\(i\)</span>接受当前<span class="math inline">\(q\)</span>的更新后的rank值。</p><p>定义矩阵<span class="math inline">\(\color {blue} {G} = \color{red} {\alpha A + (1-\alpha) \frac {1} {n} U}\)</span>，对<span class="math inline">\(A\)</span>进行修正，<span class="math inline">\(G\)</span>所有元素大于0，具有遍历性</p><ul><li><span class="math inline">\(\alpha \in[0, 1] \; (\alpha = 0.85)\)</span> 阻尼因子</li><li><span class="math inline">\(A\)</span> 一步转移矩阵</li><li><span class="math inline">\(n\)</span> 页面数量</li><li><span class="math inline">\(U\)</span> 元素全为<span class="math inline">\(1\)</span>的矩阵</li></ul><p>使用<span class="math inline">\(G\)</span>进行迭代的好处</p><ul><li>解决了很多<span class="math inline">\(A\)</span>元素为0导致的问题，如没有超链接的节点，不连接的图等</li><li><span class="math inline">\(A\)</span>所有元素大于0，具有遍历性，具有极限分布，即它的极限分布<span class="math inline">\(q\)</span>会收敛</li></ul><p>那么通过<strong>迭代</strong>就可以求出PR向量<span class="math inline">\(\color {red} {q^{next} = G q^{cur}}\)</span>，实际上<span class="math inline">\(q\)</span>是<span class="math inline">\(G\)</span>的特征值为1的<strong>特征向量</strong>。</p><p>迭代具体计算如下图(下图没有使用G，是使用A去算的，这是网上找的图[捂脸])</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/pgm/pagerank-rezultate_1.gif" alt="" title="">                </div>                <div class="image-caption"></div>            </figure><p>随着迭代，<span class="math inline">\(q\)</span>会收敛，那么称为<span class="math inline">\(q\)</span>就是<code>PageRank vector</code>。</p><p>我们知道节点1有2个backlink，3有3个backlink。但是节点1却比3更加相关，这是为什么呢？因为节点3虽然有3个backlink，但是却只有1个outgoing，只指向了页面1。这样的话它就把它所有的importance都传递给了1，所以页面1也就比页面3的相关度高。</p><h1 id="隐马尔可夫模型">隐马尔可夫模型</h1><h2 id="定义">定义</h2><p><strong>隐马尔可夫模型</strong>（Hidden Markov Model， HMM）是统计模型，它用来描述含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数，然后利用这些参数来做进一步的分析。大概形状如下</p><p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/pgm/HMM.PNG" style="display:block; margin:auto" width="50%"></p><p>一个HMM由以下5个部分构成。</p><p><strong>隐藏状态</strong></p><ul><li><code>模型的状态</code>，隐蔽不可观察</li><li>有<span class="math inline">\(N\)</span>种，隐状态种类集合<span class="math inline">\(\color {blue} {S = \{s_1, s_2, \cdots, s_N\}}\)</span>会相</li><li>隐藏状态互相互转换，<strong>一步转移</strong>。<strong><span class="math inline">\(s_i\)</span>转移到<span class="math inline">\(s_j\)</span>的概率</strong> <span class="math inline">\(\color {red} {a_{ij} = P(q_t= s_j \mid q_{t-1}=s_i)}\)</span></li><li><span class="math inline">\(q_t = s_i\)</span> 代表在<span class="math inline">\(t\)</span>时刻，系统隐藏状态<span class="math inline">\(q_t\)</span>是<span class="math inline">\(s_i\)</span></li><li><code>隐状态时间序列</code> <span class="math inline">\(\color{blue}{Q = \{q_1, q_2, \cdots, q_t, q_{t+1}\cdots \}}\)</span></li></ul><p><strong>观察状态</strong></p><ul><li>模型可以显示观察到的状态</li><li>有<span class="math inline">\(M\)</span>种，显状态种类集合<span class="math inline">\(\color{blue} {K = \{v_1, v_2, \cdots, v_M\}}\)</span>。不能相互转换，只能由隐状态产生(发射)</li><li><span class="math inline">\(o_t = v_k​\)</span> 代表在<span class="math inline">\(t​\)</span>时刻，系统的观察状态<span class="math inline">\(o_t​\)</span>是<span class="math inline">\(v_k​\)</span></li><li>每一个隐藏状态会发射一个观察状态。<strong><span class="math inline">\(s_j\)</span>发射符号<span class="math inline">\(v_k\)</span>的概率</strong><span class="math inline">\(\color {red} {b_j (k) = P(o_t = v_k \mid s_j)}\)</span></li><li><code>显状态时间序列</code> <span class="math inline">\(\color{blue} {O = \{o_1, o_2, \cdots, o_ t\}}\)</span></li></ul><p><strong>状态转移矩阵A (隐--隐)</strong></p><ul><li>从一个隐状<span class="math inline">\(s_i\)</span>转移到另一个隐状<span class="math inline">\(s_j\)</span>的概率。<span class="math inline">\(A = \{a_{ij}\}\)</span><br></li><li><span class="math inline">\(\color {red} {a_{ij} = P(q_t= s_j \mid q_{t-1}=s_i)}\)</span>，其中 <span class="math inline">\(1 \leq i, j \leq N, \; a_{ij} \geq 0, \; \sum_{j=1}^N a_{ij}=1\)</span></li></ul><p><strong>发射概率矩阵B (隐--显)</strong></p><ul><li>一个隐状<span class="math inline">\(s_j\)</span>发射出一个显状<span class="math inline">\(v_k\)</span>的概率。<span class="math inline">\(B = \{b_j(k)\}\)</span></li><li><span class="math inline">\(\color {red} {b_j(k) = P(o_t = v_k \mid s_j)}\)</span>，其中<span class="math inline">\(1 \leq j \leq N; \; 1 \leq k \leq M; \; b_{jk} \ge 0; \; \sum_{k=1}^Mb_{jk}=1\)</span></li></ul><p><strong>初始状态概率分布 <span class="math inline">\(\pi\)</span></strong></p><ul><li>最初的隐状态<span class="math inline">\(q_1=s_i\)</span>的概率是<span class="math inline">\(\pi_i = P(q_1 = s_i)\)</span></li><li>其中<span class="math inline">\(1 \leq i \leq N, \; \pi_i \ge 0, \; \sum_{i=1}^N \pi_i = 1\)</span></li></ul><p>一般地，一个HMM记作一个五元组<span class="math inline">\(\mu = (S, K, A, B, \pi)\)</span>，有时也简单记作<span class="math inline">\(\mu = (A, B, \pi)\)</span>。一般，当考虑潜在事件随机生成表面事件的时候，HMM是非常有用的。</p><p><strong>HMM中的三个问题</strong></p><ul><li><code>观察序列概率</code> 给定观察序列<span class="math inline">\(O=\{o_1, o_2, \cdots, o_T\}\)</span>和模型<span class="math inline">\(\mu = (A, B, \pi)\)</span>，求当前观察序列<span class="math inline">\(O\)</span>的出现概率<span class="math inline">\(P(O \mid \mu)\)</span></li><li><code>状态序列概率</code> 给定观察序列<span class="math inline">\(O=\{o_1, o_2, \cdots, o_T\}\)</span>和模型<span class="math inline">\(\mu = (A, B, \pi)\)</span>，求一个最优的状态序列<span class="math inline">\(Q=\{q_1, q_2, \cdots, q_T\}\)</span>的出现概率，使得最好解释当前观察序列<span class="math inline">\(O\)</span></li><li><code>训练问题或参数估计问题</code> 给定观察序列<span class="math inline">\(O=\{o_1, o_2, \cdots, o_T\}\)</span>，调节模型<span class="math inline">\(\mu = (A, B, \pi)\)</span>参数，使得<span class="math inline">\(P(O \mid u)\)</span>最大</li></ul><h2 id="前后向算法">前后向算法</h2><p>给定观察序列<span class="math inline">\(O=\{o_1, o_2, \cdots, o_T\}\)</span>和模型<span class="math inline">\(\mu = (A, B, \pi)\)</span>，求给定模型<span class="math inline">\(\mu\)</span>的情况下观察序列<span class="math inline">\(O\)</span>的出现概率。这是<code>解码问题</code>。如果直接去求，计算量会出现指数爆炸，那么会很不好求。我们这里使用<code>前向算法</code>和<code>后向算法</code>进行求解。</p><p><strong>前向算法</strong></p><p><code>前向变量</code><span class="math inline">\(\color {blue} {\alpha_t(i)}\)</span>是系统在<span class="math inline">\(t\)</span>时刻，观察序列为<span class="math inline">\(O=o_1o_2\cdots o_t\)</span>并且隐状态为<span class="math inline">\(q_t = s_i\)</span>的概率，即 <span class="math display">\[\color {red} {\alpha_t(i) = P(o_1o_2\cdots o_t, q_t = s_i \mid \mu)}\]</span> <span class="math inline">\(\color {blue} {P(O \mid \mu)}\)</span> 是在<span class="math inline">\(t\)</span>时刻，状态<span class="math inline">\(q_t=\)</span> <strong>所有隐状态的情况下，输出序列<span class="math inline">\(O\)</span>的概率之和</strong> <span class="math display">\[\color {blue} {P(O \mid \mu)} = \sum_{i=1}^N P(O, q_t = s_i \mid \mu) = \color {red} {\sum_{i=1}^{N}\alpha_t(i)}\]</span></p><p>接下来就是计算<span class="math inline">\(\color {blue} {\alpha_t(i)}\)</span>，其实是有动态规划的思想，有如下递推公式 <span class="math display">\[\color {blue} {\alpha_{t+1}(j)} = \color{red}{\underbrace{\left( \sum_{i=1}^N \alpha_t(i)a_{ij} \right)}_{所有状态i转为j的概率} \underbrace {b_j(o_{ t+1})}_{状态j发射o_{t+1}}}\]</span> 上述计算，其实是分为了下面3步</p><ul><li>从1到达时间<span class="math inline">\(t\)</span>，状态为<span class="math inline">\(s_i\)</span>，输出<span class="math inline">\(o_1o_2 \cdots o_t\)</span>。<span class="math inline">\(\color{blue}{\alpha_t(i)}\)</span></li><li>从<span class="math inline">\(t\)</span>到达<span class="math inline">\(t+1\)</span>，状态变化<span class="math inline">\(s_i \to s_j \text{。} \quad\color{blue}{a_{ij}}\)</span></li><li>在<span class="math inline">\(t+1\)</span>时刻，输出<span class="math inline">\(o_{t+1}\)</span>。<span class="math inline">\(\color{blue}{b_j(o_{ t+1})}\)</span></li></ul><p>算法的步骤如下</p><ul><li>初始化 <span class="math inline">\(\color {blue} {\alpha_1(i)} = \color {red} {\pi_ib_i(o_1)}, \; 1 \leq i \leq N\)</span></li><li>归纳计算 <span class="math inline">\(\color {blue} {\alpha_{t+1}(j)} = \color{red} {\left( \sum_{i=1}^N \alpha_t(i)a_{ij} \right) b_j(o_{ t+1})}, \; 1 \leq t \leq T-1\)</span></li><li>求和终结 <span class="math inline">\(\color {blue} {P(O \mid \mu)} = \color {red} {\sum_{i=1}^{N}\alpha_T(i)}\)</span></li></ul><p>在每个时刻<span class="math inline">\(t\)</span>，需要考虑<span class="math inline">\(N\)</span>个状态转移到<span class="math inline">\(s_{j}\)</span>的可能性，同时也需要计算<span class="math inline">\(\alpha_t(1), \cdots , \alpha_t(N)\)</span>，所以时间复杂度为<span class="math inline">\(O(N^2)\)</span>。同时在系统中有<span class="math inline">\(T\)</span>个时间，所以总的复杂度为<span class="math inline">\(O(N^2T)\)</span>。</p><p><strong>后向算法</strong></p><p><code>后向变量</code> <span class="math inline">\(\color {blue} {\beta_{t}(i)}\)</span> 是系统在<span class="math inline">\(t\)</span>时刻，状态为<span class="math inline">\(s_i\)</span>的条件下，输出为<span class="math inline">\(o_{t+1}o_{t+2}\cdots o_T\)</span>的概率，即 <span class="math display">\[\color {red} {\beta_t(i) = P(o_{t+1}o_{t+2}\cdots o_T \mid q_t = s_i , \mu)}\]</span> 递推 <span class="math inline">\(\color {blue} {\beta_{t}(i)}\)</span>的思路及公式如下</p><ul><li>从<span class="math inline">\(t \to t+1\)</span>，状态变化<span class="math inline">\(s_i \to s_j\)</span>，并从<span class="math inline">\(s_j \implies o_{t+1}\)</span>，发射<span class="math inline">\(o_{t+1}\)</span></li><li>在<span class="math inline">\(q_{t+1}=s_j\)</span>的条件下，输出序列<span class="math inline">\(o_{t+2}\cdots o_T\)</span></li></ul><p><span class="math display">\[\color {blue} {\beta_{t}(i)} = \sum_{j=1}^N\color{red}{\underbrace {a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}_{s_i转s_j \; s_j发o_{t+1} \; t+1时s_j后面\{o_{t+2}, \cdots\}} }\]</span></p><p>上面的公式个人的思路解释如下(不明白公式再看)</p><ul><li>其实要从<span class="math inline">\(\beta_{t+1}(j) \to \beta_{t}(i)\)</span></li><li><span class="math inline">\(\beta_{t+1}(j)\)</span>是<span class="math inline">\(t+1\)</span>时刻状态为<span class="math inline">\(s_j\)</span>，后面的观察序列为<span class="math inline">\(o_{t+2}, \cdots, o_{T}\)</span></li><li><span class="math inline">\(\beta_{t}(i)\)</span>是<span class="math inline">\(t\)</span>时刻状态为<span class="math inline">\(s_i\)</span>，后面的观察序列为<span class="math inline">\(\color{red}{o_{t+1}}, o_{t+2}, \cdots, o_{T}\)</span></li><li><span class="math inline">\(t \to t+1\)</span> <strong><span class="math inline">\(s_i\)</span>会变成各种<span class="math inline">\(s_j\)</span></strong>，<span class="math inline">\(\beta_t(i)\)</span>只关心t+1时刻的显示状态为<span class="math inline">\(o_{t+1}\)</span>，而不关心隐状态，<strong>所以是所有隐状态发射<span class="math inline">\(o_{t+1}\)</span>的概率和</strong></li><li><span class="math inline">\(\color{red} {a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}\)</span>，<span class="math inline">\(s_i\)</span>转为<span class="math inline">\(s_j\)</span>的概率，在t+1时刻<span class="math inline">\(s_j\)</span>发射<span class="math inline">\(o_{t+1}\)</span>的概率，t+1时刻状态为<span class="math inline">\(s_j\)</span> 观察序列为<span class="math inline">\(o_{t+2}, \cdots, o_{T}\)</span>的概率</li><li>把上述概率加起来，就得到了t时刻为<span class="math inline">\(s_i\)</span>,后面的观察为<span class="math inline">\(o_{t+1}, o_{t+2}, \cdots, o_{T}\)</span>的概率<span class="math inline">\(\beta_{t}(i)\)</span></li></ul><p>上式是把所有从<span class="math inline">\(t+1 \to t\)</span>的概率加起来，得到<span class="math inline">\(t\)</span>的概率。算法步骤如下</p><ul><li>初始化 <span class="math inline">\(\color {blue} {\beta_T(i) = 1}, \; 1 \leq i \leq N\)</span></li><li>归纳计算 <span class="math inline">\(\color {blue} {\beta_{t}(i)} = \sum_{j=1}^N\color{red}{a_{ij}b_j(o_{t+1})\beta_{t+1}(j) }, \quad 1 \leq t \leq T-1; \; 1 \leq i \leq N\)</span></li><li>求和终结 <span class="math inline">\(\color {blue} {P(O \mid \mu)} = \sum_{i=1}^{N} \color{red} {\pi_i b_i(o_1)\beta_1(i)}\)</span></li></ul><p><strong>前后向算法结合</strong></p><p>模型<span class="math inline">\(\mu\)</span>，观察序列<span class="math inline">\(O=\{o_1, o_2, \cdots, o_t, o_{t+1}\cdots, o_T\}\)</span>，<span class="math inline">\(t\)</span>时刻状态为<span class="math inline">\(q_t=s_i\)</span>的概率如下 <span class="math display">\[\color {blue} {P(O, q_t = s_i \mid \mu)} = \color{red} {\alpha_t(i) \times \beta_t(i)}\]</span> 推导过程如下 <span class="math display">\[\begin{align*}P(O, q_t = s_i \mid \mu) &amp;= P(o_1\cdots o_T, q_t=s_i \mid \mu) =P(o_1 \cdots o_t, q_t=s_i, o_{t+1} \cdots o_T \mid \mu) \\&amp;= P(o_1 \cdots o_t, q_t=s_i \mid \mu) \times P(o_{t+1} \cdots o_T \mid o_1 \cdots o_t, q_t=s_i, \mu) \\&amp;= \alpha_t(i) \times P((o_{t+1} \cdots o_T \mid q_t=s_i, \mu) \quad (显然o_1 \cdots o_t是显然成立的，概率为1，条件忽略)\\&amp;= \alpha_t(i) \times \beta_t(i)\end{align*}\]</span> 所以，把<span class="math inline">\(q_t\)</span>等于所有<span class="math inline">\(s_i\)</span>的概率加起来就可以得到观察概率<span class="math inline">\(\color{blue} {P(O \mid \mu)}\)</span> <span class="math display">\[\color{blue} {P(O \mid \mu)} = \sum_{i=1}^N\ \color{red} {\alpha_t(i) \times \beta_t(i)}, \quad 1 \leq t \leq T\]</span></p><h2 id="维特比算法">维特比算法</h2><p><code>维特比(Viterbi)算法</code>用于求解HMM的第二个问题<code>状态序列问题</code>。即给定观察序列<span class="math inline">\(O=o_1o_2\cdots o_T\)</span>和模型<span class="math inline">\(\mu = (A, B, \pi)\)</span>，<strong>求一个最优的状态序列</strong><span class="math inline">\(Q=q_1q_2 \cdots q_T\)</span>。</p><p>有两种理解最优的思路。</p><ul><li>使该状态序列中每一个状态都单独地具有最大概率，即<span class="math inline">\(\gamma_t(i) = P(q_t = s_i \mid O,\mu)\)</span>最大。但可能出现<span class="math inline">\(a_{q_tq_{t+1}}=0\)</span>的情况</li><li>另一种是，使<strong>整个状态序列概率最大</strong>，即<span class="math inline">\(P(Q \mid O, \mu)\)</span>最大。<span class="math inline">\(\hat{Q} = arg \max \limits_Q P(Q \mid O, \mu)\)</span></li></ul><p><code>维特比变量</code> <span class="math inline">\(\color{blue}{\delta_t(i)}\)</span>是，在<span class="math inline">\(t\)</span>时刻，<span class="math inline">\(q_t = s_i\)</span> ，HMM<strong>沿着某一条路径到达状态<span class="math inline">\(s_i\)</span>，并输出观察序列<span class="math inline">\(o_1o_2 \cdots o_t\)</span>的概率</strong>。 <span class="math display">\[\color{blue}{\delta_t(i)} = \arg \max \limits_{q_1\cdots q_{t-1}} P(q_1 \cdots q_{t-1}, q_t = s_i, o_1 \cdots o_t \mid \mu)\]</span> <strong>递推关系</strong> <span class="math display">\[\color{blue}{\delta_{t+1}(i)} = \max \limits_j [\delta_t(j) \cdot a_{ji}] \cdot b_i(o_{t+1})\]</span> <code>路径记忆变量</code> <span class="math inline">\(\color{blue}{\psi_t(i) = k}\)</span> 表示<span class="math inline">\(q_t = s_i, q_{t-1} = s_k\)</span>，即表示在该路径上<strong>状态<span class="math inline">\(q_t=s_i\)</span>的前一个状态<span class="math inline">\(q_{t-1} = s_k\)</span></strong>。</p><p><strong>维特比算法步骤</strong></p><p>初始化</p><p><span class="math inline">\(\delta_1(i) = \pi_ib_i(o_1), \; 1 \le i \le N\)</span>，路径变量<span class="math inline">\(\psi_1(i) = 0\)</span></p><p>归纳计算</p><p>维特比变量 <span class="math inline">\(\delta_t(j) = \max \limits_{1 \le i \le N} [\delta_{t-1}(i) \cdot a_{ij}] \cdot b_j(o_t), \quad 2 \le t \le T; 1 \le j \le N\)</span></p><p>记忆路径(记住参数<span class="math inline">\(i\)</span>就行) <span class="math inline">\(\psi_t(j) = \arg \max \limits_{1 \le i \le N} [\delta_{t-1}(i) \cdot a_{ij}] \cdot b_j(o_t), \quad 2 \le t \le T; 1 \le j \le N\)</span></p><p>终结</p><p><span class="math display">\[\hat{Q_T} = \arg \max \limits_{1 \le i \le N} [\delta_T(i)], \quad \hat P(\hat{Q_T}) =  \max \limits_{1 \le i \le N} [\delta_T(i)]\]</span> 路径（状态序列）回溯</p><p><span class="math inline">\(\hat{q_t} = \psi_{t+1}(\hat{q}_{t+1}), \quad t = T-1, T-2, \cdots, 1\)</span></p><h2 id="baum-welch算法">Baum-Welch算法</h2><p>Baum-Welch算法用于解决HMM的第3个问题，参数估计问题，给定一个观察序列<span class="math inline">\(O= o_1 o_2 \cdots o_T\)</span>，去调节模型<span class="math inline">\(\mu = (A, B, \pi)\)</span>的参数使得<span class="math inline">\(P(O\mid \mu)\)</span>最大化，即<span class="math inline">\(\mathop{argmax} \limits_{\mu} P(O_{training} \mid \mu)\)</span>。模型参数主要是<span class="math inline">\(a_{ij}, b_j(k) \text{和}\pi_i\)</span>，详细信息见上文。</p><p><strong>有完整语料库</strong></p><p>如果我们知道观察序列<span class="math inline">\(\color{blue}{O= o_1 o_2 \cdots o_T}\)</span>和状态序列<span class="math inline">\(\color{blue}{Q = q_1 q_2 \cdots q_T}\)</span>，那么我们可以根据<code>最大似然估计</code>去计算HMM的参数。</p><p>设<span class="math inline">\(\delta(x, y)\)</span>是克罗耐克函数，当<span class="math inline">\(x==y\)</span>时为1，否则为0。计算步骤如下 <span class="math display">\[\begin{align*} &amp; 初始概率\quad \color{blue}{\bar\pi_i} = \delta(q_1, s_1) \\&amp; 转移概率\quad \color{blue}{\bar {a}_{ij}} = \frac{s_i \to s_j的次数}{s_i \to all的次数} = \frac {\sum_{t=1}^{T-1} \delta(q_t, s_i) \times \delta(q_{t+1}, s_j)} { \sum_{t=1}^{T-1}\delta(q_t, s_i)} \\&amp; 发射概率 \quad \color{blue}{\bar{b}_j(k)} = \frac{s_j \to v_k 的次数}{Q到达q_j的次数} = \frac {\sum_{t=1}^T\delta(q_t, s_i) \times \delta(o_t, v_k)}{ \sum_{t=1}^{T}\delta(q_t, s_j)}\end{align*}\]</span> 但是一般情况下是不知道隐藏状态序列<span class="math inline">\(Q​\)</span>的，还好我们可以使用<a href="https://plmsmile.github.io/2017/08/13/em/">期望最大算法</a>去进行含有隐变量的参数估计。主要思路如下。</p><p>我们可以给定初始值模型<span class="math inline">\(\mu_0\)</span>，然后通过EM算法去估计隐变量<span class="math inline">\(Q\)</span>的期望来代替实际出现的次数，再通过上式去进行计算新的参数得到新的模型<span class="math inline">\(\mu_1\)</span>，再如此迭代直到参数收敛。</p><p>这种迭代爬山算法可以局部地使<span class="math inline">\(P(O \mid \mu)\)</span>最大化，BW算法就是具体实现这种EM算法。</p><p><strong>Baum-Welch算法</strong></p><p>给定HMM的参数<span class="math inline">\(\mu\)</span>和观察序列<span class="math inline">\(O= o_1 o_2 \cdots o_T\)</span>。</p><p>定义<strong>t时刻状态为<span class="math inline">\(s_i\)</span>和t+1时刻状态为<span class="math inline">\(s_j\)</span>的概率</strong>是<span class="math inline">\(\color{blue}{\xi_t(i, j)} = P(q_t =s_i, q_{t+1}=s_j \mid O, \mu)\)</span> <span class="math display">\[\begin{align}\color{blue}{\xi_t(i, j)}  &amp;= \frac{P(q_t =s_i, q_{t+1}=s_j, O \mid \mu)}{P(O \mid \mu)} = \color{red}{\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{P(O \mid \mu)}} =  \frac{\overbrace{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}^{o_1\cdots o_t, \; o_{t+1}, \; o_{t+2}\cdots o_T}}            {\underbrace{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}_{\xi_t(i, j)对ij求和，只留下P(O\mid \mu)}} \\\end{align}\]</span> 定义<strong><span class="math inline">\(t\)</span>时刻状态为<span class="math inline">\(s_i\)</span>的概率</strong>是<span class="math inline">\(\color{blue}{\gamma_t(i)} = P(q_t = s_i \mid O, \mu)\)</span> <span class="math display">\[\color{blue}{\gamma_t(i)} = \color{red}{\sum_{j=1}^N \xi_t(i, j)}\]</span> 那么有算法步骤如下（也称作前向后向算法）</p><p>1初始化</p><p>随机地给参数<span class="math inline">\(\color{blue}{a_{ij}, b_j(k), \pi_i}\)</span>赋值，当然要满足一些基本条件，各个概率和为1。得到模型<span class="math inline">\(\mu_0\)</span>，令<span class="math inline">\(i=0\)</span>，执行下面步骤</p><p>2EM步骤</p><p>2.1E步骤 使用模型<span class="math inline">\(\mu_i\)</span>计算<span class="math inline">\(\color{blue}{\xi_t(i, j)}和\color{blue}{\gamma_t(i)}\)</span> <span class="math display">\[\color{blue}{\xi_t(i, j)} = \color{red}{\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}} , \; \color{blue}{\gamma_t(i)} = \color{red}{\sum_{j=1}^N \xi_t(i, j)}\]</span> 2.2M步骤 用上面算得的期望去估计参数 <span class="math display">\[\begin{align*} &amp; 初始概率\quad \color{blue}{\bar\pi_i} = P(q_1=s_i \mid O, \mu) = \gamma_1(i) \\&amp; 转移概率\quad \color{blue}{\bar {a}_{ij}} = \frac{\sum_{t=1}^{T-1}\xi_t(i, j)}{\sum_{t=1}^{T-1} \gamma_t(i)} \\&amp; 发射概率 \quad \color{blue}{\bar{b}_j(k)}  = \frac{\sum_{t=1}^T \gamma_t(j) \times \delta(o_t, v_k)}{\sum_{t=1}^T \gamma_t(j)}\end{align*}\]</span> 3循环计算 令<span class="math inline">\(i=i+1\)</span>，直到参数收敛</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概述&quot;&gt;概述&lt;/h1&gt;
&lt;h2 id=&quot;产生式和判别式&quot;&gt;产生式和判别式&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;判别方法&lt;/strong&gt; 由数据直接去学习决策函数&lt;span class=&quot;math inline&quot;&gt;\(Y=f(X)\)&lt;
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="概率图模型" scheme="http://plmsmile.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="马尔可夫链" scheme="http://plmsmile.github.io/tags/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/"/>
    
      <category term="隐马尔科夫模型" scheme="http://plmsmile.github.io/tags/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="维特比算法" scheme="http://plmsmile.github.io/tags/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/"/>
    
      <category term="前向算法" scheme="http://plmsmile.github.io/tags/%E5%89%8D%E5%90%91%E7%AE%97%E6%B3%95/"/>
    
      <category term="后向算法，BW算法" scheme="http://plmsmile.github.io/tags/%E5%90%8E%E5%90%91%E7%AE%97%E6%B3%95%EF%BC%8CBW%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>语言模型和平滑方法</title>
    <link href="http://plmsmile.github.io/2017/07/31/nlp-notes/"/>
    <id>http://plmsmile.github.io/2017/07/31/nlp-notes/</id>
    <published>2017-07-31T00:57:52.000Z</published>
    <updated>2017-12-12T06:30:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="语言模型">语言模型</h1><p><strong>二元语法</strong>$ $</p><p>对于一个句子<span class="math inline">\(s=w_1 \cdots w_n\)</span>，近似认为一个词的概率只依赖于它前面的<strong>1个词</strong>。即一个状态只跟上一个状态有关，也称为<code>一阶马尔科夫链</code>。</p><p><span class="math display">\[\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \cdots p(w_n|w_{l-1})= \color {red} {\prod_{i=1}^l {p(w_i|w_{i-1})}}\]</span> 设<span class="math inline">\(\color {blue} {c(w_{i-1}w_i)}\)</span> 表示二元语法<span class="math inline">\(\color {green} {w_{i-1}w_i}\)</span>在给定文本中的出现次数，则上一个词是<span class="math inline">\(w_{i-1}\)</span>下一个词是<span class="math inline">\(w_i\)</span>的概率<span class="math inline">\(\color {blue} {p(w_i \mid w_{i-1})}\)</span>是当前语法<span class="math inline">\(\color {green} {w_{i-1}w_i}\)</span>出现的次数比上所有形似<span class="math inline">\(\color {green} {w_{i-1}}w\)</span>的二元语法的出现次数 <span class="math display">\[\color {blue} {p(w_i \mid w_{i-1})} =\color {red} {\frac {c(w_{i-1}w_i)} {\sum_{w} {c(w_{i-1}w)}}}，w是变量\]</span> <span class="math inline">\(n\)</span><strong>元语法</strong></p><p>认为一个词出现的概率和它<strong>前面的n个词</strong>有关系。则对于句子<span class="math inline">\(s=w_1w_2 \cdots w_l\)</span>，其概率计算公式为如下：</p><p><span class="math display">\[\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_1w_2) \cdots p(w_n|w_1w_2 \cdots w_{l-1})=\color {red} {\prod_{i=1}^n{p(w_i|w_1 \cdots w_{i-1})}}\]</span> 上述公式需要大量的概率计算，太理想了。一般取<span class="math inline">\(n=2\)</span>或者<span class="math inline">\(n=3\)</span>。</p><p>对于<span class="math inline">\(n&gt;2\)</span>的<span class="math inline">\(n\)</span>元语法模型，条件概率要考虑前面<span class="math inline">\(n-1\)</span>个词的概率，设<span class="math inline">\(w_i^j\)</span>表示<span class="math inline">\(w_i\cdots w_j\)</span>，则有 <span class="math display">\[\color{blue} {p(s)} = \prod_{i=1}^{l+1}p(w_i \mid w_{i-n+1}^{i})，\color {blue} {p(w_i \mid w_{i-n+1}^{i})}=\frac { \overbrace {c(w_{i-n+1}^i)}^{\color{red}{具体以w_i结尾的词串w[i-n+1, i]}}} { \underbrace{\sum_{w_i}{c(w_{i-n+1}^i)}}_{\color{red}{所有以w_i结尾的词串w[i-n+1, i]}}}\]</span> <strong>实际例子</strong></p><p>假设语料库<span class="math inline">\(S\)</span>是由下面3个句子组成，所求的句子t在其后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = [<span class="string">'brown read holy bible'</span>, <span class="string">'plm see a text book'</span>, <span class="string">'he read a book by david'</span>]</span><br><span class="line">t = <span class="string">'brown read a book'</span></span><br></pre></td></tr></table></figure><p>那么求句子<span class="math inline">\(t\)</span>出现的概率是</p><p><span class="math display">\[\color{blue}{p(t)}=p(\color{green}{brown\,read\, a\, book})=p(brown|BOS)p(read|brown)p(a|read)p(book|a)p(eos|book)\approx0.06\]</span></p><p><span class="math inline">\(n\)</span>元文法的一些应用如下</p><p><strong>语音识别歧义消除</strong></p><p>如给了一个拼音 <span class="math inline">\(\color{green}{ta\,shi \,yan \,jiu \,sheng\, wu\, de}\)</span>，得到了很多可能的汉字串：<font color="green">踏实研究生物的，他实验救生物的，他是研究生物的</font> ，那么求出<span class="math inline">\(arg_{str}maxP(str|pinyin)\)</span>，即返回最大概率的句子</p><p><strong>汉语分词问题</strong></p><p>给定汉字串<code>他是研究生物的</code>。可能的汉字串 <code>他 是 研究生 物 的</code>和<code>他 是 研究 生物 的</code>，这也是求最大句子的概率</p><p><strong>开发自然语言处理的统计方法的一般步骤</strong></p><ul><li>收集大量语料（基础工作，工作量最大，<strong>很重要</strong>）</li><li>对语料进行统计分析，得出知识（如n元文法，一堆概率）</li><li>针对场景建立算法，如计算概率可能也用很多复杂的算法或者直接标注</li><li>解释或者应用结果</li></ul><h1 id="模型评估参数">模型评估参数</h1><p><strong>基础</strong></p><ul><li>评价目标：语言模型计 算出的概率分布与“真实的”理想模型是否接近</li><li>难点：无法知道“真实的”理想模型的分布</li><li>常用指标：交叉熵，困惑度</li></ul><h2 id="信息量和信息熵">信息量和信息熵</h2><p><span class="math inline">\(X\)</span>是一个离散随机变量，取值空间为<span class="math inline">\(R\)</span>，其概率分布是<span class="math inline">\(p(x)=P(X=x), x \in R\)</span>。</p><p><strong>信息量</strong></p><p>概率是对事件确定性的度量，那么<strong>信息</strong>就是对<strong>不确定性</strong>的度量。<strong>信息量</strong> <span class="math inline">\(\color {blue} {I(X)}\)</span>代表特征的不确定性，定义如下 <span class="math display">\[\color {blue} {I(X)}= \color {red} {-\log {p(x)}}\]</span> <strong>信息熵</strong></p><p><strong>信息熵</strong><span class="math inline">\(\color{blue}{H(x)}\)</span>是特征<strong>不确定性的平均值</strong>，用表示，定义如下 <span class="math display">\[\color{blue}{H(X)}=\sum_{x \in R}{p(x)log\frac 1 {p(x)}}=\color {red} {-\sum_{x \in R} {p(x) \log p(x)}}\]</span> 一般是<span class="math inline">\(log_2{p(x)}\)</span>，单位是比特。若是<span class="math inline">\(\ln {p(x)}\)</span>，单位是奈特。</p><ul><li>信息熵的本质是信息量的期望</li><li><strong>信息熵是对不确定性的度量</strong></li><li>随机变量<span class="math inline">\(X\)</span>的熵越大，说明不确定性也大；若<span class="math inline">\(X\)</span>为定值，则熵为0</li><li><code>平均分布</code>是&quot;<strong>最不确定</strong>”的分布</li></ul><h2 id="联合熵和条件熵">联合熵和条件熵</h2><p><span class="math inline">\(X, Y\)</span>是一对离散型随机变量，并且<span class="math inline">\(\color{blue}{X,Y \sim p(x,y)}\)</span>。</p><p><strong>联合熵</strong></p><p>联合熵实际上描述的是一对随机变量平均所需要的信息量，定义如下。 <span class="math display">\[\color{blue}{H(X, Y)} = \color{red} {- \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y)}\]</span> <strong>条件熵</strong></p><p>给定<span class="math inline">\(X\)</span>的情况下，<span class="math inline">\(Y\)</span>的条件熵为 <span class="math display">\[\color{blue}{H(Y \mid X)} = \color{red}{ - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y \mid x)}\]</span> 其中可以推导出：<span class="math inline">\(H(X, Y) = H(X) + H(Y \mid X)\)</span>。</p><h2 id="相对熵和交叉熵">相对熵和交叉熵</h2><p><strong>相对熵</strong></p><p>随机变量<span class="math inline">\(X\)</span>的状态空间<span class="math inline">\(\Omega {x}\)</span>上有两个概率分布<span class="math inline">\(p(x)\)</span>和<span class="math inline">\(q(x)\)</span>。一般p是<code>真实分布</code>，q是<code>预测分布</code>。</p><p><strong>相对熵</strong>也称为<code>KL距离</code>，用来衡量相同事件空间里<strong>两个概率分布的差异</strong>。</p><p><span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>的<strong>相对熵</strong><span class="math inline">\(\color{blue}{D(p\mid\mid q)}\)</span>用来度量<strong>它们之间的差异</strong>，如下 <span class="math display">\[\color{blue}{D(p\mid\mid q)}=\color{red}{\sum_{x\in X} {p(x)\log{\frac {p(x)}{q(x)}}}} = E_p(\log \frac{p(X)}{q(X)}) \; (期望)\]</span> 特别地，若<span class="math inline">\(p==q\)</span>，则相对熵为0；若差别增加，相对熵的值也增加。简单理解“相对”如下： <span class="math display">\[D(p \mid\mid q)=\sum_{x \in X}{p(x)(\log p(x) - \log q(x))}= \underbrace{\left(-\sum_{x \in X}{ \color{red}{p(x)\log q(x)}}\right)}_{\color {red}{以q去近似p的熵=交叉熵}} - \underbrace{\left(-\sum_{x \in X} {\color{red}{p(x)\log p(x)}}\right)}_{\color{red} {p本身的熵}}\]</span> <strong>交叉熵</strong></p><p>交叉熵用来衡量<strong>估计模型与真实概率分布之间的差异。</strong></p><p>随机变量<span class="math inline">\(X \sim p(x)\)</span>，<span class="math inline">\(q(x)\)</span>近似于<span class="math inline">\(p(x)\)</span>。 则随机变量<span class="math inline">\(X\)</span>和模型<span class="math inline">\(q\)</span>之间的<strong>交叉熵</strong><span class="math inline">\(\color {blue} {H(X, q)}\)</span>如下：<strong>以<span class="math inline">\(q\)</span>去近似<span class="math inline">\(p\)</span>的熵</strong> <span class="math display">\[\color {blue} {H(p, q)} = H(X) + D(p \mid\mid q) = \color {red} {-\sum_{x \in X}{p(x)\log q(x)}}\]</span></p><p><strong>实际应用</strong></p><p>交叉熵的实际应用，设<span class="math inline">\(y\)</span>是预测的概率分布，<span class="math inline">\(y^\prime\)</span>为真实的概率分布。则用<strong>交叉熵去判断估计的准确程度</strong> <span class="math display">\[H(y^{\prime}, y)= - \sum_i y_i^{\prime}\log y_i = \color {red} {-\sum_i y_{真实} \log y_{预测}}\]</span> <strong>n元文法模型的交叉熵</strong></p><p>设测试集<span class="math inline">\(T=(t_1, t_2, \ldots, t_l)\)</span>包含<span class="math inline">\(l\)</span>个句子，则定义测试集的概率<span class="math inline">\(\color {blue} {p(T)}\)</span>为多个句子概率的乘积 <span class="math display">\[\color {blue} {p(T)} = \prod_{i=1}^{l} p(t_i)，  \, \text{其中}\color{blue}{p(t_i)}=\color{red}{\prod_{i=1}^{l_w} {p(w_i|w_{i-n+1}^{i-1})}}, \text{见上面}\]</span> 其中<span class="math inline">\(w_i^j\)</span>表示词<span class="math inline">\(w_i\cdots w_j\)</span>，<span class="math inline">\(\sum_{w}{c(read \, w)}\)</span>是查找出所有以<span class="math inline">\(read\)</span>开头的二元组的出现次数。</p><p>则在数据<span class="math inline">\(T\)</span>上<code>n元模型</code><span class="math inline">\(\color {green} {p(w_i|w_{i-n+1}^{i-1})}\)</span>的交叉熵<span class="math inline">\(\color {blue} {H_p(T)}\)</span>定义如下 <span class="math display">\[\color {blue} {H_p(T)} = \color {red} {-\frac {1} {W_T} \log _2 p(T)}，其中W_T是文本T中基元(词或字)的长度\]</span> 公式的推导过程如下 <span class="math display">\[-\sum_{x \in X}{p(x)\log q(x)} \implies \underbrace { -{\frac{1} {W_T}}\sum \log q(x)}_{\color{red}{使用均匀分布代替p(x)}}\implies  -{\frac{1} {W_T}} \log {\prod r(w_i|w_{i-n+1}^{i-1})}\implies  -{\frac{1} {W_T}} \log_2p(T)\]</span> 可以这么理解：利用模型<span class="math inline">\(p\)</span>对<span class="math inline">\(W_T\)</span>个词进行编码，每一个编码所需要的平均比特位数。</p><h2 id="困惑度">困惑度</h2><p><strong>困惑度</strong>是评估语言的基本准则人，也是对测试集T中每一个词汇的概率的几何平均值的倒数。 <span class="math display">\[\color{blue}{PP_T(T)} =\color{red}{ 2^{H_p(T)}= \frac {1} {\sqrt [W_T]{p(T)}}} = 2 ^{\text{交叉熵}}\]</span></p><p>当然，<font color="red">交叉熵和困惑度越小越好。语言模型设计的任务就是要找出困惑度最小的模型。</font></p><p>在英语中，n元语法模型的困惑度是<span class="math inline">\(50 \sim 1000\)</span>，交叉熵是<span class="math inline">\(6 \sim 10\)</span>个比特位。</p><h1 id="数据平滑">数据平滑</h1><h2 id="问题的提出">问题的提出</h2><p>按照上面提出的语言模型，有的句子就没有概率，但是这是不合理的，因为总有出现的可能，概率应该大于0。设<span class="math inline">\(\color {blue}{c(w)}\)</span>是<span class="math inline">\(w\)</span>在语料库中的出现次数。 <span class="math display">\[p(\color{green} {read \mid plm}) = \frac {c(plm \mid read)} {\sum_{w_i}{c(plm | w_i})} = \frac {\color{red}{0}} {1}=\color{red}{0， \, 这是不对的}\]</span> 因此，必须分配<font color="red">给所有可能出现的字符串一个非0的概率值来避免这种错误的发送。</font></p><p><code>平滑技术</code>就是用来解决这种零概率问题的。平滑指的是为了产生更准确的概率来调整最大似然估计的一种技术，也称作<code>数据平滑</code>。思想是<code>劫富济贫</code>，即<strong>提高低概率、降低高概率</strong>，尽量是概率分布趋于均匀。</p><p>数据平滑是语言模型中的核心问题</p><h2 id="加法平滑">加法平滑</h2><p>其实为了避免0概率，最简单的就是给统计次数加1。这里我们可以为每个单词的出现次数加上<span class="math inline">\(\delta，\delta \in [0, 1]\)</span>，设<span class="math inline">\(V\)</span>是所有词汇的单词表，<span class="math inline">\(|V|\)</span>是单词表的词汇个数，则有概率： <span class="math display">\[p_{add}(w_i \mid w_{i-n+1}^{i-1}) = \frac {\delta + c(w_{i-n+1}^i)} {\sum_{w_i}{(\delta*|V| + c(w_{i-n+1}^i)})}=\frac {\delta + \overbrace {c(w_{i-n+1}^i)}^{\color{red}{具体词串[i-n+1, i]}}} {\delta*|V| + \underbrace{\sum_{w_i}{c(w_{i-n+1}^i)}}_{\color{red}{所有以w_i结尾的词串[i-n+1, i]}}}\]</span></p><p>注：这个方法很原始。</p><h2 id="good-turing">Good-Turing</h2><p><code>Good-Turing</code>也称作古德-图灵方法，这是很多平滑技术的核心。</p><p>主要思想是重新分配概率，会得到一个<code>剩余概率量</code><span class="math inline">\(\color {blue} {p_0}= \color {red} {\frac {n_1} N}\)</span>，设<span class="math inline">\(n_0\)</span>为未出现的单词的个数，然后<font color="red">由这<span class="math inline">\(n_0\)</span>个单词去平均分配得到<span class="math inline">\(p_0\)</span></font>，即每个未出现的单词的概率为<span class="math inline">\(\frac {p_0} {n_0}\)</span>。</p><p>对于一个<span class="math inline">\(n\)</span>元语法，设<span class="math inline">\(\color {blue} n_r\)</span>恰好出现<span class="math inline">\(r\)</span>次的<span class="math inline">\(n\)</span>元语法的数目，下面是一些新的定义</p><ul><li>出现次数为<span class="math inline">\(r\)</span>的<span class="math inline">\(n\)</span>元语法 新的出现次数<span class="math inline">\(\color {blue} {r^*} = \color {red} {(r+1)\frac{n_{r+1}}{n_r}}\)</span></li><li>设<span class="math inline">\(N = \sum_{r=0}^{\infty}n_r r^* = \sum_{r=1}^{\infty} n_r r\)</span>，即<span class="math inline">\(N\)</span>是这个分布中最初的所有文法出现的次数，例如所有以<span class="math inline">\(read\)</span>开始的总次数</li><li>出现次数为<span class="math inline">\(r\)</span>的修正概率 <span class="math inline">\(\color {blue}p_r = \color {red} {\frac {r^*} {N}}\)</span></li></ul><p>剩余概率量<span class="math inline">\(\color {blue} {p_0}= \color {red} {\frac {n_1} N}\)</span>的推导 <span class="math display">\[总的概率 = \sum_{r&gt;0}{n_r p_r} = \sum_{r&gt;0}{n_r  (r+1)\frac{n_{r+1}}{n_r N}} = \frac {1}{N} (\sum_{r&gt;0} (r+1)n_{r+1} =  \frac {1}{N} (\sum_{r&gt;0} (r n_r - n_1) = 1 - \frac {n_1} N &lt; 1\]</span> 然后把<span class="math inline">\(p_0\)</span>平均分配给所有未见事件(r=0的事件)。</p><p><strong>缺点</strong></p><ul><li>若出现次数最大为<span class="math inline">\(k\)</span>，则无法计算<span class="math inline">\(r=k\)</span>的新的次数<span class="math inline">\(r^*\)</span>和修正概率<span class="math inline">\(p_r\)</span></li><li>高低阶模型的结合通常能获得较好的平滑效果，但是Good-Turing不能高低阶模型结合</li></ul><h2 id="jelinek-mercer">Jelinek-Mercer</h2><p><strong>问题引入</strong></p><p>假如<span class="math inline">\(c(send \, the)=c(send \, thou)=0\)</span>，则通过GT方法有<span class="math inline">\(p(the \mid send)=p(thou \mid send)\)</span>，但是实际上却应该是<span class="math inline">\(p(the \mid send)&gt;p(thou \mid send)\)</span>。</p><p>所以我们需要在二元语法模型中加入一个一元模型 <span class="math display">\[p_{ML}(w_i) = \frac {c(w_i)}{\sum_w{c(w)}}\]</span> <strong>二元线性插值</strong></p><p>使用<span class="math inline">\(r\)</span>将二元文法模型和一元文法模型进行线性插值 <span class="math display">\[p(w_i \mid w_{i-1}) = \lambda p_{ML}(w_i | w_{i-1}) + (1-\lambda)p_{ML}(w_i)，\lambda \in [0, 1]\]</span> 所以可以得到<span class="math inline">\(p(the \mid send)&gt;p(thou \mid send)\)</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;语言模型&quot;&gt;语言模型&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;二元语法&lt;/strong&gt;$ $&lt;/p&gt;
&lt;p&gt;对于一个句子&lt;span class=&quot;math inline&quot;&gt;\(s=w_1 \cdots w_n\)&lt;/span&gt;，近似认为一个词的概率只依赖于它前面的&lt;
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="各种熵" scheme="http://plmsmile.github.io/tags/%E5%90%84%E7%A7%8D%E7%86%B5/"/>
    
      <category term="语言模型" scheme="http://plmsmile.github.io/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="数据平滑" scheme="http://plmsmile.github.io/tags/%E6%95%B0%E6%8D%AE%E5%B9%B3%E6%BB%91/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer算法题</title>
    <link href="http://plmsmile.github.io/2017/07/29/aim2offer/"/>
    <id>http://plmsmile.github.io/2017/07/29/aim2offer/</id>
    <published>2017-07-29T03:42:07.000Z</published>
    <updated>2018-01-09T02:04:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数组中重复的数字-03">数组中重复的数字-03</h1><h2 id="题目1">题目1</h2><p>找到数组中重复的数字</p><blockquote><p>一个数组存放n个数字，所有数字在[0, n-1]范围内。某些数字是随机重复的。请找出任意一个重复的数字。例如[2,3,1,0,2,5,3]，输出2或3</p></blockquote><p><strong>思路1</strong></p><p>对数组进行<strong>排序</strong>，然后可以找出重复的数字。但是排序的时间复杂度是<code>O(nlogn)</code></p><p><strong>思路2</strong></p><p>使用<strong>哈希表</strong>，每次存放的时候检查是否在哈希表中，如果已经存在，那么就重复了。时间复杂度<code>O(n)</code>，空间复杂度<code>O(n)</code></p><p><strong>最优思路</strong></p><p>利用<strong>下标</strong>和值的关系，从头到尾依次扫描这个数组。扫描到下标为<code>i</code>，值为<code>m</code>。尽量把<code>m</code>放到<code>a[m]</code>的位置上。修改了原来的数组。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (a[i] != i)</span><br><span class="line">    <span class="keyword">if</span> m == i:</span><br><span class="line">        <span class="comment"># 扫描下一个数字</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 把m和a[m]进行比较</span></span><br><span class="line">        <span class="keyword">if</span> m == a[m]:</span><br><span class="line">            <span class="comment"># 找到一个相同的数字</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 把m放到a[m]的位置，交换</span></span><br><span class="line">            m &lt;--&gt; a[m]</span><br></pre></td></tr></table></figure><p>尽管有两重循环，但每个数字最多交换两次就能找到自己的位置，所以总的时间复杂度是<code>O(n)</code>，空间复杂度为<code>O(1)</code></p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/03_find_duplicate_nums/find_dup_nums.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 找到数组中重复的值</span></span><br><span class="line"><span class="comment">// Args:</span></span><br><span class="line"><span class="comment">//      a: 数组</span></span><br><span class="line"><span class="comment">//      alen: 数组长度</span></span><br><span class="line"><span class="comment">//      dup: 用于返回重复的数值</span></span><br><span class="line"><span class="comment">// Returns:</span></span><br><span class="line"><span class="comment">//      True：数据合法(长度和值)并且有重复的数字，否则返回False</span></span><br><span class="line"><span class="comment">//  </span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">duplicate</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> alen, <span class="keyword">int</span> *dup)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 遍历数组，把i都放到a[i]上</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; alen; i++) &#123;</span><br><span class="line">        <span class="keyword">while</span> (a[i] != i) &#123;</span><br><span class="line">            <span class="keyword">int</span> m = a[i];</span><br><span class="line">            <span class="keyword">if</span> (a[m] == m) &#123;</span><br><span class="line">                <span class="comment">// a[m]已经有m</span></span><br><span class="line">                *dup = m;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 把m放到a[m]上</span></span><br><span class="line">                <span class="keyword">int</span> t = a[m];</span><br><span class="line">                a[m] = m;</span><br><span class="line">                a[i] = t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="题目2">题目2</h2><p>不修改数组找出重复的数字</p><blockquote><p>数组，长度为n+1，数字范围[1, n]，数组中至少有一个是重复的，找出任意一个重复的数字，但是不能修改数组</p></blockquote><p><strong>思路1</strong></p><p>创建一个新数组b存放原数组a。遍历原数组，当前是m，如果b[m]已经没有值，则存放；如果有值，则重复。但是需要<code>O(n)</code>的辅助空间</p><p><strong>最优思路</strong></p><p>见<a href="https://plmsmile.github.io/2017/12/29/leetcode-01/#发现重复的数字-287">二分查重描述清晰版</a></p><p>把<span class="math inline">\(a[1, n]\)</span>的个数字，分为两部分。<span class="math inline">\(a[1, m]\)</span>和<span class="math inline">\(a[m+1, n]\)</span>。</p><p>在<span class="math inline">\(a[1, m]\)</span>中，统计数字<span class="math inline">\(1,2\cdots, m\)</span>在<span class="math inline">\(a[1,m]\)</span>中出现的次数<code>count</code></p><ul><li>如果是m次，则<span class="math inline">\(a[1,m]\)</span>每个数字独一无二，重复的区间在a[m+1, n]中。则<span class="math inline">\(\rm{start}=m+1\)</span>，继续查找。</li><li>否则不独一无二，则重复在<span class="math inline">\(a[1, m]\)</span>中 。则<span class="math inline">\(\rm{end}=m\)</span>， 继续查找。</li><li>直到<span class="math inline">\(\rm{start} == \rm{end}\)</span> 。count &gt; 1，则start重复，否则没有重复的。</li></ul><p><a href="https://github.com/plmsmile/aim2offer/blob/master/03_find_duplicate_nums/find_dup_nums_binary.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 二分查找数组中重复的值</span></span><br><span class="line"><span class="comment">// Args:</span></span><br><span class="line"><span class="comment">//      a: 数组</span></span><br><span class="line"><span class="comment">//      alen: 数组长度</span></span><br><span class="line"><span class="comment">// Returns:</span></span><br><span class="line"><span class="comment">//      dup: 重复的数值; 没有重复时返回-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">get_duplication</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> *a, <span class="keyword">int</span> alen)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a == <span class="literal">nullptr</span> || alen &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> start = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> end = alen - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (start &lt;= end) &#123;</span><br><span class="line">        <span class="keyword">int</span> m = ((end - start) &gt;&gt; <span class="number">1</span>) + start;</span><br><span class="line">        <span class="keyword">int</span> count = count_range(a, alen, start, m);</span><br><span class="line">        <span class="comment">// last </span></span><br><span class="line">        <span class="keyword">if</span> (start == end) &#123;</span><br><span class="line">            <span class="keyword">if</span> (count &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> start;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// continue</span></span><br><span class="line">        <span class="keyword">if</span> (count == m - start + <span class="number">1</span>) &#123;</span><br><span class="line">            start = m + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            end = m;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="二维数组查找-04">二维数组查找-04</h1><blockquote><p>​ 一个二维数组，每一行从左到右递增，每一列，从上到下递增。输入一个整数，判断二维数组中是否有这个数字</p></blockquote><p><strong>错误思路</strong></p><p>全盘扫描肯定不行，从左上角最小的开始也不行，应该从最大角的地方开始</p><p><strong>思路</strong></p><p><strong>一行的最大元素在最右边，一列的最小元素在上边</strong>。所以从右上角开始查找最好。即<strong>向左查、向下查</strong>，这样每次都能够剔除一行或者一列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span></span><br><span class="line"><span class="comment"># 当期右上角值是a[i, j]=m，查找的值是t</span></span><br><span class="line"><span class="keyword">if</span> t == a[i,j]:</span><br><span class="line">    done<span class="comment"># 查找成功</span></span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> t &gt; a[i,j]: </span><br><span class="line">    <span class="comment"># 删除当前行</span></span><br><span class="line">    m = a[i+<span class="number">1</span>, j]</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> t &lt; a[i, j]:</span><br><span class="line">    <span class="comment"># 删除当前列</span></span><br><span class="line">    m = a[i, j<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 继续查找</span></span><br></pre></td></tr></table></figure><p><strong>关键代码</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查找一个数，是否在一个矩阵中</span></span><br><span class="line"><span class="comment">// Args:</span></span><br><span class="line"><span class="comment">//      target: 要查找的数字</span></span><br><span class="line"><span class="comment">//      array: 矩阵</span></span><br><span class="line"><span class="comment">// Returns:</span></span><br><span class="line"><span class="comment">//      exists: true or false</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span> target, <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="built_in">array</span>)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> col = <span class="built_in">array</span>.size();</span><br><span class="line">    <span class="keyword">int</span> row = <span class="built_in">array</span>[<span class="number">0</span>].size();</span><br><span class="line">    <span class="keyword">bool</span> exist = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> j = col - <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 注意i,j的范围</span></span><br><span class="line">    <span class="keyword">while</span> (exist == <span class="literal">false</span> &amp;&amp; (i &lt; row &amp;&amp; i &gt;= <span class="number">0</span> &amp;&amp; j &lt; col &amp;&amp; j &gt;= <span class="number">0</span>)) &#123;</span><br><span class="line">        <span class="keyword">int</span> t = <span class="built_in">array</span>[i][j];</span><br><span class="line">        <span class="keyword">if</span> (target == t) &#123;</span><br><span class="line">            exist = <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (target &lt; t) &#123;</span><br><span class="line">            <span class="comment">// to left</span></span><br><span class="line">            --j;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(target &gt; t) &#123;</span><br><span class="line">            <span class="comment">// to down</span></span><br><span class="line">            ++i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> exist;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="字符串替换空格-05">字符串替换空格-05</h1><blockquote><p>把字符串中的每个空格替换成&quot;%20&quot;</p></blockquote><ul><li>如果在原来的字符串上修改，则会覆盖原来字符串后面的内存</li><li>如果创建新的字符串，则要分配足够的内存</li><li>C/C++中字符串最后一个字符是<code>\0</code></li></ul><p><strong>不好思路</strong></p><p>从前向后扫描，遇到一个空格替换一个。但是每次都需要大量移动后面的元素，所以时间复杂度是<span class="math inline">\(O(n^2)\)</span></p><p><strong>最优思路</strong></p><p><strong>从后向前替换</strong>。使用两个指针p1和p2。先计算出替换后的长度，p2指向替换后的长度的末尾指针。p1指向之前的字符串的指针。</p><ul><li>从p1开始向前移动</li><li>当前是普通字符，则复制到p2，p2向前移动</li><li>当前是空格，则在p2加入“%20”，p2向前移动</li><li>如果p1==p2，那么移动完毕</li></ul><p>总的来说，先找到最终的长度，从后向后拉。时间复杂度是<span class="math inline">\(O(n)\)</span></p><p><strong>技巧</strong></p><blockquote><p>合并两个数组/字符串，如果从前往后，则需要移动多次。<strong>从后向前，能够减少移动次数，提高效率</strong></p></blockquote><p><a href="https://github.com/plmsmile/aim2offer/blob/master/05_replacespace_reverse/replacespace_reverse.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 替换字符串中的空格字符，每个空格用'02%'替换</span></span><br><span class="line"><span class="comment">// 直接修改原字符串</span></span><br><span class="line"><span class="comment">// Args:</span></span><br><span class="line"><span class="comment">//      str: 字符串</span></span><br><span class="line"><span class="comment">//      len: 长度</span></span><br><span class="line"><span class="comment">// Returns:</span></span><br><span class="line"><span class="comment">//      None</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">replace_space</span><span class="params">(<span class="keyword">char</span> *str, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 统计空格的个数</span></span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++) </span><br><span class="line">    <span class="keyword">if</span> (str[i] == <span class="string">' '</span>) </span><br><span class="line">        ++count;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> newlen = (len - count) + count * <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">int</span> i = len - <span class="number">1</span>, j = newlen - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (i &gt;= <span class="number">0</span> &amp;&amp; j &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (str[i] == <span class="string">' '</span>) &#123;</span><br><span class="line">            <span class="comment">// 在j处添加替换字符</span></span><br><span class="line">            str[j--] = <span class="string">'0'</span>;</span><br><span class="line">            str[j--] = <span class="string">'2'</span>;</span><br><span class="line">            str[j--] = <span class="string">'%'</span>;</span><br><span class="line">            <span class="comment">// 向前移动</span></span><br><span class="line">            --i;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 字符复制到后面</span></span><br><span class="line">            str[j--] = str[i--];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 字符串结尾</span></span><br><span class="line">    str[newlen] = <span class="string">'0'</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="逆序打印链表-06">逆序打印链表-06</h1><h2 id="链表基础考点">链表基础考点</h2><p>链表是面试中最频繁的数据结构。动态结构很灵活，<strong>考指针、考编程功底</strong>。</p><ul><li><code>链表创建</code> ：</li><li><code>链表插入</code>： 为新节点分配内存，调整指针的指向。</li><li><code>删除链表中的节点</code></li><li><code>从尾到头打印链表</code></li><li><code>链表中倒数第k个节点</code></li><li><code>反转链表</code></li><li><code>合并两个排序的链表</code></li><li><code>两个链表的第一个公共节点</code></li><li><code>环形链表</code> ：尾节点指针指向头结点。题目62：<code>圆圈中最后剩下的数字</code></li><li><code>双向链表</code> ：题目36，<code>二叉搜索树与双向链表</code></li><li><code>复杂链表</code> ：指向下一个，指向任意节点的指针</li></ul><h2 id="从尾到头打印链表">从尾到头打印链表</h2><p>本质上是<code>先进后出</code>， 可以用<code>栈</code>和<code>递归</code>。显然，栈的效率高。</p><p><a href="https://github.com/plmsmile/aim2offer/blob/master/06_listnode/list_reverse_stack.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用栈逆序打印链表</span></span><br><span class="line"><span class="comment">// Args:</span></span><br><span class="line"><span class="comment">//      head: 头指针</span></span><br><span class="line"><span class="comment">// Returns:</span></span><br><span class="line"><span class="comment">//      res: vector&lt;int&gt;，逆序值</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; get_reverse_by_stack(ListNode *head) &#123;</span><br><span class="line">    ListNode* pnode = head;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; st;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (pnode != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        st.push(pnode-&gt;val);</span><br><span class="line">        pnode = pnode-&gt;next;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 分配定长的vector，不用</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res(count);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; count &amp;&amp; st.empty() == <span class="literal">false</span>; i++) &#123;</span><br><span class="line">        res[i] = st.top();</span><br><span class="line">        st.pop();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="重建二叉树-07">重建二叉树-07</h1><h2 id="树的考点">树的考点</h2><p><strong>树的遍历</strong></p><p>叉树涉及指针，比较难。最常问遍历。需要对下面7种了如指掌。</p><table><thead><tr class="header"><th align="center"></th><th align="center">前序</th><th align="center">中序</th><th align="center">后序</th><th align="center">层次遍历</th></tr></thead><tbody><tr class="odd"><td align="center">递归</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center">无递归</td></tr><tr class="even"><td align="center">循环</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p>考题</p><ul><li>题26，树的子结构</li><li>题34，二叉树中和为某一值的路径</li><li>题55，二叉树的深度</li><li>题7，重建二叉树</li><li>题33，二叉搜索树的后序遍历序列</li><li>题32，从上到下打印二叉树（层次遍历）</li></ul><p><strong>特别的二叉树</strong></p><p><strong>二叉搜索树</strong> ：左节点小于根节点，根节点小于右节点。查找搜索时间复杂度<span class="math inline">\(O(\log n)\)</span>。 题36，<code>二叉搜索树与双向链表</code>；题68：<code>树中两个节点的最低公共祖先</code>。</p><p><strong>堆</strong> ：最大堆和最小堆。找最大值和最小值。</p><p><strong>红黑树</strong> ： 节点定义为红黑两种颜色。根节点到叶节点的最长路径不超过最短路径的两倍。</p><h2 id="前序中序建立二叉树">前序中序建立二叉树</h2><p>前序序列：<strong>1</strong>, 2, 4, 7, 3, 5, 6, 8。 根 左 右。</p><p>中序序列：4, 7, 2, <strong>1</strong>, 5, 3, 8, 6。 左 根 右。</p><p>使用递归，先找到根节点，找到左右子树，<strong>为左右子树分别创建各自的前序和中序序列</strong>，再进行递归创建左右子树。</p><p>关键是要构建<code>下面的序列</code>，注意下标值。</p><table><thead><tr class="header"><th align="center"></th><th align="center">前序</th><th align="center">中序</th></tr></thead><tbody><tr class="odd"><td align="center">左子树</td><td align="center">2, 4, 7</td><td align="center">4, 7, 2</td></tr><tr class="even"><td align="center">右子树</td><td align="center">3, 5, 6, 8</td><td align="center">5, 3, 8, 6</td></tr></tbody></table><p><a href="https://github.com/plmsmile/aim2offer/blob/master/07_construct_binary_tree/construct_binary_tree.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 递归利用先序和中序重建二叉树</span></span><br><span class="line"><span class="comment">// Args:</span></span><br><span class="line"><span class="comment">//      vpre: 先序序列</span></span><br><span class="line"><span class="comment">//      vin: 中序序列</span></span><br><span class="line"><span class="comment">// Returns:</span></span><br><span class="line"><span class="comment">//      root: tree</span></span><br><span class="line"><span class="function">TreeNode * <span class="title">reconstruct_binary_tree</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;vpre, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vin)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 1. 为空，停止递归</span></span><br><span class="line">    <span class="keyword">if</span> (vpre.size() == <span class="number">0</span> || vin.size() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 构建根节点</span></span><br><span class="line">    TreeNode *root = <span class="keyword">new</span> TreeNode(vpre[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 找到根节点在中序中的位置</span></span><br><span class="line">    <span class="keyword">int</span> root_index = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; vin.size(); i++) &#123;</span><br><span class="line">        <span class="comment">// cout &lt;&lt; vin[i] &lt;&lt; " " &lt;&lt; vpre[0] &lt;&lt; endl;</span></span><br><span class="line">        <span class="keyword">if</span> (vin[i] == vpre[<span class="number">0</span>]) &#123;</span><br><span class="line">            root_index = i;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 简单判断一下</span></span><br><span class="line">    <span class="keyword">if</span> (root_index == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"root_index is -1"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 4. 生成左右子树的先序序列、中序序列</span></span><br><span class="line">    <span class="keyword">int</span> leftlen = root_index;</span><br><span class="line">    <span class="keyword">int</span> rightlen = vin.size() - leftlen - <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; leftvpre(leftlen), leftvin(leftlen);</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; rightvpre(rightlen), rightvin(rightlen);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 重点在这里，用实际例子去对照看</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; vin.size(); i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; root_index) &#123;</span><br><span class="line">            <span class="comment">// 左子树</span></span><br><span class="line">            leftvin[i] = vin[i];</span><br><span class="line">            leftvpre[i] = vpre[i+<span class="number">1</span>];</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (i &gt; root_index)&#123;</span><br><span class="line">            <span class="comment">// 右子树，条件特别重要</span></span><br><span class="line">            <span class="keyword">int</span> right_idx = i - root_index - <span class="number">1</span>;</span><br><span class="line">            rightvin[right_idx] = vin[i];</span><br><span class="line">            rightvpre[right_idx] = vpre[leftlen + <span class="number">1</span> + right_idx];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 5. 递归生成左右子树</span></span><br><span class="line">    root-&gt;left = reconstruct_binary_tree(leftvpre, leftvin);</span><br><span class="line">    root-&gt;right = reconstruct_binary_tree(rightvpre, rightvin);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="二叉树的下一个节点-08">二叉树的下一个节点-08</h1><p>二叉树：值，左孩子，右孩子，父亲节点指针。</p><p>给一个节点，找出中序序列的该节点的下一个节点。<strong>重在分析中序序列</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="string">"有右子树"</span>:</span><br><span class="line">    <span class="comment"># 向左走</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="string">"p有左孩子"</span>)</span><br><span class="line">    p = <span class="string">"左孩子"</span></span><br><span class="line">    t = p</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 向上走</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="string">"p有父节点 &amp;&amp; p是父节点的右节点"</span>):</span><br><span class="line">        p = <span class="string">"父节点"</span></span><br><span class="line">    t = p</span><br></pre></td></tr></table></figure><p><a href="https://github.com/plmsmile/aim2offer/blob/master/08_tree_next_node/btree_next_node.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 找到中序遍历的下一个节点</span></span><br><span class="line"><span class="comment">// Args:</span></span><br><span class="line"><span class="comment">//      pnode: 当前节点</span></span><br><span class="line"><span class="comment">// Returns:</span></span><br><span class="line"><span class="comment">//      pnext: 中序中，pnode的下一个节点</span></span><br><span class="line"><span class="function">TreeNode* <span class="title">get_next_inorder</span><span class="params">(TreeNode* pnode)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (pnode == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">    &#125; </span><br><span class="line">    TreeNode* pnext = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="keyword">if</span> (pnode-&gt;right != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        TreeNode* p = pnode-&gt;right;</span><br><span class="line">        <span class="keyword">while</span> (p-&gt;left != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            p = p-&gt;left;</span><br><span class="line">        &#125;</span><br><span class="line">        pnext = p;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        TreeNode* p = pnode;</span><br><span class="line">        <span class="keyword">while</span> (p-&gt;parent != <span class="literal">nullptr</span> &amp;&amp; p == p-&gt;parent-&gt;right) &#123;</span><br><span class="line">            p = p-&gt;parent;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (p-&gt;parent != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">            pnext = p-&gt;parent;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pnext;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="两个栈实现队列-09">两个栈实现队列-09</h1><h2 id="栈和队列">栈和队列</h2><p><code>栈</code> ：<strong>后进先出</strong></p><ul><li>题31：<code>栈的压入、弹出序列</code></li><li><span class="math inline">\(O(n)\)</span> 找到最大最小元素。若<span class="math inline">\(O(1)\)</span> ，则题30：<code>包含min函数的栈</code></li></ul><p><code>队列</code> ：<strong>先进先出</strong></p><ul><li>树的层次遍历，题32： <code>从上到下打印二叉树</code></li></ul><h2 id="用两个栈实现队列">用两个栈实现队列</h2><p>分为入栈（栈A）和出栈（栈B）。</p><ul><li>入队时：直接进入入栈</li><li>出队时：若出栈为空，则把入栈里的内容放入出栈；再从出栈里面出一个元素。</li></ul><p>[关键代码]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; stackIn;</span><br><span class="line">        <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; stackOut;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="comment">// 入队</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> node)</span> </span>&#123;</span><br><span class="line">            stackIn.push(node);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 出队</span></span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;empty()) &#123;</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"empty queue"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> node = <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (stackOut.empty() == <span class="literal">true</span>) &#123;</span><br><span class="line">                <span class="keyword">while</span> (stackIn.empty() == <span class="literal">false</span>) &#123;</span><br><span class="line">                    node = stackIn.top();</span><br><span class="line">                    stackIn.pop();</span><br><span class="line">                    stackOut.push(node);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            node = stackOut.top();</span><br><span class="line">            stackOut.pop();</span><br><span class="line">            <span class="keyword">return</span> node;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> stackIn.empty() == <span class="literal">true</span> &amp;&amp; stackOut.empty() == <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="两个队列实现栈">两个队列实现栈</h2><p>分为空队列和非空队列</p><ul><li>入栈：进入非空队列</li><li>出栈：非空队列中中前n-1个进入空队列，出非空队列最后一个元素（最新进来的）</li></ul><p><a href="https://github.com/plmsmile/aim2offer/blob/master/09_stack2queue/queue2stack.cpp" target="_blank" rel="noopener">关键代码</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span>-&gt;empty()) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 找到哪个队列有元素，注意使用指针</span></span><br><span class="line">    <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt;* qout;</span><br><span class="line">    <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt;* qin;</span><br><span class="line">    <span class="keyword">if</span> (q1.empty() == <span class="literal">true</span>) &#123;</span><br><span class="line">      qout = &amp;q2;</span><br><span class="line">      qin = &amp;q1;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      qout = &amp;q1;</span><br><span class="line">      qin = &amp;q2;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// qout的前n-1个元素放到qin中</span></span><br><span class="line">    <span class="keyword">while</span> (qout-&gt;size() &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      qin-&gt;push(qout-&gt;front());</span><br><span class="line">      qout-&gt;pop();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> res = qout-&gt;back();</span><br><span class="line">    qout-&gt;pop();</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="算法和数据操作">算法和数据操作</h1><h2 id="总览">总览</h2><table><thead><tr class="header"><th align="center">类型</th><th align="center">题型</th><th align="center">备注</th></tr></thead><tbody><tr class="odd"><td align="center">递归和循环</td><td align="center">树的遍历</td><td align="center">递归简介，循环效率高</td></tr><tr class="even"><td align="center">排序和查找</td><td align="center">二分查找、归并排序、快速排序</td><td align="center">正确、完整写出代码</td></tr><tr class="odd"><td align="center">二维数组</td><td align="center">迷宫、棋盘</td><td align="center">回溯法；栈模拟递归</td></tr><tr class="even"><td align="center">最优解</td><td align="center">动态规划，问题分解为多个子问题</td><td align="center">自上而下递归分析；自下而上循环代码实现，数组保存</td></tr><tr class="odd"><td align="center">最优解</td><td align="center">贪心算法</td><td align="center">分解时是否存在某个特殊选择：贪心得到最优解</td></tr><tr class="even"><td align="center"></td><td align="center">与、或、异或、左移、右移</td><td align="center"></td></tr></tbody></table><p>递归效率低的原因：函数调用自身，函数调用是由时间和空间的消耗；会在<strong>内存栈</strong>中分配空间以保存参数，返回地址和临时变量，往栈里弹入和弹出都需要时间。</p><p>递归和循环：题10，<code>斐波那契数列</code>；题60，<code>n个骰子的点</code>数。</p><p>动态规划</p><p>递归思路分析，递归分解的子问题中存在着大量的重复。用自下而上的循环来实现代码。题14 <code>剪绳子</code>， 题47<code>礼物的最大价值</code> ， 题48<code>最长不含重复字符的子字符串</code></p><h1 id="斐波那契数列-递归循环-10">斐波那契数列-递归循环-10</h1><h2 id="斐波那契数列">斐波那契数列</h2><p>数列定义 <span class="math display">\[f(n) = \begin{cases}&amp;0 &amp; n=0 \\&amp;1 &amp; n=1 \\&amp;f(n-1) + f(n-2) &amp; n \ge 1 \end{cases}\]</span> 递归和循环两种实现策略</p><p>关键代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="keyword">long</span> <span class="title">fibonacci_recursion</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> fibonacci_recursion(n<span class="number">-1</span>) + fibonacci_recursion(n<span class="number">-2</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="keyword">long</span> <span class="title">fibonacci_loop</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> f1 = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> f2 = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> fn = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= n; i++) &#123;</span><br><span class="line">        fn = f1 + f2;</span><br><span class="line">        f1 = f2;</span><br><span class="line">        f2 = fn;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> fn;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="青蛙跳台阶">青蛙跳台阶</h2><blockquote><p>青蛙可以一次跳1个台阶，一次跳2个台阶。问青蛙跳n个台阶有多少种跳法。</p></blockquote><p>青蛙跳到第n个台阶有两种跳法：跳1个和2个。所以<span class="math inline">\(f(n)=f(n-1)+f(n-2)\)</span> 。是斐波那契数列。</p><p><strong>扩展</strong></p><blockquote><p>青蛙一次可以跳1个台阶、2个台阶、n个台阶。问有多少种跳法？</p></blockquote><p>数学归纳法证得：<span class="math inline">\(f(n) = 2^{n-1}\)</span></p><h2 id="矩阵覆盖问题">矩阵覆盖问题</h2><blockquote><p><span class="math inline">\(2\times1\)</span>矩阵去覆盖<span class="math inline">\(2 \times 8\)</span> 矩阵，可以横着竖着覆盖，问多少种覆盖方法？</p></blockquote><p>同理，最后一个横着放或者竖着放。<span class="math inline">\(f(8)=f(7)+f(6)\)</span></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数组中重复的数字-03&quot;&gt;数组中重复的数字-03&lt;/h1&gt;
&lt;h2 id=&quot;题目1&quot;&gt;题目1&lt;/h2&gt;
&lt;p&gt;找到数组中重复的数字&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一个数组存放n个数字，所有数字在[0, n-1]范围内。某些数字是随机重复的。请找出
      
    
    </summary>
    
      <category term="leetcode" scheme="http://plmsmile.github.io/categories/leetcode/"/>
    
    
      <category term="leetcode" scheme="http://plmsmile.github.io/tags/leetcode/"/>
    
      <category term="排序" scheme="http://plmsmile.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>简单的卷积神经网络</title>
    <link href="http://plmsmile.github.io/2017/07/18/cnn-mnist/"/>
    <id>http://plmsmile.github.io/2017/07/18/cnn-mnist/</id>
    <published>2017-07-18T11:23:56.000Z</published>
    <updated>2017-09-22T03:22:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="卷积神经网络">卷积神经网络</h2><h3 id="概要">概要</h3><p><strong>卷积神经网络(Convolutional Neural Network, CNN)</strong>是人工神经网络中的一种，是一种特殊的对图像识别的方式，属于非常有效的带有前向反馈的网络。也用于音频信号、文本数据、人脸识别等等。$ $</p><p>CNN不需要把特征提取和分类训练两个过程分开，在训练的时候就提取了最有效的特征，降低了对图形数据预处理的要求。</p><blockquote><p>卷积神经网络的核心思想是将输入信息切分成一个个子采样层进行采样，然后将提取的特征和权重值作为参数，传导到下一层</p></blockquote><p><strong>主要思路</strong></p><ul><li>每一个卷积操作只处理一小块图像，提取出最有效的特征，传给紧接着的池化层</li><li>池化层用来降采样，求局部平均和二次采样</li><li>循环上面两种操作</li><li>不停地对基础特征进行组合和抽象，得到更高阶的特征</li></ul><p><strong>主要特点</strong></p><ul><li>局部连接：减少了连接数量</li><li>权值共享：大幅度减少参数数量，防止过拟合又降低了复杂度</li><li>降采样紧跟卷积层：对样本有较高的畸变容忍能力</li><li>特征分区提取、时间或空间采样等规则</li><li>理论上具有对图像缩放、平移和旋转的不变性，有着很强的<code>泛化性</code></li></ul><h3 id="网络结构">网络结构</h3><p><strong>传统网路的问题</strong></p><p>传统网络是全连接的，假如图像比较大是[200, 200, 3]，则会有<span class="math inline">\(200*200*3=12000\)</span>个神经元。这样庞大的神经元做全连接结构是非常浪费的，并且有大量的参数会导致<strong>过拟合</strong>。</p><p><strong>卷积神经网络</strong></p><p>每一个像素点在空间上和周围的像素点实际上是有紧密联系的，但是和太遥远的像素点就不一定有什么联系了。这也是<code>视觉感受野</code>的概念，每一个感受野只接受一小块区域的信号。</p><p>一个卷积神经网络通常由多个卷积层组成，每一个卷积操作只处理一小块图像（也称作卷积核滤波），提取出最有效的特征传递给后面，主要操作如下：</p><ul><li>一个卷积核可以提取出一种特征。一个图像经历一个卷积核之后，会输出一个新的图像，称作<code>特征图谱(Feature Map, FM)</code>。其实卷积操作也是<span class="math inline">\(w * x + \vec b\)</span>，只不过对于单次的卷积操作<span class="math inline">\(w\)</span>和<span class="math inline">\(\vec b\)</span>是不变的，而<span class="math inline">\(x\)</span>是会变的，每次取图片的一小块，会得到很多的结果，拼凑起来就是一张新的FM</li><li>将前面卷积的滤波结果FM，进行非线性的激活函数处理。之前是<code>sigmoid</code>，现在是<code>Relu</code>函数，比较完美解决梯度弥散的问题。</li><li>对激活函数的结果进行降采样（池化操作），比如将[2,2]将为[1,1]的图片。常用的有最大值合并、平均值合并和随机合并。思路和卷积差不多，只不过结果不用乘加，只是选择最大的就行了。</li><li>最后一个子采样层一般会全连接一个或多个全连接层，全连接层的输出就是最后的输出。一般是<code>softmax</code></li></ul><div align="center"><img src="http://otafnwsmg.bkt.clouddn.com/image/dl/cnn_structure.jpg" style="zoom:70%; margin:auto"></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;卷积神经网络&quot;&gt;卷积神经网络&lt;/h2&gt;
&lt;h3 id=&quot;概要&quot;&gt;概要&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;卷积神经网络(Convolutional Neural Network, CNN)&lt;/strong&gt;是人工神经网络中的一种，是一种特殊的对图像识别的方式，属于
      
    
    </summary>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络" scheme="http://plmsmile.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="卷积" scheme="http://plmsmile.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorflow" scheme="http://plmsmile.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>利用tensorflow实现简版word2vec</title>
    <link href="http://plmsmile.github.io/2017/07/14/word2vec/"/>
    <id>http://plmsmile.github.io/2017/07/14/word2vec/</id>
    <published>2017-07-14T12:17:50.000Z</published>
    <updated>2017-09-22T07:14:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关知识">相关知识</h2><h3 id="传统方法">传统方法</h3><p><code>One-Hot Encoder</code> 是一个词对应一个向量，向量中只有一个是1，其余是0，离散表达。$ $</p><p><code>Bag of Words</code> 标识当前单词那一位不是1，而是变成了当前单词的出现次数。</p><p><strong>存在的问题</strong> 需要大量的维数去表示，编码随机的，没有任何关联的信息。</p><h3 id="向量空间模型">向量空间模型</h3><p><code>Vector Space Models</code>可以把字词转化为连续值，并将意思相近的词被映射到向量空间相近的位置。</p><p>VSM在NLP中的一个重要假设是：在相同语境中出现的词，语义也相近。</p><p>有如下两种模型</p><ul><li><code>计数模型</code> 统计语料库中相邻出现的词的频率，再把这些计数结果转为小而稠密的矩阵。</li><li><code>预测模型</code> 根据一个词周围相邻的词汇推测出这个词。</li></ul><h3 id="word2vec">Word2Vec</h3><p><code>Word2Vec</code>是一种计算非常高效的、可以从原始语料中学习字词空间向量的预测模型。</p><p>有如下两种模型</p><ul><li><code>CBOW</code> Continuous Bag of Words 从语境推测目标词汇，适合小型数据。如“中国的首都是__”推测出“北京”。把一整段上下文信息当做一个观察对象</li><li><code>Skip-Gram</code> 从目标词汇推测语境，适合大型语料。 把每一对<code>上下文-目标词汇</code>当做一个观察对象</li></ul><p>Word2Vec的一些优点</p><ul><li><p>连续的词向量能够捕捉到更多的语义和关联信息</p></li><li>意思相近的词语在向量空间中的位置也会比较近。如北京-成都、狗-猫等词汇会分别聚集在一起。</li><li><p>能学会一些高阶语言的抽象概念。如&quot;man-woman&quot;和&quot;king-queen&quot;的向量很相似。</p></li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://www.tensorflow.org/images/linear-relationships.png" alt="Word2Vec学习的抽象概念" title="">                </div>                <div class="image-caption">Word2Vec学习的抽象概念</div>            </figure><h3 id="噪声对比训练">噪声对比训练</h3><p>神经概率化语言模型通常使用极大似然法进行训练，再使用<code>Softmax</code>函数得到在给出上下文单词<span class="math inline">\(h\)</span>的情况下，目标词<span class="math inline">\(w_t\)</span>出现的最大概率，设为<span class="math inline">\(P(w_t|h)\)</span>。</p><p>设<span class="math inline">\(score(w_t, h)\)</span>为当前词<span class="math inline">\(w_t\)</span>和上下文单词<span class="math inline">\(h\)</span>的相容性，通常使用向量积获得。</p><p><span class="math display">\[P(w_t|h) = Softmax(score(w_i, h))=\frac{e^{score(w_i, h)}} {\sum_{i=1}^v {e^{score(w_i, h)}}}\]</span></p><p>通过对数似然函数max likelihood来进行训练 <span class="math display">\[J_{ml}=\ln{P(w_t|h)}=score(w_t,h)-\ln{\sum_{i=1}^v e^{score(w_i, h)}}\]</span> 这个方法看起来可行，但是消耗太大了，因为要对当前<span class="math inline">\(h\)</span>与所有单词<span class="math inline">\(w\)</span>的相容性<span class="math inline">\(score(w, h)\)</span>。</p><p><img src="https://www.tensorflow.org/images/softmax-nplm.png" style="display:block; margin:auto" width="60%"></p><p>在使用word2vec模型中，我们并不需要对所有的特征进行学习。所以在CBOW模型和Skip-Gram模型中，会构造<span class="math inline">\(k\)</span>个噪声单词，而我们只需要从这k个中找出真正目标单词<span class="math inline">\(w_t\)</span>即可，使用了一个二分类器（lr）。下面是CBOW模型，对于Skip-Gram模型只需要相反就行了。</p><p><img src="https://www.tensorflow.org/images/nce-nplm.png" style="display:block; margin:auto" width="60%"></p><p>设<span class="math inline">\(Q_\theta(D=1|w, h)\)</span>是二元逻辑回归的概率，即在当前条件下出现词语<span class="math inline">\(w\)</span>的概率。</p><ul><li><span class="math inline">\(\theta\)</span> 输入的embedding vector</li><li><span class="math inline">\(h\)</span> 当前上下文</li><li><span class="math inline">\(d\)</span> 输入数据集</li><li><span class="math inline">\(w\)</span> 目标词汇（就是他出现的概率）</li></ul><p>此时，最大化目标函数如下： <span class="math display">\[J_{NEG}=\ln{Q_\theta(D=1|w_t, h)} + \frac {\sum_{i=1}^{k}{\ln Q_\theta(D=0|w_I, h)}} {k}\]</span> 前半部分为词<span class="math inline">\(w\)</span>出现的概率，后面为<span class="math inline">\(k\)</span>个噪声概率的期望值（如果写法有错误，希望提出，再改啦），有点像<a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration" target="_blank" rel="noopener">蒙特卡洛</a>。</p><p><code>负采样Negative Sampling</code></p><ul><li>当模型预测的真实目标词汇<span class="math inline">\(w_t\)</span>的概率越高，其他噪声词汇概率越低，模型就得到优化了</li><li>用编造的噪声词汇进行训练</li><li>计算loss效率非常高，只需要随机选择<span class="math inline">\(k\)</span>个，而不是全部词汇</li></ul><h2 id="实现skip-gram模型">实现Skip-Gram模型</h2><h3 id="数据说明">数据说明</h3><p>Skip-Gram模型是通过目标词汇预测语境词汇。如数据集如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">I hope you always find a reason to smile</span><br></pre></td></tr></table></figure><p>从中我们可以得到很多目标单词和所对应的上下文信息（多个单词）。如假设设左右词的窗口距离为1，那么相应的信息如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'hope'</span>:[<span class="string">'i'</span>, <span class="string">'you'</span>], <span class="string">'you'</span>:[<span class="string">'hope'</span>, <span class="string">'alawys'</span>]...&#125;</span><br></pre></td></tr></table></figure><p>训练时，希望给出目标词汇就能够预测出语境词汇，所以需要这样的训练数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前面是目标单词，后面是语境词汇，实际上相当于数据的label</span></span><br><span class="line">(<span class="string">'hope'</span>, <span class="string">'i'</span>)</span><br><span class="line">(<span class="string">'hope'</span>, <span class="string">'you'</span>)</span><br><span class="line">(<span class="string">'you'</span>, <span class="string">'hope'</span>)</span><br><span class="line">(<span class="string">'you'</span>, <span class="string">'always'</span>)</span><br></pre></td></tr></table></figure><p>同时在训练时，制造一些随机单词作为负样本（噪声）。我们希望预测的概率分布在正样本上尽可能大，在负样本上尽可能小。</p><p>使用<code>随机梯度下降算法(SGD)</code>来进行最优化求解，并且使用<code>mini-batch</code>的方法，这样来<strong>更新embedding中的参数<span class="math inline">\(\theta\)</span>，让损失函数(NCE loss)尽可能小</strong>。这样，每个单词的词向量就会在训练的过程中不断调整，最后会处在一个最合适的语料空间位置。</p><p>例如，假设训练第<span class="math inline">\(t\)</span>步，输入目标单词<code>hope</code>，希望预测出<code>you</code>，选择一个噪声词汇<code>reason</code>。则目标函数如下 <span class="math display">\[J_{NEG}^{(t)}=\ln {Q_\theta(D=1|hope, you)} + \ln{Q_\theta(D=0|hope, reason)}\]</span> 目标是更新embedding的参数<span class="math inline">\(\theta\)</span>以增大目标值，更新方式是计算损失函数对参数<span class="math inline">\(\theta\)</span>的导数，使得参数<span class="math inline">\(\theta\)</span>朝梯度方向进行调整。多次以后，模型就能够很好区别出真实语境单词和噪声词。</p><h3 id="构建数据集">构建数据集</h3><p>先来分析数据，对所有的词汇进行编码。对高频词汇给一个<code>id</code>，对于出现次数很少词汇，id就设置为0。高频是选择出现频率最高的50000个词汇。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(self, words)</span>:</span></span><br><span class="line">    <span class="string">''' 构建数据集</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            words: 单词列表</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            word_code: 所有word的编码，top的单词：数量；其余的：0</span></span><br><span class="line"><span class="string">            topword_id: topword-id</span></span><br><span class="line"><span class="string">            id_topword: id-word</span></span><br><span class="line"><span class="string">            topcount: 包含所有word的一个Counter对象</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">    <span class="comment"># 获取top50000频数的单词</span></span><br><span class="line">    unk = <span class="string">'UNK'</span></span><br><span class="line">    topcount = [[unk, <span class="number">-1</span>]]</span><br><span class="line">    topcount.extend(</span><br><span class="line">        collections.Counter(words).most_common(</span><br><span class="line">            self.__vocab_size - <span class="number">1</span>))</span><br><span class="line">    topword_id = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word, _ <span class="keyword">in</span> topcount:</span><br><span class="line">        topword_id[word] = len(topword_id)</span><br><span class="line">        <span class="comment"># 构建单词的编码。top单词：出现次数；其余单词：0</span></span><br><span class="line">        word_code = []</span><br><span class="line">        unk_count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">            <span class="keyword">if</span> w <span class="keyword">in</span> topword_id:</span><br><span class="line">                c = topword_id[w]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                c = <span class="number">0</span></span><br><span class="line">                unk_count += <span class="number">1</span></span><br><span class="line">            word_code.append(c)</span><br><span class="line">    topcount[<span class="number">0</span>][<span class="number">1</span>] = unk_count</span><br><span class="line">    id_topword = dict(zip(topword_id.values(), topword_id.keys()))</span><br><span class="line">    <span class="keyword">return</span> word_code, topword_id, id_topword, topcount</span><br></pre></td></tr></table></figure><h3 id="产生batch训练样本">产生batch训练样本</h3><p>由于是使用<code>mini-batch</code>的训练方法，所以每次要产生一些样本。对于每个单词，要确定要产生多少个语境单词，和最多可以左右选择多远。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(self, batch_size, single_num, skip_window, word_code)</span>:</span></span><br><span class="line">    <span class="string">'''产生训练样本。Skip-Gram模型，从当前推测上下文</span></span><br><span class="line"><span class="string">    如 i love you. (love, i), (love, you)</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        batch_size: 每一个batch的大小，即多少个()</span></span><br><span class="line"><span class="string">        single_num: 对单个单词生成多少个样本</span></span><br><span class="line"><span class="string">        skip_window: 单词最远可以联系的距离</span></span><br><span class="line"><span class="string">        word_code: 所有单词，单词以code形式表示</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        batch: 目标单词</span></span><br><span class="line"><span class="string">        labels: 语境单词</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 条件判断</span></span><br><span class="line">    <span class="comment"># 确保每个batch包含了一个词汇对应的所有样本</span></span><br><span class="line">    <span class="keyword">assert</span> batch_size % single_num == <span class="number">0</span></span><br><span class="line">    <span class="comment"># 样本数量限制</span></span><br><span class="line">    <span class="keyword">assert</span> single_num &lt;= <span class="number">2</span> * skip_window</span><br><span class="line"></span><br><span class="line">    <span class="comment"># batch label</span></span><br><span class="line">    batch = np.ndarray(shape=(batch_size), dtype=np.int32)</span><br><span class="line">    labels = np.ndarray(shape=(batch_size, <span class="number">1</span>), dtype=np.int32)</span><br><span class="line">    <span class="comment"># 目标单词和相关单词</span></span><br><span class="line">    span = <span class="number">2</span> * skip_window + <span class="number">1</span></span><br><span class="line">    word_buffer = collections.deque(maxlen=span)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(span):</span><br><span class="line">        word_buffer.append(word_code[self.__data_index])</span><br><span class="line">        self.__data_index = (self.__data_index + <span class="number">1</span>) % len(word_code)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历batchsize/samplenums次，每次一个目标词汇，一次samplenums个语境词汇</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size // single_num):</span><br><span class="line">        target = skip_window                <span class="comment"># 当前的单词</span></span><br><span class="line">        targets_to_void = [skip_window]     <span class="comment"># 已经选过的单词+自己本身</span></span><br><span class="line">        <span class="comment"># 为当前单词选取样本</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(single_num):</span><br><span class="line">            <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_void:</span><br><span class="line">                target = random.randint(<span class="number">0</span>, span - <span class="number">1</span>)</span><br><span class="line">            targets_to_void.append(target)</span><br><span class="line">            batch[i * single_num + j] = word_buffer[skip_window]</span><br><span class="line">            labels[i * single_num + j, <span class="number">0</span>] = word_buffer[target]</span><br><span class="line">        <span class="comment"># 当前单词已经选择完毕，输入下一个单词，skip_window单词也成为下一个</span></span><br><span class="line">        self.__data_index = (self.__data_index + <span class="number">1</span>) % len(word_code)</span><br><span class="line">        word_buffer.append(word_code[self.__data_index])</span><br><span class="line">    <span class="keyword">return</span> batch, labels</span><br></pre></td></tr></table></figure><h3 id="一些配置信息">一些配置信息</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 频率top50000个单词</span></span><br><span class="line">vocab_size = <span class="number">50000</span></span><br><span class="line"><span class="comment"># 一批样本的数量</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># 将单词转化为稠密向量的维度</span></span><br><span class="line">embedding_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># 为单词找相邻单词，向左向右最多能取得范围</span></span><br><span class="line">skip_window = <span class="number">1</span></span><br><span class="line"><span class="comment"># 每个单词的语境单词数量</span></span><br><span class="line">single_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证单词的数量</span></span><br><span class="line">valid_size = <span class="number">16</span></span><br><span class="line"><span class="comment"># 验证单词从频数最高的100个单词中抽取</span></span><br><span class="line">valid_window = <span class="number">100</span></span><br><span class="line"><span class="comment"># 从100个中随机选择16个</span></span><br><span class="line">valid_examples = np.random.choice(valid_window, valid_size, replace=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># 负样本的噪声数量</span></span><br><span class="line">noise_num = <span class="number">64</span></span><br></pre></td></tr></table></figure><h3 id="计算图">计算图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    <span class="comment"># 输入数据</span></span><br><span class="line">    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])</span><br><span class="line">    train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br><span class="line">    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">        <span class="comment"># 随机生成单词的词向量，50000*128</span></span><br><span class="line">        embeddings = tf.Variable(</span><br><span class="line">            tf.random_uniform([vocab_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">        <span class="comment"># 查找输入inputs对应的向量</span></span><br><span class="line">        embed = tf.nn.embedding_lookup(embeddings, train_inputs)</span><br><span class="line">        nce_weights = tf.Variable(</span><br><span class="line">            tf.truncated_normal([vocab_size, embedding_size],</span><br><span class="line">                                stddev=<span class="number">1.0</span> / math.sqrt(embedding_size)))</span><br><span class="line">        nce_biases = tf.Variable(tf.zeros([vocab_size]))</span><br><span class="line">    <span class="comment"># 为每个batch计算nceloss</span></span><br><span class="line">    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,</span><br><span class="line">                                         biases=nce_biases,</span><br><span class="line">                                         labels = train_labels,</span><br><span class="line">                                         inputs=embed,</span><br><span class="line">                                         num_sampled=noise_num,</span><br><span class="line">                                         num_classes=vocab_size))</span><br><span class="line">    <span class="comment"># sgd</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">1.0</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算embeddings的L2范式，各元素的平方和然后求平方根，防止过拟合</span></span><br><span class="line">    norm = tf.sqrt(</span><br><span class="line">        tf.reduce_sum(</span><br><span class="line">            tf.square(embeddings),</span><br><span class="line">            axis=<span class="number">1</span>,</span><br><span class="line">            keep_dims=<span class="keyword">True</span>))</span><br><span class="line">    <span class="comment"># 标准化词向量</span></span><br><span class="line">    normalized_embeddings = embeddings / norm</span><br><span class="line">    valid_embeddings = tf.nn.embedding_lookup(</span><br><span class="line">                                normalized_embeddings, valid_dataset)</span><br><span class="line">    <span class="comment"># valid单词和所有单词的相似度计算，向量相乘</span></span><br><span class="line">    similarity = tf.matmul(</span><br><span class="line">        valid_embeddings,</span><br><span class="line">        normalized_embeddings,</span><br><span class="line">        transpose_b=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><h3 id="训练过程">训练过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">100001</span></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    print(<span class="string">'Initialized'</span>)</span><br><span class="line">    avg_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        batch_inputs, batch_labels = wu.generate_batch(</span><br><span class="line">                batch_size, single_num, skip_window,  word_code)</span><br><span class="line">        feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125;</span><br><span class="line">        _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)</span><br><span class="line">        avg_loss += loss_val</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">                avg_loss /= <span class="number">2000</span></span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"avg loss at step %s : %s"</span> % (step, avg_loss)) </span><br><span class="line">            avg_loss = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 相似度，16*50000</span></span><br><span class="line">            sim = similarity.eval()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(valid_size):</span><br><span class="line">                valid_word = id_topword[valid_examples[i]]</span><br><span class="line">                <span class="comment"># 选相似的前8个</span></span><br><span class="line">                top_k = <span class="number">8</span></span><br><span class="line">                <span class="comment"># 排序，获得id</span></span><br><span class="line">                nearest = (-sim[i, :]).argsort()[<span class="number">1</span>:top_k+<span class="number">1</span>]</span><br><span class="line">                log_str = <span class="string">"Nearest to %s: "</span> % valid_word</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(top_k):</span><br><span class="line">                    close_word = id_topword[nearest[k]]</span><br><span class="line">                    log_str = <span class="string">"%s %s,"</span> % (log_str, close_word)</span><br><span class="line">                <span class="keyword">print</span> log_str</span><br><span class="line">    final_embeddings = normalized_embeddings.eval()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;相关知识&quot;&gt;相关知识&lt;/h2&gt;
&lt;h3 id=&quot;传统方法&quot;&gt;传统方法&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;One-Hot Encoder&lt;/code&gt; 是一个词对应一个向量，向量中只有一个是1，其余是0，离散表达。$ $&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Bag of Wo
      
    
    </summary>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="自然语言处理" scheme="http://plmsmile.github.io/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="深度学习" scheme="http://plmsmile.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="word2vec" scheme="http://plmsmile.github.io/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>机器学习-朴素贝叶斯</title>
    <link href="http://plmsmile.github.io/2017/05/06/ml-ch03-bayes/"/>
    <id>http://plmsmile.github.io/2017/05/06/ml-ch03-bayes/</id>
    <published>2017-05-06T06:36:58.000Z</published>
    <updated>2017-05-07T06:28:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="条件概率">条件概率</h2><h3 id="基础知识">基础知识</h3><p><strong>条件概率</strong></p><p>在<span class="math inline">\(B\)</span>发生的情况下<span class="math inline">\(A\)</span>的概率$ $</p><p>​ <span class="math inline">\(P(A|B) = \frac{P(AB)}{P(B)}\)</span></p><p>​ <span class="math inline">\(P(c_i|x)=\frac{P(c_ix)}{P(x)}\)</span>。<span class="math inline">\(c_i\)</span>是类别，<span class="math inline">\(x\)</span>是一个向量。<span class="math inline">\(x\)</span>属于类别<span class="math inline">\(c_i\)</span>的概率。</p><p><strong>贝叶斯准则</strong></p><p>交换条件概率中的条件与结果，得到想要的值。</p><p><span class="math inline">\(P(A|B) = \frac{P(AB)}{P(B)}\)</span>, <span class="math inline">\(P(B|A) = \frac{P(AB)}{P(A)}\)</span> <span class="math inline">\(\to\)</span> <span class="math inline">\(P(B|A)=\frac{P(A|B)P(B)}{P(A)}\)</span></p><p>所以可以得到<span class="math inline">\(\color{red}{P(c_i|x)}=\frac{P(x|c_i)P(c_i)}{P(x)}\)</span></p><h3 id="条件概率分类">条件概率分类</h3><p><strong>贝叶斯决策理论</strong></p><p>计算两个概率<span class="math inline">\(x\)</span>属于类别1和类别2的概率<span class="math inline">\(p_1(x)\)</span>和<span class="math inline">\(p_2(x)\)</span>。</p><ul><li>如果<span class="math inline">\(p_1(x) &gt; p_2(x)\)</span>，则<span class="math inline">\(x\)</span>属于类别1</li><li>如果<span class="math inline">\(p_2(x) &gt; p_1(x)\)</span>，则<span class="math inline">\(x\)</span>属于类别2</li></ul><p><strong>贝叶斯准则</strong></p><p><span class="math inline">\(x\)</span>属于类别<span class="math inline">\(c_i\)</span>的概率是<span class="math inline">\(\color{red}{P(c_i|x)}\)</span>。</p><ul><li>如果<span class="math inline">\(P(c_1|x) &gt; P(c_2|x)\)</span>，则<span class="math inline">\(x\)</span>属于<span class="math inline">\(c_1\)</span></li><li>如果<span class="math inline">\(P(c_2|x) &gt; P(c_1|x)\)</span>，则<span class="math inline">\(x\)</span>属于<span class="math inline">\(c_2\)</span></li></ul><h3 id="朴素贝叶斯文档分类">朴素贝叶斯文档分类</h3><p><strong>简介</strong></p><p>机器学习的一个重要应用就是<code>文档的自动分类</code>。我们可以观察文档中出现的词，并把<code>每个词出现与否</code>或者<code>出现次数</code>作为一个<strong>特征</strong>。<code>朴素贝叶斯</code>就是用于文档分类的常用算法，当然它可以用于任意场景的分类。</p><p>向量<span class="math inline">\(\color{red}{\vec{w}}={(w_1,w_2,...,w_n)}\)</span>代表一篇<strong>文章</strong>。其中<span class="math inline">\(w_i=0,1\)</span>，代表<code>词汇表</code>中第<span class="math inline">\(i\)</span>个词汇出现与否。词汇表是指一个总体的全局词汇表。文章<span class="math inline">\(\vec{w}\)</span>属于第<span class="math inline">\(i\)</span>类的概率<span class="math inline">\(\color{red}{P(c_i|\vec{w})}=\frac{P(\vec{w}|c_i)P(c_i)}{P(\vec{w})}\)</span>。</p><p>朴素贝叶斯分类器的两个假设：</p><ul><li>特征之间相互独立</li><li>每个特征同等重要</li></ul><p>尽管这有瑕疵，但是朴素贝叶斯的实际效果却很好了。</p><p>朴素贝叶斯分类器的两种实现：</p><ul><li>伯努利模型：只考虑出现或者不出现</li><li>多项式模型：考虑词在文档中的出现次数</li></ul><p>文档分类中的<code>独立</code>：每个单词出现的可能性和其他单词没有关系。独立的好处在下面概率计算中会体现出来。</p><p><strong>概率计算</strong></p><p>对每一个文章的各个分类概率计算，其实只需要计算上式的分母就行了。</p><p>对于<span class="math inline">\(P(c_i)=\frac{c_i数量}{总数量}\)</span>，即<span class="math inline">\(c_i\)</span>类文章的数量除以所有类别的文章的总数量。</p><p>对于<span class="math inline">\(P(\vec{w}|c_i)\)</span>，要稍微复杂一些。由于各个特征（单词出现否）独立，则有如下推导公式：</p><p><span class="math display">\[P(\vec{w}|c_i)=P(w_1,w_2,...,w_n|c_i)=P(w_1|c_i)P(w_2|c_i)\cdots P(w_n|c_i)\]</span></p><p>其中<span class="math inline">\(\color{red}{P(w_i|c_i)}\)</span>代表<strong>第<span class="math inline">\(i\)</span>个单词</strong>在<strong><span class="math inline">\(c_i\)</span>类别文章的总词汇</strong>里出现的<strong>概率</strong>。</p><p>实际操作的一个小技巧，由于概率都很小多个<strong>小值做乘法</strong>会导致<strong>下溢出</strong>，所以决定对概率<strong>取对数做加法</strong>，最后再比较对数的大小。</p><p><span class="math display">\[\ln(P(\vec{w}|c_i))=\ln(P(w_1|c_i))+\ln(P(w_2|c_i))+\dots+\ln(P(w_n|c_i))\]</span></p><p>如上，可以求得每个单词在各个类别文章里出现的概率。用<span class="math inline">\(\color{red}{\vec{wp_0}}\)</span>、<span class="math inline">\(\color{red}{\vec{wp_1}}\)</span>来分别表示所有单词在类别0、类别1中总词汇中的概率。当然，在程序中实际上这个概率是取对数了的。</p><p>当要求一篇新的文章<span class="math inline">\(\color{red}{\vec{w}}={(0,1,0,0,\dots)}\)</span>，此时为出现或者不出现，当然也可以统计出现次数，属于哪个类别的时候，要先求出<span class="math inline">\(\color{red}{P(w|c_0)}\)</span>和<span class="math inline">\(\color{red}{P(w|c_1)}\)</span>，然后根据贝叶斯准则选择<strong>概率大的分类为结果</strong>。</p><p><span class="math display">\[P(w|c_0)=\vec{w}\cdot\vec{wp_0}, P(w|c_1)=\vec{w}\cdot\vec{wp_1}\]</span></p><h2 id="程序实现">程序实现</h2><p>朴素贝叶斯的实例应有很多，这里主要是介绍垃圾邮件分类。数据集中的邮件有两种：垃圾邮件和正常邮件。每个类型都有25个样本，一共是50个样本。我们对数据集进行划分为训练集和测试集。训练集用来训练获得<span class="math inline">\(\vec{wp_0}\)</span>、<span class="math inline">\(\vec{wp_1}\)</span>和<span class="math inline">\(p(c_1)\)</span>。然后用测试集去进行朴素贝叶斯分类，计算错误率，查看效果。</p><h3 id="加载数据">加载数据</h3><p>数据是存放在两个文件夹中的，以txt格式的形式存储。取出来后要进行单词切割。然后得到邮件列表email_list和它对应的分类列表class_list。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_str</span><span class="params">(big_str)</span>:</span></span><br><span class="line">    <span class="string">''' 解析文本为单词列表</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        big_str: 长文本</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        单词列表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 以任何非单词字符切割</span></span><br><span class="line">    word_list = re.split(<span class="string">r'\W*'</span>, big_str)</span><br><span class="line">    <span class="comment"># 只保留长度大于3的单词，并且全部转化为小写</span></span><br><span class="line">    <span class="keyword">return</span> [word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> word_list <span class="keyword">if</span> len(word) &gt; <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">(spam_dir, ham_dir)</span>:</span></span><br><span class="line">    <span class="string">''' 从文件夹中加载文件</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        spam_dir: 垃圾邮件文件夹</span></span><br><span class="line"><span class="string">        ham_dir: 正常邮件文件夹</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        email_list: 邮件列表</span></span><br><span class="line"><span class="string">        class_list: 分类好的列表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    email_list = []</span><br><span class="line">    class_list = []</span><br><span class="line">    txt_num = <span class="number">25</span>    <span class="comment"># 每个文件夹有25个文件</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, txt_num + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            file_dir = spam_dir <span class="keyword">if</span> j == <span class="number">1</span> <span class="keyword">else</span> ham_dir</span><br><span class="line">            f = open((<span class="string">'&#123;&#125;/&#123;&#125;.txt'</span>).format(file_dir, i))</span><br><span class="line">            f_str = f.read()</span><br><span class="line">            f.close()</span><br><span class="line">            words = parse_str(f_str)</span><br><span class="line">            email_list.append(words)    <span class="comment"># 邮件列表</span></span><br><span class="line">            class_list.append(j)        <span class="comment"># 分类标签，1垃圾邮件，0非垃圾邮件</span></span><br><span class="line">    <span class="keyword">return</span> email_list, class_list</span><br></pre></td></tr></table></figure><h3 id="划分数据集">划分数据集</h3><p>由于前面email_list包含所有的邮件，下标是从0-49，所以我们划分数据集只需要获得对应的索引集合就可以了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_test_indices</span><span class="params">(data_num)</span>:</span></span><br><span class="line">    <span class="string">''' 划分训练集和测试集</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data_num: 数据集的数量</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        train_indices: 训练集的索引列表</span></span><br><span class="line"><span class="string">        test_indices: 测试集的索引列表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    train_indices = range(data_num)</span><br><span class="line">    test_ratio = <span class="number">0.3</span>        <span class="comment"># 测试数据的比例</span></span><br><span class="line">    test_num = int(data_num * test_ratio)</span><br><span class="line">    test_indices = random.sample(train_indices, test_num)<span class="comment"># 随机抽样选择</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> test_indices:</span><br><span class="line">        train_indices.remove(i)</span><br><span class="line">    <span class="keyword">return</span> train_indices, test_indices</span><br></pre></td></tr></table></figure><h3 id="获得训练矩阵">获得训练矩阵</h3><p>获得训练数据之后，要把训练数据转化为训练矩阵。</p><p><strong>获得所有的词汇</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_vocab_list</span><span class="params">(post_list)</span>:</span></span><br><span class="line">    <span class="string">''' 从数据集中获取所有的不重复的词汇列表</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        post_list: 多个文章的列表，一篇文章：由单词组成的list</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        vocab_list: 单词列表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    vocab_set = set([])</span><br><span class="line">    <span class="keyword">for</span> post <span class="keyword">in</span> post_list:</span><br><span class="line">        vocab_set = vocab_set | set(post)</span><br><span class="line">    <span class="keyword">return</span> list(vocab_set)</span><br></pre></td></tr></table></figure><p><strong>获得一篇文章的文档向量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_doc_vec</span><span class="params">(doc, vocab_list, is_bag = False)</span>:</span></span><br><span class="line">    <span class="string">''' 获得一篇doc的文档向量</span></span><br><span class="line"><span class="string">    词集模型：每个词出现为1，不出现为0。每个词出现1次</span></span><br><span class="line"><span class="string">    词袋模型：每个词出现次数，可以多次出现。</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocab_list: 总的词汇表</span></span><br><span class="line"><span class="string">        doc: 一篇文档，由word组成的list</span></span><br><span class="line"><span class="string">        is_bag: 是否是词袋模型，默认为Fasle</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        doc_vec: 文档向量，1出现，0未出现</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    doc_vec = [<span class="number">0</span>] * len(vocab_list)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> doc:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocab_list:</span><br><span class="line">            idx = vocab_list.index(word)</span><br><span class="line">            <span class="keyword">if</span> is_bag == <span class="keyword">False</span>:         <span class="comment"># 词集模型</span></span><br><span class="line">                doc_vec[idx] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                doc_vec[idx] += <span class="number">1</span>       <span class="comment"># 词袋模型</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'词汇表中没有 %s '</span> % word</span><br><span class="line">    <span class="keyword">return</span> doc_vec</span><br></pre></td></tr></table></figure><p><strong>获得训练矩阵</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">go_bayes_email</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">''' 贝叶斯垃圾邮件过滤主程序</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        error_rate: 错误率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 源数据</span></span><br><span class="line">    email_list, class_list = load_dataset(<span class="string">'email/spam'</span>, <span class="string">'email/ham'</span>)</span><br><span class="line">    <span class="comment"># 总的词汇表</span></span><br><span class="line">    vocab_list = bys.get_vocab_list(email_list)</span><br><span class="line">    <span class="comment"># 训练数据，测试数据的索引列表</span></span><br><span class="line">    data_num = len(email_list)</span><br><span class="line">    train_indices, test_indices = get_train_test_indices(data_num)</span><br><span class="line">    <span class="comment"># 训练数据的矩阵和分类列表</span></span><br><span class="line">    train_mat = []</span><br><span class="line">    train_class = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> train_indices:</span><br><span class="line">        vec = bys.get_doc_vec(email_list[i], vocab_list)</span><br><span class="line">        train_mat.append(vec)</span><br><span class="line">        train_class.append(class_list[i])</span><br><span class="line">    <span class="comment"># 后续还有训练数据和测试数据，在下文给出</span></span><br></pre></td></tr></table></figure><h3 id="贝叶斯算法">贝叶斯算法</h3><p><strong>贝叶斯训练算法</strong></p><p>通过训练数据去计算上文提到的<span class="math inline">\(\vec{wp_0}\)</span>、<span class="math inline">\(\vec{wp_1}\)</span>和<span class="math inline">\(p(c_1)\)</span>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_nb0</span><span class="params">(train_mat, class_list)</span>:</span></span><br><span class="line">    <span class="string">''' 朴素贝叶斯训练算法，二分类问题</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        train_mat: 训练矩阵，文档向量组成的矩阵</span></span><br><span class="line"><span class="string">        class_list: 每一篇文档对应的分类结果</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        p0_vec: c0中各个word占c0总词汇的概率</span></span><br><span class="line"><span class="string">        p1_vec: c1中各个word占c1总词汇的概率</span></span><br><span class="line"><span class="string">        p1: 文章是c1的概率</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 文档数目，单词数目</span></span><br><span class="line">    doc_num = len(train_mat)</span><br><span class="line">    word_num = len(train_mat[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 两个类别的总单词数量</span></span><br><span class="line">    c0_word_count = <span class="number">2.0</span></span><br><span class="line">    c1_word_count = <span class="number">2.0</span></span><br><span class="line">    <span class="comment"># 向量累加</span></span><br><span class="line">    c0_vec_sum = np.ones(word_num)</span><br><span class="line">    c1_vec_sum = np.ones(word_num)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(doc_num):</span><br><span class="line">        <span class="keyword">if</span> class_list[i] == <span class="number">0</span>:</span><br><span class="line">            c0_word_count += sum(train_mat[i])</span><br><span class="line">            c0_vec_sum += train_mat[i]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            c1_word_count += sum(train_mat[i])</span><br><span class="line">            c1_vec_sum += train_mat[i]</span><br><span class="line">    c1_num = sum(class_list)</span><br><span class="line">    p1 = c1_num / float(doc_num)</span><br><span class="line">    p0_vec = c0_vec_sum / c0_word_count</span><br><span class="line">    p1_vec = c1_vec_sum / c1_word_count</span><br><span class="line">    <span class="comment"># 由于后面做乘法会下溢出，所以取对数做加法</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(word_num):</span><br><span class="line">        p0_vec[i] = math.log(p0_vec[i])</span><br><span class="line">        p1_vec[i] = math.log(p1_vec[i])</span><br><span class="line">    <span class="keyword">return</span> p0_vec, p1_vec, p1</span><br></pre></td></tr></table></figure><p><strong>贝叶斯分类</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify_nb</span><span class="params">(w_vec, p0_vec, p1_vec, p1)</span>:</span></span><br><span class="line">    <span class="string">''' 使用朴素贝叶斯分类</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        w_vec: 要测试的向量</span></span><br><span class="line"><span class="string">        p0_vec: c0中所有词汇占c0的总词汇的概率</span></span><br><span class="line"><span class="string">        p1_vec: c1中所有词汇占c1的总词汇的概率</span></span><br><span class="line"><span class="string">        p1: 文章为类型1的概率，即P(c1)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># P(w|c0)*P(c0) = P(w1|c0)*...*P(wn|c0)*P(c0)</span></span><br><span class="line">    <span class="comment"># 由于下溢出，所以上文取了对数，来做加法</span></span><br><span class="line">    w_p0 = sum(w_vec * p0_vec) + math.log(<span class="number">1</span> - p1)</span><br><span class="line">    w_p1 = sum(w_vec * p1_vec) + math.log(p1)</span><br><span class="line">    <span class="keyword">if</span> w_p0 &gt; w_p1:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>训练数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p0_vec, p1_vec, p1 = bys.train_nb0(train_mat, train_class)</span><br></pre></td></tr></table></figure><h3 id="测试数据">测试数据</h3><p><strong>一次执行</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">go_bayes_email</span><span class="params">()</span>:</span></span><br><span class="line"><span class="comment"># 此处省略上文的部分内容</span></span><br><span class="line">    <span class="comment"># 训练数据</span></span><br><span class="line">    p0_vec, p1_vec, p1 = bys.train_nb0(train_mat, train_class)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 测试数据</span></span><br><span class="line">    error_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> test_indices:</span><br><span class="line">        vec = bys.get_doc_vec(email_list[i], vocab_list)</span><br><span class="line">        res = bys.classify_nb(vec, p0_vec, p1_vec, p1)</span><br><span class="line">        <span class="keyword">if</span> res != class_list[i]:</span><br><span class="line">            error_count += <span class="number">1</span></span><br><span class="line">    error_rate = error_count / float(data_num)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'error=%d, rate=%s, test=%d, all=%d'</span> % (error_count, error_rate, len(test_indices),</span><br><span class="line">                    data_num)</span><br><span class="line">    <span class="keyword">return</span> error_rate</span><br></pre></td></tr></table></figure><p><strong>多次执行，取平均值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_bayes_email</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">''' 执行多次go_bayes_email，计算平均错误率 '''</span></span><br><span class="line">    times = <span class="number">100</span></span><br><span class="line">    error_rate_sum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        error_rate_sum += go_bayes_email()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'average_rate = %s'</span> % (error_rate_sum / <span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><a href="https://github.com/plmsmile/study/blob/master/ml/ch04-bayes/filter_email.py" target="_blank" rel="noopener">源代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;条件概率&quot;&gt;条件概率&lt;/h2&gt;
&lt;h3 id=&quot;基础知识&quot;&gt;基础知识&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;条件概率&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在&lt;span class=&quot;math inline&quot;&gt;\(B\)&lt;/span&gt;发生的情况下&lt;span class=
      
    
    </summary>
    
      <category term="机器学习" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="朴素贝叶斯" scheme="http://plmsmile.github.io/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
  </entry>
  
  <entry>
    <title>NumPy</title>
    <link href="http://plmsmile.github.io/2017/04/15/NumPy/"/>
    <id>http://plmsmile.github.io/2017/04/15/NumPy/</id>
    <published>2017-04-15T07:32:52.000Z</published>
    <updated>2017-09-22T03:28:44.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://www.jianshu.com/p/57e3c0a92f3a" target="_blank" rel="noopener">NumPy教程</a></p><h2 id="基础">基础</h2><h3 id="简单demo">简单demo</h3><p>NumPy中最重要的对象是<code>ndarray</code>，是一个N维数组。它存储着相同类型的元素集合。通过<code>dtype</code>来获取类型，<code>索引</code>来获取值。</p><p>通过<code>numpy.array</code>来创建ndarray。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. numpy定义</span></span><br><span class="line">numpy.array(object, dtype = <span class="keyword">None</span>, copy = <span class="keyword">True</span>, order = <span class="keyword">None</span>, subok = <span class="keyword">False</span>, ndmin = <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 2. demo</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 使用dtype</span></span><br><span class="line"><span class="comment">#int8, int16, int32, int64 可替换为等价的字符串 'i1', 'i2', 'i4', 以及其他。</span></span><br><span class="line">dt = np.dtype(np.int32)</span><br><span class="line"></span><br><span class="line">student = np.dtype([(<span class="string">'name'</span>,<span class="string">'S20'</span>),  (<span class="string">'age'</span>,  <span class="string">'i1'</span>),  (<span class="string">'marks'</span>,  <span class="string">'f4'</span>)])</span><br><span class="line">a = np.array([(<span class="string">'tom'</span>, <span class="number">23</span>, <span class="number">89</span>), (<span class="string">'sara'</span>, <span class="number">22</span>, <span class="number">97</span>)], dtype=student)</span><br></pre></td></tr></table></figure><p><strong>ndarray.shape</strong> 和<strong>reshape</strong> 获取数组维度 ，也可以调整大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]) </span><br><span class="line"><span class="keyword">print</span> a.shape</span><br><span class="line"><span class="comment"># (2, 3)</span></span><br><span class="line">b = a.reshape(<span class="number">3</span>,<span class="number">2</span>)  </span><br><span class="line">b.shape</span><br><span class="line"><span class="keyword">print</span> b</span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>] </span><br><span class="line"> [<span class="number">3</span>, <span class="number">4</span>] </span><br><span class="line"> [<span class="number">5</span>, <span class="number">6</span>]]</span><br></pre></td></tr></table></figure><p><strong>ndarray.ndim</strong> 数组的维数</p><p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">12</span>)<span class="comment"># 2</span></span><br><span class="line">b = a.reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)<span class="comment"># 3</span></span><br></pre></td></tr></table></figure></p><h3 id="创建数组">创建数组</h3><p><strong>输入数组建立</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br></pre></td></tr></table></figure><p><strong>zeros, ones, empty</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># zeros创建0矩阵</span></span><br><span class="line">np.zeros((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="comment"># ones创建1矩阵</span></span><br><span class="line">np.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), dtype=np.int16)</span><br><span class="line"><span class="comment"># empty不初始化数组，值随机</span></span><br><span class="line">np.empty((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p><strong>arange, linspace</strong></p><p>创建随机数，整数和浮点数，步长</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建[0, n-1]的数组</span></span><br><span class="line">np.arange(<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 创建1-10范围类，3个数</span></span><br><span class="line">np.arange(<span class="number">1</span>, <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取均值步长</span></span><br><span class="line">np.linspace(<span class="number">0</span>, <span class="number">1.5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># array([ 0.  ,  0.75,  1.5 ])</span></span><br></pre></td></tr></table></figure><h3 id="基本操作">基本操作</h3><p><strong>基本数学操作</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>])</span><br><span class="line">b = np.arrange(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 减法</span></span><br><span class="line">c = b - a </span><br><span class="line"><span class="comment"># 乘法</span></span><br><span class="line">b * <span class="number">2</span>  <span class="comment"># 新建一个矩阵</span></span><br><span class="line">b *= <span class="number">2</span> <span class="comment"># 直接改变b，不会新建一个矩阵 a += 2 同理</span></span><br><span class="line"><span class="comment"># 次方</span></span><br><span class="line">b ** <span class="number">2</span></span><br><span class="line"><span class="comment"># 判断</span></span><br><span class="line">a &lt; <span class="number">30</span> </span><br><span class="line"><span class="comment"># array([ True, False, False, False], dtype=bool)</span></span><br><span class="line"><span class="number">10</span> * np.sin(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵乘法</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">B = np.array([[<span class="number">2</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">A.dot(B)</span><br><span class="line">B.dot(A)</span><br><span class="line">np.dot(A, B)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sum, max, min</span></span><br><span class="line">a = np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">       [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">       [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br><span class="line"><span class="comment"># 所有元素sum, min, max</span></span><br><span class="line">a.sum()</span><br><span class="line">a.max()</span><br><span class="line"><span class="comment"># 使用axis=0按列, axis=1按行</span></span><br><span class="line">a.sum(axis=<span class="number">0</span>)</span><br><span class="line">array([<span class="number">12</span>, <span class="number">15</span>, <span class="number">18</span>, <span class="number">21</span>])</span><br><span class="line">a.sum(axis=<span class="number">1</span>)</span><br><span class="line">array([ <span class="number">6</span>, <span class="number">22</span>, <span class="number">38</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通用函数</span></span><br><span class="line">B = np.arange(<span class="number">3</span>)</span><br><span class="line">np.exp(B) <span class="comment"># 求e的次方</span></span><br><span class="line">np.sqrt(B) <span class="comment"># 开方</span></span><br><span class="line">C = np.array([<span class="number">2</span>, <span class="number">-1</span>, <span class="number">4</span>])</span><br><span class="line">np.add(B, C) <span class="comment"># 相加</span></span><br></pre></td></tr></table></figure><p><strong>访问元素</strong>，index, slice, iterator</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 一维数组</span></span><br><span class="line">a = np.arange(<span class="number">4</span>)**<span class="number">2</span> <span class="comment"># array([0, 1, 4, 9])</span></span><br><span class="line"><span class="comment"># 下标访问</span></span><br><span class="line">a[<span class="number">1</span>] <span class="comment"># 从0开始 # 1</span></span><br><span class="line"><span class="comment"># 切片，同python切片</span></span><br><span class="line">a[<span class="number">1</span>:<span class="number">3</span>]<span class="comment"># array([1, 4])</span></span><br><span class="line"><span class="comment"># 迭代</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">  <span class="keyword">print</span> (i*<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 2. 多维数组</span></span><br><span class="line">a = np.fromfunction(<span class="keyword">lambda</span> i, j: i + j, (<span class="number">3</span>, <span class="number">3</span>), dtype=int)<span class="comment"># 对下标进行操作</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">a[<span class="number">1</span>, <span class="number">2</span>] <span class="comment"># 访问到 3</span></span><br><span class="line"><span class="comment"># 访问第2列</span></span><br><span class="line">a[<span class="number">0</span>:<span class="number">3</span>, <span class="number">1</span>]<span class="comment"># array([1, 2, 3])</span></span><br><span class="line">a[:, <span class="number">1</span>]<span class="comment"># array([1, 2, 3]) </span></span><br><span class="line"><span class="comment"># 第i+1行</span></span><br><span class="line">a[<span class="number">1</span>] <span class="comment"># 第2行</span></span><br><span class="line">a[<span class="number">-1</span>]<span class="comment"># 最后一行</span></span><br><span class="line">a[<span class="number">1</span>, ...]<span class="comment"># 第2行，多维的时候这样写</span></span><br><span class="line"><span class="comment"># 第2、3行</span></span><br><span class="line">a[<span class="number">1</span>:<span class="number">3</span>, :]</span><br><span class="line">a[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> a:</span><br><span class="line">    <span class="keyword">print</span> row</span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> a.flat:</span><br><span class="line">    <span class="keyword">print</span> e</span><br></pre></td></tr></table></figure><p><strong>切片</strong>(start, end, step)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. (start, end, step)</span></span><br><span class="line">a = np.arange(<span class="number">10</span>)</span><br><span class="line">s = slice(<span class="number">2</span>, <span class="number">7</span>, <span class="number">2</span>) </span><br><span class="line">b = a[s]</span><br><span class="line"><span class="comment"># 2. 1-7, step=3, 不包括7</span></span><br><span class="line">b = a[<span class="number">1</span>:<span class="number">7</span>:<span class="number">3</span>]</span><br><span class="line"><span class="comment"># 3. 从idx开始向后切，包括idx</span></span><br><span class="line">b = a[<span class="number">2</span>:]</span><br><span class="line"><span class="comment"># 4. start, end, 不包括end</span></span><br><span class="line">b = a[<span class="number">2</span>:<span class="number">5</span>]</span><br><span class="line"><span class="comment"># 5. </span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])  </span><br><span class="line">a[..., <span class="number">1</span>] <span class="comment">#第2列 [2 4 5]</span></span><br><span class="line">a[<span class="number">1</span>, ...] <span class="comment">#第2行 [3 4 5]</span></span><br><span class="line">a[...,<span class="number">2</span>:] <span class="comment">#第2列及其剩余元素</span></span><br><span class="line">[[<span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">6</span>]]</span><br></pre></td></tr></table></figure><p><strong>索引</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 一维时</span></span><br><span class="line">a = np.arange(<span class="number">5</span>)**<span class="number">2</span> <span class="comment"># array([ 0,  1,  4,  9, 16])</span></span><br><span class="line"><span class="comment"># 索引1</span></span><br><span class="line">i = np.array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># idx 为1,3,4的元素</span></span><br><span class="line">a[i]<span class="comment"># 访问元素 </span></span><br><span class="line">array([ <span class="number">1</span>,  <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line"><span class="comment"># 索引2</span></span><br><span class="line">j = np.array([ [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">a[j]</span><br><span class="line">array([[ <span class="number">1</span>,  <span class="number">4</span>],</span><br><span class="line">       [ <span class="number">9</span>, <span class="number">16</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 二维时</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,  <span class="number">2</span>],  [<span class="number">3</span>,  <span class="number">4</span>],  [<span class="number">5</span>,  <span class="number">6</span>]]) </span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line"><span class="comment"># 索引</span></span><br><span class="line">y = x[ [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]] <span class="comment"># [0, 1, 2]是行, [0, 1, 0]是对应行的列</span></span><br><span class="line">[<span class="number">1</span> <span class="number">4</span> <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 没看懂</span></span><br><span class="line">x = np.array([[  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],[  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],[  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],[  <span class="number">9</span>,  <span class="number">10</span>,  <span class="number">11</span>]]) </span><br><span class="line">[[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">2</span>]</span><br><span class="line"> [ <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>]</span><br><span class="line"> [ <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>]</span><br><span class="line"> [ <span class="number">9</span> <span class="number">10</span> <span class="number">11</span>]]</span><br><span class="line"><span class="comment"># 索引 </span></span><br><span class="line">rows = np.array([ [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">3</span>,<span class="number">3</span>] ])</span><br><span class="line">cols = np.array([ [<span class="number">0</span>,<span class="number">2</span>], [<span class="number">0</span>,<span class="number">2</span>] ])</span><br><span class="line">i = [rows, cols]</span><br><span class="line">y = x[i]</span><br><span class="line">[[ <span class="number">0</span>  <span class="number">2</span>]</span><br><span class="line"> [ <span class="number">9</span> <span class="number">11</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 切片+索引</span></span><br><span class="line">x = np.array([[  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],[  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],[  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],[  <span class="number">9</span>,  <span class="number">10</span>,  <span class="number">11</span>]]) </span><br><span class="line"><span class="comment"># 切片, 1-3行, 1-2列</span></span><br><span class="line">z = x[<span class="number">1</span>:<span class="number">4</span>,<span class="number">1</span>:<span class="number">3</span>]  </span><br><span class="line">[[ <span class="number">4</span>  <span class="number">5</span>]</span><br><span class="line"> [ <span class="number">7</span>  <span class="number">8</span>]</span><br><span class="line"> [<span class="number">10</span> <span class="number">11</span>]]</span><br><span class="line"><span class="comment"># 高级索引来切片, 1-3行,1、2列</span></span><br><span class="line">y = x[<span class="number">1</span>:<span class="number">4</span>, [<span class="number">1</span>,<span class="number">2</span>]]</span><br><span class="line">[[ <span class="number">4</span>  <span class="number">5</span>]</span><br><span class="line"> [ <span class="number">7</span>  <span class="number">8</span>]</span><br><span class="line"> [<span class="number">10</span> <span class="number">11</span>]]</span><br></pre></td></tr></table></figure><h2 id="shape-manipulation">Shape Manipulation</h2><h3 id="改变形状">改变形状</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a = np.floor(<span class="number">10</span>*np.random.random((<span class="number">3</span>,<span class="number">4</span>)))<span class="comment"># &lt;1的小数*10，取整</span></span><br><span class="line">array([[ <span class="number">9.</span>,  <span class="number">6.</span>,  <span class="number">3.</span>,  <span class="number">8.</span>],</span><br><span class="line">       [ <span class="number">2.</span>,  <span class="number">8.</span>,  <span class="number">4.</span>,  <span class="number">2.</span>],</span><br><span class="line">       [ <span class="number">5.</span>,  <span class="number">3.</span>,  <span class="number">3.</span>,  <span class="number">1.</span>]])</span><br><span class="line"><span class="comment"># 形状</span></span><br><span class="line">a.shape</span><br><span class="line">(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 打平，返回array</span></span><br><span class="line">a.ravel()</span><br><span class="line">array([ <span class="number">9.</span>,  <span class="number">6.</span>,  <span class="number">3.</span>,  <span class="number">8.</span>,  <span class="number">2.</span>,  <span class="number">8.</span>,  <span class="number">4.</span>,  <span class="number">2.</span>,  <span class="number">5.</span>,  <span class="number">3.</span>,  <span class="number">3.</span>,  <span class="number">1.</span>])</span><br><span class="line"><span class="comment"># reshape 生成新的</span></span><br><span class="line">a.reshape(<span class="number">2</span>, <span class="number">6</span>) </span><br><span class="line">a.reshape(<span class="number">3</span>, <span class="number">-1</span>) <span class="comment"># 给定一个，自动计算另外的</span></span><br><span class="line"><span class="comment"># resize 改变自己</span></span><br><span class="line">a.resize(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 转置</span></span><br><span class="line">a.T</span><br></pre></td></tr></table></figure><h3 id="堆积不同的阵列">堆积不同的阵列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a = np.floor(<span class="number">10</span>*np.random.random((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">b = np.floor(<span class="number">10</span>*np.random.random((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="comment"># 垂直堆积 (4, 2)</span></span><br><span class="line">np.vstack((a, b))</span><br><span class="line">[[ <span class="number">1.</span>,  <span class="number">7.</span>],</span><br><span class="line"> [ <span class="number">9.</span>,  <span class="number">8.</span>],</span><br><span class="line"> [ <span class="number">9.</span>,  <span class="number">0.</span>],</span><br><span class="line"> [ <span class="number">8.</span>,  <span class="number">6.</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 水平堆积 (2, 4)</span></span><br><span class="line">np.hstack((a, b))</span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">7.</span>,  <span class="number">9.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">9.</span>,  <span class="number">8.</span>,  <span class="number">8.</span>,  <span class="number">6.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特别的，针对一维的列堆积</span></span><br><span class="line">a = np.array((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">b = np.array((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">np.column_stack((a,b))</span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line"> [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line"> [<span class="number">3</span>, <span class="number">4</span>]]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://www.jianshu.com/p/57e3c0a92f3a&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NumPy教程&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;基础&quot;&gt;基础&lt;/h2&gt;
&lt;h3 id=&quot;简单demo&quot;&gt;简单de
      
    
    </summary>
    
      <category term="Python" scheme="http://plmsmile.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="http://plmsmile.github.io/tags/Python/"/>
    
      <category term="NumPy" scheme="http://plmsmile.github.io/tags/NumPy/"/>
    
  </entry>
  
  <entry>
    <title>机器学习-西瓜书-第一章习题</title>
    <link href="http://plmsmile.github.io/2017/04/04/ml-watermelon-chap1/"/>
    <id>http://plmsmile.github.io/2017/04/04/ml-watermelon-chap1/</id>
    <published>2017-04-04T04:07:22.000Z</published>
    <updated>2017-05-03T11:02:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>$ $</p><h2 id="版本空间">1.1版本空间</h2><p><strong>题目</strong></p><p><strong>表1.1中若只包含编号为1和4的两个用例，试给出相应的版本空间。</strong></p><p><strong>背景知识</strong></p><blockquote><p><strong>假设空间</strong>：假设数据有<span class="math inline">\(n\)</span>中属性，第<span class="math inline">\(i\)</span>个属性可能的取值有<span class="math inline">\(t_i\)</span>种，加上该属性的泛化取值(*)，所以可能的假设有<span class="math inline">\(\prod_{i=1}^n {(t_i+1)}\)</span>种。再用空集表示没有正例，所以<strong>假设空间</strong>一共有<span class="math inline">\(\prod_{i=1}^n {(t_i+1)} + 1\)</span>种假设。</p></blockquote><blockquote><p><strong>学习过程</strong>：在假设空间中进行搜索去找到与训练集匹配的<strong>假设</strong>。即能够将训练集中的瓜判断正确的假设。</p></blockquote><blockquote><p><strong>版本空间</strong>：多个与训练集一致的假设组成的集合。</p></blockquote><p><strong>解答</strong></p><p>西瓜数据集，本题只取1和4。</p><table><thead><tr class="header"><th align="center">编号</th><th align="center">色泽</th><th align="center">根蒂</th><th align="center">敲声</th><th align="center">好瓜</th></tr></thead><tbody><tr class="odd"><td align="center">1</td><td align="center">青绿</td><td align="center">蜷缩</td><td align="center">浊响</td><td align="center">是</td></tr><tr class="even"><td align="center">2</td><td align="center">乌黑</td><td align="center">蜷缩</td><td align="center">浊响</td><td align="center">是</td></tr><tr class="odd"><td align="center">3</td><td align="center">青绿</td><td align="center">硬挺</td><td align="center">清脆</td><td align="center">否</td></tr><tr class="even"><td align="center">4</td><td align="center">乌黑</td><td align="center">稍蜷</td><td align="center">沉闷</td><td align="center">否</td></tr></tbody></table><p>获得好瓜的布尔表达式是：<span class="math inline">\(好瓜\leftrightarrow (色泽=?)\wedge(根蒂=?)\wedge(敲声=?)\)</span>。</p><p>三个特征的取值分别是：</p><ul><li>色泽：青绿，乌黑，* (*是说什么色泽都行，下面同理)</li><li>根蒂：蜷缩，稍蜷，*</li><li>敲声，浊响，沉闷，*</li></ul><p>当然也可能本身没有“好瓜”这种东西，我们用<span class="math inline">\(\emptyset\)</span>来表示。</p><p>综上，一共有 <span class="math inline">\(3\times3\times3+1=27\)</span>种假设。所以假设空间如下(先对正样本最大泛化)：</p><table><thead><tr class="header"><th align="center">编号</th><th align="center">色泽</th><th align="center">根蒂</th><th align="center">敲声</th><th align="center">符合正样本？</th></tr></thead><tbody><tr class="odd"><td align="center">1</td><td align="center">青绿</td><td align="center">蜷缩</td><td align="center">浊响</td><td align="center">是</td></tr><tr class="even"><td align="center">2</td><td align="center">青绿</td><td align="center">蜷缩</td><td align="center">沉闷</td><td align="center"></td></tr><tr class="odd"><td align="center">3</td><td align="center">青绿</td><td align="center">蜷缩</td><td align="center">*</td><td align="center">是</td></tr><tr class="even"><td align="center">4</td><td align="center">青绿</td><td align="center">稍蜷</td><td align="center">浊响</td><td align="center"></td></tr><tr class="odd"><td align="center">5</td><td align="center">青绿</td><td align="center">稍蜷</td><td align="center">沉闷</td><td align="center"></td></tr><tr class="even"><td align="center">6</td><td align="center">青绿</td><td align="center">稍蜷</td><td align="center">*</td><td align="center"></td></tr><tr class="odd"><td align="center">7</td><td align="center">青绿</td><td align="center">*</td><td align="center">浊响</td><td align="center">是</td></tr><tr class="even"><td align="center">8</td><td align="center">青绿</td><td align="center">*</td><td align="center">沉闷</td><td align="center"></td></tr><tr class="odd"><td align="center">9</td><td align="center">青绿</td><td align="center">*</td><td align="center">*</td><td align="center">是</td></tr><tr class="even"><td align="center">10</td><td align="center">乌黑</td><td align="center">蜷缩</td><td align="center">浊响</td><td align="center"></td></tr><tr class="odd"><td align="center">11</td><td align="center">乌黑</td><td align="center">蜷缩</td><td align="center">沉闷</td><td align="center"></td></tr><tr class="even"><td align="center">12</td><td align="center">乌黑</td><td align="center">蜷缩</td><td align="center">*</td><td align="center"></td></tr><tr class="odd"><td align="center">13</td><td align="center">乌黑</td><td align="center">稍蜷</td><td align="center">浊响</td><td align="center"></td></tr><tr class="even"><td align="center">14</td><td align="center">乌黑</td><td align="center">稍蜷</td><td align="center">沉闷</td><td align="center"></td></tr><tr class="odd"><td align="center">15</td><td align="center">乌黑</td><td align="center">稍蜷</td><td align="center">*</td><td align="center"></td></tr><tr class="even"><td align="center">16</td><td align="center">乌黑</td><td align="center">*</td><td align="center">浊响</td><td align="center"></td></tr><tr class="odd"><td align="center">17</td><td align="center">乌黑</td><td align="center">*</td><td align="center">沉闷</td><td align="center"></td></tr><tr class="even"><td align="center">18</td><td align="center">乌黑</td><td align="center">*</td><td align="center">*</td><td align="center"></td></tr><tr class="odd"><td align="center">19</td><td align="center">*</td><td align="center">蜷缩</td><td align="center">浊响</td><td align="center">是</td></tr><tr class="even"><td align="center">20</td><td align="center">*</td><td align="center">蜷缩</td><td align="center">沉闷</td><td align="center"></td></tr><tr class="odd"><td align="center">21</td><td align="center">*</td><td align="center">蜷缩</td><td align="center">*</td><td align="center"></td></tr><tr class="even"><td align="center">22</td><td align="center">*</td><td align="center">稍蜷</td><td align="center">浊响</td><td align="center"></td></tr><tr class="odd"><td align="center">23</td><td align="center">*</td><td align="center">稍蜷</td><td align="center">沉闷</td><td align="center"></td></tr><tr class="even"><td align="center">24</td><td align="center">*</td><td align="center">稍蜷</td><td align="center">*</td><td align="center">是</td></tr><tr class="odd"><td align="center">25</td><td align="center">*</td><td align="center">*</td><td align="center">浊响</td><td align="center">是</td></tr><tr class="even"><td align="center">26</td><td align="center">*</td><td align="center">*</td><td align="center">沉闷</td><td align="center"></td></tr><tr class="odd"><td align="center">27</td><td align="center">*</td><td align="center">*</td><td align="center">*</td><td align="center"></td></tr><tr class="even"><td align="center">28</td><td align="center"><span class="math inline">\(\emptyset\)</span></td><td align="center"></td><td align="center"></td><td align="center"></td></tr></tbody></table><p>所以版本空间为如下，共7个。</p><table><thead><tr class="header"><th align="center">编号</th><th align="center">色泽</th><th align="center">根蒂</th><th align="center">敲声</th><th align="center">好瓜</th></tr></thead><tbody><tr class="odd"><td align="center">1</td><td align="center">青绿</td><td align="center">蜷缩</td><td align="center">浊响</td><td align="center">是</td></tr><tr class="even"><td align="center">3</td><td align="center">青绿</td><td align="center">蜷缩</td><td align="center">*</td><td align="center">是</td></tr><tr class="odd"><td align="center">7</td><td align="center">青绿</td><td align="center">*</td><td align="center">浊响</td><td align="center">是</td></tr><tr class="even"><td align="center">9</td><td align="center">青绿</td><td align="center">*</td><td align="center">*</td><td align="center">是</td></tr><tr class="odd"><td align="center">19</td><td align="center">*</td><td align="center">蜷缩</td><td align="center">浊响</td><td align="center">是</td></tr><tr class="even"><td align="center">24</td><td align="center">*</td><td align="center">稍蜷</td><td align="center">*</td><td align="center">是</td></tr><tr class="odd"><td align="center">25</td><td align="center">*</td><td align="center">*</td><td align="center">浊响</td><td align="center">是</td></tr></tbody></table><h2 id="析合范式">1.2析合范式</h2><p><strong>题目</strong></p><p><strong>1.2 与使用单个合取式来进行假设表示相比，使用“析合范式”将使得假设空间具有更强的表示能力。例如：</strong> <span class="math display">\[好瓜\leftrightarrow\left((色泽=*) \wedge(根蒂=蜷缩)\wedge(敲声=*)\right)\vee\left((色泽=乌黑)\wedge(根蒂=*)\wedge(敲声=沉闷)\right)\]</span> <strong>若使用包含<span class="math inline">\(k\)</span>个合取式的析合范式来表达表1.1(上文中有)西瓜分类问题的假设空间，试估算共有多少种假设的可能。</strong></p><p><strong>解答</strong></p><p>表1.1中4个样例，3个属性。假设空间中共有<span class="math inline">\(3\times4\times4+1=49\)</span>种假设。</p><p>不考虑冗余</p><p>在假设空间中选取<span class="math inline">\(k\)</span>个来组成析合范式，则有<span class="math inline">\(\sum_{k=1}^n {C_{49}^K}=\color{red}{2^{49}}\)</span>种可能。但是其中包含了很多冗余的情况。</p><p>考虑冗余(忽略空集)</p><p>48种假设中：</p><ul><li>具体假设：<span class="math inline">\(2\times3\times3=18\)</span>种</li><li>1个属性泛化假设：<span class="math inline">\(1\times3\times3+2\times1\times3+2\times3\times1=21\)</span>种</li><li>2个属性泛化假设：<span class="math inline">\(2\times1\times1+1\times3\times1+1\times1\times3=8\)</span>种</li><li>3个属性泛化假设：<span class="math inline">\(1\times1\times1=1\)</span>种</li></ul><p><span class="math inline">\(k\)</span>的范围是：<span class="math inline">\(1\leq k \leq18\)</span>。取1个或者所有的具体假设。</p><p>当<span class="math inline">\(k=1\)</span>时，即只选取一种假设，这样不会有冗余情况，有<span class="math inline">\(\color {red} {48}\)</span>种可能。</p><p>当<span class="math inline">\(k=18\)</span>时，即所有的具体假设，只有<span class="math inline">\(\color {red} {1}\)</span>种可能。</p><p>当<span class="math inline">\(1&lt;k&lt;18\)</span>时，进行编程去循环遍历，按照：3属性泛化、2属性泛化、1属性泛化、具体属性排序，去遍历枚举。<a href="http://blog.csdn.net/icefire_tyh/article/details/52065626" target="_blank" rel="noopener">具体参见</a>。</p><h2 id="归纳偏好">1.3归纳偏好</h2><p><strong>题目</strong></p><p><strong>若数据包含噪声，则假设空间中有可能不存在与所有训练样本都一致的假设。在此情形下，试设计一种归纳偏好用于假设选择。即不存在训练错误为0的假设。</strong></p><p><strong>解答</strong></p><p>通常认为两个数据的属性越相近，则更倾向于把它们分为同一类。若相同属性出现了两种不同的分类，则认为它属于与它最邻近几个数据的属性。也可以考虑同时去掉所有具有相同属性而不同分类的数据，留下的就是没有误差的数据，但是可能会丢失部分信息。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;$ $&lt;/p&gt;
&lt;h2 id=&quot;版本空间&quot;&gt;1.1版本空间&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;题目&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表1.1中若只包含编号为1和4的两个用例，试给出相应的版本空间。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;背景知
      
    
    </summary>
    
      <category term="机器学习" scheme="http://plmsmile.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://plmsmile.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="周志华" scheme="http://plmsmile.github.io/tags/%E5%91%A8%E5%BF%97%E5%8D%8E/"/>
    
      <category term="西瓜书" scheme="http://plmsmile.github.io/tags/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>Spark-Programming</title>
    <link href="http://plmsmile.github.io/2017/03/25/Spark-Programming/"/>
    <id>http://plmsmile.github.io/2017/03/25/Spark-Programming/</id>
    <published>2017-03-25T10:07:35.000Z</published>
    <updated>2017-09-22T03:27:10.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="总览">总览</h2><p><strong>Spark程序</strong></p><p>有一个驱动程序，会运行用户的主要功能，并且在集群上执行各种并行操作。</p><p><strong>RDD</strong></p><p>RDD是<code>跨集群节点分区</code>的、并且可以<code>并行计算</code>的分布式数据集合。可以通过外部文件系统或者内部集合来创建。可以在内存中<code>持久化</code>一个RDD，并且在并行计算中有效地<code>重用</code>。RDD可以从节点故障中<code>自动恢复</code>。</p><p><strong>共享变量</strong></p><p>当一组任务在不同的节点上并行运行一个函数时，Spark会为函数中的每个变量发送一个<code>副本</code>到各个任务中去(低效)。有时，变量需要在任务与任务、任务与驱动程序间共享。Spark有两种共享变量。</p><ul><li>累加器：将工作节点中的值聚合到驱动程序中</li><li>广播变量：在各个节点中cache一个<code>只读变量</code></li></ul><p><strong>SparkContext</strong></p><p>Spark的主要入口点。使用它可以连接到集群、创建RDD和广播变量。</p><h2 id="rdd">RDD</h2><p>RDD是Spark中最核心的概念。</p><ul><li>这是一个<code>分布式的</code>、<code>容忍错误的</code>、<code>能并行操作</code>的<strong>数据集合</strong>。</li><li>RDD是一个分布式的不可变的对象集合，可以包含任意对象。</li><li>每个RDD都会被分为<strong>多个分区</strong>，这些分区运行在不同的节点上。</li><li>Spark会自动把RDD的数据分发到集群上，并且<strong>并行化执行</strong>相关操作。</li><li>记录如何转化、计算数据的指令列表。</li></ul><p>Spark中对数据的所有操作都是<strong>创建RDD</strong>、<strong>转化已有RDD</strong>、<strong>调用RDD操作进行求值</strong>。</p><h4 id="创建rdd">创建RDD</h4><p>创建RDD有两种方式：驱动程序内部的集合，外部系统的数据集(如HDFS, HBase等)。</p><p><strong>集合</strong></p><p>从集合中创建RDD，会把集合中的元素复制去创建一个可以并行执行的分布式数据集。</p><p>Spark可以对这些并行集合进行分区，把这些数据切割到多个分区。Spark会为集群的每个分区运行一个Task。一般，我们需要为集群中的每个CPU分配2-4个分区。默认，Spark会根据集群尝试自动设置分区数。但我们也可以手动地设置分区数。(有的代码中也称partition为slice)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">rdd.reduce(<span class="keyword">lambda</span> x, y: x + y) <span class="comment"># 求和</span></span><br><span class="line">rdd2 = sc.parallelize([<span class="string">'Spark'</span>, <span class="string">'Hadoop'</span>, <span class="string">'ML'</span>, <span class="string">'Python'</span>, <span class="string">'Data'</span>], <span class="number">2</span>) <span class="comment"># 设置2个分区</span></span><br></pre></td></tr></table></figure><p><strong>外部数据集</strong></p><p>Spark可以从本地文件系统、HDFS、Cassandra、HBase、Amazon S3等创建数据。支持Text、SequenceFile和任何其他Hadoop的Input Format。</p><p>Spark读取文件<code>textFile</code>的一些说明：</p><ul><li>本地文件使用本地路径读取文件时，该文件也得在<strong>其它的worker node的相同路径上访问到</strong>。可以把文件复制过去或者使用network-mounted的文件共享系统。</li><li>支持文件 、文件夹、通配符、压缩文件(.gz)。</li><li>可以设置分区数。默认，Spark为文件的每一个块创建一个分区。(HDFS的block是128MB)。可以传递一个更大的值来请求更多的分区。</li></ul><h4 id="rdd操作">RDD操作</h4><p>RDD主要有2个操作。</p><ul><li>转化操作：由一个RDD生成一个新的RDD(Dataset)。<strong>惰性求值</strong>。</li><li>行动操作：会对RDD(Dataset)计算出一个结果或者写到外部系统。会触发<strong>实际的计算</strong>。</li></ul><p>Spark会<strong>惰性计算</strong>这些RDD，只有第一次在一个行动操作中用到时才会真正计算。</p><p>一般，Spark会在每次行动操作时<strong>重新计算</strong>转换RDD。如果想<strong>复用</strong>，则用<code>persist</code>把RDD<strong>持久化缓存</strong>下来。可以持久化到内存、到磁盘、在多个节点上进行复制。这样，在下次查询时，集群可以更快地访问。</p><p>Spark程序大体步骤如下。</p><ul><li>从外部数据创建输入RDD。如<code>textFile</code></li><li>使用转化操作得到新的RDD。如<code>map</code>，<code>filter</code></li><li>对重用的中间结果RDD进行持久化。如<code>persist</code></li><li>使用行动操作来触发一次并行计算。如<code>count</code>, <code>first</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从外部创建一个rdd。此时并没有把数据加载到内存中。lines只是一个指向文件的指针</span></span><br><span class="line">lines = sc.textFile(<span class="string">"data.txt"</span>)</span><br><span class="line"><span class="comment"># 转化。没有进行真实的计算，因为惰性求值</span></span><br><span class="line">lineLengths = lines.map(<span class="keyword">lambda</span> s: len(s))</span><br><span class="line"><span class="comment"># 持久化</span></span><br><span class="line">lineLengths.persist()</span><br><span class="line"><span class="comment"># 行动。Spark把计算分解为一些任务，这些任务在单独的机器上进行运算。</span></span><br><span class="line"><span class="comment"># 每个机器只做属于自己map的部分，并且在本地reduce。返一个结果给DriverProgram</span></span><br><span class="line">totalLength = lineLengths.reduce(<span class="keyword">lambda</span> a, b: a + b)</span><br></pre></td></tr></table></figure><h4 id="传递函数给spark">传递函数给Spark</h4><p>Spark的API很多都依赖于传递函数来在集群上面运行。有下面3种方式可以使用：</p><ul><li>Lambda表达式：简单功能。不支持多语句函数、不支持没有返回值的语句。</li><li>本地def函数，调用spark。</li><li>模块的顶级函数。</li></ul><p><strong>代码较多时</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span><span class="params">(s)</span>:</span></span><br><span class="line">    words = s.split(<span class="string">" "</span>)</span><br><span class="line">    <span class="keyword">return</span> len(words)</span><br><span class="line">len_rdd = sc.textFile(<span class="string">"word.txt"</span>).map(my_func)</span><br></pre></td></tr></table></figure><p><strong>对象方法时</strong></p><p>千万不要引用self，这样会把整个对象序列化发送过去。而我们其实只需要一个方法或者属性就可以了，我们可以copy一份<strong>局部变量</strong>传递过去。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SearchFunctions</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, query)</span>:</span></span><br><span class="line">        self.query = query</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_match</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.query <span class="keyword">in</span> s</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">get_matches_func_ref</span><span class="params">(self, rdd)</span>:</span></span><br><span class="line">        <span class="string">"""问题: self.is_match引用了整个self</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> rdd.filter(self.is_match)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_matches_attr_ref</span><span class="params">(self, rdd)</span>:</span></span><br><span class="line">        <span class="string">"""问题：self.query引用了整个self</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> rdd.filter(<span class="keyword">lambda</span> s: self.query <span class="keyword">in</span> s)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_matches_no_ref</span><span class="params">(self, rdd)</span>:</span></span><br><span class="line">        <span class="string">"""正确做法：使用局部变量</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        query = self.query</span><br><span class="line">        <span class="keyword">return</span> rdd.filter(<span class="keyword">lambda</span> s: query <span class="keyword">in</span> s)</span><br></pre></td></tr></table></figure><h4 id="理解闭包">理解闭包</h4><p>当在集群上面执行代码时，理解变量和方法的范围和生命周期是很重要并且困难的。先看一段代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">counter = <span class="number">0</span></span><br><span class="line">rdd = sc.parallelize(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Wrong: Don't do this!!请使用Accumulator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">increment_counter</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> counter</span><br><span class="line">    counter += x</span><br><span class="line">rdd.foreach(increment_counter)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Counter value: "</span>, counter)</span><br></pre></td></tr></table></figure><p>执行job的时候，Spark会把处理RDD的操作分解为多个任务，每个任务会由一个<code>执行器executor</code>执行。执行前，Spark会计算任务的闭包。闭包其实就是一些变量和方法，为了计算RDD，它们对于执行器是可见的。Spark会把闭包序列化并且发送到每一个执行器。</p><p>发送给执行器的闭包里的变量其实是一个<strong>副本</strong>，这些执行器程序却看不到驱动器程序节点的内存中的变量(counter)，只能看到自己的副本。当foreach函数引用counter的时候，它使用的不是驱动器程序中的counter，而是自己的副本。</p><p>本地执行时，有时候foreach函数会在和driver同一个JVM里面执行，那么访问的就是最初的counter，也会对其进行修改。</p><p>一般，我们可以使用累加器<code>Accumulator</code>，它可以安全地修改一个变量。闭包不应该修改全局变量。如果要进行全局聚合，则应该使用<strong>累加器</strong>。</p><p>在本地模式，rdd.foreach(println)的时候，会打印出所有的RDD。但是在集群模式的时候，执行器会打印出它自己的那一部分，在driver中并没有。如果要在driver中打印，则需要collect().foreach()，但是只适用于数据量小的情况。因为collect会拿出所有的数据。</p><p><strong>键值对RDD</strong></p><p>详细的知识参见<a href="https://plmspark.github.io/2017/03/13/Spark-PairRDD/" target="_blank" rel="noopener">Spark的键值对RDD</a>。</p><h4 id="shuffle操作">Shuffle操作</h4><p><strong>shuffle说明</strong></p><p>Shuffle是Spark中重新分布数据的机制，因此它在分区之间分组也不同。主要是复制数据到执行器和机器上，这个很复杂而且很耗费。</p><p>以<code>reduceByKey</code>为例，一个key的所有value不一定在同一个partition甚至不在同一个machine，但是却需要把这些values放在一起进行计算。单个任务会在单个分区上执行。为了reduceByKey的reduce任务，需要获得所有的数据。Spark执行一个<code>all-to-all</code>操作，会在所有分区上，查找所有key的所有value，然后跨越分区汇总，去执行reduce任务。这就是shuffle。</p><p>shuffle后，分区的顺序和分区里的元素是确定的，但是分区里元素的顺序却不是确定的。可以去设置确定顺序。</p><p><strong>性能影响</strong></p><p>Shuffle涉及到磁盘IO、数据序列化、网络IO。组织data：一系列map任务；shuffle这些data；聚合data：一系列reduce任务。</p><p>一些map的结果会写到内存里，当太大时，会以分区排好序，然后写到单个文件里。在reduce端，task会读取相关的有序的block。</p><p>Shuffle操作会占用大量的堆内存，在传输data之前或者之后，都会使用内存中的数据结构去组织这些record。也就是说，在map端，会创建这些structures，在reduce端会生成这些structures。在内存中存不下时，就会写到磁盘中。</p><p>Shuffle操作会在磁盘上生成大量的中间文件，并且在RDD不再被使用并且被垃圾回收之前，这些文件都将被一直保留。因为lineage(血统,DAG图)要被重新计算的话，就不会再次shuffle了。如果保留RDD的引用或者垃圾回收不频繁，那么Spark会占用大量的磁盘空间。文件目录可由<code>spark.local.dir</code>配置。</p><p>我们可以在<a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">Spark的配配置指南</a>中配置各种参数。</p><h4 id="rdd持久化">RDD持久化</h4><p><strong>介绍</strong></p><p>Spark一个重要的特性是可以在操作的时候<strong>持久化缓存RDD到内存</strong>中。<code>Persist</code>一个RDD后，每个节点都会将这个RDD计算的所有分区存储在内存中，并且会在后续的计算中进行复用。这可以让future actions快很多(一般是10倍)。<strong>缓存</strong>是<code>迭代算法</code>和快速交互使用的关键工具。</p><p>持久化RDD可以使用<code>persist</code>或<code>cache</code>方法。会先进行行动操作计算，然后缓存到各个节点的内存中。Spark的缓存是<code>fault-tolerant</code>的，如果RDD的某些分区丢失了，它会自动使用产生这个RDD的transformation进行重新计算。</p><p><strong>类别</strong></p><p>出于不同的目的，持久化可以设置不同的级别。例如可以缓存到磁盘，缓存到内存(以序列化对象存储，节省空间)等，然后会复制到其他节点上。可以对<code>persist</code>传递<code>StorageLevel</code>对象进行设置缓存级别，而<code>cache</code>方法默认的是MEMORY_ONLY，下面是几个常用的。</p><ul><li><p>MEMORY_ONLY(default): RDD作为<code>反序列化的</code>Java对象存储在JVM中。如果not fit in memory，那么一些分区就不会存储，并且会在每次使用的时候<strong>重新计算</strong>。<strong>CPU时间快</strong>，但<strong>耗内存</strong>。</p></li><li><p>MEMORY_ONLY_SER: RDD作为<code>序列化的</code>Java对象存储在JVM中，每个分区一个字节数组。很<strong>省内存</strong>，可以选择一个快速的序列化器。<strong>CPU计算时间多</strong>。只是Java和Scala。</p></li><li><p>MEMORY_AND_DISK: <code>反序列化的</code>Java对象存在内存中。如果not fit in memory，那么把不适合在磁盘中存放的分区存放在内存中。</p></li><li><p>MEMORY_AND_DISK_SER: 和MEMORY_ONLY_SER差不多，只是存不下的再存储到磁盘中，而不是再重新计算。只是Java和Scala。</p></li></ul><table><thead><tr class="header"><th align="left">名字</th><th align="center">占用空间</th><th align="center">CPU时间</th><th align="center">在内存</th><th align="center">在磁盘</th></tr></thead><tbody><tr class="odd"><td align="left">MEMORY_ONLY</td><td align="center">高</td><td align="center">低</td><td align="center">是</td><td align="center">否</td></tr><tr class="even"><td align="left">MEMORY_ONLY_SER</td><td align="center">低</td><td align="center">高</td><td align="center">是</td><td align="center">否</td></tr><tr class="odd"><td align="left">MEMORY_AND_DISK</td><td align="center">高</td><td align="center">中等</td><td align="center">部分</td><td align="center">部分</td></tr><tr class="even"><td align="left">MEMORY_AND_DISK_SER</td><td align="center">低</td><td align="center">高</td><td align="center">部分</td><td align="center">部分</td></tr></tbody></table><p>所有的类别都通过<code>重新计算</code>丢失的数据来保证<code>容错能力</code>。完整的配置见<a href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" target="_blank" rel="noopener">官方RDD持久化</a>。</p><p>在Python中，我们会始终序列化要存储的数据，使用的是<a href="https://docs.python.org/2/library/pickle.html" target="_blank" rel="noopener">Pickle</a>，所以不用担心选择serialized level。</p><p>在shuffle中，Spark会<strong>自动持久化一些中间结果</strong>，即使用户没有使用<code>persist</code>。这样是因为，如果一个节点failed，可以避免重新计算整个input。如果要<code>reuse</code>一个RDD的话，推荐使用<code>persist</code>这个RDD。</p><p><strong>选择</strong></p><p>Spark的不同storage level是为了在CPU和内存的效率之间不同的权衡，按照如下去选择：</p><ul><li>如果适合<code>MEMORY_ONLY</code>，那么就这个。CPU效率最高了。RDD的操作<strong>速度会很快</strong>！</li><li>如果不适合MEMORY_ONLY，则尽量使用<code>MEMORY_ONLY_SER</code>，然后<a href="http://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="noopener">选个快速序列化库</a>。这样更加节省空间，理论上也能够快速访问。</li><li>不要溢写到磁盘。只有这两种才溢写到磁盘：计算数据集非常耗费资源；会过滤掉大量的数据。</li><li>如果要快速故障恢复，那么使用复制的storage level。虽然有容错能力，但是复制了，却可以直接继续执行任务，而不需要等待重新计算丢失的分区。</li></ul><p><strong>移除数据</strong></p><p>Spark会自动监视每个节点上的缓存使用情况，并且以<code>LRU</code>最近最少使用的策略把最老的分区从内存中移除。当然也可以使用<code>rdd.unpersist</code>手动移除。</p><ul><li>内存策略：移除分区，再次使用的时候，就需要重新计算。</li><li>内存和磁盘策略：移除的分区会写入磁盘。</li></ul><h3 id="共享变量">共享变量</h3><p>一般，把一个函数f传给Spark的操作，f会在远程集群节点上执行。当函数f在节点上执行的时候，会对所有的变量<strong>复制一份副本到该节点</strong>，然后利用这些副本单独地工作。对这些副本变量的<strong>更新修改不会传回驱动程序</strong>，只是修改这些副本。如果要在任务之间支持一般读写共享的变量是很<strong>低效</strong>的。</p><p>Spark支持两种共享变量：</p><ul><li>广播变量：用来高效地分发较大的只读对象</li><li>累加器：用来对信息进行聚合</li></ul><h4 id="广播变量">广播变量</h4><p><strong>简介</strong></p><p>广播变量可以让程序高效地向<strong>所有工作节点</strong>发送<strong>一个较大的只读值</strong>，供一个或多个Spark操作共同使用。</p><p>例如较大的只读查询表、机器学习中的一个很大的特征向量，使用广播变量就很方便。这会在每台机器上<strong>cache这个变量</strong>，而不是发送一个副本。</p><p>Spark的Action操作由一组stage组成，由分布式的&quot;shuffle&quot;操作隔离。Spark会自动广播每个stage的tasks需要的common data。这种广播的数据，是以<strong>序列化格式缓存的</strong>，并且会在每个<strong>任务运行之前反序列化</strong>。</p><p>创建广播变量只有下面两种情况<strong>有用</strong>：</p><ul><li>多个stage的task需要相同的数据</li><li>以反序列化形式缓存数据很重要</li></ul><p>存在的问题：</p><ul><li>Spark会自动把闭包中引用到的变量发送到工作节点。方便但是<strong>低效</strong>。</li><li>可能在并行操作中使用同一个变量，但是Spark会为每个操作都发送一次这个变量。</li><li>有的变量可能很大，为每个任务都发送一次代价很大。后面再用的话，则还要<strong>重新发送</strong>。</li></ul><p>广播变量来解决：</p><ul><li>其实就是一个类型为<code>spark.broadcast.BroadCast[T]</code>的变量。</li><li>可以<strong>在Task中进行访问</strong>。</li><li>广播变量只会发送到节点一次，只读。</li><li>一种高效地类似BitTorrent的通信机制。</li></ul><p><strong>使用方法</strong></p><ul><li>对于一个类型为T的对象，使用<code>SparkContext.broadcast</code>创建一个<code>BroadCast[T]</code>。要可以序列化</li><li>通过<code>value</code>属性访问值</li><li>变量作为<strong>只读值</strong>会发送到各个节点<strong>一次</strong>，在自己的节点上修改不会影响到其他变量。</li></ul><h4 id="累加器">累加器</h4><p><strong>简介</strong></p><p>累加器可以把工作节点中的数据聚合到驱动程序中。类似于<code>reduce</code>，但是更简单。常用作对事件进行计数。累加器仅仅通过关联和交换的操作来实现<code>累加</code>。可以有效地支持并行操作。Spark本身支持数值类型的累加器，我们也可以添加新的类型。</p><p><strong>用法</strong></p><ul><li>在驱动器程序中，调用<code>SparkContext.accumulator(initialValue)</code>创建一个有初始值的累加器。返回值为<code>org.apache.spark.Accumulator[T]</code></li><li>Spark的闭包里的执行器代码可以用累加器的<code>+=</code>来累加。</li><li>驱动器程序中，调用累加器的<code>value</code>属性来访问累加器的值</li><li>工作节点上的任务不能访问累加器的值</li></ul><p><strong>例子</strong></p><p>累加空行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">file = sc.textFile(<span class="string">"callsign_file"</span>)</span><br><span class="line"><span class="comment"># 创建累加器Accumulator[Int]并且赋初值0</span></span><br><span class="line">blank_line_count = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_callsigns</span><span class="params">(line)</span>:</span></span><br><span class="line">    <span class="string">"""提取callsigns"""</span></span><br><span class="line">    <span class="keyword">global</span> blank_line_count<span class="comment"># 访问全局变量</span></span><br><span class="line">    <span class="keyword">if</span> line == <span class="string">""</span>:</span><br><span class="line">        blank_line_count += <span class="number">1</span><span class="comment"># 累加</span></span><br><span class="line">    <span class="keyword">return</span> line.split(<span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line">callsigns = file.flatMap(extract_callsigns)</span><br><span class="line">callsigns.saveAsTextFile(output_dir + <span class="string">"/callsigns"</span>)</span><br><span class="line"><span class="comment"># 读取累加器的值 由于惰性求值，只有callsigns的action发生后，才能读取到值</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"Blank lines count: %d"</span> % blank_line_count.value</span><br></pre></td></tr></table></figure><p>进行错误计数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建用来验证呼号的累加器</span></span><br><span class="line">valid_signcount = sc.accumulator(<span class="number">0</span>)</span><br><span class="line">invalid_signcount = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">valid_datesign</span><span class="params">(sign)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> valid_signcount, invalid_sign_count</span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">r"\A\d?[a-zA-Z]&#123;1,2&#125;\d&#123;1,4&#125;[a-zA-Z]&#123;1, 3&#125;\Z"</span>, sign):</span><br><span class="line">        valid_signcount += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">        invalid_signcount += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个呼号的联系次数进行计数</span></span><br><span class="line">valid_signs = callsigns.filter(valid_datesign)</span><br><span class="line">contact_count = valid_signs.map(<span class="keyword">lambda</span> sign: (sign, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> (x, y): x+y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 强制求值计算计数</span></span><br><span class="line">contact_count.count()</span><br><span class="line"><span class="keyword">if</span> invalid_signcount.value &lt; <span class="number">0.1</span> * valid_signcount.value:</span><br><span class="line">    contact_count.saveAsTextFile(output_dir + <span class="string">"/contactcount"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Too many errors: %d in %d"</span> % (invalid_signcount.value, valid_signcount.value)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sign_prefixes = sc.broadcast(load_callsign_table())</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_sign_count</span><span class="params">(sign_count, sign_prefixes)</span>:</span></span><br><span class="line">    country = lookup_country(sign_count[<span class="number">0</span>], sign_prefixes.value)</span><br><span class="line">    count = sign_count[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> (country, count)</span><br><span class="line"></span><br><span class="line">country_contack_counts =</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;总览&quot;&gt;总览&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Spark程序&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有一个驱动程序，会运行用户的主要功能，并且在集群上执行各种并行操作。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RDD&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RDD是&lt;code&gt;跨
      
    
    </summary>
    
      <category term="大数据" scheme="http://plmsmile.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://plmsmile.github.io/tags/Spark/"/>
    
      <category term="RDD" scheme="http://plmsmile.github.io/tags/RDD/"/>
    
  </entry>
  
  <entry>
    <title>Spark-SQL</title>
    <link href="http://plmsmile.github.io/2017/03/19/Spark-SQL/"/>
    <id>http://plmsmile.github.io/2017/03/19/Spark-SQL/</id>
    <published>2017-03-19T14:34:44.000Z</published>
    <updated>2017-09-22T03:27:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基础">基础</h2><h3 id="概念">概念</h3><p>Spark SQL是用来处理结构化数据的模块。与基本RDD相比，Spark SQL提供了更多关于数据结构和计算方面的信息(在内部有优化效果)。通常通过SQL和Dataset API来和Spark SQL进行交互。</p><ul><li>SQL: 进行SQL查询，从各种结构化数据源(Json, Hive, Parquet)读取数据。返回Dataset/DataFrame。</li><li>Dataset: 分布式的数据集合。</li><li>DataFrame</li><li>是一个组织有列名的Dataset。类似于关系型数据库中的表。</li><li>可以使用结构化数据文件、Hive表、外部数据库、RDD等创建。</li><li>在Scala和Java中，DataFrame由Rows和Dataset组成。在Scala中，DataFrame只是Dataset[Row]的类型别名。在Java中，用Dataset<row>表示DataFrame</row></li></ul><h3 id="开始">开始</h3><p><strong>SparkSession</strong></p><p><code>SparkSession</code>是Spark所有功能的入口点，用<code>SparkSession.builder()</code>就可以。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">"Python Spark SQL basic example"</span>) \</span><br><span class="line">    .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>) \</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure><p><strong>DataFrameReader</strong></p><p>从外部系统加载数据，返回DataFrame。例如文件系统、键值对等等。通过<code>spark.read</code>来获取Reader。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. json 键值对</span></span><br><span class="line">df1 = spark.read.json(<span class="string">"python/test_support/sql/people.json"</span>)</span><br><span class="line">df1.dtypes</span><br><span class="line"><span class="comment"># [('age', 'bigint'), ('name', 'string')]</span></span><br><span class="line">df2 = sc.textFile(<span class="string">"python/test_support/sql/people.json"</span>)</span><br><span class="line"><span class="comment"># df1.dtypes 和 df2.dtypes是一样的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. text 文本文件 </span></span><br><span class="line"><span class="comment"># 每一行就是一个Row，默认的列名是Value</span></span><br><span class="line">df = spark.read.text(<span class="string">"python/test_support/sql/text-test.txt"</span>)</span><br><span class="line">df.collect()</span><br><span class="line"><span class="comment"># [Row(value=u'hello'), Row(value=u'this')]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. load</span></span><br><span class="line"><span class="comment"># 从数据源中读取数据</span></span><br></pre></td></tr></table></figure><p><strong>创建DataFrames</strong></p><p>从RDD、Hive Table、Spark data source、外部文件中都可以创建DataFrames。</p><p>通过<code>DataFrameReader</code>，读取外部文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spark.read返回一个DataFrameReader</span></span><br><span class="line">df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line">df.show()</span><br><span class="line">df.dtypes</span><br></pre></td></tr></table></figure><p>通过<code>spark.createDataFrame()</code>，读取RDD、List或<code>pandas.DataFrame</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">person_list = [(<span class="string">'AA'</span>, <span class="number">18</span>), (<span class="string">'PLM'</span>, <span class="number">23</span>)]</span><br><span class="line">rdd = sc.parallelize(person_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. list</span></span><br><span class="line">df = spark.createDataFrame(person_list) <span class="comment"># 没有指定列名，默认为_1 _2</span></span><br><span class="line">df = spark.createDataFrame(person_list, [<span class="string">'name'</span>, <span class="string">'age'</span>]) <span class="comment"># 指定了列名</span></span><br><span class="line">df.collect() <span class="comment"># df.show()</span></span><br><span class="line"><span class="comment">#[Row(name='AA', age=18), Row(name='PLM', age=23)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. RDD</span></span><br><span class="line">rdd = sc.parallelize(person_list)</span><br><span class="line">df = spark.createDataFrame(rdd, [<span class="string">'name'</span>, <span class="string">'age'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Row</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line">Person = Row(<span class="string">'name'</span>, <span class="string">'age'</span>)<span class="comment"># 指定模板属性</span></span><br><span class="line">persons = rdd.map(<span class="keyword">lambda</span> r: Person(*r))<span class="comment"># 把每一行变成Person</span></span><br><span class="line">df = spark.createDataFrame(persons)</span><br><span class="line">df.collect()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 指定类型StructType</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line">field_name = StructField(<span class="string">'name'</span>, StringType(), <span class="keyword">True</span>) <span class="comment"># 名，类型，非空</span></span><br><span class="line">field_age = StructField(<span class="string">'age'</span>, IntegerType, <span class="keyword">True</span>)</span><br><span class="line">person_type = StructType([field_name, field_age])</span><br><span class="line"><span class="comment"># 通过链式add也可以</span></span><br><span class="line"><span class="comment"># person_type = StructType.add("name", StringType(), True).add(...)</span></span><br><span class="line">df = spark.createDataFrame(rdd, person_type)</span><br></pre></td></tr></table></figure><p><strong>Row</strong></p><p><code>Row</code>是<code>DataFrame</code>中的，它可以定义一些属性，这些属性在DataFrame里面可以被访问。比如：<code>row.key</code>(像属性)和<code>row['key']</code>(像dict)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> Row</span><br><span class="line"><span class="comment"># 1. 创建一个模板</span></span><br><span class="line">Person = Row(<span class="string">'name'</span>, <span class="string">'age'</span>)<span class="comment"># &lt;Row(name, age)&gt;</span></span><br><span class="line"><span class="string">'name'</span> <span class="keyword">in</span> Person <span class="comment"># True，有这个属性</span></span><br><span class="line"><span class="string">'sex'</span> <span class="keyword">in</span> Person <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 以Person为模板，创建alice, plm</span></span><br><span class="line">alice = Person(<span class="string">'Alice'</span>, <span class="number">22</span>) <span class="comment"># Row(name='Alice', age=22)</span></span><br><span class="line">plm = Person(<span class="string">'PLM'</span>, <span class="number">23</span>)</span><br><span class="line"><span class="comment"># 访问属性</span></span><br><span class="line">name, age = alice[<span class="string">'name'</span>], alice[<span class="string">'age'</span>]</span><br><span class="line"><span class="comment"># 返回dict</span></span><br><span class="line">plm.asDict()</span><br><span class="line"><span class="comment"># &#123;'age': 23, 'name': 'PLM'&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 多个person创建一个DataFrame</span></span><br><span class="line">p_list = [alice, plm]</span><br><span class="line">p_df = spark.createDataFrame(p_list)</span><br><span class="line">p_df.collect()</span><br><span class="line"><span class="comment"># [Row(name=u'Alice', age=22), Row(name=u'PLM', age=23)]</span></span><br></pre></td></tr></table></figure><p><strong>DataFrame的操作</strong></p><p>在2.0中，DataFrames只是Scala和Java API中的Rows数据集。它的操作称为非类型转换，与带有强类型Scala和Java数据集的类型转换相反。</p><p>Python tips: <code>df.age</code>和<code>df['age']</code>都可以使用，前者在命令行里面方便，但是建议使用后者。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema()</span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line">df.select(df[<span class="string">'name'</span>], df[<span class="string">'age'</span>] + <span class="number">1</span>).show()</span><br><span class="line">df.filter(df[<span class="string">'age'</span>] &gt; <span class="number">21</span>).show()</span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br></pre></td></tr></table></figure><p><a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">DataFrame的Python Api</a>，<a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions" target="_blank" rel="noopener">DataFrame的函数API</a></p><p><strong>编程方式运行SQL</strong></p><p>通过<code>spark.sql()</code>执行，返回一个DataFrame</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过df创建一个本地临时视图，与创建这个df的SparkSession同生命周期</span></span><br><span class="line">df.createOrReplaceTempview(<span class="string">"people"</span>)</span><br><span class="line">sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="comment"># 展示为Table</span></span><br><span class="line">sqlDf.collect()</span><br><span class="line"><span class="comment"># [Row(age=None, name=u'Michael'), Row(age=30, name=u'Andy'), Row(age=19, name=u'Justin')]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局临时视图</span></span><br><span class="line"><span class="comment"># 在所有session中共享，直到spark application停止</span></span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基础&quot;&gt;基础&lt;/h2&gt;
&lt;h3 id=&quot;概念&quot;&gt;概念&lt;/h3&gt;
&lt;p&gt;Spark SQL是用来处理结构化数据的模块。与基本RDD相比，Spark SQL提供了更多关于数据结构和计算方面的信息(在内部有优化效果)。通常通过SQL和Dataset API来和Spa
      
    
    </summary>
    
      <category term="大数据" scheme="http://plmsmile.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://plmsmile.github.io/tags/Spark/"/>
    
      <category term="Spark SQL" scheme="http://plmsmile.github.io/tags/Spark-SQL/"/>
    
  </entry>
  
  <entry>
    <title>博客搭建过程及问题</title>
    <link href="http://plmsmile.github.io/2017/03/13/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B%E5%8F%8A%E9%97%AE%E9%A2%98/"/>
    <id>http://plmsmile.github.io/2017/03/13/博客搭建过程及问题/</id>
    <published>2017-03-13T14:52:41.000Z</published>
    <updated>2018-03-03T02:00:06.174Z</updated>
    
    <content type="html"><![CDATA[<p>一直都想搭建一个博客，今天终于把博客给初步搭建好了。搭建的过程其实不那么顺利，所以简答记录一下。</p><h2 id="搭建过程">搭建过程</h2><ul><li>根据<a href="http://www.jianshu.com/p/f4cc5866946b" target="_blank" rel="noopener">手把手搭建博客教程</a>这篇文章来进行搭建，其中目前只看了它的第一页</li><li>后期在<a href="https://hexo.io/themes/" target="_blank" rel="noopener">hexo主题</a>里面选择了<a href="https://github.com/ahonn/hexo-theme-even" target="_blank" rel="noopener">even</a>主题</li><li>依照<a href="https://github.com/ahonn/hexo-theme-even/wiki" target="_blank" rel="noopener">even-wiki</a>来添加标签、分类和about页面</li><li>修改主题目录下的<code>_config.yml</code>的menu，把home、tags等手动改成中文了</li></ul><h2 id="遇到的坑">遇到的坑</h2><ul><li>没有看wiki，自己去谷歌建立tags、categories等页面</li><li>建立好tags，却在tags页面没有看到相应的标签。是因为没有为<code>tags/index.md</code>设置<code>layout</code>为tags</li><li>中文语言，在站点目录下的<code>_config.yml</code>中设置<code>language: zh-cn</code>。</li></ul><h2 id="博客重新搭建">博客重新搭建</h2><h3 id="配置及源文件">配置及源文件</h3><p>因为经常重装系统，所以博客也需要重新恢复。先配置git相关信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name "plmsmile"</span><br><span class="line">git config --global user.email "pulimingspark@163.com"</span><br><span class="line">ssh-keygen -t rsa -C "pulimingspark@163.com"</span><br><span class="line"><span class="meta">#</span> 去GitHub上添加sshkey</span><br><span class="line">cat ~/.ssh/id_rsa.pub</span><br><span class="line"><span class="meta">#</span> 完成后，进行测试</span><br><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>然后把之前的<code>PLMBlogs</code>拷贝到D盘，一般目录是<code>d/PLMBlogs</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd PLMBlogs</span><br><span class="line"><span class="meta">#</span> 安装hexo</span><br><span class="line">npm install hexo-cli -g</span><br><span class="line"><span class="meta">#</span> 安装插件</span><br><span class="line">npm install hexo-deployer-git --save</span><br><span class="line"><span class="meta">#</span> 安装依赖</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><h3 id="数学公式渲染">数学公式渲染</h3><p>执行完上面的操作后，执行如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate# 这一步会报错</span><br><span class="line">hexo server</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><p>错误信息如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html</span><br><span class="line">Error: write EPIPE</span><br><span class="line">    at exports._errnoException (util.js:1020:11)</span><br><span class="line">    at Socket._writeGeneric (net.js:711:26)</span><br><span class="line">    at Socket._write (net.js:730:8)</span><br><span class="line">    at doWrite (_stream_writable.js:331:12)</span><br><span class="line">    at writeOrBuffer (_stream_writable.js:317:5)</span><br><span class="line">    at Socket.Writable.write (_stream_writable.js:243:11)</span><br><span class="line">    at Socket.write (net.js:657:40)</span><br><span class="line">    at Hexo.pandocRenderer (D:\PLMBlogs\node_modules\hexo-renderer-pandoc\index.js:64:15)</span><br><span class="line">    at Hexo.tryCatcher (D:\PLMBlogs\node_modules\bluebird\js\release\util.js:16:23)</span><br><span class="line">    at Hexo.ret (eval at makeNodePromisifiedEval (C:\Users\PLM\AppData\Roaming\npm\node_modules\hexo-cli\node_modules\bluebird\js\release\promisify.js:184:12), &lt;anonymous&gt;:13:39)</span><br><span class="line">    at D:\PLMBlogs\node_modules\hexo\lib\hexo\render.js:61:21</span><br><span class="line">    at tryCatcher (D:\PLMBlogs\node_modules\bluebird\js\release\util.js:16:23)</span><br><span class="line">    at Promise._settlePromiseFromHandler (D:\PLMBlogs\node_modules\bluebird\js\release\promise.js:512:31)</span><br><span class="line">    at Promise._settlePromise (D:\PLMBlogs\node_modules\bluebird\js\release\promise.js:569:18)</span><br><span class="line">    at Promise._settlePromiseCtx (D:\PLMBlogs\node_modules\bluebird\js\release\promise.js:606:10)</span><br><span class="line">    at Async._drainQueue (D:\PLMBlogs\node_modules\bluebird\js\release\async.js:138:12)</span><br><span class="line">    at Async._drainQueues (D:\PLMBlogs\node_modules\bluebird\js\release\async.js:143:10)</span><br><span class="line">    at Immediate.Async.drainQueues (D:\PLMBlogs\node_modules\bluebird\js\release\async.js:17:14)</span><br><span class="line">    at runCallback (timers.js:672:20)</span><br><span class="line">    at tryOnImmediate (timers.js:645:5)</span><br><span class="line">    at processImmediate [as _immediateCallback] (timers.js:617:5)</span><br><span class="line">events.js:160</span><br><span class="line">      throw er; // Unhandled 'error' event</span><br><span class="line">      ^</span><br><span class="line"></span><br><span class="line">Error: spawn pandoc ENOENT</span><br><span class="line">    at exports._errnoException (util.js:1020:11)</span><br><span class="line">    at Process.ChildProcess._handle.onexit (internal/child_process.js:193:32)</span><br><span class="line">    at onErrorNT (internal/child_process.js:367:16)</span><br><span class="line">    at _combinedTickCallback (internal/process/next_tick.js:80:11)</span><br><span class="line">    at process._tickCallback (internal/process/next_tick.js:104:9)</span><br></pre></td></tr></table></figure><p>原因是有大量的数学公式，所以需要对网页进行渲染。</p><p>一般是使用<code>pandoc</code>进行渲染，先去官网下载，然后下一步安装即可。最后，执行下面的命令，安装就好了。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-pandoc --save</span><br><span class="line"><span class="meta">#</span> 再次应该就不会报错了</span><br><span class="line">hexo generate</span><br></pre></td></tr></table></figure><h2 id="配置git">配置git</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name plmsmile</span><br><span class="line">git config --global user.email plmspark@163.com</span><br><span class="line"><span class="meta">#</span> 生成Key，一路回车</span><br><span class="line">ssh-keygen -t rsa -C "plmspark@163.com" </span><br><span class="line">cat ~/.ssh/id_rsa.pub</span><br><span class="line"><span class="meta">#</span> 到github上添加该sshkey</span><br><span class="line"><span class="meta">#</span> 测试</span><br><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><h2 id="潜在问题">潜在问题</h2><ul><li>本站没有搜索功能，安装插件失败了</li><li>tags页面，标签数量错误</li></ul><h2 id="期望">期望</h2><ul><li>认真学习</li><li>好好做笔记</li><li>要更新博客</li><li>自己常来看看之前的知识点</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一直都想搭建一个博客，今天终于把博客给初步搭建好了。搭建的过程其实不那么顺利，所以简答记录一下。&lt;/p&gt;
&lt;h2 id=&quot;搭建过程&quot;&gt;搭建过程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;根据&lt;a href=&quot;http://www.jianshu.com/p/f4cc5866946b
      
    
    </summary>
    
      <category term="心得" scheme="http://plmsmile.github.io/categories/%E5%BF%83%E5%BE%97/"/>
    
    
      <category term="心得" scheme="http://plmsmile.github.io/tags/%E5%BF%83%E5%BE%97/"/>
    
  </entry>
  
  <entry>
    <title>Spark的键值对RDD</title>
    <link href="http://plmsmile.github.io/2017/03/13/Spark-PairRDD/"/>
    <id>http://plmsmile.github.io/2017/03/13/Spark-PairRDD/</id>
    <published>2017-03-13T11:37:06.000Z</published>
    <updated>2017-09-22T03:26:56.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="pairrdd及其创建">PairRDD及其创建</h3><p>键值对RDD称为PairRDD，通常用来做<code>聚合计算</code>。Spark为Pair RDD提供了许多专有的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建pair rdd: map 或者 读取键值对格式自动转成pairrdd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每行的第一个单词作为key，line作为value</span></span><br><span class="line">pairs = lines.map(<span class="keyword">lambda</span> line: (line.split(<span class="string">' '</span>)[<span class="number">0</span>], line))</span><br></pre></td></tr></table></figure><h3 id="转化操作">转化操作</h3><p>Pair RDD 的转化操作分为单个和多个RDD的转化操作。</p><h4 id="单个pair-rdd转化">单个Pair RDD转化</h4><p><strong>reduceByKey(func)</strong></p><p><code>合并</code>含有相同键的值，也称作<code>聚合</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line">rdd = sc.parallelize([(<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)])</span><br><span class="line">sorted(rdd.reduceByKey(add).collect())</span><br><span class="line"><span class="comment"># [('a', 2), ('b', 1)]</span></span><br><span class="line"><span class="comment"># 这种写法也可以</span></span><br><span class="line">rdd.reduceByKey(<span class="keyword">lambda</span> x, y: x+y).collect()</span><br></pre></td></tr></table></figure><p><strong>groupByKey</strong></p><p>对具有相同键的值进行<code>分组</code>。会生成hash分区的RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)])</span><br><span class="line">sorted(rdd.groupByKey().mapValues(len).collect())</span><br><span class="line"><span class="comment"># [('a', 2), ('b', 1)]</span></span><br><span class="line">sorted(rdd.groupByKey().mapValues(list).collect())</span><br><span class="line">[(<span class="string">'a'</span>, [<span class="number">1</span>, <span class="number">1</span>]), (<span class="string">'b'</span>, [<span class="number">1</span>])]</span><br></pre></td></tr></table></figure><p>说明：如果对键进行分组以便对每个键进行聚合（如sum和average），则用<code>reduceByKey</code>和<code>aggregateByKey</code>性能更好</p><p><strong>combineByKey</strong></p><p>合并具有相同键的值，但是<code>返回不同类型</code> (K, V) - (K, C)。最常用的聚合操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b)</span>:</span> <span class="keyword">return</span> a + str(b)</span><br><span class="line">sorted(x.combineByKey(str, add, add).collect())</span><br><span class="line">[(<span class="string">'a'</span>, <span class="string">'11'</span>), (<span class="string">'b'</span>, <span class="string">'1'</span>)]</span><br></pre></td></tr></table></figure><p>下面是combineByKey的源码和参数说明</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,<span class="comment">// V =&gt; C的转变 / 初始值 / 创建one-element的list</span></span><br><span class="line">      mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,<span class="comment">// 将原V累加到新的C</span></span><br><span class="line">      mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,<span class="comment">// 两个C合并成一个</span></span><br><span class="line">      partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">      mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">      serializer: <span class="type">Serializer</span> = <span class="literal">null</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">    <span class="comment">//实现略</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 求平均值</span></span><br><span class="line"><span class="keyword">val</span> scores = sc.parallelize(</span><br><span class="line"><span class="type">List</span>((<span class="string">"chinese"</span>, <span class="number">88.0</span>) , (<span class="string">"chinese"</span>, <span class="number">90.5</span>) , (<span class="string">"math"</span>, <span class="number">60.0</span>), (<span class="string">"math"</span>, <span class="number">87.0</span>))</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> avg = scores.combineByKey(</span><br><span class="line">  (v) =&gt; (v, <span class="number">1</span>),</span><br><span class="line">  (acc: (<span class="type">Float</span>, <span class="type">Int</span>), <span class="type">V</span>) =&gt; (acc._1 + v, acc._2 + <span class="number">1</span>),</span><br><span class="line">  (acc1: (<span class="type">Float</span>, <span class="type">Int</span>), acc2: (<span class="type">Float</span>, <span class="type">Int</span>) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2))</span><br><span class="line">).map&#123;<span class="keyword">case</span> (key, value) =&gt; (key, value._1 / value._2.toFloat)&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求平均值</span></span><br><span class="line">nums = sc.parallelize([(<span class="string">'c'</span>, <span class="number">90</span>), (<span class="string">'m'</span>, <span class="number">95</span>), (<span class="string">'c'</span>, <span class="number">80</span>)])</span><br><span class="line">sum_count = nums.combineByKey(</span><br><span class="line"><span class="keyword">lambda</span> x: (x, <span class="number">1</span>),</span><br><span class="line">    <span class="keyword">lambda</span> acc, x: (acc[<span class="number">0</span>] + x, acc[<span class="number">1</span>] + <span class="number">1</span>),</span><br><span class="line">    <span class="keyword">lambda</span> acc1, acc2: (acc1[<span class="number">0</span>] + acc2[<span class="number">0</span>], acc1[<span class="number">1</span>] + acc2[<span class="number">1</span>])</span><br><span class="line">)</span><br><span class="line"><span class="comment"># [('c', (170, 2)), ('m', (95, 1))]</span></span><br><span class="line">avg_map = sum_count.mapValues(<span class="keyword">lambda</span> (sum, count): sum/count).collectAsMap()</span><br><span class="line"><span class="comment"># &#123;'c': 85, 'm': 95&#125;</span></span><br><span class="line">avg_map = sum_count.map(<span class="keyword">lambda</span> key, s_c: (key, s_c[<span class="number">0</span>]/s_c[<span class="number">1</span>])).collectAsMap()</span><br></pre></td></tr></table></figure><p><strong>mapValues(f)</strong></p><p>对每个pair RDD中的每个Value应用一个func，不改变Key。其实也是<code>对value做map</code>操作。一般我们只想访问pair的值的时候，可以用<code>mapValues</code>。类似于map{case (x, y): (x, <em>func(y)</em>)}</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">"a"</span>, [<span class="string">"apple"</span>, <span class="string">"banana"</span>, <span class="string">"lemon"</span>]), (<span class="string">"b"</span>, [<span class="string">"grapes"</span>])])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> len(x)</span><br><span class="line">x.mapValues(f).collect()</span><br><span class="line">[(<span class="string">'a'</span>, <span class="number">3</span>), (<span class="string">'b'</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure><p><strong>mapPartitions(f)</strong></p><p>是<code>map</code>的一个变种，都需要传入一个函数f，去处理数据。不同点如下：</p><ul><li>map: f应用于每一个元素。</li><li>mapPartitions: f应用于每一个分区。分区的内容以Iterator[T]传入f，f的输出结果是Iterator[U]。最终RDD的由所有分区经过输入函数处理后的结果得到的。</li></ul><p>优点：我们可以为每一个分区做一些初始化操作，而不用为每一个元素做初始化。例如，初始化数据库，次数n。map时：n=元素数量，mapPartitions时：n=分区数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 每个元素加1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(iterator)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"called f"</span>)</span><br><span class="line">    <span class="keyword">return</span> map(<span class="keyword">lambda</span> x: x + <span class="number">1</span>, iterator)</span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">2</span>)</span><br><span class="line">rdd.mapPartitions(f).collect()<span class="comment"># 只调用2次f</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">called f</span></span><br><span class="line"><span class="string">called f</span></span><br><span class="line"><span class="string">[2, 3, 4, 5, 6]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 分区求和</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="number">2</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(iterator)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"call f"</span></span><br><span class="line">    <span class="keyword">yield</span> sum(iterator)</span><br><span class="line">rdd.mapPartitions(f).collect() <span class="comment"># 调用2次f，分区求和</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">call f</span></span><br><span class="line"><span class="string">call f</span></span><br><span class="line"><span class="string">[6, 15]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p><strong>mapPartitionsWithIndex(f)</strong></p><p>和<code>mapPartitions</code>一样，只是多了个partition的index。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="string">"yellow"</span>,<span class="string">"red"</span>,<span class="string">"blue"</span>,<span class="string">"cyan"</span>,<span class="string">"black"</span>], <span class="number">3</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">g</span><span class="params">(index, item)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"id-&#123;&#125;, &#123;&#125;"</span>.format(index, item)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(index, iterator)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'called f'</span></span><br><span class="line">    <span class="keyword">return</span> map(<span class="keyword">lambda</span> x: g(index, x), iterator)</span><br><span class="line">rdd.mapPartitionsWithIndex(f).collect()</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">called f</span></span><br><span class="line"><span class="string">called f</span></span><br><span class="line"><span class="string">called f</span></span><br><span class="line"><span class="string">['id-0, yellow', 'id-1, red', 'id-1, blue', 'id-2, cyan', 'id-2, black']</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p><strong>repartition(n)</strong></p><p>生成新的RDD，分区数目为n。会增加或者减少 RDD的并行度。会对分布式数据集进行<code>shuffle</code>操作，<strong>效率低</strong>。如果只是想减少分区数，则使用<code>coalesce</code>，不会进行shuffle操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd = sc.parallelize([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>], <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted(rdd.glom().collect())</span><br><span class="line">[[<span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(rdd.repartition(<span class="number">2</span>).glom().collect())</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(rdd.repartition(<span class="number">10</span>).glom().collect())</span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure><p><strong>coalesce(n)</strong></p><p>合并，减少分区数，默认不执行shuffle操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">3</span>).glom().collect()</span><br><span class="line"><span class="comment"># [[1], [2, 3], [4, 5]]</span></span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">3</span>).coalesce(<span class="number">1</span>).glom().collect()</span><br><span class="line"><span class="comment"># [[1, 2, 3, 4, 5]]</span></span><br></pre></td></tr></table></figure><p><strong>flatMapValues(f)</strong></p><p>打平values，[(&quot;k&quot;, [&quot;v1&quot;, &quot;v2&quot;])] -- [(&quot;k&quot;,&quot;v1&quot;), (&quot;k&quot;, &quot;v2&quot;)]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">"a"</span>, [<span class="string">"x"</span>, <span class="string">"y"</span>, <span class="string">"z"</span>]), (<span class="string">"b"</span>, [<span class="string">"p"</span>, <span class="string">"r"</span>])])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> <span class="keyword">return</span> x</span><br><span class="line">x.flatMapValues(f).collect()</span><br><span class="line"><span class="comment"># [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]</span></span><br></pre></td></tr></table></figure><p><strong>keys</strong></p><p><strong>values</strong></p><p><strong>sortByKey</strong></p><p>返回一个对键进行排序的RDD。会生成范围分区的RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tmp = [(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">2</span>), (<span class="string">'1'</span>, <span class="number">3</span>), (<span class="string">'d'</span>, <span class="number">4</span>), (<span class="string">'2'</span>, <span class="number">5</span>)]</span><br><span class="line">sc.parallelize(tmp).sortByKey().first()</span><br><span class="line"><span class="comment"># ('1', 3)</span></span><br><span class="line">sc.parallelize(tmp).sortByKey(<span class="keyword">True</span>, <span class="number">1</span>).collect()</span><br><span class="line"><span class="comment"># [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]</span></span><br><span class="line">sc.parallelize(tmp).sortByKey(<span class="keyword">True</span>, <span class="number">2</span>).collect()</span><br><span class="line"><span class="comment"># [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]</span></span><br><span class="line"></span><br><span class="line">tmp2 = [(<span class="string">'Mary'</span>, <span class="number">1</span>), (<span class="string">'had'</span>, <span class="number">2</span>), (<span class="string">'a'</span>, <span class="number">3</span>), (<span class="string">'little'</span>, <span class="number">4</span>), (<span class="string">'lamb'</span>, <span class="number">5</span>)]</span><br><span class="line">tmp2.extend([(<span class="string">'whose'</span>, <span class="number">6</span>), (<span class="string">'fleece'</span>, <span class="number">7</span>), (<span class="string">'was'</span>, <span class="number">8</span>), (<span class="string">'white'</span>, <span class="number">9</span>)])</span><br><span class="line">sc.parallelize(tmp2).sortByKey(<span class="keyword">True</span>, <span class="number">3</span>, keyfunc=<span class="keyword">lambda</span> k: k.lower()).collect()</span><br><span class="line"><span class="comment"># [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]</span></span><br></pre></td></tr></table></figure><h4 id="两个pair-rdd转化">两个Pair RDD转化</h4><p><strong>substract</strong></p><p>留下在x中却不在y中的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">4</span>), (<span class="string">"b"</span>, <span class="number">5</span>), (<span class="string">"a"</span>, <span class="number">3</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">"a"</span>, <span class="number">3</span>), (<span class="string">"c"</span>, <span class="keyword">None</span>)])</span><br><span class="line">sorted(x.subtract(y).collect())</span><br><span class="line"><span class="comment">#[('b', 4), ('b', 5)]</span></span><br></pre></td></tr></table></figure><p><strong>substractByKey</strong></p><p>删掉X中在Y中也存在的Key所包含的所有元素</p><p><strong>join</strong></p><p>内连接，从x中去和y中一个一个的匹配，(k, v1), (k, v2) -- (k, (v1, v2))</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">4</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">"a"</span>, <span class="number">2</span>), (<span class="string">"a"</span>, <span class="number">3</span>)])</span><br><span class="line">sorted(x.join(y).collect())</span><br><span class="line"><span class="comment"># [('a', (1, 2)), ('a', (1, 3))]</span></span><br></pre></td></tr></table></figure><p><strong>leftOuterJoin</strong></p><p>左边RDD的键都有，右边没有的配None</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">4</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">'a'</span>, <span class="number">2</span>)])</span><br><span class="line">sorted(x.leftOuterJoin(y).collect())</span><br><span class="line"><span class="comment"># [('a', (1, 2)), ('b', (4, None))]</span></span><br></pre></td></tr></table></figure><p><strong>rightOuterJoin</strong></p><p>右边RDD的键都有，左边没有的配None</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">4</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">'a'</span>, <span class="number">2</span>)])</span><br><span class="line">sorted(x.rightOuterJoin(y).collect())</span><br><span class="line"><span class="comment"># [('a', (1, 2))]</span></span><br><span class="line">sorted(y.rightOuterJoin(x).collect())</span><br><span class="line"><span class="comment"># [('a', (2, 1)), ('b', (None, 4))]</span></span><br></pre></td></tr></table></figure><p><strong>cogroup</strong></p><p>将两个RDD中拥有相同键的value分组到一起，即使两个RDD的V不一样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">4</span>)])</span><br><span class="line">y = sc.parallelize([(<span class="string">'a'</span>, <span class="number">2</span>)])</span><br><span class="line">x.cogroup(y).collect()</span><br><span class="line"><span class="comment"># 上面显示的是16进制</span></span><br><span class="line">[(x, tuple(map(list, y))) <span class="keyword">for</span> x, y <span class="keyword">in</span> sorted(x.cogroup(y).collect())]</span><br><span class="line"><span class="comment"># [('a', ([1], [2])), ('b', ([4], []))]</span></span><br></pre></td></tr></table></figure><h3 id="行动操作">行动操作</h3><p><strong>countByKey</strong></p><p>对每个键对应的元素分别计数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>)])</span><br><span class="line">rdd.countByKey().items() <span class="comment"># 转换成一个dict，再取所有元素</span></span><br><span class="line"><span class="comment"># [('a', 2), ('b', 1)]</span></span><br></pre></td></tr></table></figure><p><strong>collectAsMap</strong></p><p>返回一个map</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = sc.parallelize([(<span class="number">1</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">4</span>)]).collectAsMap()</span><br><span class="line">m[<span class="number">1</span>] - <span class="number">2</span> <span class="comment"># 当做map操作即可</span></span><br><span class="line">m[<span class="number">3</span>] - <span class="number">4</span></span><br></pre></td></tr></table></figure><p><strong>lookup(key)</strong></p><p>返回键的RDD中的值列表。如果RDD具有已知的分区器，则通过仅搜索key映射到的分区来高效地完成该操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">l = range(<span class="number">1000</span>) <span class="comment"># 1,2,3,...,1000</span></span><br><span class="line">rdd = sc.parallelize(zip(l, l), <span class="number">10</span>) <span class="comment"># 键和值一样，10个数据分片，10个并行度，10个task</span></span><br><span class="line">rdd.lookup(<span class="number">42</span>) <span class="comment"># slow</span></span><br><span class="line"><span class="comment"># [42]</span></span><br><span class="line">sorted_rdd = rdd.sortByKey()</span><br><span class="line">sorted_rdd.lookup(<span class="number">42</span>) <span class="comment"># fast</span></span><br><span class="line"><span class="comment"># [42]</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="string">'a1'</span>), (<span class="string">'a'</span>, <span class="string">'a2'</span>), (<span class="string">'b'</span>, <span class="string">'b1'</span>)])</span><br><span class="line">rdd.lookup(<span class="string">'a'</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 'a1'</span></span><br></pre></td></tr></table></figure><h3 id="聚合操作">聚合操作</h3><p>当数据是键值对组织的时候，<strong>聚合具有相同键的元素</strong>是很常见的操作。基础RDD有<code>fold()</code>, <code>combine()</code>, <code>reduce()</code>，Pair RDD有<code>combineByKey()</code><strong>最常用</strong>,<code>reduceByKey()</code>, <code>foldByKey()</code>等。</p><p><strong>计算均值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 方法一：mapValues和reduceByKey</span></span><br><span class="line">rdd = sc.parallelize([(<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">3</span>), (<span class="string">'b'</span>, <span class="number">4</span>)])</span><br><span class="line">maprdd = rdd.mapValues(<span class="keyword">lambda</span> x : (x, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># [('a', (1, 1)), ('a', (3, 1)), ('b', (4, 1))]</span></span><br><span class="line">reducerdd = maprdd.reduceByKey(<span class="keyword">lambda</span> x, y: (x[<span class="number">0</span>] + y[<span class="number">0</span>], x[<span class="number">1</span>] + y[<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># [('a', (4, 2)), ('b', (4, 1))]</span></span><br><span class="line">reducerdd.mapValues(<span class="keyword">lambda</span> x : x[<span class="number">0</span>]/x[<span class="number">1</span>]).collect()</span><br><span class="line"><span class="comment"># [('a', 2), ('b', 4)] </span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 方法二 combineByKey 最常用的</span></span><br><span class="line">nums = sc.parallelize([(<span class="string">'c'</span>, <span class="number">90</span>), (<span class="string">'m'</span>, <span class="number">95</span>), (<span class="string">'c'</span>, <span class="number">80</span>)])</span><br><span class="line">sum_count = nums.combineByKey(</span><br><span class="line"><span class="keyword">lambda</span> x: (x, <span class="number">1</span>),</span><br><span class="line">    <span class="keyword">lambda</span> acc, x: (acc[<span class="number">0</span>] + x, acc[<span class="number">1</span>] + <span class="number">1</span>),</span><br><span class="line">    <span class="keyword">lambda</span> acc1, acc2: (acc1[<span class="number">0</span>] + acc2[<span class="number">0</span>], acc1[<span class="number">1</span>] + acc2[<span class="number">1</span>])</span><br><span class="line">)</span><br><span class="line"><span class="comment"># [('c', (170, 2)), ('m', (95, 1))]</span></span><br><span class="line">avg_map = sum_count.mapValues(<span class="keyword">lambda</span> (sum, count): sum/count).collectAsMap()</span><br><span class="line"><span class="comment"># &#123;'c': 85, 'm': 95&#125;</span></span><br><span class="line">avg_map = sum_count.map(<span class="keyword">lambda</span> key, s_c: (key, s_c[<span class="number">0</span>]/s_c[<span class="number">1</span>])).collectAsMap()</span><br></pre></td></tr></table></figure><p><strong>统计单词计数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.textFile(<span class="string">'README.md'</span>)</span><br><span class="line">words = rdd.flatMap(<span class="keyword">lambda</span> x: x.split(<span class="string">' '</span>))</span><br><span class="line"><span class="comment"># 568个</span></span><br><span class="line">result = words.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> x, y: x+y)</span><br><span class="line"><span class="comment"># 289</span></span><br><span class="line">result.top(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># [('your', 1), ('you', 4)]</span></span><br></pre></td></tr></table></figure><h3 id="数据分区">数据分区</h3><p><strong>分区说明</strong></p><p>在分布式程序中，通信的代价是很大的。因此控制数据分布以获得<strong>最少的网络传输</strong>可以极大地提升整体性能。Spark程序通过<strong>控制RDD分区方式来减少通信开销</strong>。使用<code>partitionBy</code>进行分区</p><ul><li>不需分区：给定RDD只需要被扫描一次</li><li>需要分区：数据集在<strong>多次基于键的操作</strong>中使用，比如连接操作。<code>partitionBy</code>是<strong>转化操作</strong>，生成新的RDD，为了多次计算，一般要进行持久化<code>persist</code></li></ul><p>Spark中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分组。Spark不能显示控制具体每个键落在哪一个工作节点上，但是Spark可以确保同一组的键出现在同一个节点上。</p><ul><li>Hash分区：将一个RDD分成了100个分区，hashcode(key)%100 相同的，会在同一个节点上面</li><li>范围分区：将key在同一个范围区间内的记录放在同一个节点上</li></ul><p>一个简单的例子，内存中有一张很大的用户信息表 -- 即(UserId, UserInfo)组成的RDD，UserInfo包含用户订阅了的所有Topics。还有一张(UserId, LinkInfo)存放着过去5分钟用户浏览的Topic。现在要找出用户浏览了但是没有订阅的Topic数量。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(...)</span><br><span class="line"><span class="keyword">val</span> userData = sc.sequenceFile[<span class="type">UserId</span>, <span class="type">UserInfo</span>](<span class="string">"hdfs://..."</span>).persist()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processNewLog</span></span>(logFileName: <span class="type">String</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> events = sc.sequenceFile[<span class="type">UserId</span>, <span class="type">LinkInfo</span>](logFileName)</span><br><span class="line">  <span class="keyword">val</span> joined = userData.join(events)<span class="comment">// (UserId, (UserInfo, LinkInfo))</span></span><br><span class="line">  <span class="keyword">val</span> offTopicVisits = joined.filter&#123;</span><br><span class="line">    <span class="keyword">case</span> (<span class="type">UserId</span>, (<span class="type">UserInfo</span>, <span class="type">LinkInfo</span>)) =&gt;</span><br><span class="line">    !<span class="type">UserInfo</span>.topics.contains(<span class="type">LinkInfo</span>.topic)</span><br><span class="line">  &#125;.count()</span><br><span class="line">  print (<span class="string">"浏览了且未订阅的数量："</span> + offTopicVisits)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码<strong>不够高效</strong>。因为每次调用<code>processNewLog</code>都会用<code>join</code>操作，但我们却不知道数据集是如何分区的。</p><blockquote><p><strong>连接操作</strong>，会将两个数据集的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在机器上对所有键相同的记录进行操作。</p></blockquote><p>因为userData比events要大的多并且基本不会变化，所以有很多浪费效率的事情：每次调用时都对userData表进行计算hash值计算和跨节点数据混洗。</p><p><strong>解决方案</strong>：在程序开始的时候，对userData表使用<code>partitionBy()</code>转换操作，将<strong>这张表转换为哈希分区</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> userData = sc.sequenceFile[<span class="type">UserId</span>, <span class="type">UserInfo</span>](<span class="string">"hdfs://..."</span>)</span><br><span class="line">.partitionBy(<span class="keyword">new</span> <span class="type">HashPartioner</span>(<span class="number">100</span>))<span class="comment">// 构造100个分区</span></span><br><span class="line">.persist()<span class="comment">// 持久化当前结果</span></span><br></pre></td></tr></table></figure><p>events是本地变量，并且只使用一次，所以为它指定分区方式没有什么用处。</p><p>userData使用了<code>partitionBy()</code>，Spark就知道该RDD是根据键的hash值来分区的。在<code>userData.join(events)</code>时，Spark<strong>只会对events进行数据混洗操作</strong>。将events中特定UserId的记录发送到userData的对应分区所在的那台机器上。需要网络传输的数据就大大减少了，速度也就显著提升了。</p><p><strong>分区相关的操作</strong></p><p>Spark的许多操作都有将数据根据跨节点进行混洗的过程。所有这些操作都会从数据分区中获益。类似<code>join</code>这样的二元操作，预先进行数据分区会导致其中至少一个RDD<code>不发生数据混洗</code>。</p><blockquote><p>获取好处的操作：<code>cogroup</code>, <code>groupWith</code>, <code>join</code>, <code>leftOuterJoin</code> , <code>rightOuterJoin</code>, <code>groupByKey</code>, <code>reduceByKey</code> , <code>combineByKey</code>, <code>lookup</code></p><p>为结果设好分区的操作：<code>cogroup</code>, <code>groupWith</code>, <code>join</code>, <code>leftOuterJoin</code> , <code>rightOuterJoin</code>, <code>groupByKey</code>, <code>reduceByKey</code> , <code>combineByKey</code>, <code>partitionBy</code>, <code>sort</code>, （<code>mapValues</code>, <code>flatMapValues</code>, <code>filter</code> 如果父RDD有分区方式的话）</p></blockquote><p>其他所有的操作的结果都不会存在特定的分区方式。对于二元操作，输出数据的分区方式取决于父RDD的分区方式。默认情况结果会采取hash分区。</p><p><strong>PageRank</strong></p><p>PageRank的python版本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="string">""" PageRank算法</span></span><br><span class="line"><span class="string">author = PuLiming</span></span><br><span class="line"><span class="string">运行: bin/spark-submit files/pagerank.py data/mllib/pagerank_data.txt 10</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_contribs</span><span class="params">(urls, rank)</span>:</span></span><br><span class="line">    <span class="string">""" 给urls计算</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        urls: 目标url相邻的urls集合</span></span><br><span class="line"><span class="string">        rank: 目标url的当前rank</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        url: 相邻urls中的一个url</span></span><br><span class="line"><span class="string">        rank: 当前url的新的rank</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    num_urls = len(urls)</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">        <span class="keyword">yield</span> (url, rank / num_urls)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_url</span><span class="params">(url_line)</span>:</span></span><br><span class="line">    <span class="string">""" 把一行url切分开来</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        url_line: 一行url，如 1 2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        url, neighbor_url</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parts = re.split(<span class="string">r'\s+'</span>, url_line) <span class="comment"># 正则</span></span><br><span class="line">    <span class="keyword">return</span> parts[<span class="number">0</span>], parts[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_pagerank</span><span class="params">(sc, url_data_file, iterations)</span>:</span></span><br><span class="line">    <span class="string">""" 计算各个page的排名</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        sc: SparkContext</span></span><br><span class="line"><span class="string">        url_data_file: 测试数据文件</span></span><br><span class="line"><span class="string">        iterations: 迭代次数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        status: 成功就返回0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取url文件 ['1 2', '1 3', '2 1', '3 1'] </span></span><br><span class="line">    lines = sc.textFile(url_data_file).map(<span class="keyword">lambda</span> line: line.encode(<span class="string">'utf8'</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 建立Pair RDD (url, neighbor_urls) [(1,[2,3]), (2,[1]), (3, [1])]</span></span><br><span class="line">    links = lines.map(<span class="keyword">lambda</span> line : split_url(line)).distinct().groupByKey().mapValues(<span class="keyword">lambda</span> x: list(x)).cache()</span><br><span class="line">    <span class="comment"># 初始化所有url的rank为1 [(1, 1), (2, 1), (3, 1)]</span></span><br><span class="line">    ranks = lines.map(<span class="keyword">lambda</span> line : (line[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iterations):</span><br><span class="line">        <span class="comment"># (url, [(neighbor_urls), rank]) join neighbor_urls and rank </span></span><br><span class="line">        <span class="comment"># 把当前url的rank分别contribute到其他相邻的url (url, rank)</span></span><br><span class="line">        contribs = links.join(ranks).flatMap(</span><br><span class="line">            <span class="keyword">lambda</span> url_urls_rank: compute_contribs(url_urls_rank[<span class="number">1</span>][<span class="number">0</span>], url_urls_rank[<span class="number">1</span>][<span class="number">1</span>])</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 把url的所有rank加起来，再赋值新的</span></span><br><span class="line">        ranks = contribs.reduceByKey(add).mapValues(<span class="keyword">lambda</span> rank : rank * <span class="number">0.85</span> + <span class="number">0.15</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (link, rank) <span class="keyword">in</span> ranks.collect():</span><br><span class="line">        print(<span class="string">"%s has rank %s."</span> % (link, rank)) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">        print(<span class="string">"Usage: python pagerank.py &lt;data.txt&gt; &lt;iterations&gt;"</span>, file = sys.stderr)</span><br><span class="line">        sys.exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 数据文件和迭代次数</span></span><br><span class="line">    url_data_file = sys.argv[<span class="number">1</span>]</span><br><span class="line">    iterations = int(sys.argv[<span class="number">2</span>])</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 配置 SparkContext</span></span><br><span class="line">    conf = SparkConf().setAppName(<span class="string">'PythonPageRank'</span>)</span><br><span class="line">    conf.setMaster(<span class="string">'local'</span>)</span><br><span class="line">    sc = SparkContext(conf=conf)</span><br><span class="line"></span><br><span class="line">    ret = compute_pagerank(sc, url_data_file, iterations)</span><br><span class="line">    sys.exit(ret)</span><br></pre></td></tr></table></figure><p>PageRank的scala版本</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(...)</span><br><span class="line"><span class="keyword">val</span> links = sc.objectFile[(<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">String</span>])](<span class="string">"links"</span>)</span><br><span class="line">.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">100</span>))</span><br><span class="line">.persist()</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> ranks = links.mapValues(_ =&gt; <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 迭代10次</span></span><br><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> until <span class="number">10</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> contributions = links.join(ranks).flatMap &#123;</span><br><span class="line">    <span class="keyword">case</span> (pageId, (links, rank)) =&gt;</span><br><span class="line">    links.map(dest =&gt; (dest, rank / links.size))</span><br><span class="line">  &#125;</span><br><span class="line">  ranks = contributions.reduceByKey(_ + _).mapValues(<span class="number">0.15</span> + <span class="number">0.85</span>* _)</span><br><span class="line">&#125;</span><br><span class="line">ranks.saveAsTextFile(<span class="string">"ranks"</span>)</span><br></pre></td></tr></table></figure><p>当前scala版本的PageRank算法的优点：</p><ul><li>links每次都会和ranks发生连接操作，所以一开始就对它进行分区<code>partitionBy</code>，就不会通过网络进行数据混洗了，节约了相当可观的网络通信开销</li><li>对links进行<code>persist</code>，留在内存中，每次迭代使用</li><li>第一次创建ranks，使用<code>mapValues</code>保留了父RDD的分区方式，第一次连接开销就会很小</li><li><code>reduceByKey</code>后已经是分区了，再使用<code>mapValues</code>分区方式，再次和links进行<code>join</code>就会更加<strong>高效</strong></li></ul><p>所以对分区后的RDD尽量使用<code>mapValues</code>保留父分区方式，而不要用<code>map</code>丢失分区方式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;pairrdd及其创建&quot;&gt;PairRDD及其创建&lt;/h3&gt;
&lt;p&gt;键值对RDD称为PairRDD，通常用来做&lt;code&gt;聚合计算&lt;/code&gt;。Spark为Pair RDD提供了许多专有的操作。&lt;/p&gt;
&lt;figure class=&quot;highlight pyt
      
    
    </summary>
    
      <category term="大数据" scheme="http://plmsmile.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://plmsmile.github.io/tags/Spark/"/>
    
      <category term="PairRDD" scheme="http://plmsmile.github.io/tags/PairRDD/"/>
    
  </entry>
  
  <entry>
    <title>Spark的基础RDD</title>
    <link href="http://plmsmile.github.io/2017/03/13/Spark-BaseRDD/"/>
    <id>http://plmsmile.github.io/2017/03/13/Spark-BaseRDD/</id>
    <published>2017-03-13T11:26:18.000Z</published>
    <updated>2017-09-22T03:26:44.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="rdd基础">RDD基础</h3><p><strong>RDD</strong>是Spark中的核心抽象——弹性分布式数据集(Resilient Distributed Dataset)。其实RDD是<strong>分布式的元素集合</strong>，是一个不可变的分布式对象集合。每个RDD都会被分为<strong>多个分区</strong>，这些分区运行在不同的节点上。RDD可以包含任意对象。Spark会自动将这些RDD的数据分发到集群上，并将操作<strong>并行化执行</strong>。</p><p>RDD当做我们通过转化操作构建出来的、记录如何计算数据的指令列表。</p><p>对数据的所有操作都是<strong>创建RDD</strong>、<strong>转化已有RDD</strong>、<strong>调用RDD操作进行求值</strong>。</p><p>RDD主要有2个操作。</p><ul><li>转化操作：由一个RDD生成一个新的RDD。惰性求值。</li><li>行动操作：会对RDD计算出一个结果或者写到外部系统。会触发<strong>实际的计算</strong>。</li></ul><p>Spark会<strong>惰性计算</strong>这些RDD，只有第一次在一个行动操作中用到时才会真正计算。</p><p>Spark的RDD会在每次行动操作时<strong>重新计算</strong>。如果想<strong>复用</strong>，则用<code>persist</code>把RDD<strong>持久化缓存</strong>下来。</p><p>下面是总的大体步骤</p><ul><li>从外部数据创建输入RDD。如<code>textFile</code></li><li>使用转化操作得到新的RDD。如<code>map</code>，<code>filter</code></li><li>对重用的中间结果RDD进行持久化。如<code>persist</code></li><li>使用行动操作来触发一次并行计算。如<code>count</code>, <code>first</code></li></ul><h3 id="创建rdd">创建RDD</h3><p>创建RDD主要有两个方法：读取集合，读取外部数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取集合</span></span><br><span class="line">words = sc.parallelize([<span class="string">"hello"</span>, <span class="string">"spark"</span>, <span class="string">"good"</span>, <span class="string">"study"</span>])</span><br><span class="line"><span class="comment"># 读取外部数据</span></span><br><span class="line">lines = sc.textFile(<span class="string">"README.md"</span>)</span><br></pre></td></tr></table></figure><h3 id="转化操作">转化操作</h3><p>RDD的转化操作会返回新的RDD，是<strong>惰性求值</strong>的。即只有真正调用这些RDD的行动操作这些RDD才会被计算。许多转化操作是<strong>针对各个元素</strong>的，即每次只会操作RDD中的一个元素。</p><p>通过转化操作，会从已有RDD派生出新的RDD。Spark会使用<code>谱系图</code>(lineage graph)来记录这些不同<strong>RDD之间的依赖关系</strong>。Spark会利用这些关系按需计算每个RDD，或者恢复所丢失的数据。</p><p>最常用的转化操作是<code>map()</code>和<code>filter()</code>。下面说明一下常用的转化操作。</p><p><strong>map(f)</strong></p><p>对每个元素使用func函数，将返回值构成新的RDD。不会保留父RDD的分区。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rdd =  sc.parallelize([<span class="string">"b"</span>, <span class="string">"a"</span>, <span class="string">"c"</span>])</span><br><span class="line">rddnew = rdd.map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># [('b', 1), ('a', 1), ('c', 1)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可使用sorted()进行排序</span></span><br><span class="line">sorted(rddnew.collect())</span><br><span class="line"><span class="comment"># [('a', 1), ('b', 1), ('c', 1)]</span></span><br></pre></td></tr></table></figure><p><strong>flatMap(f)</strong></p><p>对每个元素使用func函数，然后展平结果。通常用来切分单词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lines = sc.textFile(<span class="string">"README.md"</span>)</span><br><span class="line"><span class="comment"># 104个</span></span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line : line.split(<span class="string">" "</span>))</span><br><span class="line"><span class="comment"># 568个</span></span><br><span class="line">rdd = sc.parallelize([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">rdd2 = rdd.flatMap(<span class="keyword">lambda</span> x: range(<span class="number">1</span>, x))<span class="comment"># range(1, x) 生成1-x的数，不包括x</span></span><br><span class="line"><span class="comment"># [1, 1, 2, 1, 2, 3]</span></span><br></pre></td></tr></table></figure><p><strong>filter(f)</strong></p><p>元素满足f函数，则放到新的RDD里</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">rdd.filter(<span class="keyword">lambda</span> x: x % <span class="number">2</span> == <span class="number">0</span>).collect()</span><br><span class="line"><span class="comment"># [2, 4]</span></span><br></pre></td></tr></table></figure><p><strong>distinct</strong></p><p>去重。开销很大，会进行数据混洗。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sorted(sc.parallelize([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]).distinct().collect())</span><br><span class="line"><span class="comment"># [1, 2, 3]</span></span><br></pre></td></tr></table></figure><p><strong>union</strong></p><p>合并两个RDD。会包含重复的元素</p><p><strong>intersection</strong></p><p>求两个RDD的共同的元素。会去掉重复的元素</p><p><strong>subtract</strong></p><p>留下在自己却不在other里面的元素</p><p><strong>cartesian</strong></p><p>两个RDD的笛卡尔积</p><h3 id="行动操作">行动操作</h3><p>行动操作会把最终求得的结果返回到驱动程序，或者写入外部存储系统中。</p><p><strong>collect</strong></p><p>返回RDD中的所有元素。只适用于数据小的情况，因为会把所有数据加载到驱动程序的内存中。通常只在单元测试中使用</p><p><strong>count</strong></p><p>RDD中元素的个数</p><p><strong>countByValue</strong></p><p>各元素在RDD中出现的次数，返回一个<code>dictionary</code>。在pair RDD中有<code>countByKey</code>方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]).countByValue().items()</span><br><span class="line"><span class="comment"># [(1, 2), (2, 3)]</span></span><br></pre></td></tr></table></figure><p><strong>take(num)</strong></p><p>返回RDD中的n个元素。它会首先扫描一个分区，在这个分区里面尽量满足n个元素，不够再去查别的分区。只能用于数据量小的情况下。得到的顺序可能和你预期的不一样</p><p><strong>takeOrdered(num, key=None)</strong></p><p>获取n个元素，按照<strong>升序</strong>或者按照<strong>传入的key function</strong>。只适用于数据小的RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]).takeOrdered(<span class="number">6</span>)</span><br><span class="line"><span class="comment"># [1, 2, 3, 4, 5, 6]</span></span><br><span class="line">sc.parallelize([<span class="number">10</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], <span class="number">2</span>).takeOrdered(<span class="number">6</span>, key=<span class="keyword">lambda</span> x: -x)</span><br><span class="line"><span class="comment"># [10, 9, 7, 6, 5, 4]</span></span><br></pre></td></tr></table></figure><p><strong>top(num, key=None)</strong></p><p>从RDD只获取前N个元素。降序排列。只适用于数据量小的RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]).top(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># [6, 5]</span></span><br><span class="line">sc.parallelize([<span class="number">10</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">12</span>, <span class="number">3</span>]).top(<span class="number">3</span>, key=str)</span><br><span class="line"><span class="comment"># [4, 3, 2]</span></span><br></pre></td></tr></table></figure><p><strong>reduce(f)</strong></p><p>并行整合RDD中的所有数据，得到一个结果。接收一个f函数。目前在本地reduce partitions。返回结果类型不变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]).reduce(add)</span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]).reduce(<span class="keyword">lambda</span> x, y: x+y)</span><br><span class="line"><span class="comment"># 15</span></span><br></pre></td></tr></table></figure><p><strong>fold(zeroValue, op)</strong></p><p>和reduce一样，但是需要提供<strong>初始值</strong>。op(t1, t2)，t1可以变，t2不能变</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]).fold(<span class="number">0</span>, add)</span><br><span class="line"><span class="comment"># 15</span></span><br></pre></td></tr></table></figure><p><strong>aggregate(zeroValue, seqOp, combOp)</strong></p><p>聚合所有分区的元素，然后得到一个结果。和<code>reduce</code>相似，但是通常<strong>返回不同的类型</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">seqOp = (<span class="keyword">lambda</span> x, y: (x[<span class="number">0</span>] + y, x[<span class="number">1</span>] + <span class="number">1</span>))<span class="comment"># 累加</span></span><br><span class="line">combOp = (<span class="keyword">lambda</span> x, y: (x[<span class="number">0</span>] + y[<span class="number">0</span>], x[<span class="number">1</span>] + y[<span class="number">1</span>]))<span class="comment"># combine多个</span></span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).aggregate((<span class="number">0</span>, <span class="number">0</span>), seqOp, combOp)</span><br><span class="line"><span class="comment"># (10, 4)</span></span><br></pre></td></tr></table></figure><p><strong>foreach(f)</strong></p><p>对rdd中的每个元素使用f函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (x)</span><br><span class="line">sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).foreach(f)</span><br></pre></td></tr></table></figure><p><strong>glom</strong></p><p>将分区中的元素合并到一个list</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], <span class="number">2</span>)</span><br><span class="line">rdd.glom().collect()</span><br><span class="line"><span class="comment"># [[1, 2], [3, 4, 5]]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;rdd基础&quot;&gt;RDD基础&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;RDD&lt;/strong&gt;是Spark中的核心抽象——弹性分布式数据集(Resilient Distributed Dataset)。其实RDD是&lt;strong&gt;分布式的元素集合&lt;/strong&gt;，是一个不
      
    
    </summary>
    
      <category term="大数据" scheme="http://plmsmile.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="http://plmsmile.github.io/tags/Spark/"/>
    
      <category term="RDD" scheme="http://plmsmile.github.io/tags/RDD/"/>
    
  </entry>
  
  <entry>
    <title>HDFS初步学习</title>
    <link href="http://plmsmile.github.io/2016/12/05/HDFS%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/"/>
    <id>http://plmsmile.github.io/2016/12/05/HDFS初步学习/</id>
    <published>2016-12-05T11:59:45.000Z</published>
    <updated>2017-09-22T03:23:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>HDFS是Hadoop的一个分布式文件系统</p><h2 id="hdfs设计原理">HDFS设计原理</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ohps10obl.bkt.clouddn.com/HDFS%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86.png" alt="HDFS设计原理" title="">                </div>                <div class="image-caption">HDFS设计原理</div>            </figure><blockquote><ul><li>提供了一个抽象访问界面，通过<code>路径</code>访问文件</li><li>抽象界面上所展示的文件，存储在很多个<code>服务器</code>上面</li><li>抽象路径与实际存储的映射关系，由<code>元数据管理系统</code>来进行管理</li><li>为了数据的安全性，数据被存成多个<code>副本</code></li><li>为了负载均衡和吞吐量，这些文件被分隔成为<code>若干个块</code></li></ul></blockquote><h2 id="元数据存储细节">元数据存储细节</h2><ul><li>职责及存储格式</li><li>响应客户端请求，维护hdfs目录树</li><li>管理元数据，维护映射（HDFS上的文件 --- Blocks --- DataNode</li><li>fileName, replicas, blockIds, idToHosts</li><li><p>/test/a.txt, 3, {blk_1, blk_2}, [{blk1 : [h0, h1, h2]}, {blk2 : [{h1, h2}]}]</p></li><li>元数据存储信息</li><li>Meta.data 内存中的元数据</li><li>Meta.edits 元数据最新的修改信息，存在磁盘上</li><li><p>Meta.data.image</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;HDFS是Hadoop的一个分布式文件系统&lt;/p&gt;
&lt;h2 id=&quot;hdfs设计原理&quot;&gt;HDFS设计原理&lt;/h2&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
    
      
    
    </summary>
    
      <category term="大数据" scheme="http://plmsmile.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hadoop" scheme="http://plmsmile.github.io/tags/Hadoop/"/>
    
      <category term="HDFS" scheme="http://plmsmile.github.io/tags/HDFS/"/>
    
  </entry>
  
</feed>
