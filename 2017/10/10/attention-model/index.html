<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>图文介绍注意力机制 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="注意力,自然语言处理,机器翻译,Attention Model,Encoder-Decoder">
    <meta name="description" content="Encoder-Decoder 基本介绍 举个翻译的例子，原始句子\(X = (x_1, x_2, \cdots, x_m)\) ，翻译成目标句子\(Y = (y_1, y_2, \cdots, y_n)\) 。 现在采用Encoder-Decoder架构模型，如下图  Encoder会利用整个原始句子生成一">
<meta name="keywords" content="注意力,自然语言处理,机器翻译,Attention Model,Encoder-Decoder">
<meta property="og:type" content="article">
<meta property="og:title" content="图文介绍注意力机制">
<meta property="og:url" content="http://plmsmile.github.io/2017/10/10/attention-model/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="Encoder-Decoder 基本介绍 举个翻译的例子，原始句子\(X = (x_1, x_2, \cdots, x_m)\) ，翻译成目标句子\(Y = (y_1, y_2, \cdots, y_n)\) 。 现在采用Encoder-Decoder架构模型，如下图  Encoder会利用整个原始句子生成一个语义向量，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/01-encoder-decoder-n">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/02-encoder-decoder-am-n">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/03-ci-compute-n">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/04-rnn-encoder-decoder-n">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/05-aij-compute-n">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/06-summary-am-n">
<meta property="og:updated_time" content="2018-03-18T10:30:58.053Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="图文介绍注意力机制">
<meta name="twitter:description" content="Encoder-Decoder 基本介绍 举个翻译的例子，原始句子\(X = (x_1, x_2, \cdots, x_m)\) ，翻译成目标句子\(Y = (y_1, y_2, \cdots, y_n)\) 。 现在采用Encoder-Decoder架构模型，如下图  Encoder会利用整个原始句子生成一个语义向量，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句">
<meta name="twitter:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/01-encoder-decoder-n">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.0">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="https://plmsmile.github.io/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">图文介绍注意力机制</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">图文介绍注意力机制</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-10-10T03:40:01.000Z" itemprop="datePublished" class="page-time">
  2017-10-10
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#encoder-decoder"><span class="post-toc-number">1.</span> <span class="post-toc-text">Encoder-Decoder</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#基本介绍"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">基本介绍</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#缺点"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">缺点</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#attention-model"><span class="post-toc-number">2.</span> <span class="post-toc-text">Attention Model</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#基本架构"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">基本架构</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#语义向量的计算"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">语义向量的计算</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#注意力分配概率计算"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">注意力分配概率计算</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#am模型的意义"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">AM模型的意义</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#文本摘要例子"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">文本摘要例子</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#pytorch翻译am实现"><span class="post-toc-number">3.</span> <span class="post-toc-text">PyTorch翻译AM实现</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#思想"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">思想</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#实现"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">实现</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-attention-model"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">图文介绍注意力机制</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-10-10 11:40:01" datetime="2017-10-10T03:40:01.000Z"  itemprop="datePublished">2017-10-10</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="encoder-decoder">Encoder-Decoder</h1>
<h2 id="基本介绍">基本介绍</h2>
<p>举个翻译的例子，原始句子<span class="math inline">\(X = (x_1, x_2, \cdots, x_m)\)</span> ，翻译成目标句子<span class="math inline">\(Y = (y_1, y_2, \cdots, y_n)\)</span> 。</p>
<p>现在采用<code>Encoder-Decoder</code>架构模型，如下图</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/01-encoder-decoder-n" style="display:block; margin:auto" width="70%"></p>
<p>Encoder会利用整个原始句子生成一个<code>语义向量</code>，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句子的意思、句法结构、性别信息等等。具体框架可以参考<a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247484196&amp;idx=1&amp;sn=efa6b79d24b138ff79f33ec3426c79e2&amp;chksm=ebb43bf0dcc3b2e6e69ff3b7d7cabf28744e276ee08ccfcfc1dc2c3f0bf52a122c9bd77ff319&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Encoder-Decoder框架</a>。</p>
<p>Encoder对<span class="math inline">\(X\)</span> 进行非线性变换得到<code>中间语义向量c</code> ： <span class="math display">\[
c = G(x_1, x_2, \cdots, x_n)
\]</span> Decoder根据<strong>语义<span class="math inline">\(c\)</span> 和生成的历史单词</strong><span class="math inline">\((y_1, y_2, \cdots, y_{i-1})\)</span> 来<strong>生成第<span class="math inline">\(i\)</span> 个单词 <span class="math inline">\(y_i\)</span>：</strong> <span class="math display">\[
y_i = f(c, y_1, y_2, \cdots, y_{i-1})
\]</span> Encoder-Decoder是个<strong>创新大杀器</strong>，是个通用的计算框架。Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN，RNN，BiRNN，GRU，LSTM， Deep LSTM。上面的内容<strong>任意组合</strong>，只要得到的<strong>效果好</strong>，就是一个<strong>创新</strong>，就可以毕业了。（当然别人没有提出过）</p>
<h2 id="缺点">缺点</h2>
<p>在生成目标句子<span class="math inline">\(Y\)</span>的单词时，<strong>所有的单词<span class="math inline">\(y_i\)</span>使用的语义编码<span class="math inline">\(c\)</span> 都是一样的</strong>。而语义编码<span class="math inline">\(c\)</span>是由句子<span class="math inline">\(X\)</span> 的每个单词经过Encoder编码产生，也就是说<strong>每个<span class="math inline">\(x_i\)</span>对所有<span class="math inline">\(y_j\)</span>的影响力都是相同的</strong>，没有任何区别的。所以上面的是<strong>注意力不集中的分心模型</strong>。</p>
<p>句子较短时问题不大，但是较长时，所有语义完全通过一个中间语义向量来表示，<strong>单词自身的信息已经消失，会丢失更多的细节信息</strong>。</p>
<p><strong>例子</strong></p>
<p>比如输入<span class="math inline">\(X\)</span>是<code>Tom chase Jerry</code>，模型翻译出 <code>汤姆 追逐 杰瑞</code>。在翻译“杰瑞”的时候，“Jerry”对“杰瑞”的贡献<strong>更重要</strong>。但是显然普通的Encoder-Decoder模型中，三个单词对于翻译“Jerry-杰瑞”的贡献是一样的。</p>
<p>解决方案应该是，每个单词对于翻译“杰瑞”的<strong>贡献应该不一样</strong>，如翻译“杰瑞”时： <span class="math display">\[
(Tom, 0.3), \;  (Chase, 0.2), \;  (Jerry, 0.5)
\]</span></p>
<h1 id="attention-model">Attention Model</h1>
<h2 id="基本架构">基本架构</h2>
<p>Attention Model的架构如下：</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/02-encoder-decoder-am-n" style="display:block; margin:auto" width="60%"></p>
<p>如图所示，生成每个单词<span class="math inline">\(y_i\)</span>时，都有<strong>各自的语义向量<span class="math inline">\(C_i\)</span></strong>，不再是统一的<span class="math inline">\(C\)</span> 。 <span class="math display">\[
y_i = f(C_i, y_1, \cdots, y_{i-1})
\]</span> 例如，前3个单词的生成： <span class="math display">\[
\begin{align}
&amp; y_1 = f(C_1) \\
&amp; y_2 = f(C_2, y_1) \\
&amp; y_3 = f(C_3, y_1, y_2) \\
\end{align}
\]</span></p>
<h2 id="语义向量的计算">语义向量的计算</h2>
<p><code>注意力分配概率</code> <strong><span class="math inline">\(a_{ij}\)</span> 表示 <span class="math inline">\(y_i\)</span>收到<span class="math inline">\(x_j\)</span> 的注意力概率</strong>。</p>
<p>例如<span class="math inline">\(X=(Tom, Chase, Jerry)\)</span>，<span class="math inline">\(Y = (汤姆, 追逐, 杰瑞)\)</span> 。<span class="math inline">\(a_{12}=0.2\)</span>表示<code>汤姆</code> 收到来自<code>Chase</code>的注意力概率是0.2。</p>
<p>有下面的注意力分配矩阵： <span class="math display">\[
A = [a_{ij}] = 
\begin {bmatrix}
0.6 &amp; 0.2 &amp; 0.2  \\
0.2 &amp; 0.7 &amp; 0.1 \\
0.3 &amp; 0.1 &amp; 0.5 \\
\end {bmatrix}
\]</span> 第<span class="math inline">\(i\)</span>行表示<strong><span class="math inline">\(y_i\)</span> 收到的所有来自输入单词的注意力分配概率</strong>。<span class="math inline">\(y_i\)</span> 的语义向量<span class="math inline">\(C_i\)</span> 由这些<strong>注意力分配概率</strong>和Encoder对<strong>单词<span class="math inline">\(x_j\)</span>的转换函数</strong>相乘，计算而成，例如： <span class="math display">\[
\begin {align}
&amp; C_1 = C_{汤姆} = g(0.6 \cdot h(Tom),\; 0.2 \cdot h(Chase),\; 0.2 \cdot h(Jerry)) \\
&amp; C_2 = C_{追逐} = g(0.2 \cdot h(Tom) ,\;0.7 \cdot h(Chase) ,\;0.1 \cdot h(Jerry)) \\
&amp; C_3 = C_{汤姆} = g(0.3 \cdot h(Tom),\; 0.2 \cdot h(Chase) ,\;0.5 \cdot h(Jerry)) \\
\end {align}
\]</span> <span class="math inline">\(\color{blue}{h(x_j)}\)</span> 就表示Encoder<strong>对输入英文单词的某种变换函数</strong>。比如Encoder使用RNN的话，<span class="math inline">\(h(x_j)\)</span>往往都是某个时刻输入<span class="math inline">\(x_j\)</span> 后<strong>隐层节点的状态值</strong>。</p>
<p><code>g函数</code> 表示注意力分配后的整个句子的语义转换信息，一般都是<strong>加权求和</strong>，则有<strong>语义向量</strong>计算公式： <span class="math display">\[
C_i  = \sum_{j=1}^{T_x} a_{ij} \cdot h_j, \quad h_j = h(x_j)
\]</span> 其中<span class="math inline">\(\color{blue}{T_x}\)</span> 代表<strong>输入句子的长度</strong>。形象来看计算过程如下图：</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/03-ci-compute-n" style="display:block; margin:auto" width="50%"></p>
<h2 id="注意力分配概率计算">注意力分配概率计算</h2>
<p>语义向量需要注意力分配概率和Encoder输入单词变换函数来共同计算得到。</p>
<p>但是比如<code>汤姆</code>收到的分配概率<span class="math inline">\(a_1 = (0.6, 0.2, 0.2)\)</span>是怎么计算得到的呢？</p>
<p>这里采用RNN作为Encoder和Decoder来说明。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/04-rnn-encoder-decoder-n" style="display:block; margin:auto" width="60%"></p>
<p>注意力分配概率如下图计算</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/05-aij-compute-n" style="display:block; margin:auto" width="60%"></p>
<p>对于<span class="math inline">\(a_{ij}\)</span> 其实是通过一个<code>对齐函数F</code>来进行计算的，两个参数：输入节点<span class="math inline">\(j\)</span>，和输出节点<span class="math inline">\(i\)</span>，当然一般是取<strong>隐层状态</strong>。 <span class="math display">\[
a_i = F(i, j), \quad j \in [1, T_x], \quad h(j)\,Encoder, \; H(i)\,Decoder
\]</span> <span class="math inline">\(\color{blue}{F(i, j)}\)</span>代表<span class="math inline">\(y_i\)</span>和<span class="math inline">\(x_j\)</span>的<strong>对齐可能性</strong>。一般F输出后，再经过<code>softmax</code>就得到了注意力分配概率。</p>
<h2 id="am模型的意义">AM模型的意义</h2>
<p>一般地，会把AM模型看成单词对齐模型，输入句子单词和这个目标生句子成单词的对齐概率。</p>
<p>其实，理解为<code>影响力模型</code>也是合理的。就是在生成目标单词的时候，<strong>输入句子中的每个单词，对于生成当前目标单词有多大的影响程度。</strong></p>
<p>AM模型有很多的应用，思想大都如此。</p>
<h2 id="文本摘要例子">文本摘要例子</h2>
<p>比如<code>文本摘要</code>的例子，输入一个长句，提取出重要的信息。</p>
<p>输入&quot;russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism&quot;。</p>
<p>输出&quot;russia calls for joint front against terrorism&quot;。</p>
<p>下图代表着输入单词对输出单词的影响力，颜色越深，影响力越大，注意力分配概率也越大。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/am/06-summary-am-n" style="display:block; margin:auto" width="60%"></p>
<h1 id="pytorch翻译am实现">PyTorch翻译AM实现</h1>
<h2 id="思想">思想</h2>
<p>参考<a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">这篇论文</a> 。</p>
<p>生成目标单词<span class="math inline">\(y_i\)</span> 的计算概率是 <span class="math display">\[
p(y_i \mid (y_1,\cdots, y_{i-1}), x) = g(y_{i-1}, s_i, c_i)
\]</span> 符号意义说明</p>
<ul>
<li><span class="math inline">\(y_i\)</span> 当前应该生成的目标单词，<span class="math inline">\(y_{i-1}\)</span> 上一个节点的输出单词</li>
<li><span class="math inline">\(s_i\)</span> 当前节点的<strong>隐藏状态</strong></li>
<li><span class="math inline">\(c_i\)</span> 生成当前单词应该有的<strong>语义向量</strong></li>
<li><span class="math inline">\(g\)</span> 全连接层的函数</li>
</ul>
<p><strong>隐层状态<span class="math inline">\(s_i\)</span></strong></p>
<p>求当前Decoder隐层状态<span class="math inline">\(s_i\)</span>：由上一层的隐状态<span class="math inline">\(s_{i-1}\)</span>，输出单词<span class="math inline">\(y_{i-1}\)</span> ，语义向量<span class="math inline">\(c_i\)</span> <span class="math display">\[
s_i = f(s_{i-1}, y_{i-1}, c_i)
\]</span> <strong>语义向量<span class="math inline">\(c_i\)</span></strong></p>
<p>语义向量：分配权值<span class="math inline">\(a_{ij}\)</span>，Encoder的输出 <span class="math display">\[
c_i  = \sum_{j=1}^{T_x} a_{ij} \cdot h_j, \quad h_j = h(x_j)
\]</span> <strong>分配概率<span class="math inline">\(a_{ij}\)</span></strong></p>
<p>注意力分配概率<span class="math inline">\(a_{ij} ，\)</span> <span class="math inline">\(y_i\)</span> 收到<span class="math inline">\(x_j\)</span> 的注意力：分配能量<span class="math inline">\(e_{ij}\)</span> <span class="math display">\[
a_{ij} = \frac{\exp(e_{ij})} {\sum_{k=1}^{T_x} \exp (e_{ik})}
\]</span> <strong>分配能量<span class="math inline">\(e_{ij}\)</span></strong></p>
<p><span class="math inline">\(x_j\)</span> 注意<span class="math inline">\(y_i\)</span> 的能量，由encoder的隐状态<span class="math inline">\(h_j\)</span> 和 decoder的上一层的隐状态<span class="math inline">\(s_{i-1}\)</span> 计算而成。a函数就是一个线性层。也就是上面的F函数。 <span class="math display">\[
e_{ij} = a(s_{i-1}, h_j)
\]</span></p>
<h2 id="实现">实现</h2>
<p>Decoder由4层组成</p>
<ul>
<li>embedding : word2vec</li>
<li>attention layer: 为每个encoder的output计算Attention</li>
<li>RNN layer:</li>
<li>output layer:</li>
</ul>
<p>Decoder输入 <span class="math inline">\(s_{i-1}\)</span> , <span class="math inline">\(y_{i-1}\)</span> 和encoder的所有outputs <span class="math inline">\(h_*\)</span></p>
<p><strong>Embedding Layer</strong></p>
<p>输入<span class="math inline">\(y_{i-1}\)</span>，对其进行编码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># y(i-1)</span></span><br><span class="line">embedded = embedding(last_rnn_output)</span><br></pre></td></tr></table></figure>
<p><strong>Attention Layer</strong></p>
<p>输入<span class="math inline">\(s_{i-1}, h_j\)</span>，输出分配能量<span class="math inline">\(e_{ij}\)</span>， 计算出<span class="math inline">\(a_{ij}\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attn_weights[j] = attn_layer(last_hidden, encoder_outputs[j])</span><br><span class="line">attn_weights = normalize(attn_weights)</span><br></pre></td></tr></table></figure>
<p><strong>计算语义向量</strong></p>
<p>求语义向量<span class="math inline">\(c_i\)</span>， 一般是加权求和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context = sum(attn_weights * encoder_outputs)</span><br></pre></td></tr></table></figure>
<p><strong>RNN Layer</strong></p>
<p>输入<span class="math inline">\(s_{i-1}, y_{i-1}, c_i\)</span> ，内部隐层状态，输出<span class="math inline">\(s_i\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rnn_input = concat(embeded, context)</span><br><span class="line">rnn_output, rnn_hidden = rnn(rnn_input, last_hidden)</span><br></pre></td></tr></table></figure>
<p><strong>输出层</strong></p>
<p>输入<span class="math inline">\(y_{i-1}, s_i, c_i\)</span> ，输出<span class="math inline">\(y_i\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = out(embedded, rnn_output, context)</span><br></pre></td></tr></table></figure>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-03-18T10:30:58.053Z" itemprop="dateUpdated">2018-03-18 18:30:58</time>
</span><br>


        
        <br>原始链接：<a href="/2017/10/10/attention-model/" target="_blank" rel="external">http://plmsmile.github.io/2017/10/10/attention-model/</a>
        
    </div>
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention-Model/">Attention Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Encoder-Decoder/">Encoder-Decoder</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器翻译/">机器翻译</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/注意力/">注意力</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/自然语言处理/">自然语言处理</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/10/10/attention-model/&title=《图文介绍注意力机制》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/10/10/attention-model/&title=《图文介绍注意力机制》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/10/10/attention-model/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《图文介绍注意力机制》 — PLM's Notes&url=http://plmsmile.github.io/2017/10/10/attention-model/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/10/10/attention-model/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/10/12/Attention-based-NMT/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">注意力机制和PyTorch实现机器翻译</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/10/05/pytorch-start/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">PyTorch快速上手</h4>
      </a>
    </div>
  
</nav>



    














</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <!-- <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div> -->
    <div class="bottom">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
          <span>

          <!-- PLM  -->
          PLM's Notes &nbsp;
          &copy;
          &nbsp;
          2016 - 2018

          </span>
            <!-- <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span> -->
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/10/10/attention-model/&title=《图文介绍注意力机制》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/10/10/attention-model/&title=《图文介绍注意力机制》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/10/10/attention-model/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《图文介绍注意力机制》 — PLM's Notes&url=http://plmsmile.github.io/2017/10/10/attention-model/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/10/10/attention-model/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACL0lEQVR42u3aQY6EMAxE0b7/pZltSzOEKlcYKfbPqoWA5LFwx44/H3lcX+P7yu977u7X37l+KhowYMA4lnEthzLB3VP6h1DmfZgFBgwYAxh3E69/r98jTb9pLhgwYMDQQ+06TCckGDBgwKgFwV2BWE+VYcCAMZmhhE4lNc2vv56Lw4AB40CGXnX//9+vnG/AgAHjKMZlDrfQryzRDbJ/rAoGDBitGXqAS3hKEFeuP8wCAwaMpoxam4Xe7FVrC9OT3qhbBAYMGIcw3PJZctCYHHau3wYDBozejNpy9SOEXSmrtDWEAQNGa4abiNaaLWrNYdJhJwwYMMYwkjYL91l9iym9EwYMGK0ZSktW7dhSD6C1dFeqHcKAAaMRI296yMNlXoyDAQNGb0atiOYeK7plNTe5hQEDxhyGEv7yJbr/BkoyDAMGjK6MWguFW0rTF/pKswUMGDAOZ+QNFruaJ5QK/8OnhAEDRlNGrRzvnh4qbRZJYQ4GDBi9GUk7l7uhTLaDRucIDBgwWjPc0piyuGhZbiINAwaMAQy3kUtJRJXPlGxPYcCAMY3hNkDoJX43TXU/EAwYMHozLnMkm0J3u2mwYcCA0Zrhhrldhwe1fwYl3MOAAaMrw12inV6amGLAhQEDxgCGkfVuaqSobfse9rkwYMCAIYRXd3Fuc8a2gAsDBowBDL1pzN1Q1hrIYMCAMYGht3PVrrjFOPsAFQYMGK0ZUepY2hrqsM3tGjBgwDiP8QM6CNE/jsMDUgAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.0"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.0" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
