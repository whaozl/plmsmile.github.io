<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>cs224n-assignment-1 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="word2vec,cbow,skip-gram,cs224n,assignment">
    <meta name="description" content="Softmax Softmax常数不变性 \[ \rm{softmax}(\mathbf{x})_i = \frac{e^{\mathbf x_i}}{\sum_{j}e^{\mathbf{x}_j}} \] 一般在计算softmax的时候，避免太大的数，要加一个常数。 一般是减去最大的数。 \[ \rm{">
<meta name="keywords" content="word2vec,cbow,skip-gram,cs224n,assignment">
<meta property="og:type" content="article">
<meta property="og:title" content="cs224n-assignment-1">
<meta property="og:url" content="http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="Softmax Softmax常数不变性 \[ \rm{softmax}(\mathbf{x})_i = \frac{e^{\mathbf x_i}}{\sum_{j}e^{\mathbf{x}_j}} \] 一般在计算softmax的时候，避免太大的数，要加一个常数。 一般是减去最大的数。 \[ \rm{softmax}(x) = \rm{softmax}(x+c) \] 关">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/note3/three_layer.png">
<meta property="og:updated_time" content="2017-12-21T14:13:14.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="cs224n-assignment-1">
<meta name="twitter:description" content="Softmax Softmax常数不变性 \[ \rm{softmax}(\mathbf{x})_i = \frac{e^{\mathbf x_i}}{\sum_{j}e^{\mathbf{x}_j}} \] 一般在计算softmax的时候，避免太大的数，要加一个常数。 一般是减去最大的数。 \[ \rm{softmax}(x) = \rm{softmax}(x+c) \] 关">
<meta name="twitter:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/note3/three_layer.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.0">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="https://plmsmile.github.io/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">cs224n-assignment-1</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">cs224n-assignment-1</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-12-17T04:47:06.000Z" itemprop="datePublished" class="page-time">
  2017-12-17
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/cs224n/">cs224n</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#softmax"><span class="post-toc-number">1.</span> <span class="post-toc-text">Softmax</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#softmax常数不变性"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">Softmax常数不变性</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#关键代码"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">关键代码</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#神经网络基础"><span class="post-toc-number">2.</span> <span class="post-toc-text">神经网络基础</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#sigmoid实现"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">Sigmoid实现</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#softmax求梯度"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">Softmax求梯度</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#交叉熵求梯度"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">交叉熵求梯度</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#简单网络"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">简单网络</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#梯度检查"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">梯度检查</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#word2vec"><span class="post-toc-number">3.</span> <span class="post-toc-text">Word2Vec</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#词向量的梯度"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">词向量的梯度</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-cs224n-assignment-1"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">cs224n-assignment-1</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-12-17 12:47:06" datetime="2017-12-17T04:47:06.000Z"  itemprop="datePublished">2017-12-17</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/cs224n/">cs224n</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p><img src="" style="display:block; margin:auto" width="60%"></p>
<h1 id="softmax">Softmax</h1>
<h2 id="softmax常数不变性">Softmax常数不变性</h2>
<p><span class="math display">\[
\rm{softmax}(\mathbf{x})_i = \frac{e^{\mathbf x_i}}{\sum_{j}e^{\mathbf{x}_j}}
\]</span></p>
<p>一般在计算<code>softmax</code>的时候，<strong>避免太大的数，要加一个常数</strong>。 一般是减去最大的数。 <span class="math display">\[
\rm{softmax}(x) = \rm{softmax}(x+c)
\]</span></p>
<h2 id="关键代码">关键代码</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    exp_func = <span class="keyword">lambda</span> x: np.exp(x - np.max(x))</span><br><span class="line">    sum_func = <span class="keyword">lambda</span> x: <span class="number">1.0</span> / np.sum(x)</span><br><span class="line">    x = np.apply_along_axis(exp_func, <span class="number">-1</span>, x)</span><br><span class="line">    denom = np.apply_along_axis(sum_func, <span class="number">-1</span>, x)</span><br><span class="line">    denom = denom[..., np.newaxis]</span><br><span class="line">    x = x * denom</span><br><span class="line">   	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h1 id="神经网络基础">神经网络基础</h1>
<h2 id="sigmoid实现">Sigmoid实现</h2>
<p><a href="https://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/#sigmoid">我的sigmoid笔记</a> <span class="math display">\[
\begin{align}
&amp; \sigma (z) = \frac {1} {1 + \exp(-z)}, \; \sigma(z) \in (0,1) \\ \\
&amp; \sigma^\prime (z) = \sigma(z) (1 - \sigma(z)) \\
\end{align}
\]</span> <strong>关键代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    s = <span class="number">1.0</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_grad</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">""" 对sigmoid的函数值，求梯度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ds = s * (<span class="number">1</span> - s)</span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure>
<h2 id="softmax求梯度">Softmax求梯度</h2>
<p>交叉熵和softmax如下，记softmax的输入为<span class="math inline">\(\theta\)</span> ，<span class="math inline">\(y\)</span>是真实one-hot向量。 <span class="math display">\[
\begin{align}
&amp; \rm{CE}(y, \hat y) = - \sum_{i} y_i \times \log (\hat y_i) \\ \\
&amp; \hat y = \rm{softmax} (\theta)\\
\end{align}
\]</span> <strong>softmax求导</strong></p>
<p>引入记号： <span class="math display">\[
\begin{align}
&amp; f_i = e^{\theta_i} &amp; \text{分子} \\
&amp; g_i = \sum_{k=1}^{K}e^{\theta_k} &amp; \text{分母，与i无关} \\
&amp; \hat y_i = S_i = \frac{f_i}{g_i} &amp; \text{softmax}\\
\end{align}
\]</span> 则有<span class="math inline">\(S_i​\)</span>对其中的一个数据<span class="math inline">\(\theta_j​\)</span> 求梯度： <span class="math display">\[
\frac{\partial S_i}{\partial \theta_j} = \frac{f_i^{\prime} g_i - f_i g_i^{\prime}}{g_i^2}
\]</span> 其中两个导数 <span class="math display">\[
f^{\prime}_i(\theta_j) =
\begin{cases}
&amp; e^{\theta_j}, &amp; i = j\\
&amp; 0, &amp; i \ne j \\
\end{cases}
\]</span></p>
<p><span class="math display">\[
g^{\prime}_i(\theta_j) = e^{\theta_j}
\]</span></p>
<p><strong><span class="math inline">\(i=j\)</span>时</strong> <span class="math display">\[
\begin{align}
\frac{\partial S_i}{\partial \theta_j}
&amp; = \frac{e^{\theta_j} \cdot \sum_{k}e^{\theta_k}-  e^{\theta_i} \cdot e^{\theta_j}}{\left( \sum_ke^{\theta_k}\right)^2} \\ \\
&amp; = \frac{e^{\theta_j}}{\sum_ke^{\theta_k}} \cdot \left( 1 - \frac{e^{\theta_j}}{\sum_k e^{\theta_k}} \right)  \\ \\
&amp; = S_i \cdot (1 - S_i)
\end{align}
\]</span> <strong><span class="math inline">\(i \ne j\)</span>时</strong> <span class="math display">\[
\begin{align}
\frac{\partial S_i}{\partial \theta_j}
&amp; = \frac{ -  e^{\theta_i} \cdot e^{\theta_j}}{\left( \sum_ke^{\theta_k}\right)^2}  = - S_i \cdot  S_j
\end{align}
\]</span></p>
<h2 id="交叉熵求梯度">交叉熵求梯度</h2>
<p><span class="math display">\[
\begin{align}
&amp; \rm{CE}(y, \hat y) = - \sum_{i} y_i \times \log (\hat y_i) \\ \\
&amp; \hat y = \rm{S} (\theta)\\
\end{align}
\]</span></p>
<p>只关注有关系的部分，带入<span class="math inline">\(y_i =1\)</span> ： <span class="math display">\[
\begin{align}
\frac{\partial CE}{\partial \theta_i}
&amp; =  -\frac{\partial \log \hat y_i}{\partial \theta_i} 
 = - \frac{1}{\hat y_i} \cdot  \frac{\partial \hat y_i}{\partial \theta_i} \\ \\
&amp; = - \frac{1}{S_i} \cdot \frac{\partial S_i}{\partial \theta_i} 
 = S_i - 1  \\ \\
 &amp; = \hat y_i - y_i
\end{align}
\]</span> 不带入求导 <span class="math display">\[
\begin{align}
\frac{\partial CE}{\partial \theta_i}
&amp; = - \sum_{k}y_k \times \frac{\partial \log S_k}{\partial \theta_i} \\
&amp; = - \sum_{k}y_k \times \frac{1}{S_k}\times \frac{\partial S_k}{\partial \theta_i}   \\ 
&amp; = - y_i (1 - S_i) - \sum_{k \ne i} y_k \cdot \frac{1}{S_k} \cdot (- S_i \cdot S_k) \\ 
&amp; = - y_i (1 - S_i) + \sum_{k \ne i} y_k \cdot S_i \\
&amp; =  S_i - y_i 
\end{align}
\]</span> 所以，交叉熵的导数是 <span class="math display">\[
\frac{\partial CE}{\partial \theta_i} = \hat y_i  - y_i, \quad \quad \frac{\partial CE(y, \hat y)}{\partial \theta} = \hat y  - y
\]</span></p>
<p>即 <span class="math display">\[
\frac{\partial CE(y, \hat y)}{\partial \theta_i} = 
\begin{cases}
&amp; \hat y_i - 1, &amp; \text{i是label} \\
&amp;\hat y_i, &amp; \text{其它}\\
\end{cases}
\]</span></p>
<h2 id="简单网络">简单网络</h2>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/note3/three_layer.png" style="display:block; margin:auto" width="40%"></p>
<p><strong>前向计算</strong> <span class="math display">\[
\begin{align}
&amp; z_1 = xW_1 + b_1 \\ \\
&amp; h = \rm{sigmoid}(z1) \\ \\
&amp; z_2 = hW_2 + b_2 \\ \\
&amp; \hat y = \rm{softmax}(z_2) 
\end{align}
\]</span> 关键代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></span><br><span class="line">    h = sigmoid(np.dot(data, W1) + b1)</span><br><span class="line">    yhat = softmax(np.dot(h, W2) + b2)</span><br></pre></td></tr></table></figure>
<p><strong>loss函数</strong> <span class="math display">\[
J = \rm{CE}(y, \hat y)
\]</span> 关键代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></span><br><span class="line">    <span class="comment"># yhat[labels==1]实际上是boolean索引，见我的numpy_api.ipynb</span></span><br><span class="line">    cost = np.sum(-np.log(yhat[labels == <span class="number">1</span>])) / data.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><strong>反向传播</strong> <span class="math display">\[
\begin {align}
&amp; \delta_2 = \frac{\partial J}{\partial z_2} = \hat y - y \\ \\
&amp; \frac{\partial J}{\partial h} = \delta_2 \cdot \frac{\partial z_2}{\partial h} = \delta_2  W_2^T \\ \\
&amp; \delta_1 = \frac{\partial J}{\partial z_1} 
= \frac{\partial J}{\partial h} \cdot \frac{\partial h}{\partial z_1} = \delta_2  W_2^T \circ \sigma^{\prime}(z_1) \\ \\
&amp;  \frac{\partial J}{\partial x} = \delta_1 W_1^T
\end{align}
\]</span> 一共有<span class="math inline">\((d_x + 1) \cdot d_h + (d_h +1) \cdot d_y\)</span> 个参数。</p>
<p>关键代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_backward_prop</span><span class="params">(data, labels, params, dimensions)</span>:</span></span><br><span class="line">    <span class="comment"># 前面推导的softmax梯度公式</span></span><br><span class="line">    gradyhat = (yhat - labels) / data.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 链式法则</span></span><br><span class="line">    gradW2 = np.dot(h.T, gradyhat)</span><br><span class="line">    <span class="comment"># 本地导数是1，把第1维的所有加起来</span></span><br><span class="line">    gradb2 = np.sum(gradyhat, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    gradh = np.dot(gradyhat, W2.T)</span><br><span class="line">    gradz1 = gradh * sigmoid_grad(h)</span><br><span class="line">    gradW1 = np.dot(data.T, gradz1)</span><br><span class="line">    gradb1 = np.sum(gradz1, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),</span><br><span class="line">            gradW2.flatten(), gradb2.flatten()))</span><br><span class="line">    <span class="keyword">return</span> cost, grad</span><br></pre></td></tr></table></figure>
<h2 id="梯度检查">梯度检查</h2>
<p><a href="https://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/#梯度检查">我的梯度检查</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradcheck_naive</span><span class="params">(f, x)</span>:</span></span><br><span class="line">    fx, grad = f(x) <span class="comment"># Evaluate function value at original point</span></span><br><span class="line">    h = <span class="number">1e-4</span>        <span class="comment"># Do not change this!</span></span><br><span class="line">    <span class="comment"># Iterate over all indexes in x</span></span><br><span class="line">    it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line">        ix = it.multi_index</span><br><span class="line">        <span class="comment"># 关键代码</span></span><br><span class="line">        x[ix] += h</span><br><span class="line">        random.setstate(rndstate)</span><br><span class="line">        new_f1 = f(x)[<span class="number">0</span>]</span><br><span class="line">        x[ix] -= <span class="number">2</span> * h</span><br><span class="line">        random.setstate(rndstate)</span><br><span class="line">        new_f2 = f(x)[<span class="number">0</span>]</span><br><span class="line">        x[ix] += h</span><br><span class="line">        numgrad = (new_f1 - new_f2) / (<span class="number">2</span> * h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compare gradients</span></span><br><span class="line">        reldiff = abs(numgrad - grad[ix]) / max(<span class="number">1</span>, abs(numgrad), abs(grad[ix]))</span><br><span class="line">        <span class="keyword">if</span> reldiff &gt; <span class="number">1e-5</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Gradient check failed."</span>)</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"First gradient error found at index %s"</span> % str(ix))</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Your gradient: %f \t Numerical gradient: %f"</span> % (</span><br><span class="line">                grad[ix], numgrad))</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        it.iternext() <span class="comment"># Step to next dimension</span></span><br></pre></td></tr></table></figure>
<h1 id="word2vec">Word2Vec</h1>
<p><a href="https://plmsmile.github.io/2017/11/12/cs224n-notes1-word2vec/#word2vec">我的word2vec笔记</a></p>
<h2 id="词向量的梯度">词向量的梯度</h2>
<p>符号定义</p>
<ul>
<li><span class="math inline">\(v_c\)</span> 中心词向量，输入词向量，<span class="math inline">\(V\)</span>， <span class="math inline">\(\mathbb{R}^{W\times d}\)</span><br>
</li>
<li><span class="math inline">\(u_o\)</span> 上下文词向量，输出词向量，<span class="math inline">\(U=[u_1, u_2, \cdots, u_w]\)</span> , <span class="math inline">\(\mathbb{R}^{d\times W}\)</span></li>
</ul>
<p><strong>前向</strong></p>
<p>预测o是c的上下文概率，o为正确单词 <span class="math display">\[
\hat y_o = p(o \mid c) = \rm{softmax}(o) = \frac{\exp(u_o^T v_c)}{\sum_{w} \exp(u_w^T v_c)}
\]</span> 得分向量： <span class="math display">\[
z=U^T \cdot v_c, \quad  [W,d] \times[ d] \in ,\mathbb{R}^{W }
\]</span> <strong>loss及梯度</strong> <span class="math display">\[
J_{\rm{softmax-CE}}(v_c, o, U) = CE(y, \hat y), \quad \text{其中} \; \frac{\partial CE(y, \hat y)}{\partial \theta} = \hat y  - y
\]</span></p>
<table>
<colgroup>
<col width="35%">
<col width="8%">
<col width="42%">
<col width="13%">
</colgroup>
<thead>
<tr class="header">
<th align="center">梯度</th>
<th align="center">中文</th>
<th align="center">计算</th>
<th align="center">维数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\frac{\partial J}{\partial z}\)</span></td>
<td align="center">softmax</td>
<td align="center"><span class="math inline">\(\hat y - y\)</span></td>
<td align="center"><span class="math inline">\(W\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\frac{\partial J}{\partial v_c}\)</span></td>
<td align="center">中心词</td>
<td align="center"><span class="math inline">\(\frac{\partial J}{\partial z} \cdot \frac{\partial z}{\partial v_c} = (\hat y - y) \cdot U^T\)</span></td>
<td align="center"><span class="math inline">\(d\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\frac{\partial J}{\partial U}\)</span></td>
<td align="center">上下文</td>
<td align="center"><span class="math inline">\(\frac{\partial J}{\partial z} \cdot \frac{\partial z}{\partial U^T}= (\hat y - y) \cdot v_c\)</span></td>
<td align="center"><span class="math inline">\(d \times W\)</span></td>
</tr>
</tbody>
</table>
<p><strong>关键代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmaxCostAndGradient</span><span class="params">(predicted, target, outputVectors, dataset)</span>:</span></span><br><span class="line">    <span class="string">""" Softmax cost function for word2vec models</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        predicted: 中心词vc</span></span><br><span class="line"><span class="string">        target: 上下文uo, index</span></span><br><span class="line"><span class="string">        outputVectors: 输出，上下文矩阵U，W*d，未转置</span></span><br><span class="line"><span class="string">        dataset: </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cost: 交叉熵loss</span></span><br><span class="line"><span class="string">        gradv: 一维向量</span></span><br><span class="line"><span class="string">        gradU: W*d</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    vhat = predicted</span><br><span class="line">    z = np.dot(outputVectors,vhat)</span><br><span class="line">    preds = softmax(z)</span><br><span class="line">    <span class="comment">#  Calculate the cost:</span></span><br><span class="line">    cost = -np.log(preds[target])</span><br><span class="line">    <span class="comment">#  Gradients</span></span><br><span class="line">    gradz = preds.copy()</span><br><span class="line">    gradz[target] -= <span class="number">1.0</span></span><br><span class="line">    gradU = np.outer(z, vhat)</span><br><span class="line">    gradv = np.dot(outputVectors.T, z)</span><br><span class="line">    <span class="comment">### END YOUR CODE</span></span><br><span class="line">    <span class="keyword">return</span> cost, gradv, gradU</span><br></pre></td></tr></table></figure>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2017-12-21T14:13:14.000Z" itemprop="dateUpdated">2017-12-21 22:13:14</time>
</span><br>


        
        <br>原始链接：<a href="/2017/12/17/cs224n-assignment-1/" target="_blank" rel="external">http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/</a>
        
    </div>
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/assignment/">assignment</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cbow/">cbow</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cs224n/">cs224n</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/skip-gram/">skip-gram</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/word2vec/">word2vec</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/&title=《cs224n-assignment-1》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/&title=《cs224n-assignment-1》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《cs224n-assignment-1》 — PLM's Notes&url=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/12/25/cpp-pointer-object-reference/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">cpp-pointer-object-reference</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/12/03/nlp-labels/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">nlp-labels</h4>
      </a>
    </div>
  
</nav>



    














</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>PLM &copy; 2016 - 2018</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/&title=《cs224n-assignment-1》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/&title=《cs224n-assignment-1》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《cs224n-assignment-1》 — PLM's Notes&url=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/12/17/cs224n-assignment-1/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACNUlEQVR42u3aMW7DMAwF0Nz/0i7QqUPsfJJOAclPU+Gkjp4GgiL5esXr+F1nT44/6+xJ8ub8v5oLAwNjWcZxua4ZyffPNnq26eT7b6gYGBgPYFRDYYK8DqO94zt9joGBgXG5iZz6IWgGv4uBgYGRp2jJLXJyJcbAwMDopWjVBDHZei9MY2BgPI1RbQz8599f7G9gYGAswjiKa9KAzN9c3hUGBsbWjORKORmwqF5Hq+8s3IwxMDAWZ8znrHoNgF77oYzBwMBYnFEd58q3kp9i9aRPk0IMDIxNGXnI69W4ekE5b3BiYGBg5I2BaltxMrQRldswMDA2ZeQjDpPi/qRU17zKYmBgLM7oFfGraV+1nFcuw2FgYGzNqJbSqr2FagNyEqAxMDB2ZUwA+fWyt/X88oyBgfEERn4dnRTuewNehaQQAwNjU8b8Z/LUcLLyAhwGBsZ+jDzkVQthvfSxGvpf1ciNgYGxIKNQkxsPk1XHNZIGQ6ETi4GBsThjnt7l7+ldj29IWzEwMJZlTMYdeknkPIh/uH9jYGBsypg8yduTSYJ4/Sunh46BgbE1o7r1PDW89zhuDrsYGBhLMXqtxPyE8lCbF9revAcDA2NrRrXo1mtDJhuqthAwMDCexqiOZM2blHn4Lpw9BgbGAxjVIJu3E6rNzmoTFAMDA+Ouql5vtKIwsoaBgYERkJJWQW+cIgriGBgYD2A0w1zw6TeGMAozbhgYGFswJo2B/LLau+j2jhUDA2Mjxg+K//oIPtuEqAAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.0"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.0" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
