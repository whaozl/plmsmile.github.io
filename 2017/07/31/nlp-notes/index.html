<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>语言模型和平滑方法 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="自然语言处理,各种熵,语言模型,数据平滑">
    <meta name="description" content="语言模型 二元语法$ $ 对于一个句子\(s=w_1 \cdots w_n\)，近似认为一个词的概率只依赖于它前面的1个词。即一个状态只跟上一个状态有关，也称为一阶马尔科夫链。 \[ \color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \cdots p(w_n|w_{l-">
<meta name="keywords" content="自然语言处理,各种熵,语言模型,数据平滑">
<meta property="og:type" content="article">
<meta property="og:title" content="语言模型和平滑方法">
<meta property="og:url" content="http://plmsmile.github.io/2017/07/31/nlp-notes/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="语言模型 二元语法$ $ 对于一个句子\(s=w_1 \cdots w_n\)，近似认为一个词的概率只依赖于它前面的1个词。即一个状态只跟上一个状态有关，也称为一阶马尔科夫链。 \[ \color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \cdots p(w_n|w_{l-1})= \color {red} {\prod_{i=1}^l {p(w_i|">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2017-12-12T06:30:52.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="语言模型和平滑方法">
<meta name="twitter:description" content="语言模型 二元语法$ $ 对于一个句子\(s=w_1 \cdots w_n\)，近似认为一个词的概率只依赖于它前面的1个词。即一个状态只跟上一个状态有关，也称为一阶马尔科夫链。 \[ \color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \cdots p(w_n|w_{l-1})= \color {red} {\prod_{i=1}^l {p(w_i|">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.0">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="https://plmsmile.github.io/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">语言模型和平滑方法</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">语言模型和平滑方法</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-07-31T00:57:52.000Z" itemprop="datePublished" class="page-time">
  2017-07-31
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#语言模型"><span class="post-toc-number">1.</span> <span class="post-toc-text">语言模型</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#模型评估参数"><span class="post-toc-number">2.</span> <span class="post-toc-text">模型评估参数</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#信息量和信息熵"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">信息量和信息熵</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#联合熵和条件熵"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">联合熵和条件熵</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#相对熵和交叉熵"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">相对熵和交叉熵</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#困惑度"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">困惑度</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#数据平滑"><span class="post-toc-number">3.</span> <span class="post-toc-text">数据平滑</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#问题的提出"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">问题的提出</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#加法平滑"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">加法平滑</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#good-turing"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">Good-Turing</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#jelinek-mercer"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">Jelinek-Mercer</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-nlp-notes"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">语言模型和平滑方法</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-07-31 08:57:52" datetime="2017-07-31T00:57:52.000Z"  itemprop="datePublished">2017-07-31</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="语言模型">语言模型</h1>
<p><strong>二元语法</strong>$ $</p>
<p>对于一个句子<span class="math inline">\(s=w_1 \cdots w_n\)</span>，近似认为一个词的概率只依赖于它前面的<strong>1个词</strong>。即一个状态只跟上一个状态有关，也称为<code>一阶马尔科夫链</code>。</p>
<p><span class="math display">\[
\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \cdots p(w_n|w_{l-1})= \color {red} {\prod_{i=1}^l {p(w_i|w_{i-1})}}
\]</span> 设<span class="math inline">\(\color {blue} {c(w_{i-1}w_i)}\)</span> 表示二元语法<span class="math inline">\(\color {green} {w_{i-1}w_i}\)</span>在给定文本中的出现次数，则上一个词是<span class="math inline">\(w_{i-1}\)</span>下一个词是<span class="math inline">\(w_i\)</span>的概率<span class="math inline">\(\color {blue} {p(w_i \mid w_{i-1})}\)</span>是当前语法<span class="math inline">\(\color {green} {w_{i-1}w_i}\)</span>出现的次数比上所有形似<span class="math inline">\(\color {green} {w_{i-1}}w\)</span>的二元语法的出现次数 <span class="math display">\[
\color {blue} {p(w_i \mid w_{i-1})} =\color {red} {\frac {c(w_{i-1}w_i)} {\sum_{w} {c(w_{i-1}w)}}}，w是变量
\]</span> <span class="math inline">\(n\)</span><strong>元语法</strong></p>
<p>认为一个词出现的概率和它<strong>前面的n个词</strong>有关系。则对于句子<span class="math inline">\(s=w_1w_2 \cdots w_l\)</span>，其概率计算公式为如下：</p>
<p><span class="math display">\[
\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_1w_2) \cdots p(w_n|w_1w_2 \cdots w_{l-1})=\color {red} {\prod_{i=1}^n{p(w_i|w_1 \cdots w_{i-1})}}
\]</span> 上述公式需要大量的概率计算，太理想了。一般取<span class="math inline">\(n=2\)</span>或者<span class="math inline">\(n=3\)</span>。</p>
<p>对于<span class="math inline">\(n&gt;2\)</span>的<span class="math inline">\(n\)</span>元语法模型，条件概率要考虑前面<span class="math inline">\(n-1\)</span>个词的概率，设<span class="math inline">\(w_i^j\)</span>表示<span class="math inline">\(w_i\cdots w_j\)</span>，则有 <span class="math display">\[
\color{blue} {p(s)} = \prod_{i=1}^{l+1}p(w_i \mid w_{i-n+1}^{i})，\color {blue} {p(w_i \mid w_{i-n+1}^{i})}=\frac { \overbrace {c(w_{i-n+1}^i)}^{\color{red}{具体以w_i结尾的词串w[i-n+1, i]}}} { \underbrace{\sum_{w_i}{c(w_{i-n+1}^i)}}_{\color{red}{所有以w_i结尾的词串w[i-n+1, i]}}}
\]</span> <strong>实际例子</strong></p>
<p>假设语料库<span class="math inline">\(S\)</span>是由下面3个句子组成，所求的句子t在其后：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = [<span class="string">'brown read holy bible'</span>, <span class="string">'plm see a text book'</span>, <span class="string">'he read a book by david'</span>]</span><br><span class="line">t = <span class="string">'brown read a book'</span></span><br></pre></td></tr></table></figure>
<p>那么求句子<span class="math inline">\(t\)</span>出现的概率是</p>
<p><span class="math display">\[\color{blue}{p(t)}=p(\color{green}{brown\,read\, a\, book})=p(brown|BOS)p(read|brown)p(a|read)p(book|a)p(eos|book)\approx0.06\]</span></p>
<p><span class="math inline">\(n\)</span>元文法的一些应用如下</p>
<p><strong>语音识别歧义消除</strong></p>
<p>如给了一个拼音 <span class="math inline">\(\color{green}{ta\,shi \,yan \,jiu \,sheng\, wu\, de}\)</span>，得到了很多可能的汉字串：<font color="green">踏实研究生物的，他实验救生物的，他是研究生物的</font> ，那么求出<span class="math inline">\(arg_{str}maxP(str|pinyin)\)</span>，即返回最大概率的句子</p>
<p><strong>汉语分词问题</strong></p>
<p>给定汉字串<code>他是研究生物的</code>。可能的汉字串 <code>他 是 研究生 物 的</code>和<code>他 是 研究 生物 的</code>，这也是求最大句子的概率</p>
<p><strong>开发自然语言处理的统计方法的一般步骤</strong></p>
<ul>
<li>收集大量语料（基础工作，工作量最大，<strong>很重要</strong>）</li>
<li>对语料进行统计分析，得出知识（如n元文法，一堆概率）</li>
<li>针对场景建立算法，如计算概率可能也用很多复杂的算法或者直接标注</li>
<li>解释或者应用结果</li>
</ul>
<h1 id="模型评估参数">模型评估参数</h1>
<p><strong>基础</strong></p>
<ul>
<li>评价目标：语言模型计 算出的概率分布与“真实的”理想模型是否接近</li>
<li>难点：无法知道“真实的”理想模型的分布</li>
<li>常用指标：交叉熵，困惑度</li>
</ul>
<h2 id="信息量和信息熵">信息量和信息熵</h2>
<p><span class="math inline">\(X\)</span>是一个离散随机变量，取值空间为<span class="math inline">\(R\)</span>，其概率分布是<span class="math inline">\(p(x)=P(X=x), x \in R\)</span>。</p>
<p><strong>信息量</strong></p>
<p>概率是对事件确定性的度量，那么<strong>信息</strong>就是对<strong>不确定性</strong>的度量。<strong>信息量</strong> <span class="math inline">\(\color {blue} {I(X)}\)</span>代表特征的不确定性，定义如下 <span class="math display">\[
\color {blue} {I(X)}= \color {red} {-\log {p(x)}}
\]</span> <strong>信息熵</strong></p>
<p><strong>信息熵</strong><span class="math inline">\(\color{blue}{H(x)}\)</span>是特征<strong>不确定性的平均值</strong>，用表示，定义如下 <span class="math display">\[
\color{blue}{H(X)}=\sum_{x \in R}{p(x)log\frac 1 {p(x)}}=\color {red} {-\sum_{x \in R} {p(x) \log p(x)}}
\]</span> 一般是<span class="math inline">\(log_2{p(x)}\)</span>，单位是比特。若是<span class="math inline">\(\ln {p(x)}\)</span>，单位是奈特。</p>
<ul>
<li>信息熵的本质是信息量的期望</li>
<li><strong>信息熵是对不确定性的度量</strong></li>
<li>随机变量<span class="math inline">\(X\)</span>的熵越大，说明不确定性也大；若<span class="math inline">\(X\)</span>为定值，则熵为0</li>
<li><code>平均分布</code>是&quot;<strong>最不确定</strong>”的分布</li>
</ul>
<h2 id="联合熵和条件熵">联合熵和条件熵</h2>
<p><span class="math inline">\(X, Y\)</span>是一对离散型随机变量，并且<span class="math inline">\(\color{blue}{X,Y \sim p(x,y)}\)</span>。</p>
<p><strong>联合熵</strong></p>
<p>联合熵实际上描述的是一对随机变量平均所需要的信息量，定义如下。 <span class="math display">\[
\color{blue}{H(X, Y)} = \color{red} {- \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(x, y)}
\]</span> <strong>条件熵</strong></p>
<p>给定<span class="math inline">\(X\)</span>的情况下，<span class="math inline">\(Y\)</span>的条件熵为 <span class="math display">\[
\color{blue}{H(Y \mid X)} = \color{red}{ - \sum_{x \in X} \sum_{y \in Y} p(x, y) \log p(y \mid x)}
\]</span> 其中可以推导出：<span class="math inline">\(H(X, Y) = H(X) + H(Y \mid X)\)</span>。</p>
<h2 id="相对熵和交叉熵">相对熵和交叉熵</h2>
<p><strong>相对熵</strong></p>
<p>随机变量<span class="math inline">\(X\)</span>的状态空间<span class="math inline">\(\Omega {x}\)</span>上有两个概率分布<span class="math inline">\(p(x)\)</span>和<span class="math inline">\(q(x)\)</span>。一般p是<code>真实分布</code>，q是<code>预测分布</code>。</p>
<p><strong>相对熵</strong>也称为<code>KL距离</code>，用来衡量相同事件空间里<strong>两个概率分布的差异</strong>。</p>
<p><span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>的<strong>相对熵</strong><span class="math inline">\(\color{blue}{D(p\mid\mid q)}\)</span>用来度量<strong>它们之间的差异</strong>，如下 <span class="math display">\[
\color{blue}{D(p\mid\mid q)}
=\color{red}{\sum_{x\in X} {p(x)\log{\frac {p(x)}{q(x)}}}} 
= E_p(\log \frac{p(X)}{q(X)}) \; (期望)
\]</span> 特别地，若<span class="math inline">\(p==q\)</span>，则相对熵为0；若差别增加，相对熵的值也增加。简单理解“相对”如下： <span class="math display">\[
D(p \mid\mid q)=\sum_{x \in X}{p(x)(\log p(x) - \log q(x))}
= \underbrace{\left(-\sum_{x \in X}{ \color{red}{p(x)\log q(x)}}\right)}_{\color {red}{以q去近似p的熵=交叉熵}} - \underbrace{\left(-\sum_{x \in X} {\color{red}{p(x)\log p(x)}}\right)}_{\color{red} {p本身的熵}}
\]</span> <strong>交叉熵</strong></p>
<p>交叉熵用来衡量<strong>估计模型与真实概率分布之间的差异。</strong></p>
<p>随机变量<span class="math inline">\(X \sim p(x)\)</span>，<span class="math inline">\(q(x)\)</span>近似于<span class="math inline">\(p(x)\)</span>。 则随机变量<span class="math inline">\(X\)</span>和模型<span class="math inline">\(q\)</span>之间的<strong>交叉熵</strong><span class="math inline">\(\color {blue} {H(X, q)}\)</span>如下：<strong>以<span class="math inline">\(q\)</span>去近似<span class="math inline">\(p\)</span>的熵</strong> <span class="math display">\[
\color {blue} {H(p, q)} = H(X) + D(p \mid\mid q) = \color {red} {-\sum_{x \in X}{p(x)\log q(x)}}
\]</span></p>
<p><strong>实际应用</strong></p>
<p>交叉熵的实际应用，设<span class="math inline">\(y\)</span>是预测的概率分布，<span class="math inline">\(y^\prime\)</span>为真实的概率分布。则用<strong>交叉熵去判断估计的准确程度</strong> <span class="math display">\[
H(y^{\prime}, y)= - \sum_i y_i^{\prime}\log y_i = \color {red} {-\sum_i y_{真实} \log y_{预测}}
\]</span> <strong>n元文法模型的交叉熵</strong></p>
<p>设测试集<span class="math inline">\(T=(t_1, t_2, \ldots, t_l)\)</span>包含<span class="math inline">\(l\)</span>个句子，则定义测试集的概率<span class="math inline">\(\color {blue} {p(T)}\)</span>为多个句子概率的乘积 <span class="math display">\[
\color {blue} {p(T)} = \prod_{i=1}^{l} p(t_i)，  \, \text{其中}\color{blue}{p(t_i)}=\color{red}{\prod_{i=1}^{l_w} {p(w_i|w_{i-n+1}^{i-1})}}, \text{见上面}
\]</span> 其中<span class="math inline">\(w_i^j\)</span>表示词<span class="math inline">\(w_i\cdots w_j\)</span>，<span class="math inline">\(\sum_{w}{c(read \, w)}\)</span>是查找出所有以<span class="math inline">\(read\)</span>开头的二元组的出现次数。</p>
<p>则在数据<span class="math inline">\(T\)</span>上<code>n元模型</code><span class="math inline">\(\color {green} {p(w_i|w_{i-n+1}^{i-1})}\)</span>的交叉熵<span class="math inline">\(\color {blue} {H_p(T)}\)</span>定义如下 <span class="math display">\[
\color {blue} {H_p(T)} = \color {red} {-\frac {1} {W_T} \log _2 p(T)}，其中W_T是文本T中基元(词或字)的长度
\]</span> 公式的推导过程如下 <span class="math display">\[
-\sum_{x \in X}{p(x)\log q(x)} 
\implies \underbrace { -{\frac{1} {W_T}}\sum \log q(x)}_{\color{red}{使用均匀分布代替p(x)}}
\implies  -{\frac{1} {W_T}} \log {\prod r(w_i|w_{i-n+1}^{i-1})}
\implies  -{\frac{1} {W_T}} \log_2p(T)
\]</span> 可以这么理解：利用模型<span class="math inline">\(p\)</span>对<span class="math inline">\(W_T\)</span>个词进行编码，每一个编码所需要的平均比特位数。</p>
<h2 id="困惑度">困惑度</h2>
<p><strong>困惑度</strong>是评估语言的基本准则人，也是对测试集T中每一个词汇的概率的几何平均值的倒数。 <span class="math display">\[
\color{blue}{PP_T(T)} =\color{red}{ 2^{H_p(T)}= \frac {1} {\sqrt [W_T]{p(T)}}} = 2 ^{\text{交叉熵}}
\]</span></p>
<p>当然，<font color="red">交叉熵和困惑度越小越好。语言模型设计的任务就是要找出困惑度最小的模型。</font></p>
<p>在英语中，n元语法模型的困惑度是<span class="math inline">\(50 \sim 1000\)</span>，交叉熵是<span class="math inline">\(6 \sim 10\)</span>个比特位。</p>
<h1 id="数据平滑">数据平滑</h1>
<h2 id="问题的提出">问题的提出</h2>
<p>按照上面提出的语言模型，有的句子就没有概率，但是这是不合理的，因为总有出现的可能，概率应该大于0。设<span class="math inline">\(\color {blue}{c(w)}\)</span>是<span class="math inline">\(w\)</span>在语料库中的出现次数。 <span class="math display">\[
p(\color{green} {read \mid plm}) = \frac {c(plm \mid read)} {\sum_{w_i}{c(plm | w_i})} = \frac {\color{red}{0}} {1}=\color{red}{0， \, 这是不对的}
\]</span> 因此，必须分配<font color="red">给所有可能出现的字符串一个非0的概率值来避免这种错误的发送。</font></p>
<p><code>平滑技术</code>就是用来解决这种零概率问题的。平滑指的是为了产生更准确的概率来调整最大似然估计的一种技术，也称作<code>数据平滑</code>。思想是<code>劫富济贫</code>，即<strong>提高低概率、降低高概率</strong>，尽量是概率分布趋于均匀。</p>
<p>数据平滑是语言模型中的核心问题</p>
<h2 id="加法平滑">加法平滑</h2>
<p>其实为了避免0概率，最简单的就是给统计次数加1。这里我们可以为每个单词的出现次数加上<span class="math inline">\(\delta，\delta \in [0, 1]\)</span>，设<span class="math inline">\(V\)</span>是所有词汇的单词表，<span class="math inline">\(|V|\)</span>是单词表的词汇个数，则有概率： <span class="math display">\[
p_{add}(w_i \mid w_{i-n+1}^{i-1}) = \frac {\delta + c(w_{i-n+1}^i)} {\sum_{w_i}{(\delta*|V| + c(w_{i-n+1}^i)})}=\frac {\delta + \overbrace {c(w_{i-n+1}^i)}^{\color{red}{具体词串[i-n+1, i]}}} {\delta*|V| + \underbrace{\sum_{w_i}{c(w_{i-n+1}^i)}}_{\color{red}{所有以w_i结尾的词串[i-n+1, i]}}}
\]</span></p>
<p>注：这个方法很原始。</p>
<h2 id="good-turing">Good-Turing</h2>
<p><code>Good-Turing</code>也称作古德-图灵方法，这是很多平滑技术的核心。</p>
<p>主要思想是重新分配概率，会得到一个<code>剩余概率量</code><span class="math inline">\(\color {blue} {p_0}= \color {red} {\frac {n_1} N}\)</span>，设<span class="math inline">\(n_0\)</span>为未出现的单词的个数，然后<font color="red">由这<span class="math inline">\(n_0\)</span>个单词去平均分配得到<span class="math inline">\(p_0\)</span></font>，即每个未出现的单词的概率为<span class="math inline">\(\frac {p_0} {n_0}\)</span>。</p>
<p>对于一个<span class="math inline">\(n\)</span>元语法，设<span class="math inline">\(\color {blue} n_r\)</span>恰好出现<span class="math inline">\(r\)</span>次的<span class="math inline">\(n\)</span>元语法的数目，下面是一些新的定义</p>
<ul>
<li>出现次数为<span class="math inline">\(r\)</span>的<span class="math inline">\(n\)</span>元语法 新的出现次数<span class="math inline">\(\color {blue} {r^*} = \color {red} {(r+1)\frac{n_{r+1}}{n_r}}\)</span></li>
<li>设<span class="math inline">\(N = \sum_{r=0}^{\infty}n_r r^* = \sum_{r=1}^{\infty} n_r r\)</span>，即<span class="math inline">\(N\)</span>是这个分布中最初的所有文法出现的次数，例如所有以<span class="math inline">\(read\)</span>开始的总次数</li>
<li>出现次数为<span class="math inline">\(r\)</span>的修正概率 <span class="math inline">\(\color {blue}p_r = \color {red} {\frac {r^*} {N}}\)</span></li>
</ul>
<p>剩余概率量<span class="math inline">\(\color {blue} {p_0}= \color {red} {\frac {n_1} N}\)</span>的推导 <span class="math display">\[
总的概率 = \sum_{r&gt;0}{n_r p_r} = \sum_{r&gt;0}{n_r  (r+1)\frac{n_{r+1}}{n_r N}} = \frac {1}{N} (\sum_{r&gt;0} (r+1)n_{r+1} =  \frac {1}{N} (\sum_{r&gt;0} (r n_r - n_1) = 1 - \frac {n_1} N &lt; 1
\]</span> 然后把<span class="math inline">\(p_0\)</span>平均分配给所有未见事件(r=0的事件)。</p>
<p><strong>缺点</strong></p>
<ul>
<li>若出现次数最大为<span class="math inline">\(k\)</span>，则无法计算<span class="math inline">\(r=k\)</span>的新的次数<span class="math inline">\(r^*\)</span>和修正概率<span class="math inline">\(p_r\)</span></li>
<li>高低阶模型的结合通常能获得较好的平滑效果，但是Good-Turing不能高低阶模型结合</li>
</ul>
<h2 id="jelinek-mercer">Jelinek-Mercer</h2>
<p><strong>问题引入</strong></p>
<p>假如<span class="math inline">\(c(send \, the)=c(send \, thou)=0\)</span>，则通过GT方法有<span class="math inline">\(p(the \mid send)=p(thou \mid send)\)</span>，但是实际上却应该是<span class="math inline">\(p(the \mid send)&gt;p(thou \mid send)\)</span>。</p>
<p>所以我们需要在二元语法模型中加入一个一元模型 <span class="math display">\[
p_{ML}(w_i) = \frac {c(w_i)}{\sum_w{c(w)}}
\]</span> <strong>二元线性插值</strong></p>
<p>使用<span class="math inline">\(r\)</span>将二元文法模型和一元文法模型进行线性插值 <span class="math display">\[
p(w_i \mid w_{i-1}) = \lambda p_{ML}(w_i | w_{i-1}) + (1-\lambda)p_{ML}(w_i)，\lambda \in [0, 1]
\]</span> 所以可以得到<span class="math inline">\(p(the \mid send)&gt;p(thou \mid send)\)</span></p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2017-12-12T06:30:52.000Z" itemprop="dateUpdated">2017-12-12 14:30:52</time>
</span><br>


        
        <br>原始链接：<a href="/2017/07/31/nlp-notes/" target="_blank" rel="external">http://plmsmile.github.io/2017/07/31/nlp-notes/</a>
        
    </div>
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/各种熵/">各种熵</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据平滑/">数据平滑</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/自然语言处理/">自然语言处理</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/语言模型/">语言模型</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/07/31/nlp-notes/&title=《语言模型和平滑方法》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/07/31/nlp-notes/&title=《语言模型和平滑方法》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/07/31/nlp-notes/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《语言模型和平滑方法》 — PLM's Notes&url=http://plmsmile.github.io/2017/07/31/nlp-notes/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/07/31/nlp-notes/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/08/04/pgm-01/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">马尔可夫模型</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/07/29/aim2offer/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">剑指Offer算法题</h4>
      </a>
    </div>
  
</nav>



    














</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <!-- <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div> -->
    <div class="bottom">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
          <span>

          <!-- PLM  -->
          PLM's Notes &nbsp;
          &copy;
          &nbsp;
          2016 - 2018

          </span>
            <!-- <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span> -->
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/07/31/nlp-notes/&title=《语言模型和平滑方法》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/07/31/nlp-notes/&title=《语言模型和平滑方法》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/07/31/nlp-notes/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《语言模型和平滑方法》 — PLM's Notes&url=http://plmsmile.github.io/2017/07/31/nlp-notes/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/07/31/nlp-notes/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACJUlEQVR42u3aS26EMBAFQO5/6ck2UoJ5bZuRaIpVlBhwEanl/hxHfH2G1+81Z+vHT/5713HHhYGB8VhGvsWzF4w3fUZNnpDsDQMD4z2MJBQm+Oqa8buS9RgYGBjjNXm4zI+MGBgYGHczqsjxczAwMDDWS2krsPxouDkXx8DAeCAjr7p//+db+hsYGBiPYnyKV5KarifA5V1hYGC0ZuQBrlogy585F3AxMDDew8jTxWo6uo4Zf5TT/wMGBkZTRl6Or4bX6meaK8NhYGD0ZswNT1TX5AF0WwMAAwOjEeOOAlmeAM8V6f5Zg4GB8RpGvt08vVzfenRsxcDAeAHjopi1tTCX/LWaGGNgYPRm5CWtccDNt76CuXgOBgbGCxhzBfq8TL/eHL14FwYGRlNGEkbHYa7aNpgLx4VWJQYGRjvG3JDWenEt/3yTwxYYGBiNGHOl/7lxjfxp5WCNgYHRmrErBa2OU1R/EyExMDBaM/LhrWro3Eu6GLnAwMBozcjLZ/ldc4E4aQ9MDltgYGA0YuRHsb0DFtVhiyPveWJgYLRjVBPIvOm4oTGZnG0xMDDaMT7FKw/B1RLeSojHwMDozVgvlt3xhDypLsMwMDAey6iOZK1vMS/GlZNYDAyM1ow88M2VyZIPsS0Xx8DAwAgY47s2Ny8xMDAw4jx4/fhYbTNEwxYYGBiNGNVmQPLK6l358REDA+NtjGrquD5yMZfWbmhqYmBgPI/xA50mQc8gIkNEAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.0"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.0" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
