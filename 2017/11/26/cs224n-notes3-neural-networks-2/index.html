<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>神经网络-过拟合-预处理-BN | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="数据预处理,正则化,范数,Dropout,PCA,白化,BN">
    <meta name="description" content="过拟合 过拟合 训练数据很少，或者训练次数很多，会导致过拟合。避免过拟合的方法有如下几种：  early stop 数据集扩增 正则化 （L1， L2权重衰减） Dropout 决策树剪枝（尽管不属于神经网络）  现在一般用L2正则化+Dropout。 过拟合时，拟合系数一般都很大。过拟合需要顾及">
<meta name="keywords" content="数据预处理,正则化,范数,Dropout,PCA,白化,BN">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络-过拟合-预处理-BN">
<meta property="og:url" content="http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="过拟合 过拟合 训练数据很少，或者训练次数很多，会导致过拟合。避免过拟合的方法有如下几种：  early stop 数据集扩增 正则化 （L1， L2权重衰减） Dropout 决策树剪枝（尽管不属于神经网络）  现在一般用L2正则化+Dropout。 过拟合时，拟合系数一般都很大。过拟合需要顾及到所有的数据点，意味着拟合函数波动很大。  看到，在某些很小的区间内里，函数">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/overfit.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/compare-neurons-num.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/compare-lambda-regularize.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/dropout.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-process.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-pca-process.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB1.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB2.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB3.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB4.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB5.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB6.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB8.png">
<meta property="og:updated_time" content="2018-03-30T06:23:39.828Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络-过拟合-预处理-BN">
<meta name="twitter:description" content="过拟合 过拟合 训练数据很少，或者训练次数很多，会导致过拟合。避免过拟合的方法有如下几种：  early stop 数据集扩增 正则化 （L1， L2权重衰减） Dropout 决策树剪枝（尽管不属于神经网络）  现在一般用L2正则化+Dropout。 过拟合时，拟合系数一般都很大。过拟合需要顾及到所有的数据点，意味着拟合函数波动很大。  看到，在某些很小的区间内里，函数">
<meta name="twitter:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/overfit.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.0">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="https://plmsmile.github.io/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">神经网络-过拟合-预处理-BN</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">神经网络-过拟合-预处理-BN</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-11-26T08:21:23.000Z" itemprop="datePublished" class="page-time">
  2017-11-26
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#过拟合"><span class="post-toc-number">1.</span> <span class="post-toc-text">过拟合</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#过拟合-1"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">过拟合</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#范数"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">范数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#l2正则化权重衰减"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">L2正则化权重衰减</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#实例说明"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">实例说明</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#l1正则化"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">L1正则化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#随机失活dropout"><span class="post-toc-number">1.6.</span> <span class="post-toc-text">随机失活Dropout</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#数据预处理"><span class="post-toc-number">2.</span> <span class="post-toc-text">数据预处理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#中心化"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">中心化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#标准化"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">标准化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#pca"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">PCA</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#白化"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">白化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#权重初始化"><span class="post-toc-number">3.</span> <span class="post-toc-text">权重初始化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#小随机数初始化"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">小随机数初始化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#稀疏和偏置初始化"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">稀疏和偏置初始化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#batch-normalization"><span class="post-toc-number">4.</span> <span class="post-toc-text">Batch Normalization</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#训练速度分析"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">训练速度分析</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#敏感度问题"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">敏感度问题</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#bn算法"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">BN算法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#bn的优点"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">BN的优点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#效果图片展示"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">效果图片展示</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-cs224n-notes3-neural-networks-2"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">神经网络-过拟合-预处理-BN</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-11-26 16:21:23" datetime="2017-11-26T08:21:23.000Z"  itemprop="datePublished">2017-11-26</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="过拟合">过拟合</h1>
<h2 id="过拟合-1">过拟合</h2>
<p>训练数据很少，或者训练次数很多，会导致<code>过拟合</code>。避免过拟合的方法有如下几种：</p>
<ul>
<li>early stop</li>
<li>数据集扩增</li>
<li>正则化 （L1， <strong>L2权重衰减</strong>）</li>
<li>Dropout</li>
<li>决策树剪枝（尽管不属于神经网络）</li>
</ul>
<p>现在一般用L2正则化+Dropout。</p>
<p><strong>过拟合时</strong>，<strong>拟合系数一般都很大</strong>。过拟合需要顾及到所有的数据点，意味着<strong>拟合函数波动很大</strong>。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/overfit.png" style="display:block; margin:auto" width="50%"></p>
<p>看到，在某些很小的区间内里，函数值的<strong>变化很剧烈</strong>。意味着这些小区间的<strong>导数值（绝对值）非常大</strong>。由于自变量值可大可小，所以只有<strong>系数足够大</strong>，才能保证导数值足够大。</p>
<p>所以：<strong>过拟合时，参数一般都很大</strong>。<strong>参数较小时，意味着模型复杂度更低，对数据的拟合刚刚好</strong>， 这也是<code>奥卡姆剃刀</code>法则。</p>
<h2 id="范数">范数</h2>
<p><strong>向量范数</strong></p>
<p><span class="math inline">\(x \in \mathbb {R}^d\)</span></p>
<table>
<colgroup>
<col width="19%">
<col width="80%">
</colgroup>
<thead>
<tr class="header">
<th>范数</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1-范数</td>
<td><span class="math inline">\(\left \| x\right\|_1 = \sum_i^d \|x_i\|\)</span>， <strong>绝对值之和</strong></td>
</tr>
<tr class="even">
<td>2-范数</td>
<td><span class="math inline">\(\left \| x\right\|_2 = \left(\sum_i^d \vert x_i\vert^2\right)^{\frac{1}{2}}\)</span>， <strong>绝对值平方之和再开方</strong></td>
</tr>
<tr class="odd">
<td>p-范数</td>
<td><span class="math inline">\(\left \| x\right\|_p = \left(\sum_i^d \|x_i\|^p\right)^{\frac{1}{p}}\)</span>， <strong>绝对值的p次方之和的<span class="math inline">\(\frac{1}{p}​\)</span>次幂</strong></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\infty\)</span>-范数</td>
<td><span class="math inline">\(\left \| x\right\|_\infty = \max_\limits i \|x_i\|\)</span> ，绝对值的最大值</td>
</tr>
<tr class="odd">
<td>-<span class="math inline">\(\infty\)</span>-范数</td>
<td><span class="math inline">\(\left \| x\right\|_{-\infty} = \min_\limits i \|x_i\|\)</span> ，绝对值的最小值</td>
</tr>
</tbody>
</table>
<p><strong>矩阵范数</strong></p>
<p><span class="math inline">\(A \in \mathbb R^{m \times n}\)</span></p>
<table style="width:74%;">
<colgroup>
<col width="16%">
<col width="56%">
</colgroup>
<thead>
<tr class="header">
<th>范数</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1-范数</td>
<td><span class="math inline">\(\left \| A\right\|_1 = \max \limits_{j}\sum_i^m \|a_{ij}\|\)</span>，<strong>列和范数</strong>，矩阵列向量绝对值之和的最大值。</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\infty\)</span>-范数</td>
<td><span class="math inline">\(\left \| A\right\|_\infty = \max_\limits i \sum_{j}^{n}\|a_{ij}\|\)</span> ，<strong>行和范数</strong>，所有行向量绝对值之和的最大值。</td>
</tr>
<tr class="odd">
<td>2-范数</td>
<td><span class="math inline">\(\left \| A\right\|_2 = \sqrt{\lambda_{m}}\)</span> ， 其中<span class="math inline">\(\lambda_m\)</span>是<span class="math inline">\(A^TA\)</span>的<strong>最大特征值</strong>。</td>
</tr>
<tr class="even">
<td>F-范数</td>
<td><span class="math inline">\(\left \| A\right\|_F = \left(\sum_i^m \sum_j^n a_{ij}^2\right)^{\frac{1}{2}}\)</span>，<strong>所有元素的平方之和，再开方。或者不开方</strong>， L2正则化就直接平方，不开方。</td>
</tr>
</tbody>
</table>
<h2 id="l2正则化权重衰减">L2正则化权重衰减</h2>
<p><strong>为了避免过拟合</strong>，使用L2正则化参数。<span class="math inline">\(\lambda\)</span>是正则项系数，<strong>用来权衡正则项和默认损失的比重</strong>。<span class="math inline">\(\lambda\)</span> 的选取很重要。 <span class="math display">\[
J_R = J + \lambda \sum_{i=1}^L \left \| W^{(i)}\right \|_F
\]</span> L2惩罚更倾向于<strong>更小更分散</strong>的权重向量，鼓励使用所有维度的特征，而不是只依赖其中的几个，这也避免了过拟合。</p>
<p><strong>标准L2正则化</strong></p>
<p><span class="math inline">\(\lambda\)</span> 是<code>正则项系数</code>，<span class="math inline">\(n\)</span>是数据数量，<span class="math inline">\(w\)</span>是模型的参数。 <span class="math display">\[
C = C_0 + \frac {\lambda} {2n} \sum_w w^2
\]</span> <span class="math inline">\(C\)</span>对参数<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>的<code>偏导</code>： <span class="math display">\[
\begin {align}
&amp; \frac{\partial C}{\partial w}  =  \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} w  \\
&amp; \frac{\partial C}{\partial b}  =  \frac{\partial C_0}{\partial b} \\
\end{align}
\]</span> <strong>更新参数</strong> ：可以看出，正则化<span class="math inline">\(C\)</span>对<span class="math inline">\(w\)</span>有影响，对<span class="math inline">\(b\)</span>无影响。<br>
<span class="math display">\[
\begin{align}
w 
&amp;= w - \alpha \cdot  \frac{\partial C}{\partial w} \\
&amp;=  (1 - \frac{\alpha \lambda}{n})w - \alpha \frac{\partial C_0}{\partial w} \\
\end{align}
\]</span> 从上式可以看出：</p>
<ul>
<li>不使用正则化时，<span class="math inline">\(w\)</span>的系数是1</li>
<li>使用正则化时</li>
<li><span class="math inline">\(w\)</span>的系数是<span class="math inline">\(1 - \frac{\alpha \lambda}{n} &lt; 1\)</span> ，效果是减小<span class="math inline">\(w\)</span>， 所以是<strong>权重衰减</strong> <code>weight decay</code></li>
<li>当然，<span class="math inline">\(w\)</span>具体增大或减小，还取决于后面的导数项</li>
</ul>
<p><strong>mini-batch随机梯度下降</strong></p>
<p>设<span class="math inline">\(m\)</span> 是这个batch的样本个数，有更新参数如下，即求<strong>batch个C对w的平均偏导值</strong> <span class="math display">\[
\begin {align}
&amp; w =  (1 - \frac{\alpha \lambda}{n})w - \frac{\alpha}{m} \cdot \sum_{i=1}^{m}\frac{\partial C_i}{\partial w} \\
&amp; b = b - \frac{\alpha}{m} \cdot \sum_{i=1}^{m}\frac{\partial C_i}{\partial b} \\
\end{align}
\]</span> 所以，权重衰减后一般可以减小过拟合。 L2正则化比L1正则化<strong>更加发散</strong>，权值也会被限制的更小。 一般使用L2正则化。</p>
<p>还有一种方法是<code>最大范数限制</code>：给范数一个上界<span class="math inline">\(\left \| w \right \| &lt; c\)</span> ， 可以在学习率太高的时候网络不会爆炸，因为更新总是有界的。</p>
<h2 id="实例说明">实例说明</h2>
<p>增加网络的层的数量和尺寸时，网络的容量上升，多个神经元一起合作，可以表达各种复杂的函数。</p>
<p>如下图，2分类问题，有噪声数据。</p>
<p>一个隐藏层。神经元数量分别是3、6、20。很明显20<strong>过拟合</strong>了，拟合了所有的数据。<code>正则化</code>就是<strong>处理过拟合</strong>的非常好的办法。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/compare-neurons-num.jpg" style="display:block; margin:auto" width="60%"></p>
<p>对20个神经元的网络，使用正则化，解决过拟合问题。正则化强度<span class="math inline">\(\lambda\)</span>很重要。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/compare-lambda-regularize.jpg" style="display:block; margin:auto" width="60%"></p>
<h2 id="l1正则化">L1正则化</h2>
<p>正则化loss如下： <span class="math display">\[
C = C_0 + \frac {\lambda} {n} \sum_w |w|
\]</span> 对<span class="math inline">\(w\)</span>的<strong>偏导</strong>， 其中<span class="math inline">\(\rm{sgn}(w)\)</span>是<code>符号函数</code>： <span class="math display">\[
\frac{\partial C}{\partial w}  =  \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} \cdot \rm{sgn}(w)
\]</span> 更新参数： <span class="math display">\[
w 
=  w  - \frac{\alpha \lambda}{n} \cdot \rm{sgn}(w) - \alpha \frac{\partial C_0}{\partial w} 
\]</span> 分析：<span class="math inline">\(w\)</span>为正，减小；<span class="math inline">\(w\)</span>为负，增大。所以<strong>L1正则化就是使参数向0靠近</strong>，是权重尽可能为0，减小网络复杂度，防止过拟合。</p>
<p>特别地：当<span class="math inline">\(w=0\)</span>时，不可导，就不要正则化项了。L1正则化更加稀疏。</p>
<h2 id="随机失活dropout">随机失活Dropout</h2>
<p>Dropout是<strong>非常有用</strong>的<strong>正则化</strong>的办法，它<strong>改变了网络结构</strong>。一般采用<strong>L2正则化+Dropout来防止过拟合</strong>。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/dropout.png" style="display:block; margin:auto" width="50%"></p>
<p>训练的时候，输出不变，<strong>随机以概率<span class="math inline">\(p\)</span>保留神经元，<span class="math inline">\(1-p\)</span>删除神经元</strong>（<strong>置位0</strong>）。每次迭代删除的神经元都不一样。</p>
<p>BP的时候，<strong>置位0的神经元的参数就不再更新</strong>， <strong>只更新前向时alive的神经元</strong>。</p>
<p>预测的时候，要保留所有的神经元，即不使用Dropout。</p>
<p>相当于训练了很多个（<strong>指数级数量</strong>）小网络（<code>半数网络</code>），在预测的时候综合它们的结果。随着训练的进行，大部分的半数网络都可以给出正确的分类结果。</p>
<h1 id="数据预处理">数据预处理</h1>
<p>用的很多的是0中心化。CNN中很少用PCA和白化。</p>
<p>应该：线划分训练、验证、测试集，<strong>只是从训练集中求平均值</strong>！<strong>然后各个集再减去这个平均值</strong>。</p>
<h2 id="中心化">中心化</h2>
<p>也称作<code>均值减法</code>， 把数据所有维度变成<code>0均值</code>，其实就是减去均值。就是将<strong>数据迁移到原点</strong>。 <span class="math display">\[
x = x - \rm{avg}(x) = x - \bar x
\]</span></p>
<h2 id="标准化">标准化</h2>
<p>也称作<code>归一化</code>， 数据所有维度都归一化，使其数值<strong>变化范围都近似相等</strong>。</p>
<ul>
<li><strong>除以标准差</strong></li>
<li>最大值和最小值按照比例缩放到<span class="math inline">\((-1 ,1)\)</span> 之间</li>
</ul>
<p><code>方差</code><span class="math inline">\(s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2\)</span> ，<code>标准差</code>就是<span class="math inline">\(s\)</span> 。数据<strong>除以标准差</strong>，接近<code>标准高斯分布</code>。 <span class="math display">\[
x = \frac{x}{s}
\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-process.jpg" style="display:block; margin:auto" width="70%"></p>
<h2 id="pca">PCA</h2>
<p><a href="http://ufldl.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90" target="_blank" rel="noopener">斯坦福PCA</a> ，<a href="http://blog.csdn.net/wangjian1204/article/details/50642732" target="_blank" rel="noopener">CSDNPCA和SVD的区别和联系</a></p>
<p><strong>协方差</strong></p>
<p><code>协方差</code>就是<strong>乘积的期望-期望的乘积</strong>。 <span class="math display">\[
\rm{Cov}(X, Y) = E(XY) - E(X)E(Y)
\]</span> 协方差的性质如下： <span class="math display">\[
\begin{align}
&amp; \rm{Cov}(X, Y) = \rm{Conv}(Y, X) \\ \\
&amp; \rm{Cov}(aX, bY) = ab \cdot \rm{Conv}(Y, X) \\ \\
&amp; \rm{Cov}(X, X) = E(X^2) - E^2(X) = D(X) , \quad \text{三方公式}\\ \\
&amp; \rm{Cov}(X, C)  = 0 \\ \\ 
&amp; \rm{Cov}(X, Y) = 0 \leftrightarrow X与Y独立
\end{align}
\]</span> 还有别的性质就看考研笔记吧。</p>
<p><strong>奇异值分解</strong> <span class="math display">\[
A_{m \times n} = U_{m \times m} \Sigma_{m \times n}  V^T_{n \times n}
\]</span> <span class="math inline">\(V_{n \times n}\)</span> ：<span class="math inline">\(V\)</span>的列，一组对A正交输入或分析的基向量（线性无关）。这些向量是<span class="math inline">\(M^TM\)</span> 的特征向量。</p>
<p><span class="math inline">\(U_{m \times m}\)</span> ：<span class="math inline">\(U\)</span>的列，一组对A正交输出的<code>基向量</code> 。是<span class="math inline">\(MM^T\)</span>的特征向量。</p>
<p><span class="math inline">\(\Sigma_{m \times n}\)</span>：<code>对角矩阵</code>。对角元素按照从小到大排列，这些<strong>对角元素</strong>称为<code>奇异值</code>。 是<span class="math inline">\(M^TM, MM^T\)</span> 的特征值的非负平方根，并且与U和V的行向量对应。</p>
<p>记<span class="math inline">\(r\)</span>是<strong>非0奇异值的个数</strong>，则A中仅有<strong><span class="math inline">\(r\)</span>个重要特征</strong>，其余特征都是噪声和冗余特征。</p>
<p><a href="https://www.zhihu.com/question/22237507" target="_blank" rel="noopener">奇异值的物理意义</a></p>
<p><strong>利用SVD进行PCA</strong></p>
<p><strong>先将数据中心化</strong>。输入是<span class="math inline">\(X \in \mathbb R^ {N \times D}\)</span> ，则<code>协方差矩阵</code> 如下： <span class="math display">\[
\mathrm{Cov}(X) = \frac{X^TX}{N}  \;  \in  \mathbb R^{D \times D}
\]</span> 比如X有a和b两维，<strong>均值均是0</strong>。那么<span class="math inline">\(\rm{Cov}(ab)=E(ab)-0=(a_0b_0+a_1b_1+\cdots + a_nb_n) /n\)</span> ，就得到了协方差值。</p>
<ul>
<li>中心化</li>
<li>计算<span class="math inline">\(x\)</span>的<code>协方差矩阵cov</code></li>
<li>对协方差矩阵cov进行<code>svd分解</code>，得到<code>u, s, v</code></li>
<li><strong>去除x的相关性，旋转</strong>，<span class="math inline">\(xrot = x \cdot u\)</span> ，此时xrot的协方差矩阵只有对角线才有值，其余均为0</li>
<li>选出大于0的奇异值</li>
<li>数据降维</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_pca</span><span class="params">()</span>:</span></span><br><span class="line">    x = np.random.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="comment"># 中心化</span></span><br><span class="line">    x -= np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">print</span> (x.shape)</span><br><span class="line">    <span class="comment"># 协方差</span></span><br><span class="line">    conv = np.dot(x.T, x) / x.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">print</span> (conv.shape)</span><br><span class="line">    <span class="keyword">print</span> (conv)</span><br><span class="line">    u, s, v = np.linalg.svd(conv)</span><br><span class="line">    <span class="keyword">print</span> (s)</span><br><span class="line">    <span class="keyword">print</span> (u.shape, s.shape, v.shape)</span><br><span class="line">    <span class="comment"># 大于0的奇异值</span></span><br><span class="line">    n_sv = np.where(s &gt; <span class="number">1e-5</span>)[<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">    print(n_sv)</span><br><span class="line">    <span class="comment"># 对数据去除相关性</span></span><br><span class="line">    xrot = np.dot(x, u)</span><br><span class="line">    <span class="keyword">print</span> (xrot.shape)</span><br><span class="line">    <span class="comment"># 数据降维</span></span><br><span class="line">    xrot_reduced = np.dot(x, u[:, :n_sv])</span><br><span class="line">    <span class="comment"># 降到了4维</span></span><br><span class="line">    <span class="keyword">print</span> (xrot_reduced.shape)</span><br></pre></td></tr></table></figure>
<h2 id="白化">白化</h2>
<p><a href="http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96" target="_blank" rel="noopener">斯坦福白化</a></p>
<p>白化希望特征之间的相关性较低，所有特征具有相同的协方差。白化后，得到均值为0，协方差相等的矩阵。对<span class="math inline">\(xrot\)</span>除以特征值。 <span class="math display">\[
x_{white} = \frac{x_{rot}}{\sqrt{\lambda + \epsilon}}
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_white = xrot / np.sqrt(s + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure>
<p>缺陷是：可能会夸大数据中的早上，因为把所有维度都拉伸到了相同的数值范围。可能有一些极少差异性（方差小）但大多数是噪声的维度。可以使用平滑来解决。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-pca-process.jpg" style="display:block; margin:auto" width="60%"></p>
<h1 id="权重初始化">权重初始化</h1>
<p>如果数据恰当归一化以后，可以假设所有权重数值中大约一半为正数，一半为负数。所以期望参数值是0。</p>
<p><strong>千万不能够全零初始化</strong>。因为每个神经元的输出相同，BP时梯度也相同，参数更新也<strong>相同</strong>。神经元之间就<strong>失去了不对称性的源头</strong>。</p>
<h2 id="小随机数初始化">小随机数初始化</h2>
<blockquote>
<p>如果神经元刚开始的时候是随机且不相等的，那么它们将<strong>计算出不同的更新</strong>，并<strong>成为网络的不同部分</strong>。</p>
</blockquote>
<p>参数接近于0单不等于0。使用<strong>零均值和标准差的高斯分布</strong>来生成随机数初始化参数，这样就<strong>打破了对称性</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)</span><br></pre></td></tr></table></figure>
<p>注意：不是参数值初始越小就一定好。参数小，意味着会减小BP中的梯度信号，在深度网络中，就会有问题。</p>
<p><strong>校准方差</strong></p>
<p>随着数据集的增长，随机初始化的神经元的<strong>输出数据分布的方差也会增大</strong>。可以<strong>使用<span class="math inline">\(\frac{1}{\sqrt{n}}\)</span> 校准方差</strong>。n是数据的数量。这样就能保证网络中<strong>所有神经元起始时有近似同样的输出分布</strong>。这样也能够提高收敛的速度。 感觉实际上就是做了一个归一化。</p>
<p>数学详细推导见<code>cs231n</code> ， <span class="math inline">\(s = \sum_{i}^nw_ix_i\)</span> ，假设w和x都服从同样的分布。想要输出s和输入x有同样的方差。 <span class="math display">\[
\begin {align}
&amp; \because D(s) = n \cdot D(w)D(x),  \; D(s) = D(x)    \\
&amp; \therefore D(w) = \frac{1}{n} \\
&amp; \because D(w_{old}) = 1, \; D(aX) = a^2 D(X) \\
&amp; \therefore D(w) = \frac{1}{n}D(w_{old}) = D(\frac{1}{\sqrt n} w_{old}) \\
&amp; \therefore w = \frac{1}{\sqrt n} w_{old}
\end{align}
\]</span> 所以要使用<span class="math inline">\(\frac{1}{\sqrt{n}}\)</span>来标准化参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H)/ sqrt(n)</span><br></pre></td></tr></table></figure>
<p><strong>经验公式</strong></p>
<p>对于某一层的方差，应该取决于两层的输入和输出神经元的数量，如下： <span class="math display">\[
\rm{D}(w) = \frac{2}{n_{in} + n_{out}}
\]</span> <code>ReLU</code>来说，<strong>方差应该是<span class="math inline">\(\frac{2}{n}\)</span></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.randn(D,H) * sqrt(<span class="number">2.0</span> / n)</span><br></pre></td></tr></table></figure>
<h2 id="稀疏和偏置初始化">稀疏和偏置初始化</h2>
<p>一般稀疏初始化用的比较少。一般偏置都初始化为0。</p>
<h1 id="batch-normalization">Batch Normalization</h1>
<p><a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-08-batch-normalization/" target="_blank" rel="noopener">莫凡python BN讲解</a> 和 <a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">CSDN-BN论文介绍</a> 。Batch Normalization和普通数据标准化类似，是将分散的数据标准化。</p>
<p><code>Batch Normalization</code>在神经网络非常流行，<strong>已经成为一个标准了</strong>。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB1.png" style="display:block; margin:auto" width="40%"></p>
<h2 id="训练速度分析">训练速度分析</h2>
<p>网络训练的时候，每一层<strong>网络参数更新</strong>，会导致<strong>下一层输入数据分布的变化</strong>。这个称为<code>Internal Convariate Shift</code>。</p>
<p>需要对<strong>数据归一化的原因</strong> ：</p>
<ul>
<li>神经网络的本质是<strong>学习数据分布</strong>。如果训练数据与测试数据的分布不同，那么<code>泛化能力</code>也大大降低</li>
<li>如果每个batch数据分布不同（batch 梯度下降），每次迭代都要去<strong>学习适应不同的分布</strong>，会大大<strong>降低训练速度</strong></li>
</ul>
<p>深度网络，前几层数据微小变化，后面几层数据<strong>差距会积累放大</strong>。</p>
<p>一旦某一层网络输入<strong>数据发生改变</strong>，这层网络就需要去<strong>适应学习这个新的数据分布</strong>。如果训练数据的分布一直变化，那么就会<strong>影响网络的训练速度</strong>。</p>
<h2 id="敏感度问题">敏感度问题</h2>
<p>神经网络中，如果使用<code>tanh</code>激活函数，初始权值是0.1。</p>
<p>输入<span class="math inline">\(x=1\)</span>， 正常更新： <span class="math display">\[
z = wx = 0.1, \quad a(z_1) = 0.1 \quad  \to \quad a^\prime(z) = 0.99
\]</span> 但是如果一开始输入 <span class="math inline">\(x=20\)</span> ，会导致梯度消失，不更新参数。 <span class="math display">\[
z = wx = 2 ,\quad a(z) \approx 1  \quad \to \quad   a^\prime(z) = 0
\]</span> 同样地，如果再输入<span class="math inline">\(x=100\)</span> ，神经元的输出依然是接近于1，不更新参数。 <span class="math display">\[
z = wx = 10 ,\quad a(z) \approx 1  \quad \to \quad   a^\prime(z) = 0
\]</span> 对于一个<strong>变化范围比较大</strong>特征维度，神经网络在初始阶段<strong>对它已经不敏感没有区分度</strong>了！</p>
<p>这样的问题，在神经网络的输入层和中间层都存在。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB2.png" style="display:block; margin:auto" width="50%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB3.png" style="display:block; margin:auto" width="50%"></p>
<h2 id="bn算法">BN算法</h2>
<p>BN算法在每一次迭代中，对每一层的输入都进行归一化。<strong>把数据转换为均值为0、方差为1的高斯分布</strong>。 <span class="math display">\[
\hat x = \frac{x - E(x)} {\sqrt{D(x) + \epsilon}}
\]</span> 非常大的<code>缺陷</code>：<strong>强行归一化会破坏掉刚刚学习到的特征</strong>。 把每层的数据分布都固定了，但不一定是前面一层学习到的数据分布。</p>
<p><code>牛逼的地方</code> ：设置两个可以学习的变量<code>扩展参数</code><span class="math inline">\(\gamma\)</span> ，和<code>平移参数</code> <span class="math inline">\(\beta\)</span> ，<strong>用这两个变量去还原上一层应该学习到的数据分布</strong>。（但是芳芳说，这一步其实可能没那么重要，要不要都行，CNN的本身会处理得更好）。 <span class="math display">\[
y = \gamma \hat x+ \beta
\]</span> 这样理解：用这两个参数，让神经网络自己去学习琢磨是前面的标准化是否有优化作用，<strong>如果没有优化效果，就用<span class="math inline">\(\gamma, \beta\)</span>来抵消标准化的操作。</strong></p>
<p>这样，BN就把原来不固定的数据分布，全部转换为固定的数据分布，而这种数据分布恰恰就是要学习到的分布。从而<strong>加速了网络的训练</strong>。</p>
<p>对一个<code>mini-batch</code>进行更新， 输入一个<span class="math inline">\(batchsize=m\)</span>的数据，学习两个参数，输出<span class="math inline">\(y\)</span> <span class="math display">\[
\begin{align}
&amp; \mu = \frac{1}{m} \sum_{i=1}^m x_i &amp;  \text{求均值} \\
&amp; \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 &amp; \text{求方差} \\
&amp; \hat x = \frac{x - E(x)} {\sqrt{\sigma^2 + \epsilon}} &amp;  \text{标准化} \\
&amp; y =  \gamma \hat x+ \beta &amp; \text{scale and shfit}
 \end{align}
\]</span> 其实就是对输入数据做个归一化： <span class="math display">\[
z = wx+b \to z = \rm{BN}(wx + b) \to a = f(z)
\]</span> 一般<strong>在全连接层和激活函数之间</strong>添加BN层。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB4.png" style="display:block; margin:auto" width="50%"></p>
<p>在测试的时候，由于是没有batch，所以使用固定的均值和标准差，也就是对训练的各个batch的均值和标准差做批处理。 <span class="math display">\[
E(x) = E(\mu), \quad D(x) = \frac{b}{b-1} E(\sigma^2)
\]</span></p>
<h2 id="bn的优点">BN的优点</h2>
<p><strong>1 训练速度快</strong></p>
<p><strong>2 选择大的初始学习率</strong></p>
<p>初始大学习率，学习率的衰减也很快。<strong>快速训练收敛</strong>。小的学习率也可以。</p>
<p><strong>3 不再需要Dropout</strong></p>
<p>BN本身就可以<strong>提高网络泛化能力</strong>，可以不需要Dropout和L2正则化。源神说，<strong>现在主流的网络都没有dropout了</strong>。但是<strong>会使用L2正则化</strong>，比较小的正则化。</p>
<p><strong>4 不再需要局部相应归一化</strong></p>
<p><strong>5 可以把训练数据彻底打乱</strong></p>
<h2 id="效果图片展示">效果图片展示</h2>
<p>对所有数据标准化到一个范围，这样<strong>大部分的激活值都不会饱和</strong>，都不是-1或者1。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB5.png" style="display:block; margin:auto" width="50%"></p>
<p>大部分的激活值在各个分布区间都有值。再传递到后面，数据更有价值。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB6.png" style="display:block; margin:auto" width="50%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NB8.png" style="display:block; margin:auto" width="50%"></p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-03-30T06:23:39.828Z" itemprop="dateUpdated">2018-03-30 14:23:39</time>
</span><br>


        
        <br>原始链接：<a href="/2017/11/26/cs224n-notes3-neural-networks-2/" target="_blank" rel="external">http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/</a>
        
    </div>
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BN/">BN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dropout/">Dropout</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PCA/">PCA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据预处理/">数据预处理</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/正则化/">正则化</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/白化/">白化</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/范数/">范数</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&title=《神经网络-过拟合-预处理-BN》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&title=《神经网络-过拟合-预处理-BN》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《神经网络-过拟合-预处理-BN》 — PLM's Notes&url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/11/27/cs231n-linear-notes/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">cs231n线性分类器和损失函数</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/11/23/cs224n-notes3-neural-networks/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">神经网络基础-反向传播-激活函数</h4>
      </a>
    </div>
  
</nav>



    














</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <!-- <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div> -->
    <div class="bottom">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
          <span>

          <!-- PLM  -->
          PLM's Notes &nbsp;
          &copy;
          &nbsp;
          2016 - 2018

          </span>
            <!-- <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span> -->
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&title=《神经网络-过拟合-预处理-BN》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&title=《神经网络-过拟合-预处理-BN》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《神经网络-过拟合-预处理-BN》 — PLM's Notes&url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACuElEQVR42u3awWrkMBAE0Pn/n87CsoeF4KGqpSYDeT6FiW3p+SA11Xq94uvr75X/8vT3/798v56e/X7P5QsPDw9vNPVkyHbg/PfkzvejP84EDw8Pb403e3X7hmQbeL+2tx8ODw8P72d5SWmbTB0PDw/vt/GS4dt72m0DDw8P7xN4ebSabBV5WJDHGetZCx4eHl7Maxtgn/D3Yn8PDw8P76Crnl/tsYDZ++un8PDw8BZ4bVybl7BtMJEEH/mBhmEajYeHhxfw3r8oB59MPT9ekIweJcd4eHh4l3izOLUNL3LGSXCMh4eHt8fLU9B8WW/T1PNSuz0WhoeHh3fOmw3TdpruHsAq8Hh4eHgLvLZ91ZbO5yEvHh4e3ufw8iX7ZOpt6NDO7fEpPDw8vAVe/rr2v0lzq42Mj2IRPDw8vKu8vGnUhgt52ywJMtoQBA8PD2+DlxS7+RGo2XTbjztMVvDw8PAu8fIS9mRjmIXCbYMt6u/h4eHhXeLlsezs0EB7xCr/pYiG8fDw8K7ykiW7PTI1m1wbbURbDh4eHt4Cb9awT4KAdvhZPBGV1Hh4eHhXeW07v40wcvZsg3ndHQwPDw+vbIAlxWs7xaRAv3uAAA8PD2+bd6tV324J7XTzD/HvTjw8PLxl3q1FP3/qVszxOC4eHh7eAi95RX6YICm+W8BJzoCHh4e3wWuX6dmk25Z/W3Y/zhMPDw9vgdc2nE4ChXyPqovj5Ck8PDy8S7yv8mqPDuQfKI9xiy0BDw8Pb4HXLrtteywvkfOCOy+y8fDw8PZ4+VGnk1J7trXMNoxi98PDw8M74M0OXZ1POgEcZS14eHh4H8DLm/p5iTw7oBB9Gjw8PLwf5bXsE1LbbIsODeDh4eFd5eVhRLt8J58vz5vbqAIPDw9vg9c2wGbHp9qYOI+Mj2B4eHh43f1/AC1IIhbhOc9gAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.0"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.0" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
