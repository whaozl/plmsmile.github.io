<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>神经网络基础-反向传播-激活函数 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="神经网络,反向传播,激活函数">
    <meta name="description" content="神经网络基础 很多数据都是非线性分割的，所以需要一种非线性non-linear决策边界 来分类。神经网络包含很多这样的非线性的决策函数。  神经元 神经元其实就是一个计算单元。  输入向量 \(x \in \mathbb R^n\) \(z = w^T x + b\) \(a = f(z)\) 激活函数">
<meta name="keywords" content="神经网络,反向传播,激活函数">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络基础-反向传播-激活函数">
<meta property="og:url" content="http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="神经网络基础 很多数据都是非线性分割的，所以需要一种非线性non-linear决策边界 来分类。神经网络包含很多这样的非线性的决策函数。  神经元 神经元其实就是一个计算单元。  输入向量 \(x \in \mathbb R^n\) \(z = w^T x + b\) \(a = f(z)\) 激活函数，sigmoid, relu等，后文有讲。  Sigmoid神经元 传">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NonlinearBoundary.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/sigmoidneuron.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/SingleLayerNeuralNetwork.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/SimpleFF.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ner1.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/421nnet.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/421nnet.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ErrorSignal.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ErrorSignal2.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ErrorSignal3.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/sigmoid_2.gif">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/tanh-2.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/relu-3.png">
<meta property="og:updated_time" content="2018-03-20T08:14:48.112Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络基础-反向传播-激活函数">
<meta name="twitter:description" content="神经网络基础 很多数据都是非线性分割的，所以需要一种非线性non-linear决策边界 来分类。神经网络包含很多这样的非线性的决策函数。  神经元 神经元其实就是一个计算单元。  输入向量 \(x \in \mathbb R^n\) \(z = w^T x + b\) \(a = f(z)\) 激活函数，sigmoid, relu等，后文有讲。  Sigmoid神经元 传">
<meta name="twitter:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NonlinearBoundary.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.0">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="https://plmsmile.github.io/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">神经网络基础-反向传播-激活函数</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">神经网络基础-反向传播-激活函数</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-11-23T04:01:08.000Z" itemprop="datePublished" class="page-time">
  2017-11-23
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#神经网络基础"><span class="post-toc-number">1.</span> <span class="post-toc-text">神经网络基础</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经元"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">神经元</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#网络层"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">网络层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#前向计算"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">前向计算</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#max-magin目标函数"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">Max magin目标函数</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#反向传播训练"><span class="post-toc-number">2.</span> <span class="post-toc-text">反向传播训练</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#符号定义"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">符号定义</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#w梯度推导"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">W梯度推导</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#w元素实例"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">W元素实例</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#梯度反向传播"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">梯度反向传播</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#bp向量化"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">BP向量化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#神经网络常识"><span class="post-toc-number">3.</span> <span class="post-toc-text">神经网络常识</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#梯度检查"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">梯度检查</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#激活函数"><span class="post-toc-number">4.</span> <span class="post-toc-text">激活函数</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#sigmoid"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">Sigmoid</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#tanh"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">Tanh</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#relu"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">ReLU</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#maxout"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">Maxout</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-cs224n-notes3-neural-networks"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">神经网络基础-反向传播-激活函数</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-11-23 12:01:08" datetime="2017-11-23T04:01:08.000Z"  itemprop="datePublished">2017-11-23</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p><img src="" style="display:block; margin:auto" width="40%"></p>
<h1 id="神经网络基础">神经网络基础</h1>
<p>很多数据都是非线性分割的，所以需要一种<code>非线性non-linear决策边界</code> 来分类。<code>神经网络</code>包含很多这样的<strong>非线性的决策函数</strong>。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/NonlinearBoundary.png" style="display:block; margin:auto" width="30%"></p>
<h2 id="神经元">神经元</h2>
<p>神经元其实就是一个<code>计算单元</code>。</p>
<ul>
<li>输入向量 <span class="math inline">\(x \in \mathbb R^n\)</span></li>
<li><span class="math inline">\(z = w^T x + b\)</span></li>
<li><span class="math inline">\(a = f(z)\)</span> 激活函数，<code>sigmoid</code>, <code>relu</code>等，后文有讲。</li>
</ul>
<p><strong>Sigmoid神经元</strong></p>
<p>传统用<code>sigmoid</code>多，但是现在一定不要使用啦。大多使用<code>Relu</code>作为<code>激活函数</code>。<br>
<span class="math display">\[
z = \mathbf{w}^T \mathbf{x} + b , \;  a = \frac {1}{1 + \exp (-z)}
\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/sigmoidneuron.png" style="display:block; margin:auto" width="40%"></p>
<h2 id="网络层">网络层</h2>
<p><strong>一个网络层有很多个神经元</strong>。输入<span class="math inline">\(\mathbf x\)</span>向量，<strong>会传递到多个神经元</strong>。如</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/SingleLayerNeuralNetwork.png" style="display:block; margin:auto" width="30%"></p>
<p>输入是<span class="math inline">\(n\)</span>维，隐层是<span class="math inline">\(m\)</span>维，有<span class="math inline">\(m\)</span>个神经元。则有 <span class="math display">\[
\begin{align}
&amp; z = W x + b , &amp; W \in \mathbb{R}^{m \times n}, x \in \mathbb R^n, b \in \mathbb R^m\\
&amp; a = f (z)  &amp; a \in \mathbb R^m\\
&amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\ 
\end{align}
\]</span> <strong>激活函数的意义</strong></p>
<p>每个神经元</p>
<ul>
<li>输入<span class="math inline">\(z = w^Tx+b\)</span> ：对特征进行加权组合的结果</li>
<li>激活<span class="math inline">\(a = f(z)\)</span>： 对<span class="math inline">\(z\)</span>是否继续保留</li>
</ul>
<p>最后会把所有的神经元的<strong>所有<span class="math inline">\(z\)</span>的激活信息<span class="math inline">\(a\)</span>综合起来</strong>，得到最终的分类结果。比如<span class="math inline">\(s = U^T a\)</span>。</p>
<h2 id="前向计算">前向计算</h2>
<p>输入<span class="math inline">\(x \in \mathbb R^n\)</span>， 激活信息<span class="math inline">\(a \in \mathbb R^m\)</span>。一般前向计算如下： <span class="math display">\[
\begin{align}
&amp; z = W x + b , &amp; W \in \mathbb{R}^{m \times n}, x \in \mathbb R^n, b \in \mathbb R^m\\\\
&amp; a = f (z)  &amp; a \in \mathbb R^m\\\\
&amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\ 
\end{align}
\]</span> 下面是一个简单的<code>全连接</code>，最后的圆圈里的1代表<strong>等价输出</strong>。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/SimpleFF.png" style="display:block; margin:auto" width="30%"></p>
<p><strong>NER例子</strong></p>
<p><code>NER</code>(named-entity recognition)，<code>命名实体识别</code>。对于一个句子<code>Museums in Paris are amazing</code>。 要判断中心单词<code>Paris</code><strong>是否是个命名实体</strong>。</p>
<p>既要看window里的所有<strong>词向量</strong>，也要看这些词的<strong>交互关系</strong>。比如：<code>Paris</code>出现在<code>in</code>的后面。 因为可能有<code>Paris</code>和<code>Paris Hilton</code>。这就需要<code>non-linear decisions</code>。</p>
<p>如果直接把input给到softmax，是很难获取到非线性决策的。所以需要添加中间层使用神经网络。如上图所示。</p>
<p><strong>维数分析</strong></p>
<p>每个单词4维，输入整个窗口就是20维。在隐层使用8个神经元。计算过程如下，最终得到一个分类的得分。 <span class="math display">\[
\begin {align}
&amp; z = Wx + b \\
&amp; a = f(z) \\
&amp; s = U^T a \\
\end{align}
\]</span> 维数如下： <span class="math display">\[
x \in \mathbb R^{20}, \; W \in \mathbb R^{8\times20}, \; U \in \mathbb R^{8\times1}, s \in R
\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ner1.png" style="display:block; margin:auto" width="40%"></p>
<h2 id="max-magin目标函数">Max magin目标函数</h2>
<p>正样本<span class="math inline">\(s\)</span> ：<code>Museums in Paris are amazing</code> ，负样本<span class="math inline">\(s_c\)</span>： <code>Not all museums in Paris</code> 。</p>
<p>只关心：<strong>正样本的得分高于负样本的得分</strong>， <strong>其它的不关注</strong>。即要<span class="math inline">\(s - s_c &gt; 0\)</span>： <span class="math display">\[
\mathrm{maxmize}(s -s_c) \leftrightarrow \mathrm{minmize}(s_c - s)
\]</span> <code>优化目标函数</code>如下： <span class="math display">\[
\rm{minimize} \; J =
\max(s_c - s, 0) \; 
= \begin{cases}
&amp; s_c - s, &amp; s &lt; s_c \\
&amp; 0, &amp; s \ge s_c
\end{cases}
\]</span> 上式其实有风险，更需要<span class="math inline">\(s - s_c &gt; \Delta\)</span>， 即<span class="math inline">\(s\)</span>比<span class="math inline">\(s_c\)</span>得分大于<span class="math inline">\(\Delta\)</span>，来保证一个安全的间距。 <span class="math display">\[
\rm{minimize} \; J = \max(\Delta + s_c - s, 0)
\]</span> 给具体间距<span class="math inline">\(\Delta=1\)</span>， 所以<code>优化目标函数</code>：详情见SVM。 <span class="math display">\[
\rm{minimize} \; J = \max(1 + s_c - s, 0)
\]</span> 其中<span class="math inline">\(s_c = U^T f(Wx_c + b), \; s = U^T f(Wx+b)\)</span> 。</p>
<h1 id="反向传播训练">反向传播训练</h1>
<p><a href="https://plmsmile.github.io/2017/08/20/ml-ng-notes/#梯度下降">梯度下降</a> ，或者SGD： <span class="math display">\[
\theta^{(t+1)} = \theta^{(t)} - \alpha \cdot \Delta_{\theta^{(t)}} J
\]</span> <code>反向传播</code> 使用<code>链式法则</code> 来计算<code>前向计算</code>中用到的参数的<code>梯度</code>。</p>
<h2 id="符号定义">符号定义</h2>
<p>如下图，一个简单的网络：</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/421nnet.png" style="display:block; margin:auto" width="40%"></p>
<p>网络在输入层和输出层是<strong>等价输入和等价输出</strong>，只有<strong>中间层会使用激活函数</strong>进行非线性变换。</p>
<table>
<thead>
<tr class="header">
<th>符号</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(x\)</span></td>
<td>网络输入，这里是4维</td>
</tr>
<tr class="even">
<td><span class="math inline">\(s\)</span></td>
<td>网络输出，这里是1维，即一个数字</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(W^{(k)}\)</span></td>
<td>第<span class="math inline">\(k \to k+1\)</span>层的转移矩阵。<span class="math inline">\(W \in \mathbb R^{n \times m}\)</span>。 k层m个神经元，k+1层n个神经元</td>
</tr>
<tr class="even">
<td><span class="math inline">\(W_{ij}^{(k)}\)</span></td>
<td>k+1层的<span class="math inline">\(i\)</span> 神经元 到 到<span class="math inline">\(k\)</span>层<span class="math inline">\(j\)</span>神经元的 的权值</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(b_i^{(k)}\)</span></td>
<td><span class="math inline">\(k \to k+1\)</span> 转移， k+1层的<span class="math inline">\(i\)</span> 神经元的接收偏置</td>
</tr>
<tr class="even">
<td><span class="math inline">\(z^{(k)}_j\)</span></td>
<td>第<span class="math inline">\(k\)</span>层的第<span class="math inline">\(j\)</span>个神经元的输入</td>
</tr>
<tr class="odd">
<td>计算输入</td>
<td><span class="math inline">\(z_j^{(k+1)} = \sum_i W_{ji}^{(k)} \cdot a^{(k)}_i + b^{(k)}_j\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(a_j^{(k)}\)</span></td>
<td>第<span class="math inline">\(k\)</span>层的第<span class="math inline">\(j\)</span>个神经元的输入。<span class="math inline">\(a = f(z)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\delta_j^{(k)}\)</span></td>
<td>BP时，<strong>在<span class="math inline">\(z_j^{(k)}\)</span>处的梯度。即<span class="math inline">\(f^\prime(z_j^{(k)}) \cdot g\)</span> ，<span class="math inline">\(g\)</span>是传递来的梯度</strong></td>
</tr>
</tbody>
</table>
<h2 id="w梯度推导">W梯度推导</h2>
<p><code>误差函数</code><span class="math inline">\(J = \max (1 + s_c - s, 0)\)</span> ，<strong>当<span class="math inline">\(J &gt; 0\)</span>的时候</strong>，<span class="math inline">\(J = 1 + s_c - s\)</span>要去更新参数W和b。 <span class="math display">\[
\frac{\partial J} {\partial s} = - \frac{\partial J} {\partial s_c} = -1
\]</span> 反向传播时，必须<strong>知道参数在前向时所贡献所关联的对象</strong>，即知道<code>路径</code>。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/421nnet.png" style="display:block; margin:auto" width="40%"></p>
<p>这里是等价输出： <span class="math display">\[
s = a_1^{(3)} = z_1^{(3)}  = W_1^{(2)}a_1^{(2) } + W_2^{(2)}a_2^{(2) }
\]</span> 这里对<span class="math inline">\(W_{ij}^{(1)}\)</span>的偏导进行反向传播<strong>推导</strong>： <span class="math display">\[
\begin{align}
\frac{\partial s}{\partial W_{ij}^{(1)}}
&amp;
= \frac{\partial W^{(2)} a^{(2)}}{\partial W_{ij}^{(1)}} 
\\
&amp;= \frac{ \color{blue} {\partial W_i^{(2)} a_i^{(2)}}} {\partial W_{ij}^{(1)}}
= \color{blue}{W_i^{(2)}} \cdot \frac{\partial  a_i^{(2)}}{\partial W_{ij}^{(1)}} 
\\
&amp; = W_i^{(2)} \cdot \color{blue} {\frac{\partial  a_i^{(2)}}{\partial z_i^{(2)}} \cdot  \frac{\partial  z_i^{(2)}}{\partial W_{ij}^{(1)}}} 
\\
&amp; = W_i^{(2)} \cdot \color{blue}{f^\prime(z_i^{(2)})} \cdot \frac{\partial }{\partial W_{ij}^{(1)}} \left(\color{blue}{b_i^{(2)} + \sum_k^4 a_k^{(1)}W_{ik}^{(1)}}\right)
\\
&amp; =  W_i^{(2)}f^\prime(z_i^{(2)}) \color{blue}{a_j^{(1)}} 
\\
&amp; = \color{blue}{\delta^{(2)}_i} \cdot a_j^{(1)}
\end{align}
\]</span> <strong>结果分析</strong></p>
<p>我们知道<span class="math inline">\(z_i^{(2)} = \sum_k^4 a_k^{(1)}W_{ik}^{(1)} + b_i^{(2)}\)</span>。 <strong>单纯</strong><span class="math inline">\(z_i^{(2)}\)</span>对<span class="math inline">\(W_{ij}^{(2)}\)</span>的导数是<span class="math inline">\(a_j^{(1)}\)</span>。<strong>反向时</strong>，在<span class="math inline">\(z_i^{(2)}\)</span>处的梯度是<span class="math inline">\(\delta_i^{(2)}\)</span>。</p>
<p>反向时，<span class="math inline">\(\frac{\partial s}{\partial W_{ij}^{(1)}} = \delta^{(2)}_i \cdot a_j^{(1)}\)</span>，是<strong>传来的梯度和当前梯度的乘积</strong>。这正好应证了<code>反向传播</code>。 传来的梯度也作<code>error signal</code>。 反向过程也是<code>error sharing/distribution</code>。</p>
<h2 id="w元素实例">W元素实例</h2>
<p><span class="math inline">\(W_{14}^{(1)}\)</span> 只直接贡献于<span class="math inline">\(z_1^{(2)}\)</span>和<span class="math inline">\(a_1^{(2)}\)</span></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ErrorSignal.png" style="display:block; margin:auto" width="30%"></p>
<table style="width:97%;">
<colgroup>
<col width="40%">
<col width="56%">
</colgroup>
<thead>
<tr class="header">
<th>步骤</th>
<th>梯度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(s \to a_1^{(3)}\)</span></td>
<td>梯度<span class="math inline">\(g=1\)</span>。开始为1。</td>
</tr>
<tr class="even">
<td><span class="math inline">\(a_1^{(3)} \to z_1^{(3)}\)</span></td>
<td>在<span class="math inline">\(z_1^{(3)}\)</span>处的梯度<span class="math inline">\(g = 1 \cdot 1 = \delta_1^{(3)}\)</span> 。<span class="math inline">\(local \; g= 1\)</span> ，等价变换</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(z_1^{(3)} \to a_1^{(2)}\)</span></td>
<td><span class="math inline">\(g = \delta_1^{(3)} \cdot W_1^{(2)} = W_1^{(2)}\)</span> 。<span class="math inline">\(lg = w\)</span>, <span class="math inline">\(z=wa+b\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(a_1^{(2)} \to z_1^{(2)}\)</span></td>
<td><span class="math inline">\(g = W_1^{(2)} \cdot f^\prime(z_1^{(2)}) = \delta_1^{(2)}\)</span>。 <span class="math inline">\(lg=f^\prime(z_1^{(2)})\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(z_1^{(2)} \to W_{14}^{(1)}\)</span></td>
<td><span class="math inline">\(g =W_1^{(2)} \cdot f^\prime(z_1^{(2)}) \cdot a_4^{(1)} = \delta_1^{(2)} \cdot a_4^{(1)}\)</span>。 <span class="math inline">\(lg = a_4^{(1)}\)</span> ， 因为<span class="math inline">\(z =wa+b\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(z_1^{(2)} \to b_1^{(1)}\)</span></td>
<td><span class="math inline">\(g = W_1^{(2)} \cdot f^\prime(z_1^{(2)}) \cdot 1 = \delta_1^{(2)} \cdot a_4^{(1)}\)</span>。 <span class="math inline">\(lg = 1\)</span> ， 因为<span class="math inline">\(z =wa+b\)</span></td>
</tr>
</tbody>
</table>
<p>对于上式的梯度计算，有两种理解方法，通过这<strong>两种思路去思考</strong>能更深入了解。</p>
<ul>
<li><code>链式法则</code></li>
<li><code>error sharing and distributed flow approach</code></li>
</ul>
<h2 id="梯度反向传播">梯度反向传播</h2>
<p><span class="math inline">\(\delta_i^{(k)} \to \delta_j^{(k-1)}\)</span> 传播图如下：</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ErrorSignal2.png" style="display:block; margin:auto" width="30%"></p>
<p>但是更多时候，当前层的某个神经元的信息会传播到下一层的多个节点上，如下图：</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/ErrorSignal3.png" style="display:block; margin:auto" width="30%"></p>
<p><strong>梯度推导公式</strong>如下： <span class="math display">\[
\begin{align}
&amp; g_w = \delta_i^{(k)} \cdot a_j^{(k-1)} &amp; W_{ij}^{(k-1)}的梯度\\\\
&amp; g_a = \sum_i \delta_i^{(k)}W_{ij}^{(k-1)} &amp; a_j^{(k-1)}的梯度 \\\\
&amp; g_z  = \delta_j^{(k-1)} = f^\prime(z_j^{(k-1)}) \cdot \sum_i \delta_i^{(k)}W_{ij}^{(k-1)}  &amp; z_j^{(k-1)}的梯度 \\\\
\end{align}
\]</span></p>
<h2 id="bp向量化">BP向量化</h2>
<p>很明显，不能一个一个参数地去更新<code>element-wise</code>。所以需要用矩阵和向量去表达，去一次性全部更新<code>matrix-vector level</code>。</p>
<p><strong>梯度计算</strong>， <span class="math inline">\(W_{ij}^{(k)}\)</span>的梯度是<span class="math inline">\(\delta_i^{(k+1)} \cdot a_j^{(k)}\)</span> 。向量表达如下： <span class="math display">\[
\Delta _{W^{(k)}} = 
\begin{bmatrix}
\delta_1^{(k+1)} \cdot a_1^{(k)} &amp; \delta_1^{(k+1)} \cdot a_2^{(k)}  &amp; \cdots\\
\delta_2^{(k+1)} \cdot a_1^{(k)} &amp; \delta_2^{(k+1)} \cdot a_2^{(k)} &amp; \cdots \\
\vdots &amp; \vdots &amp; \ddots
\end{bmatrix}
= \delta^{(k+1)}  a^{(k)T}
\]</span> <strong>梯度传播</strong>，<span class="math inline">\(\delta_j^{(k)} = f^\prime(z_j^{(k)}) \cdot \sum_i \delta_i^{(k+1)}W_{ij}^{(k)}\)</span>。向量表达如下： <span class="math display">\[
\delta^{(k)} = f^\prime(z^{(k)}) \circ (\delta^{(k+1)}W^{(k)})
\]</span> 其中<span class="math inline">\(\circ\)</span>是叉积向量积<code>element-wise</code>，是各个位置相乘， 即<span class="math inline">\(\mathbb R^N \times \mathbb R^N \to \mathbb R^N\)</span>。 点积和数量积是各个位置相乘求和。</p>
<p><strong>计算效率</strong></p>
<p>很明显，在计算的时候要把上一层的<span class="math inline">\(\delta^{(k+1)}\)</span>存起来，去计算<span class="math inline">\(\delta^{(k)}\)</span> ，这样可以减少大量的多余的计算。</p>
<h1 id="神经网络常识">神经网络常识</h1>
<h2 id="梯度检查">梯度检查</h2>
<p>使用导数的定义来估计导数，去和BP算出来的梯度做对比。<br>
<span class="math display">\[
f ^\prime (\theta) \approx \frac{J(\theta^{(i+)}) - J(\theta^{(i-)})} {2 \epsilon }
\]</span> 由于这样计算非常，效率特别低，所以只用这种办法来检查梯度。具体实现代码见原notes。</p>
<h1 id="激活函数">激活函数</h1>
<p>激活函数有很多，现在<strong>主要用</strong><code>ReLu</code>，<strong>不要用</strong><code>sigmoid</code>。</p>
<p>用ReLU学习率一定不要设置太大！同一个网络中都使用同一种类型的激活函数。</p>
<h2 id="sigmoid">Sigmoid</h2>
<p>数学形式和导数如下： <span class="math display">\[
\begin{align}
&amp; \sigma (z) = \frac {1} {1 + \exp(-z)}, \; \sigma(z) \in (0,1) \\ \\
&amp; \sigma^\prime (z) = \sigma(z) (1 - \sigma(z)) \\
\end{align}
\]</span> 图像</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/sigmoid_2.gif" style="display:block; margin:auto" width="40%"></p>
<p>优点是具有好的解释性，将实数挤压到<span class="math inline">\((0,1)\)</span>中，很大的负数变成0，很大的正数变成1 。但现在用的已经越来越少了。有下面2个缺点。</p>
<p>Sigmoid会<strong>造成梯度消失</strong></p>
<ul>
<li><strong>靠近0和1两端时</strong>，梯度会变成0。 BP链式法则，<span class="math inline">\(0 \times g_{from} = 0\)</span> ，后面的<strong>梯度接近0</strong>， <strong>将没有信息去更新参数</strong>。</li>
<li><strong>初始化权重过大</strong>，<strong>大部分神经元会饱和，无法更新参数</strong>。因为输入值很大，靠近1了。<span class="math inline">\(f^\prime(z) = 0\)</span>， 没法传播了。</li>
</ul>
<p>Sigmoid输出<strong>不是以0为均值</strong></p>
<ul>
<li>如果输出<span class="math inline">\(x\)</span>全是正的，<span class="math inline">\(z=wx+b\)</span>， 那么<span class="math inline">\(\frac{\partial z}{\partial w} = x\)</span> 梯度就全是正的</li>
<li>不过一般是batch训练，其实问题也还好</li>
</ul>
<p>Sigmoid梯度消失的问题最严重。</p>
<h2 id="tanh">Tanh</h2>
<p>数学公式和导数如下： <span class="math display">\[
\begin{align}
&amp; \tanh (z) = \frac{\exp(z) - \exp(-z)}{\exp(z) + \exp(-z)} = 2 \sigma(2z) - 1, \; \tanh(z) \in(-1, 1) \\ \\
&amp;  \tanh^\prime (z) = 1 -  \tanh^2 (z)  
\end{align}
\]</span> 图像：</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/tanh-2.png" style="display:block; margin:auto" width="40%"></p>
<p>Tanh是Sigmoid的代替，它是0均值的，但是依然存在梯度消失的问题。</p>
<h2 id="relu">ReLU</h2>
<p>ReLU<code>Rectified Linear Unit</code> 最近越来越流行，不会对于大值<span class="math inline">\(z\)</span>就导致神经元饱和的问题。在CV取得了很大的成功。 <span class="math display">\[
\begin{align}
&amp; \rm{rect}(z) = \max(z, 0) \\ \\
&amp;  \rm{rect}^\prime (z) = 
\begin{cases} &amp;1, &amp;z &gt; 0 \\&amp; 0, &amp; z \le 0 \end{cases} 
\\
\end{align}
\]</span> 其实ReLU是一个关于0的阈值，<strong>现在一般都用ReLU</strong>：</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/relu-3.png" style="display:block; margin:auto" width="40%"></p>
<p>ReLU的<strong>优点</strong></p>
<ul>
<li>加速收敛（6倍）。线性的，不存在梯度消失的问题。一直是1。</li>
<li>计算简单</li>
</ul>
<p>ReLU的<strong>缺点</strong></p>
<p>训练的时候很脆弱</p>
<ul>
<li>BP时，如果有大梯度经过ReLU，当前在z处的梯度<span class="math inline">\(\delta^{(k+1)} = 1 \times g_m\)</span> 就很大</li>
<li>对参数<span class="math inline">\(w\)</span>的梯度 <span class="math inline">\(\Delta_{W^{(k)}} =\delta^{(k)} a^{(k)T}\)</span> 也就很大</li>
<li>参数<span class="math inline">\(w\)</span>会更新的特别小 <span class="math inline">\(W^{(k)} = W^{(k)} - \alpha \cdot \Delta_{W^{(k)}}\)</span></li>
<li>前向时，<span class="math inline">\(z =wx+b \le 0\)</span> 也就特别小，激活函数就不会激活</li>
<li>不激活，梯度就为0。</li>
<li>再BP的时候，就无法更新参数了</li>
</ul>
<p>总结也就是：大梯度<span class="math inline">\(\to\)</span>小参数<span class="math inline">\(w\)</span> ，新小$z = wx+b $ ReLU不激活， 不激活梯度为0 <span class="math inline">\(\to\)</span> 不更新参数w了。</p>
<p>当然可以使用比<strong>较小的学习率</strong>来解决这个问题。</p>
<h2 id="maxout">Maxout</h2>
<p><code>maxout</code> 有ReLU的优点，同时避免了它的缺点。但是maxout加倍了模型的参数，导致了模型的存储变大。 <span class="math display">\[
\begin{align}
&amp; \rm{mo}(x) = \max(w_1x+b_1, w_2x+b_2) \\ \\
&amp;  \rm{mo}^\prime (x) = 
\begin{cases} &amp;w_1, &amp;w_1x+b_1 大 \\&amp; w_2, &amp; 其它 \\\end{cases} 
\\
\end{align}
\]</span></p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-03-20T08:14:48.112Z" itemprop="dateUpdated">2018-03-20 16:14:48</time>
</span><br>


        
        <br>原始链接：<a href="/2017/11/23/cs224n-notes3-neural-networks/" target="_blank" rel="external">http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/</a>
        
    </div>
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/反向传播/">反向传播</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/激活函数/">激活函数</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/&title=《神经网络基础-反向传播-激活函数》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/&title=《神经网络基础-反向传播-激活函数》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《神经网络基础-反向传播-激活函数》 — PLM's Notes&url=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2017/11/26/cs224n-notes3-neural-networks-2/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">神经网络-过拟合-预处理-BN</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/11/12/cs224n-notes1-word2vec/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">Word2vec详细介绍</h4>
      </a>
    </div>
  
</nav>



    














</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <!-- <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div> -->
    <div class="bottom">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
          <span>

          <!-- PLM  -->
          PLM's Notes &nbsp;
          &copy;
          &nbsp;
          2016 - 2018

          </span>
            <!-- <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span> -->
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/&title=《神经网络基础-反向传播-激活函数》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/&title=《神经网络基础-反向传播-激活函数》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《神经网络基础-反向传播-激活函数》 — PLM's Notes&url=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2017/11/23/cs224n-notes3-neural-networks/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACtUlEQVR42u3a0W7jMAwEwPz/T+deD2jj7JJWEBTjp8J1ZY0KiDSpxyO+nv9dP+9f33n12+eP6/qN1yOvLjw8PLzF1F+9MuG9eiZ58vpvk6V5OQIeHh7eMV4yXDK52dTzUJHPGQ8PD+97ePXWXDKSJ/Hw8PD+Bm+WQCfhBw8PD+87eflwbUi4HueLai14eHh484z3sZniZ34+0t/Dw8PDW3fV83T2OiTMxm/LH7+MhoeHh3eAl2+4bVHg3oNTswIxHh4e3mneJqndtMo2zydFZDw8PLx7eTksaTXlU2/LuMPyBB4eHt4xXnsnWaC7Gl2zkgceHh7eaV6OPJF8JyGhXXQ8PDy807y2aJszZvj27W/6e3h4eHgHeO0RgTzlbQ8EtOO8WTI8PDy8A7w8DORpdN7cao8ItI00PDw8vBO8tkHVptFt+MmXIwokeHh4eId5+UCbyc2OI0SlBzw8PLyP8NrUeba5z5LjWcmj/mLAw8PDW/DaD/78+NR+unma/qYkgYeHh3cTb5bstkXYzfP53IqSLh4eHt6Itzl01TarZkcQ2kIzHh4e3mleO+isyNs21fLQUoQZPDw8vJt47Ra/KUO0S7YvDePh4eGd4OWbbD7RdiGuAflSDjt+eHh4eAvebOPOV649oDBrfRVVajw8PLwFrx0035qTILF/+5tYh4eHh3eYV2ffZTt/s/UngeGXMfHw8PAO82ZF0rZomx9B2CwrHh4e3gnes7zaFn6egm+KxS//VXh4eHgHeO2Gu9m+90Xh/cEsPDw8vLt49ef9qIw7Cyez4wt4eHh4n+Elpdi8aLspyz7uvfDw8PC+gJdPaBNg2lT7tsCAh4eHd4CX770zxuzQwJtDV3h4eHgHePnQeQGiTZdnjbcoMODh4eHdytt/0SchJF+Othx8QwkDDw8PL53bP74oFtsakS3hAAAAAElFTkSuQmCC" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.0"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.0" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
