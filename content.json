[{"title":"linux-notes","date":"2018-01-25T08:07:07.000Z","path":"2018/01/25/linux-notes/","text":"Tmux使用 tmux list-keys 可以看到所有的按键。 使用tmux进行复制粘贴 1234567891011121314# 1. 复制# 进入vim模式ctrl x + [ # 输入Space开始选择space# 输入enter选择完成enter# 2. 粘贴到另外一个窗口的vim文件里面# i进入vim的插入模式i# 粘贴到文件里ctrl x + ] 即ctrl x + [ 是进入复制模式，可以进行上下翻页。ctrl x + ] 是粘贴。 交换window 123# &#123;&#125;实际操作是shift+[&#123;按键，和复制模式有点相似ctrl x + &#123;ctrl x + &#125;","tags":[{"name":"linux","slug":"linux","permalink":"http://plmsmile.github.io/tags/linux/"},{"name":"tmux","slug":"tmux","permalink":"http://plmsmile.github.io/tags/tmux/"}]},{"title":"Aim2offer3(21-40)","date":"2018-01-07T04:22:49.000Z","path":"2018/01/07/aim2offer3/","text":"把奇数放在偶数的前面-21 问题 输入：一个数组，乱序，有奇数和偶数 输出：把所有的奇数放在前面，所有的偶数放在后面 暴力思路 从头到尾遍历数组 遇到奇数，访问下一个 遇到偶数，把它后面的数字都向前挪动以为，该偶数放到末尾 冒泡思路 for (int i = n-1; i &gt; 0; i--) 依次放置好后面n-1个数即可 从for (int j = 0; j &lt; i; j++) ，遇到j-1是偶数，j是奇数，则交换 辅助数组 新数组存偶数，原数组删除偶数，最后把新数组的偶数追加到原数组 遍历原数组，遇到偶数，存到新数组，删除原数组中的偶数 双指针快排思路 类似于快速排序的思路，但不是稳定的。 指针1，在前面，向后移动，前面应是奇数 指针2，在后面，向前移动，后面应是偶数 指针1指偶数，指针2指奇数，交换两个数 直到指针相遇 关键代码 1234567891011121314151617181920212223242526/* * 对数组进行重新排序，把奇数放在前面，偶数在后面 * Args: * a -- 数组 * f -- 函数指针，什么样的条件放在后面，如是偶数、是正数，解耦 */void reorder_array(vector&lt;int&gt; &amp; a, bool (*f)(int)) &#123; int l = 0; int r = a.size() - 1; while (l &lt; r) &#123; // 从左到右找到第一个偶数 while (l &lt; r &amp;&amp; !f(a[l])) &#123; l++; &#125; // 从右到左，找到第一个奇数 while (r &gt; l &amp;&amp; f(a[r])) &#123; r--; &#125; // 交换 if (l &lt; r) &#123; int t = a[l]; a[l] = a[r]; a[r] = t; &#125; &#125;&#125; 归并排序思路 归并排序稳定，也很快，所以使用归并排序。 分成长度为1、2、4的序列，各自都排好，依次两两合并。 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class Solution &#123;public: /* * 奇偶性 */ int get_parity(int n) &#123; return (n &amp; 1) == 1; &#125; /* * a中前后有两个序列，分别有序，对其进行merge */ void merge(vector&lt;int&gt; &amp;a, int start, int mid, int end) &#123; // 临时存储新的排序数据 int *t = new int[end - start + 1]; int i = start; int j = mid + 1; // 临时数组的索引 int k = 0; while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; // 奇偶性 int pi = get_parity(a[i]); int pj = get_parity(a[j]); if (pi == 1 &amp;&amp; pj == 0) &#123; t[k++] = a[i++]; &#125; else if (pi == 0 &amp;&amp; pj == 1)&#123; t[k++] = a[j++]; &#125; else if (pi == 1 &amp;&amp; pj == 1) &#123; t[k++] = a[i++]; &#125; else if (pi == 0 &amp;&amp; pj == 0) &#123; t[k++] = a[i++]; &#125; &#125; while (i &lt;= mid) &#123; t[k++] = a[i++]; &#125; while (j &lt;= end) &#123; t[k++] = a[j++]; &#125; for (i = 0; i &lt; end - start + 1; i++) &#123; a[start + i] = t[i]; &#125; delete [] t; return; &#125; /* * 对a的长度为gap的子序列，两两合并 * Args: * a -- 数组 * gap -- 1个子序列的长度 * Returns: * None */ void merge_groups(vector&lt;int&gt; &amp;a, int gap) &#123; // 两组的长度 int twolen = 2 * gap; int i; // 对相邻的两个gap进行合并 for (i = 0; i + twolen - 1 &lt; a.size(); i += twolen) &#123; int start = i; int mid = start + gap - 1; int end = i + twolen - 1; merge(a, start, mid, end); &#125; // 若最后一次不足两个gap，即1个gap和部分gap if (i + gap - 1 &lt; a.size() - 1) &#123; merge(a, i, i + gap - 1, a.size() - 1); &#125; &#125; void reOrderArray(vector&lt;int&gt; &amp;a) &#123; // 分割为长度为i的子序列，两两进行合并 for (int i = 1; i &lt; a.size(); i *= 2) &#123; merge_groups(a, i); &#125; &#125; &#125;; 链表中倒数第k个节点-22 不知道链表长度，要求在\\(O(n)\\)内找到倒数第k个节点，注意代码的鲁棒性。 双指针思路 两个指针，l先走k-1步，r再从头节点出发，始终保持距离为k-1。 r走到末尾时，l就是倒数第k个节点。 注意头结点为空和k非法(为0、k超出等)的情况。 双指针求链表中间节点 两个指针，l走一步，r走两步。r走到末尾的时候，l正好走到中间。 双指针总结 当一个指针遍历链表不能解决问题的时候，就使用两个指针。 同时走、一个速度快 一个先走、速度一样 关键代码 123456789101112131415161718192021222324252627282930313233343536373839/* * 返回链表中的倒数第k个节点 * 双指针思路，注意代码的鲁棒性 * Args: * phead -- 头指针 * k -- 无符号整数 * Returns: * nullptr or 第k个节点 */ListNode* kth_from_end(ListNode* phead, unsigned int k) &#123; // 1. phead为空或者k不合法，都返回空，k不能为0，否则k-1是一个巨大的数 if (phead == nullptr || k == 0) &#123; return nullptr; &#125; // 2. 双指针 ListNode* pr = phead; ListNode* pl = phead; unsigned int count = 1; // 3. pl先走 while (pl &amp;&amp; count &lt;= k - 1) &#123; pl = pl-&gt;next; count++; &#125; // 4. 不足k个节点，返回空 if (pl == nullptr) &#123; return nullptr; &#125; // 5. 左右指针一起走，保持k-1的距离 while (pl-&gt;next) &#123; pl = pl-&gt;next; pr = pr-&gt;next; &#125; return pr;&#125; 链表中环的入口节点-23 如果一个链表有环，则返回这个环的入口节点 双指针法 确定有环 双指针，同时走，l一次1步，r一次2步。 r走到末尾，则无环 r与l相遇，则有环。相遇节点在环内 确定环内节点数量 相遇节点在环内，从相遇节点开始遍历一圈，计数。再次回到相遇节点，就能知道环内节点数量k。 找到环的入口节点 双指针，r先走k步，l再走。l与r相遇时，就是环的入口节点 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/* * 双指针判断链表是否有环时，返回两个指针相遇的节点 * Args: * phead -- 头节点 * Returns: * pmeet -- nullptr or 相遇的节点 */ListNode* meet_node(ListNode* phead) &#123; // 空节点 or 1个节点，无法成环 if (phead == nullptr || phead-&gt;next == nullptr) &#123; return nullptr; &#125; // 两个指针，l一次走一步，r一次走两步 ListNode* pl = phead; ListNode* pr = phead-&gt;next-&gt;next; ListNode* pmeet = nullptr; while (pl &amp;&amp; pr) &#123; if (pl == pr) &#123; pmeet = pl; break; &#125; pl = pl-&gt;next; pr = pr-&gt;next; // pr走第二步的时候，先判断一下 if (pr) &#123; pr = pr-&gt;next; &#125; &#125; return pmeet;&#125;/* * 获取链表中环内节点的数量，已经确保有环 * Args: * pmeet -- 链表中环内的一个节点 * Returns: * count -- 环内的节点的数量 */int get_circle_node_count(ListNode* pmeet) &#123; if (pmeet == nullptr || pmeet-&gt;next == nullptr) &#123; return 0; &#125; ListNode* p = pmeet-&gt;next; int count = 1; while (p != pmeet) &#123; p = p-&gt;next; count++; &#125; return count;&#125;/* * 双指针法获得链表中环的入口节点 * Args: * phead -- 头节点 * Returns: * pentry -- nullptr 或者 环的入口节点 */ListNode* circle_entry_node(ListNode* phead) &#123; ListNode* pmeet = meet_node(phead); // 无环 if (pmeet == nullptr) &#123; return nullptr; &#125; // 后面的操作已经确保有环 // 环内节点的数量 int k = get_circle_node_count(pmeet); // pl先走k步 ListNode* pl = phead; ListNode* pr = phead; for (int i = 1; i &lt;= k; i++) &#123; pr = pr-&gt;next; &#125; // 同时走，直到相遇在入口节点 ListNode* pentry = nullptr; while (pl &amp;&amp; pr) &#123; if (pl == pr) &#123; pentry = pl; break; &#125; pl = pl-&gt;next; pr = pr-&gt;next; &#125; return pentry;&#125; 推导法和断链法 leetcode推导法 翻转链表-24 Reverse Linked List 思路 每次遍历保存三个节点：pre, pnow, pnext ，遍历到pnow 保存pnow的next pnow指向pre pre = pnow 把pnext赋值给pnow，下一次循环 注意当pnext为空时，已走到末尾，此时的pnow应该是新的head。 12345678910111213141516171819ListNode* reverseList(ListNode* head) &#123; ListNode* pre = nullptr; ListNode* pnow = head; ListNode* pnext = nullptr; while (pnow) &#123; // 保存pnow的next pnext = pnow-&gt;next; pnow-&gt;next = pre; // 新的头节点 if (pnext == nullptr) &#123; head = pnow; &#125; // pnow作为新的pre pre = pnow; // pnext作为下一轮遍历的pnow pnow = pnext; &#125; return head;&#125; 合并两个有序链表-25 见这里 树的子结构-26 Subtree of Another Tree 两棵树s和t，判断t是否是s的一个子结构 思路 层次遍历s的每个节点p 判断p是否和t完全相同 判断两颗树相同 两个都为空，相同 其中一个为空，不同 值不同，不同 则 return 左子树相同 &amp;&amp; 右子树相同 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/* * 判断两颗树是否相等 */bool is_same(TreeNode* p1, TreeNode* p2) &#123; if (p1 == nullptr &amp;&amp; p2 == nullptr) &#123; return true; &#125; if (p1 == nullptr || p2 == nullptr) &#123; return false; &#125; if (p1-&gt;val != p2-&gt;val) &#123; return false; &#125; return is_same(p1-&gt;left, p2-&gt;left) &amp;&amp; is_same(p1-&gt;right, p2-&gt;right);&#125;/* * 层次遍历以s的每个节点为根节点的子树是否和t相同 */bool isSubtree(TreeNode* s, TreeNode* t) &#123; queue&lt;TreeNode*&gt; nodes; // 放根节点 if (s != nullptr) &#123; nodes.push(s); &#125; while (!nodes.empty()) &#123; // 访问根节点 TreeNode* p = nodes.front(); nodes.pop(); if (is_same(p, t)) &#123; return true; &#125; // 向队列中追加左右孩子 if (p-&gt;left) &#123; nodes.push(p-&gt;left); &#125; if (p-&gt;right) &#123; nodes.push(p-&gt;right); &#125; &#125; return false;&#125; 二叉树的镜像-27 二叉树的镜像，就是左右孩子交换节点嘛。 思路 使用先序遍历的思想，这里是二叉树各种遍历 使用栈，根节点入栈 栈不为空，出栈一个元素p 交换其左右孩子节点 右孩子入栈，左孩子入栈 [关键代码] 1234567891011121314151617181920212223242526/* * 先序遍历求二叉树的镜像 */void mirror(TreeNode *head) &#123; TreeNode* p = head; stack&lt;TreeNode*&gt; st; if (p != nullptr) &#123; st.push(p); &#125; // 栈不为空 while (!st.empty()) &#123; TreeNode* now = st.top(); st.pop(); // 交换其左右节点 TreeNode* t = now-&gt;left; now-&gt;left = now-&gt;right; now-&gt;right = t; // 左右节点入栈 if (now-&gt;right) &#123; st.push(now-&gt;right); &#125; if (now-&gt;left) &#123; st.push(now-&gt;left); &#125; &#125;&#125; 对称的二叉树-28 leetcode对称的二叉树 判断一个二叉树是不是对称的 思路 普通的遍历都是先左后右， 对称的话，得用先右后左。 一棵树对称，则先左后右的先序序列、先右后左的先序序列应该一样。 即左右子树对称相等。 遍历的时候 根节点为空，对称 否则，看sym(root.left, root.right) 两个节点其中一个为空，则看p1 == p2 两个都不为空，则先看根节点的值 最后则交叉看左右子树，sym(p1.left, p2.right) &amp;&amp; sym(p1.right, p2.left) [关键代码] 123456789101112131415161718192021222324252627282930/* * 判断一棵树是否对称 */bool isSymmetric(TreeNode* root) &#123; if (root == nullptr) &#123; return true; &#125; return symmetric_equal(root-&gt;left, root-&gt;right);&#125;/* * 判断两棵树对称相等 * Args: * p1, p2 -- 一般是一棵树的左右子树 * Returns: * true or false */bool symmetric_equal(TreeNode* p1, TreeNode* p2) &#123; // 有空的 if (p1 == nullptr || p2 == nullptr) &#123; return p1 == p2; &#125; // 先看根节点的值 if (p1-&gt;val != p2-&gt;val) &#123; return false; &#125; // 看左右子树对称相等 return symmetric_equal(p1-&gt;left, p2-&gt;right) &amp;&amp; symmetric_equal(p1-&gt;right, p2-&gt;left); &#125; 非递归代码 12345678910111213141516public boolean isSymmetric(TreeNode root) &#123; if (root == null) return true; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root.left); stack.push(root.right); while (!stack.empty()) &#123; TreeNode n1 = stack.pop(), n2 = stack.pop(); if (n1 == null &amp;&amp; n2 == null) continue; if (n1 == null || n2 == null || n1.val != n2.val) return false; stack.push(n1.left); stack.push(n2.right); stack.push(n1.right); stack.push(n2.left); &#125; return true;&#125; 顺时针打印矩阵-29 牛客网打印矩阵 思路 用左上和右下的坐标定位出一圈要打印的数据。一圈打完以后，分别沿着对角线向中间靠拢一个单位。 关键代码 12345678910111213141516171819202122232425262728293031vector&lt;int&gt; print_marix(vector&lt;vector&lt;int&gt;&gt; matrix) &#123; vector&lt;int&gt; res; if (matrix.empty() || matrix[0].empty()) &#123; return res; &#125; int row = matrix.size(); int col = matrix[0].size(); // 通过左上、右下去锁定当前圈的元素 int left = 0, top = 0, right = col - 1, bottom = row - 1; while (left &lt;= right &amp;&amp; top &lt;= bottom) &#123; // 左到右 for (int i = left; i &lt;= right; i++) res.push_back(matrix[top][i]); // 上到下 for (int i = top + 1; i &lt;= bottom; i++) res.push_back(matrix[i][right]); // 右到左，有多行时 if (top != bottom) for (int i = right - 1; i &gt;= left; i--) res.push_back(matrix[bottom][i]); // 下到上，有多列时 if (left != right) for (int i = bottom - 1; i &gt; top; i--) res.push_back(matrix[i][left]); // 左上角、右下角移动 left++, top++, right--, bottom--; &#125; return res;&#125; 包含min函数的栈-30 Min Stack 实现能够得到栈中最小元素的数据结构，要求入栈、出栈、获得最小元素都是O(1) 思考过程 定义一个变量，去存储最小元素，每次对其更新。可是，当最小元素出栈以后呢，怎么去得到新的最小元素呢？这样就需要把次最小元素也存储下来。就需要把每次最小的元素，按顺序存储下来， 按照入栈的顺序 入栈时，入栈当前最小的元素 出栈时，把当前最小的元素也出栈，留下的是下一个状态的最小元素 思路 数据栈，正常存数据 最小栈，存放各个时刻的最小元素，栈顶一直是当前的最小元素 入栈： 当前最小元素：min(min_st.top(), new) ，最小栈压入当前的最小元素 出栈：数据栈元素出栈，同时最小栈出掉当前的最小元素 [关键代码] 12345678910111213141516171819202122232425262728293031323334353637383940class MinStack &#123;private: // 数据栈 stack&lt;int&gt; st_data; // 存储每个状态最小元素的栈 stack&lt;int&gt; st_min;public: // 初始化数据结构 MinStack() &#123; &#125; void push(int x) &#123; st_data.push(x); // 当前的最小元素入栈 if (st_min.empty() || x &lt; st_min.top()) &#123; st_min.push(x); &#125; else &#123; st_min.push(st_min.top()); &#125; &#125; void pop() &#123; if (st_data.empty() || st_min.empty()) &#123; return; &#125; // 当前数据栈和最小元素栈都出栈 st_data.pop(); st_min.pop(); &#125; int top() &#123; return st_data.top(); &#125; int getMin() &#123; return st_min.top(); &#125;&#125;; 栈的压入和弹出序列-31 输入两个序列，第一个为栈的入栈序列，判断第二个是否为其出栈序列。入栈所有数字都不相等 思路 入栈序列，出栈序列。当前需要出栈元素为i i在栈内，则直接出栈 i不在栈内，则把如栈序列 前面到i 的元素全部入栈，再重复1 入栈序列全都入栈了，依然没有i，则不是弹出序列 示例 入栈：1 2 3 4 5， 出栈 4 5 3 2 1 4不在栈顶，前面元素入栈 操作 栈内元素 剩余出栈元素 剩余入栈元素 4不在栈顶，4入栈 1 2 3 4 4 , 5 3 2 1 5 4出栈 1 2 3 5 3 2 1 5 5不在栈顶，5入栈 1 2 3 5 5, 3 2 1 - 5出栈 1 2 3 3 2 1 - 3出栈 1 2 2 1 - 2出栈 1 1 - 1出栈 - - - 入栈：1 2 3 4 5， 出栈 4 3 5 1 2 操作 栈内元素 剩余出栈元素 剩余入栈元素 4不在栈顶，4入栈 1 2 3 4 4 , 3 5 1 2 5 4出栈 1 2 3 3 5 1 2 5 3出栈 1 2 5 1 2 5 5出栈，不在栈顶， 入栈 1 2 5 5, 1 2 5出栈 1 2 1 2 1出栈，不在栈顶，已经无可入元素， 终止 [关键代码] 遍历每个出栈元素now 使其在栈顶，对如栈序列进行入栈，直到now 如果now依然不在栈顶，则不是 如果now在栈顶，则出栈 继续遍历下一个出栈now 12345678910111213141516171819202122232425262728293031323334353637383940414243/* * 判断vpop是否是入栈序列vpush的出栈序列 * Args: * vpush -- 入栈序列 * vpop -- 要判断的出栈序列 * Returns: * true or false */bool is_poporder(const vector&lt;int&gt;&amp; vpush, const vector&lt;int&gt;&amp; vpop) &#123; bool res = false; stack&lt;int&gt; st; // 入栈的元素 int k = 0; for (int i = 0; i &lt; vpop.size(); i++) &#123; // 当前要出栈的元素 int now = vpop[i]; // now不在栈顶，则从入栈序列中入栈 if (st.empty() || st.top() != now) &#123; while (k &lt; vpush.size()) &#123; st.push(vpush[k]); if (vpush[k] == now) &#123; k++; break; &#125; k++; &#125; &#125; // now依然不在栈顶 if (st.empty() || now != st.top()) &#123; res =false; break; &#125; // now 在栈顶，出栈 st.pop(); if (i == vpop.size() - 1) &#123; res = true; &#125; &#125; return res;&#125; 从上到下打印二叉树-32 leetcode层次遍历 有3个题 层次遍历序列 每次遍历一层 分行层次遍历打印 每次打印一层 z型遍历二叉树 见leetcode的z型遍历二叉树 二叉搜索树的后序遍历-33 给一个数组，判断是否是二叉搜索树的后序遍历 后序遍历：最后一个是根节点 BST：左 &lt; 根 &lt; 右 123 8 6 105 7 9 11 给一个数组：5 7 6 9 11 10 8 根节点是8， 5 7 6前面小于8的，是左子树，全部小于8 9 11 10中间大于8的，是右子树，全部大于8 再依次取判断左右子树是否是BST 思路 判断nums[start, end] 是否是BST的后序遍历序列 空返回false，一个元素返回true。 找到根节点root = nums[end] 从start开始，找左子树的最后一个元素i-1， 直到end-1， 每个元素都小于根节点 从i开始， 找右子树，直到end-1， 每个元素都要大于根节点。 右子树时，前面左子树已经ok（有或者没有），所以后面的元素都是右子树的，都要大于根节点。 如果后面有小于根节点的，则不满足右子树，返回false 递归判断左右子树是否是BST，返回结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/* * 是否是BST的后序遍历序列 * Args: * nums: 要判断的目标序列，没有重复的数字 * start: 序列的起始值 * end: 序列的结束位置 * Returns: * true or false */bool is_bst_postorder(const vector&lt;int&gt; &amp;nums, int start, int end) &#123; // 不合法 if (nums.empty() || start &lt; 0 || end &gt; nums.size() - 1 || start &gt; end) &#123; return false; &#125; // 只有一个元素 if (start == end) &#123; return true; &#125; int root = nums[end]; // 1. 找到左子树 int i = start; for (; i &lt;= end - 1; i++) &#123; if (nums[i] &gt; root) break; &#125; // 2. 找到右子树，全部都大于root int j = i; for (; j &lt;= end - 1; j++) &#123; // 右子树中有小于root的值，不合法 if (nums[i] &lt; root) &#123; return false; &#125; &#125; // 3. 判断左右子树 bool left = true; if (i &gt; start) &#123; // 判断右子树 left = is_bst_postorder(nums, start, i - 1); &#125; bool right = true; if (j &gt; i) &#123; // 判断左子树 right = is_bst_postorder(nums, i, j - 1); &#125; return left &amp;&amp; right;&#125; 二叉树中的路径求和-34 参考leetcode中各种pathsum汇总 复杂链表的复制-35 链表：值，下一个节点，任意一个节点。复制这样的一个链表。 思路1 暴力解法 先把链表连接起来，不管任意指针 遍历链表，为每个新节点找到任意指针的节点，连接起来 同时知道旧节点N、新节点N1、旧节点的任意指针节点S 遍历新链表，找到任意指针S1 把N1和S1连接起来 时间复杂度\\(O(n^2)\\)，遍历新链表，找到任意指针S1的节点。 思路2 Hash解法 上面耗时间：找任意指针S1。用HashMap建立(N, N1)的配对，就能够\\(O(1)\\)查找到S1。从而总时间为\\(O(n)\\) 思路3 最优-新旧链表先连再断 连接新节点。把N1连接到N后面，a-a1-b-b1-c-c1 设置随机节点。设置N1的S1，实际上，a-s, a-a1，s-s1 ，所以能够找到a1-s1 新旧节点断开。把N和N1断开，得到a1-b1-c1 注意：设置随机节点时，随机节点可能为空。断链时，下一个节点可能为空。 [关键代码] 123456789101112131415161718192021222324252627282930313233343536373839404142434445RandomListNode* clone(RandomListNode* head) &#123; if (head == nullptr) &#123; return nullptr; &#125; // 1. 新建节点，连接到原节点的后面 RandomListNode* p = head; while (p) &#123; RandomListNode* p1 = new RandomListNode(p-&gt;label); // 连接 p1-&gt;next = p-&gt;next; p-&gt;next = p1; p = p1-&gt;next; &#125; // 2. 为新节点设置随机节点 p = head; while (p) &#123; RandomListNode* p1 = p-&gt;next; RandomListNode* s = p-&gt;random; // 注意random可能为空 if (s != nullptr) &#123; p1-&gt;random = s-&gt;next; &#125; p = p1-&gt;next; &#125; // 3. 新旧节点断开 p = head; RandomListNode* head1 = p-&gt;next; while (p) &#123; // 新节点 RandomListNode* p1 = p-&gt;next; // 原节点，连接到原下一个节点 p-&gt;next = p1-&gt;next; // 新节点的下一个节点，可能下一个节点为空 if (p-&gt;next) &#123; p1-&gt;next = p-&gt;next-&gt;next; &#125; else &#123; p1-&gt;next = nullptr; &#125; p = p-&gt;next; &#125; return head1;&#125; 二叉搜索树转双向链表-36 给一个二叉搜索树，转化为一个排好序的双向链表。左孩子-前一个节点，右孩子-后一个节点。 牛客网二叉搜索树与双向链表 ， 类似题型：有序链表转平衡BST BST的中序遍历就是自动有序的。 递归思路 中序遍历，使用pre来记录链表中的最后一个元素。 遍历到根节点时，递归转换左子树 pre与root连接，pre.right=root, root.left=pre, pre=root 。注意pre为空时，pre=root 递归创建右子树 再从pre找到第一个节点。 [关键代码] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/* * 把树转化为链表，递归版本 * Args: * root -- 树 * Returns: * 转换后的链表的头结点 */TreeNode* convert(TreeNode* root) &#123; if (root == nullptr) &#123; return nullptr; &#125; TreeNode* pre = nullptr; // 转换为链表，pre为最后一个节点 convert_inorder(root, pre); // 从末尾找到第一个节点 while (pre &amp;&amp; pre-&gt;left) &#123; pre = pre-&gt;left; &#125; return pre;&#125;/* * 递归转换 * Args: * root -- 当前节点 * pre -- 上一个节点，引用类型，改变值。 * Returns: * None */void convert_inorder(TreeNode* root, TreeNode* &amp;pre) &#123; if (root == nullptr) &#123; return; &#125; // 左子树 convert_inorder(root-&gt;left, pre); // 当前节点 if (pre != nullptr) &#123; root-&gt;left = pre; pre-&gt;right = root; pre = root; &#125; else &#123; pre = root; &#125; // 右子树 convert_inorder(root-&gt;right, pre);&#125; 非递归思路 中序遍历时，记录pre节点，每次进行修改即可，访问到p时，则连接到pre即可。 p不为空，p入栈，p=p.left， 扫描左孩子 p为空，p从栈顶获得，p=st.top()， 显然p没有左孩子或者左孩子已经出栈遍历过，p访问出栈。此时，把p与pre连接即可， 注意pre为空 扫描右孩子，p = p.right 最后从末尾，找到头结点 12345678910111213141516171819202122232425262728293031323334353637TreeNode* convert_stack(TreeNode* root) &#123; if (root == nullptr) &#123; return nullptr; &#125; stack&lt;TreeNode*&gt; st; TreeNode* p = root; // 上一个节点 TreeNode* pre = nullptr; while (p || !st.empty()) &#123; if (p) &#123; // p入栈 st.push(p); // 扫描左孩子 p = p-&gt;left; &#125; else &#123; // p为空，出栈元素，p为根节点，左孩子已经访问结束或者没有左孩子 p = st.top(); st.pop(); if (pre == nullptr) &#123; pre = p; &#125; else &#123; pre-&gt;right = p; p-&gt;left = pre; pre = p; &#125; // 扫描右孩子 p = p-&gt;right; &#125; &#125; // 找到头结点 while (pre &amp;&amp; pre-&gt;left) &#123; pre = pre-&gt;left; &#125; return pre;&#125; 序列化和反序列化二叉树-37 leetcode序列化和反序列化二叉树笔记 数字全排列-38 leetcode数字全排列笔记 数组中出现次数超过一半的数-39 Leetcode笔记 最小的k个数-40 牛客最小的k个数 给一个数组，找到最小的k个数 思路1 快排思路 修改原数组。partition左边即可。O(n) 。快速排序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/* * 通过快速排序来找到最小的k个数，改变原数组 * Args: * a -- 数组 * k -- 最小的k个数 * Returns: * res -- 最小的k个数 */vector&lt;int&gt; get_leastnums_by_partition(vector&lt;int&gt;&amp; a, int k) &#123; // 条件合法性判断 if (a.empty() || k &lt; 1 || k &gt; a.size()) &#123; return vector&lt;int&gt;(); &#125; int l = 0, r = a.size() - 1; int i = partition(a, l, r); // 直到左边是最小的k个数，包含a[i] while (i + 1 != k) &#123; if (i + 1 &gt; k) &#123; // 左边有超过k个数，左边继续划分 r = i - 1; &#125; else if (i + 1 &lt; k) &#123; // 左边不足k个，划分右边，加一些给左边 l = i + 1; &#125; i = partition(a, l, r); &#125; // 把左边给到res中 vector&lt;int&gt; res(k); std::copy(a.begin(), a.begin() + k, res.begin()); return res;&#125;/* * 快排的partition，左边小于，中间x，右边大于 * Args: * a -- 数组 * l -- 左边起始值 * r -- 右边结束值 * Returns: * a[l]的最终位置 */int partition(vector&lt;int&gt;&amp; a, int l, int r) &#123; int x = a[l]; while (l &lt; r) &#123; // 从右向左，找到小于x的值，放到a[l]上 while (l &lt; r &amp;&amp; a[r] &gt;= x) &#123; r--; &#125; if (l &lt; r) &#123; a[l++] = a[r]; &#125; // 从左向右，找到大于x的值，放到a[r]上 while (l &lt; r &amp;&amp; a[l] &lt;= x) &#123; l++; &#125; if (l &lt; r) &#123; a[r--] = a[l]; &#125; &#125; a[l] = x; return l;&#125; 思路2 最大堆 不修改原数组，用最大堆找到最小的k个数。O(nlogk) 。堆排序 123456789101112131415161718192021222324252627282930/* * 通过最大堆来获得数组中最小的k个数 */vector&lt;int&gt; get_leastknums_by_heap(vector&lt;int&gt;&amp; a, int k) &#123; if (a.empty() || k &lt;= 0 || k &gt; a.size()) &#123; return vector&lt;int&gt;(); &#125; // 选择前k个元素，初始化堆 vector&lt;int&gt; res(k); std::copy(a.begin(), a.begin() + k, res.begin()); std::make_heap(res.begin(), res.end()); // 剩余元素入堆 for (auto it = a.begin() + k; it != a.end(); it++) &#123; auto n = *it; printf(\"n=%d, max=%d\\n\", n, res[0]); // 大于最大值，无需入堆 if (n &gt;= res[0]) &#123; continue; &#125; // n小于最大堆的最大值，入堆 // 最大元素出堆，默认放到末尾 std::pop_heap(res.begin(), res.end()); // 新元素入堆 res[k - 1] = n; std::push_heap(res.begin(), res.end()); &#125; return res;&#125;","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"}]},{"title":"dfs","date":"2017-12-31T07:20:28.000Z","path":"2017/12/31/algorithm-dfs/","text":"搜索算法 解空间树 子集树 比如n种可选物品的01背包，对其进行选择。解空间为长度为n的01向量表示。解空间树是一颗完全二叉树。 排列树 比如旅行商问题，对n进行排列。 解空间搜索方式 DFS 深度优先。回溯法。 BFS 广度优先或最小耗费优先。分支限界法。 在扩展节点处，先生成所有的儿子节点，再从活节点列表中选择一个最有利的节点作为新的扩展节点。 回溯法 就是穷举，深度优先去找到解空间中满足约束条件的所有解，再选择一个最好的出来。 类似于走迷宫：走到死路或者已经达不到最优解就及时回头。要注意剪枝。 搜索子集树 12345678910111213void dfs(int t) &#123; if (t &gt; n) &#123; output(x); return; &#125; // 0和1两种选择 for (int i = 0; i &lt;= 1; i++) &#123; x[t] = i; if (legal(t)) &#123; bfs(t+1); &#125; &#125;&#125; 搜索排列树 1234567891011121314void dfs(int t) &#123; if (t &gt; n) &#123; output(x); return; &#125; // 遍历所有t的可能：还剩下的可选择的内容，前面t-1已经确定了 for (int i = t; i &lt;= n; i++) &#123; swap(x[t], x[i]); if (legal(t)) &#123; bfs(t + 1); &#125; swap(x[t], x[i]); &#125;&#125; 分支限界法 广度优先或者最小消费优先去搜索解空间树，去找到使目标函数达到极大或者极小解，某种意义下的最优解。 每个活节点只有一次机会成为扩展节点，成为时，就一次性产生其所有儿子节点。舍弃不可行或导致非最优解的儿子节点，其余儿子节点加入活节点列表。从活节点列表中取出下一个节点作为新的扩展节点。 广度优先搜索 队列式分支限界法 最小消费优先搜索 优先队列式分支限界法。每个活节点有一个优先级。通常用最大堆来实现最大优先队列。 装载问题 问题描述 \\(n\\)个集装箱质量分别为\\(w_i\\)， 要装上总容量为\\(c\\)的轮船。 问，怎样装，才能使装得最多？ \\[ \\begin{align} &amp; \\max \\sum_{i}^nx_i\\cdot w_i \\\\ &amp; \\sum_{i}^nx_i\\cdot w_i \\le c \\\\ &amp; x_i \\in\\{0,1\\}, \\quad \\text{表示装或不装} \\end{align} \\] 贪心解法 贪心策略：集装箱从轻到重排序，轻者先装， 直到超重。 BFS 有2艘轮船，\\(\\sum_i^n w_i \\le c_1 + c_2\\)， 怎样才能把n个集装箱装到这2艘轮船。 可知最优方案：尽量把第一艘轮船装满，剩余的装到第二艘上。 定义Solution 12345678910111213141516171819202122232425262728class Solution &#123; public: // 集装箱数量 int n; // 集装箱的重量 vector&lt;int&gt; w; // 船的载重 int c; // 当前重量 int cw; // 最优重量 int bestw; // 回溯遍历index=i的箱子 void backtrack(int i); // 构造函数 Solution(const vector&lt;int&gt; &amp;w, int c):w(w), c(c) &#123; this-&gt;cw = 0; this-&gt;bestw = 0; this-&gt;n = w.size(); &#125;&#125;;int max_loading(const vector&lt;int&gt; &amp;w, int c) &#123; Solution solu(w, c); solu.backtrack(0); return solu.bestw;&#125; 无剪枝的dfs 12345678910111213141516void Solution::backtrack(int i) &#123; if (i == n) &#123; if (cw &gt; bestw) &#123; bestw = cw; &#125; return; &#125; // 选择i if (cw + w[i] &lt;= c) &#123; cw = cw + w[i]; backtrack(i + 1); cw = cw - w[i]; &#125; // 不选择i backtrack(i + 1);&#125; 剪枝的dfs 进入下一步遍历之前，检查\\(\\rm{cw+rest &gt; bestw}\\) ，如果剩下的所有加上都小于最优解的话，那么就不用进入了。 12345678910111213141516171819202122232425262728/* * 剪枝的dfs */void Solution::dfs(int i) &#123; if (i == n) &#123; if (cw &gt; bestw) &#123; bestw = cw; &#125; return; &#125; // 剩余 r = r - w[i]; // 选择i if (cw + w[i] &lt;= c) &#123; cw += w[i]; // 剪枝 if (cw + r &gt; bestw) &#123; dfs(i + 1); &#125; cw -= w[i]; &#125; // 不选择i，剪枝 if (cw + r &gt; bestw) &#123; dfs(i + 1); &#125; r = r + w[i];&#125;","tags":[{"name":"搜索","slug":"搜索","permalink":"http://plmsmile.github.io/tags/搜索/"}]},{"title":"树的总结","date":"2017-12-29T06:25:09.000Z","path":"2017/12/29/trees/","text":"二叉树 二叉树的性质 第i层，节点最多\\(2^{i-1}\\)个 深度为k的二叉树，最多有\\(2^k-1\\)个 节点 二叉树有n个节点，高度至少为\\(\\log_2(n+1)\\) \\(n_0 =n_2 + 1\\)，叶子节点和度为2的节点的关系 种类 满二叉树 高为h，有\\(2^h-1\\)个节点 完全二叉树 二叉查找树 \\(\\rm{left &lt; root &lt; right}\\) ， 没有相等的节点 二叉平衡树 左右子树的高度差的绝对值小于等于1 二叉查找树","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"},{"name":"树","slug":"树","permalink":"http://plmsmile.github.io/tags/树/"},{"name":"数据结构","slug":"数据结构","permalink":"http://plmsmile.github.io/tags/数据结构/"}]},{"title":"leetcode-01","date":"2017-12-29T06:08:03.000Z","path":"2017/12/29/leetcode-01/","text":"有序列表转二叉查找树-109 Convert Sorted List to Binary Search Tree 二叉树的最小深度-111 Minimum Depth of Binary Tree 最小深度：根节点到某个叶子节点的最短路径。 为空，返回0 左孩子为空，则结果在右孩子 右孩子为空，则结果在左孩子 左右均不为空，返回小的+1 二叉树遍历 先序遍历-144 Binary Tree Preorder Traversal 根、左、右。栈。 先把根节点入栈 栈不为空时，出栈一个元素 访问该元素，右孩子进栈，左孩子进栈。 因为出栈，先出左孩子，再出右孩子。 关键代码 1234567891011121314151617181920vector&lt;int&gt; pre_order(TreeNode* root) &#123; vector&lt;int&gt; vpre; stack&lt;TreeNode*&gt; st; if (root != nullptr) &#123; st.push(root); &#125; while (!st.empty()) &#123; TreeNode* p = st.top(); vpre.push_back(p-&gt;val); st.pop(); // 右进、左进；出时：左先出 if (p-&gt;right) &#123; st.push(p-&gt;right); &#125; if (p-&gt;left) &#123; st.push(p-&gt;left); &#125; &#125; return vpre;&#125; 中序遍历-094 Binary Tree Inorder Traversal 思路 左、根、右。使用栈。 p=root p不为空，p入栈，一直向左走p = p.left，扫描它的左孩子，所有左孩子依次入栈 p为空时，p = st.top() ，p位于栈顶，显然没有左孩子或者左孩子已经遍历过，p访问出栈。 扫描右孩子 p = p.right 从根节点开始，一直向左，所有的左孩子入栈， 出栈一个节点，访问，它的右孩子入栈。 关键代码 123456789101112131415161718192021vector&lt;int&gt; inorder_traversal(TreeNode* root) &#123; vector&lt;int&gt; res; stack&lt;TreeNode*&gt; st; TreeNode* p = root; while (p || !st.empty()) &#123; if (p) &#123; // 根节点入栈 st.push(p); // 扫描左孩子 p = p-&gt;left; &#125; else &#123; // p位于栈顶，左孩子已经被遍历过或者没有左孩子，直接出栈访问 p = st.top(); res.push_back(p-&gt;val); st.pop(); // 扫描右孩子 p = p-&gt;right; &#125; &#125; return res;&#125; 后序遍历-145 Binary Tree Postorder Traversal 思路 左孩子、右孩子、根节点。使用栈。使用pre记录上一次遍历的节点。 根节点入栈 栈不为空，访问栈顶元素p 直接访问p的条件：p没有左右孩子 or 左右孩子刚刚遍历结束，只要pre是左或者右孩子即可 p可以直接访问，则访问出栈 p不能直接访问，则左右孩子入栈 关键代码 12345678910111213141516171819202122232425262728293031323334vector&lt;int&gt; post_order(TreeNode* root) &#123; vector&lt;int&gt; res; stack&lt;TreeNode*&gt; st; // 前一次访问的节点 TreeNode* pre = nullptr; if (root != nullptr) &#123; st.push(root); &#125; while (!st.empty()) &#123; TreeNode* p = st.top(); // 0. 检查是否可以直接访问p bool no_child = (p-&gt;left == nullptr &amp;&amp; p-&gt;right == nullptr); bool pre_is_child = (pre == p-&gt;left || pre == p-&gt;right); if (nullptr == pre) &#123; pre_is_child = false; &#125; // 1. p无左右子树 or 左右子树刚刚遍历完，直接访问p if (no_child || pre_is_child) &#123; res.push_back(p-&gt;val); pre = p; st.pop(); &#125; // 2. 需要将p的左右孩子入栈 else &#123; if (p-&gt;right) &#123; st.push(p-&gt;right); &#125; if (p-&gt;left) &#123; st.push(p-&gt;left); &#125; &#125; &#125; return res;&#125; 层次遍历-102 层次遍历，使用队列。 从上到下 Binary Tree Level Order Traversal 和从下到上 Binary Tree Level Order Traversal II。 如果只需要顺序放在一个数组里面，则不需要分层，直接层次遍历即可。 但是此题，需要分层构建vector。 数量记录思路 不是很好。 队列层次遍历 当前层在队列中的数量：cur_remain 下一层的数量：next_level cur_remain == 0时， 就切换到下一层 [关键代码] 1234567891011121314151617181920212223242526272829303132333435363738vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (root == nullptr) &#123; return res; &#125; queue&lt;TreeNode*&gt; q; q.push(root); res.push_back(vector&lt;int&gt;()); // 当前层，在队列里面的元素数量 int cur_remain = 1; // 下一层的元素数量 int next_level = 0; while (!q.empty()) &#123; TreeNode* now = q.front(); q.pop(); // 存入队列 res[res.size() - 1].push_back(now-&gt;val); // 左右孩子入队 if (now-&gt;left) &#123; q.push(now-&gt;left); next_level++; &#125; if (now-&gt;right) &#123; q.push(now-&gt;right); next_level++; &#125; // 当前层数量-- cur_remain--; // 切换到下一层 if (cur_remain == 0 &amp;&amp; !q.empty()) &#123; res.push_back(vector&lt;int&gt;()); cur_remain = next_level; next_level = 0; &#125; &#125; return res;&#125; 一次遍历一层的思路 很好，掌握！ 一次while循环，保证当前队列里面只有当前层的元素，用vector记录当前层的序列 q.size() 获得当前层元素数量，然后本次循环，只从队列里面出这么多元素。 依次遍历当前层的所有元素，出队，同时左右孩子入队 q为空，则所有层遍历结束 123456789101112131415161718192021222324252627282930/* * 保证当前队列的循环只有当前层的 */vector&lt;vector&lt;int&gt;&gt; level_order(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (root == nullptr) &#123; return res; &#125; queue&lt;TreeNode*&gt; q; q.push(root); // 一次while循环，出掉当前层的所有元素，下一层的元素全部入队 while (!q.empty()) &#123; // 当前层的数量和遍历结果序列 int level_num = q.size(); vector&lt;int&gt; curv; for (int i = 0; i &lt; level_num; i++) &#123; TreeNode* p = q.front(); // p的左右孩子入队列 if (p-&gt;left) q.push(p-&gt;left); if (p-&gt;right) q.push(p-&gt;right); // p出队，放到当前层的vector中 q.pop(); curv.push_back(p-&gt;val); &#125; // 放到末尾，就是从下到上 // res.insert(res.begin(), curv); res.push_back(curv); &#125; return res;&#125; 发现重复的数字-287 Find the Duplicate Number， 类似于aim2offer中查找重复的数字 数组a，有n+1个数，都在[1,n]范围内，只有一个重复的元素。找到它 二分思路 [1, n]这个范围有n个数。划分为两个范围[1, m]和[m+1, n] 每次去遍历整个数组，统计两个范围内的数字的数目 统计整个数组中元素在[1, m]范围内的个数\\(c_1\\) 统计整个数组中元素在[m+1, n]范围内的个数\\(c_2\\) [1, m]这m个数字的数量是c 如果 c &gt; m， 则1, m]内一定存在重复的数，e = m 否则，[m+1, n]一定存在重复的数，s = m + 1 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * 找到一个重复的数字 * Args: * a -- 数组，n+1个元素，范围[1,n]，至少有一个重复的数字 * Returns: * dup -- 重复的数字 */int find_duplicate(const vector&lt;int&gt;&amp; a) &#123; int n = a.size() - 1; if (n &lt;= 0) &#123; return 0; &#125; int l = 1; int r = n; int dup = -1; // 不断缩小范围 while (l &lt;= r) &#123; int m = (l + r) &gt;&gt; 1; // 统计[l,m]在a中的出现次数 int count = count_range(a, l, m); if (l == r) &#123; if (count &gt;= 2) &#123; dup = l; break; &#125; &#125; // [l, m]有重复的 if (count &gt; (m - l + 1)) &#123; r = m; &#125; // [m+1, r]有重复的 else &#123; l = m + 1; &#125; &#125; return dup;&#125;/* * 找到数组a中，[min, max]这些数的出现次数 */int count_range(const vector&lt;int&gt;&amp; a, int min, int max) &#123; int count = 0; for (int i = 0; i &lt; a.size(); i++) &#123; if (a[i] &gt;= min &amp;&amp; a[i] &lt;= max) &#123; count++; &#125; &#125; return count;&#125; 合并两条有序链表-021 Merge Two Sorted Lists ，和归并排序的Merge操作 很类似。 考虑鲁棒性 思路 \\(l_1\\)与\\(l_2\\)若有一个为空的，则返回另一个 初始化新的head，选择\\(l_1\\)与\\(l_2\\)中第一个节点较小的那个 while循环，谁小选谁 结束之后，直接把未空的链表链接上即可 关键代码 123456789101112131415161718192021222324252627282930313233343536373839ListNode* mergeTwoLists(ListNode* l1, ListNode* l2) &#123; if (l1 == nullptr) &#123; return l2; &#125; if (l2 == nullptr) &#123; return l1; &#125; // 初始化head ListNode* head = nullptr; if (l1-&gt;val &lt; l2-&gt;val) &#123; head = l1; l1 = l1-&gt;next; &#125; else &#123; head = l2; l2 = l2-&gt;next; &#125; // 遍历两条链表，每次选择小的追加到p的后面 ListNode* p = head; while (l1 &amp;&amp; l2) &#123; if (l1-&gt;val &lt; l2-&gt;val) &#123; p-&gt;next = l1; l1 = l1-&gt;next; &#125; else &#123; p-&gt;next = l2; l2 = l2-&gt;next; &#125; p = p-&gt;next; &#125; // 某一条链表还有剩余 if (l1) &#123; p-&gt;next = l1; &#125; if (l2) &#123; p-&gt;next = l2; &#125; return head;&#125; 合并多条有序链表-023 Merge k Sorted Lists 我们已经会合并2条链表了，可以使用归并排序和二分查找的思想来合并多个列表。 示例 现在有1, 2, 3, 4, 5, 6条链表 第一步：1-6，2-5，3-4合并，得到新的1, 2, 3 第二步：1-3合并，2不动，得到新的1, 2 第三步：1-2合并， 得到新的2， 合并完成 返回list[0] 总结 直到len==1 合并到只有一条链表，合并到list[0] 对于当前len，折半两两合并，i和len-i-1合并，放到前面lists[i] len缩减一半，len=(len+1)/2 关键代码 12345678910111213141516ListNode* mergeKLists(vector&lt;ListNode*&gt;&amp; lists) &#123; if (lists.empty()) &#123; return nullptr; &#125; int len = lists.size(); // 直到只有一条链表 while (len &gt; 1) &#123; // 依次合并前后两条链表 for (int i = 0; i &lt; len / 2; i++) &#123; // 合并放到list[i] lists[i] = mergeTwoLists(lists[i], lists[len - i - 1]); &#125; len = (len + 1) / 2; &#125; return lists[0];&#125; Z型打印二叉树-103 Binary Tree Zigzag Level Order Traversal 参考层次遍历 一次遍历一层的思路。 一次遍历一层，得到当前层的遍历结果 单数，从左向右；偶数，从右向左 每次遍历一个元素，把左右孩子入队 根节点，向右走 第二层，向左走 向右走，从队头出，孩子先左后右，加到队尾 向左走，从队尾出，孩子先右后左，加到队首 两个栈的思路 栈1初始存放根节点，栈2为空 向右走，栈1全部出栈，先左后右孩子依次压入栈2，栈底-栈顶，栈2为2 3 向左走，栈2全部出栈，先右后左孩子依次压入栈1，栈1为7 6 5 4 向右走，栈1出栈，左右孩子依次压入栈2，栈2为8 9 10 11 12 13 14 15 向左走，栈2出栈，结束 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041/* * 使用两个栈z型层次打印二叉树 */vector&lt;vector&lt;int&gt;&gt; zigzagLevelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (root == nullptr) &#123; return res; &#125; stack&lt;TreeNode*&gt; st1; stack&lt;TreeNode*&gt; st2; st1.push(root); while (!st1.empty() || !st2.empty()) &#123; // 向右走 vector&lt;int&gt; curv; if (!st1.empty()) &#123; while (!st1.empty()) &#123; TreeNode* p = st1.top(); st1.pop(); curv.push_back(p-&gt;val); if (p-&gt;left) st2.push(p-&gt;left); if (p-&gt;right) st2.push(p-&gt;right); &#125; &#125; // 向左走 else &#123; while (!st2.empty()) &#123; TreeNode* p = st2.top(); st2.pop(); curv.push_back(p-&gt;val); if (p-&gt;right) st1.push(p-&gt;right); if (p-&gt;left) st1.push(p-&gt;left); &#125; &#125; res.push_back(curv); &#125; return res;&#125; 二叉树路径求和-112.113.437 题目1 从根节点到叶子求和-112 Path Sum， EASY 给一颗二叉树和一个sum值，判断是否有从根节点到叶子节点的路径，使得路径上的节点求和等于sum 思路 做减法 根节点为空，False 没有孩子，判断root.val == sum 有孩子，把sum减掉根节点的值，去判断左右子树是否有 ，sum = sum - root.val 关键代码 12345678910111213bool hasPathSum(TreeNode* root, int sum) &#123; // 1. 节点为空 if (root == nullptr) &#123; return false; &#125; // 2. 没有左右子树，直接判断 if (!root-&gt;left &amp;&amp; !root-&gt;right) &#123; return root-&gt;val == sum; &#125; // 3. 减小sum，去递归判断左右子树 int newsum = sum - root-&gt;val; return hasPathSum(root-&gt;left, newsum) || hasPathSum(root-&gt;right, newsum);&#125; 题目2 从根节点到叶子节点求和-保存路径-113 Path Sum-113， Medium 给二叉树和sum值，找到root-to-leaf的路径，使得和为sum。保存该路径 思路 用vector&lt;int&gt; path 来记录当前路径， vector&lt;vector&lt;int&gt;&gt; res 记录最终结果 根节点为空，返回 到达叶子节点，val == sum， 把当前节点加入path，当前path加入res， 否则返回 非叶子节点，把当前节点加入path，去左右子树中遍历，继续追加path直到叶子节点 非叶子节点，结束后，把当前节点从path中删除 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* * 找到树种从root到leaf的所有和为sum的路径 */vector&lt;vector&lt;int&gt;&gt; pathSum(TreeNode* root, int sum) &#123; vector&lt;vector&lt;int&gt;&gt; res; if (root == nullptr) &#123; return res; &#125; vector&lt;int&gt; path; find_path(root, sum, path, res); return res;&#125;/* * 递归遍历节点，逐渐添加节点到当前的path，叶子节点，满足要求时，则把path追加到res中 * Args: * root -- 当前节点 * path -- 当前路径 * res -- 所有路径 * Returns: * None */void find_path(TreeNode* root, int sum, vector&lt;int&gt;&amp; path, vector&lt;vector&lt;int&gt;&gt; &amp;res) &#123; if (root == nullptr) &#123; return; &#125; // 到达叶子节点 if (!root-&gt;left &amp;&amp; !root-&gt;right) &#123; if (root-&gt;val == sum) &#123; path.push_back(root-&gt;val); res.push_back(path); path.pop_back(); &#125; return; &#125; // 当前节点加到path中 path.push_back(root-&gt;val); // 更新sum，到左右子树中去添加path int newsum = sum - root-&gt;val; if (root-&gt;left) &#123; find_path(root-&gt;left, newsum, path, res); &#125; if (root-&gt;right) &#123; find_path(root-&gt;right, newsum, path, res); &#125; // 当前节点从path中移除 path.pop_back();&#125; 题目3 求二叉树的所有和为sum的路径，任意起始节点-437 给一颗二叉树和sum，求出所有和为sum的路径数量，从任意节点开始和结束。 思路 层次遍历，以每一颗节点为起始值，找到以它开始的路径数量 节点为空，0 无孩子，不相等，0 无孩子，相等，1 有孩子，相等，c = 1， 不相等c = 0。 更新sum，继续递归查找左右孩子的count。返回c+count 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940/* * 找到树中，和为sum的所有路径数量 */int pathSum(TreeNode* root, int sum) &#123; if (root == nullptr) &#123; return 0; &#125; queue&lt;TreeNode*&gt; q; q.push(root); int count = 0; // 前序遍历 while (!q.empty()) &#123; TreeNode* now = q.front(); q.pop(); count += count_from_root(now, sum); if (now-&gt;left) q.push(now-&gt;left); if (now-&gt;right) q.push(now-&gt;right); &#125; return count;&#125;/* * 以root为起始节点，向下走，和为sum的路径的条数 */int count_from_root(TreeNode* root, int sum) &#123; // 空 if (root == nullptr) return 0; // 直接根节点就满足，无需看孩子 int c = 0; if (sum == root-&gt;val) &#123; // 相等 c = 1; &#125; else if (!root-&gt;left &amp;&amp; !root-&gt;right) &#123; // 无孩子，不相等 return 0; &#125; // c+左右孩子的 int newsum = sum - root-&gt;val; return c + count_from_root(root-&gt;left, newsum) + count_from_root(root-&gt;right, newsum);&#125; 有序链表转平衡BST-109 Convert Sorted List to Binary Search Tree， Medium。 类似题型：BST转有序双向链表 给一个有序链表，转化为平衡的二叉搜索树 思路 有序 -- BST的中序遍历；平衡 -- 以中间节点为根节点，分为左右子树递归去创建。 计算总结点数量 -- size， 递归去构建树go(0, size - 1) go(head, start, end) ，计算出中间节点mid-node， 构造树根root 左孩子root.left = go(head, start, mid-1)， 右孩子root.right = go(now.next, mid+1, end) 返回当前树根节点root 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * 把有序链表转化为平衡的BST * Args: * head -- 链表 * Returns: * root -- 树的头结点 */ TreeNode* sortedListToBST(ListNode* head) &#123; if (head == nullptr) &#123; return nullptr; &#125; int size = 0; ListNode* p = head; while (p) &#123; ++size; p = p-&gt;next; &#125; return create_tree(head, 1, size);&#125;/* * 递归中序创建树 * Args: * head -- 链表头节点 * start -- 起始节点编号，从1开始 * end -- 结束节点编号 * Returns: * root -- 树的根节点 */TreeNode* create_tree(ListNode* head, int start, int end) &#123; // 0. 递归终止条件 if (head == nullptr || start &gt; end) &#123; return nullptr; &#125; // 1. 找到中间节点，构建根节点 int mid = (end + start) / 2; ListNode* node = head; for (int i = start + 1; i &lt;= mid; i++) &#123; node = node-&gt;next; &#125; TreeNode* root = new TreeNode(node-&gt;val); // 2. 递归构造左右子树 root-&gt;left = create_tree(head, start, mid - 1); root-&gt;right = create_tree(node-&gt;next, mid + 1, end); return root;&#125; 序列化二叉树-297 Serialize and Deserialize Binary Tree， Hard 把二叉树序列化为字符串，把字符串反序列化为一棵树 思路 前序遍历来保存序列，保存成一颗完全二叉树，空节点用$表示，使用空格进行分割。 序列化 序列化为一颗完全二叉树，先序递归。遇到空指针，则用$代替 先把字符放到stringstream里面，&lt;&lt;输入， 最后s.str()得到字符串 12345678910111213141516171819202122232425262728293031323334/* * 把一棵树序列化为一个字符串，前序完全二叉树序列 * Args: * root -- 树 * Returns: * str -- 序列化后的字符串 */string serialize(TreeNode* root) &#123; if (root == nullptr) &#123; return \"\"; &#125; stringstream buf; build_string(root, buf); return buf.str();&#125;/* * 递归把二叉树序列化到buf字符串中 * Args: * root -- 当前的根节点 * buf -- 字符串buffer * Returns: * None，都写到了buf中 */void build_string(TreeNode* root, stringstream&amp; buf) &#123; if (root == nullptr) &#123; buf &lt;&lt; \"$\" &lt;&lt; \" \"; return; &#125; buf &lt;&lt; root-&gt;val &lt;&lt; \" \"; build_string(root-&gt;left, buf); build_string(root-&gt;right, buf); return;&#125; 从字符串中解析得到序列，存到队列中 123456789101112/* * 分割字符串，把字符写到容器q里面 */void split(const string&amp; str, queue&lt;string&gt; &amp;q, const char delim = ' ') &#123; istringstream input; input.str(str); string line; while (std::getline(input, line, delim)) &#123; q.push(line); &#125; return;&#125; 反序列化 得到队列序列之后，可以对其进行递归反序列化构建树。先序序列，不是层次序列。 根-左-右，队列。出队，建立根节点 左-右，队列，递归建立左孩子 右，队列，建立右孩子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/* * 把String解析为一棵树 * Args: * data -- 序列化后的字符串 * Returns: * root -- 树 */TreeNode* deserialize(const string&amp; data) &#123; if (data.empty()) &#123; return nullptr; &#125; // 先序序列 queue&lt;string&gt; preorder; split(data, preorder); return build_tree(preorder);&#125;/* * 递归先序构造树 * Args: * prev -- 先序遍历序列 * Retursn: * root -- prev[i]为根构建的树 */TreeNode* build_tree(queue&lt;string&gt;&amp; pres) &#123; if (pres.size() == 0) &#123; return nullptr; &#125; string val = pres.front(); pres.pop(); // 当前为空节点 if (val == \"$\") &#123; return nullptr; &#125; // 有值 TreeNode* root = new TreeNode(std::stoi(val)); // 递归按照顺序构建左右子树 root-&gt;left = build_tree(pres); root-&gt;right = build_tree(pres); return root;&#125; 数字全排列-046 Permutations, Medium。 搜索树 给一个数组，返回全排列。每个数字都不相同 思路 全排列回溯法搜索。一个数组，搜索第t层的时候 前面t-1层都已经ok 遍历后面的所有元素，给到t层，去搜索 每次进行交换 github代码 123456789101112131415161718192021222324252627282930/* * 数组的全排列 */vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt; &amp;nums) &#123; vector&lt;vector&lt;int&gt;&gt; res; dfs(nums, 0, res); return res;&#125;/* * 回溯搜索排列树，遍历当前第i层的所有可能性，前面i-1已经全部确定好 * Args: * t -- 第几层，[0, n-1] * path -- 当前路径，[0,i-1]已经确定好，[i,n-1]是剩余的数字，遍历每一种可能给到i * res -- 总的结果 * Returns: * None */void dfs(vector&lt;int&gt;&amp; path, int t, vector&lt;vector&lt;int&gt;&gt;&amp; res) &#123; if (t &gt;= path.size()) &#123; res.push_back(path); return; &#125; for (int i = t; i &lt; path.size(); i++) &#123; std::swap(path[t], path[i]); dfs(path, t + 1, res); std::swap(path[t], path[i]); &#125;&#125; 重复数字全排列-047 重复数字全排列-047 给一个数组，里面有一些重复的数字，给出所有的排列可能 思路 重复的原因：当为t设置值的时候，遍历后面的所有元素给t赋值，但是后面都有一些重复的数值。 比如说 1 2 1 1， 开始是1，1会与最后的两个1再进行交换，然而其实是一样的。没必要了。 每次遍历交换的时候，只交换遍历后面不重复的元素。 github代码 123456789101112131415161718192021222324252627282930/* * 回溯搜索排列树，遍历当前第i层的所有可能性，前面i-1已经全部确定好 * Args: * t -- 第几层，[0, n-1] * path -- 当前路径，[0,i-1]已经确定好，[i,n-1]是剩余的数字，遍历每一种可能给到i * res -- 总的结果 * Returns: * None */void dfs(vector&lt;int&gt;&amp; path, int t, vector&lt;vector&lt;int&gt;&gt;&amp; res) &#123; if (t &gt;= path.size()) &#123; res.push_back(path); return; &#125; // 不重复的元素与其索引 set&lt;int&gt; vals; set&lt;int&gt; idx; for (int i = t; i &lt; path.size(); i++) &#123; if (vals.find(path[i]) == vals.end()) &#123; vals.insert(path[i]); idx.insert(i); &#125; &#125; for_each(idx.begin(), idx.end(), [&amp;](int i) &#123; std::swap(path[t], path[i]); dfs(path, t + 1, res); std::swap(path[t], path[i]); &#125;);&#125; 组合问题 给n个字符，要组成m个字符，问有多少种组成方法 有点类似于01背包的选择。 选择第一个字符，则在后面选择m-1个字符 不选择第一个字符，则在后面选择m个字符 正方体顶点和相等问题 给8个数字，正方体有8个顶点，数字放在顶点上。使得3对对面的顶点和相等。 也就是搜索，然后限定一些条件。 8皇后问题 8*8的象棋摆8个皇后，任意两个皇后不能在同一行、同一列或同一对角线上。问有多少种摆法 N皇后问题-051 n*n的棋盘摆n个皇后，任意两个皇后不能在同一行、同一列或同一对角线上。返回摆法。 N-Queens 每一行一个皇后，每一行有n个选择。就去dfs搜索所有的排列树。 path[t]=k， 第t行的皇后在第k列 k不能在前面皇后的：同一列、主对角线、副对角线 。别忘记副对角线。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/* * 返回n皇后的解法 */vector&lt;vector&lt;string&gt;&gt; solveNQueens(int n) &#123; // 1. 获得所有可能的位置 vector&lt;vector&lt;int&gt;&gt; locations; vector&lt;int&gt; path(n); std::iota(path.begin(), path.end(), 0); dfs(0, n, path, locations); // 2. 构造返回结果 vector&lt;vector&lt;string&gt;&gt; res; for (auto loc : locations) &#123; vector&lt;string&gt; solu; for (int i : loc) &#123; string line(n, '.'); line[i] = 'Q'; solu.push_back(line); &#125; res.push_back(solu); &#125; show(res[0]); return res;&#125;/* * dfs，设置t行的皇后位置 * Args: * t -- 第t行，从0开始 * n -- n皇后 * path -- 当前的路径方案 * res -- 总的方案 * Returns: * None */void dfs(int t, int n, vector&lt;int&gt;&amp; path, vector&lt;vector&lt;int&gt;&gt;&amp; res) &#123; if (t == n) &#123; res.push_back(path); return; &#125; // 前面t-1行已经ok，再后面的t-n个选择中选择遍历t for (int i = t; i &lt; n; i++) &#123; if (legal(t, path[i], path) == true) &#123; swap(path[i], path[t]); //cout &lt;&lt; \"t=\" &lt;&lt; t &lt;&lt; \", k=\" &lt;&lt; path[i] &lt;&lt; endl; //for_each(path.begin(), path.end(), [](int i)&#123;cout &lt;&lt; i &lt;&lt; \" \";&#125;); dfs(t + 1, n, path, res); swap(path[i], path[t]); &#125; &#125;&#125;/* * 合法性判断，同一列、主对角线、副对角线 * Args: * t -- 第t行，从0开始 * k -- 放在第k个列，从0开始 * path -- 当前的路径，[0,t-1]行已经放好 * Returns: * true or false */bool legal(int t, int k, const vector&lt;int&gt;&amp; path) &#123; for (int i = 0; i &lt;= t - 1; i++) &#123; // 1. 不能和之前的在同一列 if (path[i] == k) &#123; return false; &#125; // 2. 不能在主对角线上 if (t - i == k - path[i]) &#123; return false; &#125; // 3. 不能在副对角线上 if (t + k == i + path[i]) &#123; return false; &#125; printf(\"p[%d]=%d,p[%d]=%d\\n\", t, k, i, path[i]); &#125; return true;&#125; N皇后问题-052 返回有多少种解法 做了上面的题，那这个就很简单了，返回数量就行了。 1234567891011121314/* * 返回n皇后的解法 */int solveNQueens(int n) &#123; // 所有的结果 vector&lt;vector&lt;int&gt;&gt; locations; // 当前的位置 vector&lt;int&gt; path(n); // 初始化为0-n-1 std::iota(path.begin(), path.end(), 0); // dfs遍历搜索 dfs(0, n, path, locations); return locations.size();&#125; 查找第k大的数总结 给N个数，确定第k个最大值 1 排序 排好序，取出第k大的值。\\(O(n\\log n + k)\\) 2 简单选择排序 简单选择。第k次选择，就是第k大的数字。\\(O(n*k)\\) 3 快速排序思想 每次partition，会把x放到位置i上。注意partition要从大到小排列，左大右小，而不是普通排序的左小右大。 i == k， 则就是a[i] k &gt; i， 则在i的右边 k &lt; i， 则在i的左边 [关键代码] 123456789101112131415161718192021222324/* * 使用快排思想查找第k大的数字，从大到小排列！！ * Args: * a -- 数组 * l -- 范围的开始 * r -- 范围的结束 * k -- 该范围内第k大的数 * Returns: * 第k大的数 */int find_kth_num(vector&lt;int&gt; &amp;a, int l, int r, int k) &#123; // 1. 划分。左边大，中间a[l]，右边小 int i = partition(a, l, r); // 2. 通过i+1==k来判断是否是第k大的数 if (i + 1 == k) &#123; return a[i]; &#125; else if (i + 1 &gt; k) &#123; // 在左边 return find_kth_num(a, l, i - 1, k); &#125; else &#123; // 在右边 return find_kth_num(a, i + 1, r, k); &#125;&#125; 4 最大堆 \\(O(4*n)\\)的空间建立最大堆，pop k次即可。\\(O(4 \\times n + k\\times\\log n)\\) 5 最小堆 维护大小为k的最小堆，遍历数组 堆顶元素大，则不管 堆顶元素小，则把当前值插入堆中 最后的堆顶，就是第k大的元素 \\(O(n \\times \\log k)\\) 6 Hash法 查找最小的k个数的总结 给一个数组，找到最小的k个数。注意改变或不改变原数组 1 排序思路 对n个数字从小到大排好序，再取前k个数。O(nlogn + k)=O(nlogn) 。排序算法总结 2 快速排序 排好前k个即可，改变原数组。 3 最大堆 建立大小为k的最大堆。不改变原数组。遇到新的元素，小于堆顶，则加入 4 堆排序 对整个数组n进行堆排序，每次取堆顶，取k次。 数组中次数超过一半的数-169 Majority Element-169， easy 一个数组，有一个元素出现次数超过一半，找到它。 思路0 先排序再找 \\(O(n\\log n)\\) 思路1 快速排序查找第k大元素思想 快速排序笔记 。 如果排好序，则该重复的数字应该在数组中间\\(a_{\\frac{n}{2}}\\)。 也就是中位数，第n/2大的数字 问题就转化为查找数组中的K大的元素 查找第k大的元素 partition(a, l, r) 会把x=a[l]放到中间去，小于的在右边，大于的在左边。返回x的最终位置i i == k， 则x就是第k大的元素 k &lt; i， 则k在右边 k &gt; i， 则k在左边 继续查找，知道 i == k 思路2 count加加减减思想 遇见友军（相同的），就++ 遇见敌军（不同的），就-- 最后剩余的肯定就是人数最多的那个（数字） [关键代码] 1234567891011121314151617181920212223242526/* * 查找主元素，阵地攻守思想。相同加价，不同减减，为0重新赋值 */int majority_element(vector&lt;int&gt;&amp; a) &#123; if (a.size() == 0) &#123; return 0; &#125; // 当前数值与计数 int res = a[0]; int count = 1; for (int i = 1; i &lt; a.size(); i++) &#123; if (a[i] == res) &#123; // 相同++ count++; &#125; else &#123; // 不同--或者重置为1 if (count == 0) &#123; res = a[i]; count = 1; &#125; else &#123; count--; &#125; &#125; &#125; return res;&#125; 主元素2-229 给一个数组，找到所有出现次数超过n/3的数 Majority Element II, medium 。 思路 当然最终结果只有2个或1个。思路同阵地攻守。 用两个变量去记录两个主元素 有一个相同，对应加1 两个都不同，有一个count==0， 则重置 两个都不听，两个都有count，则都减减 遍历之后，得到两个数，两个count 返回count &gt; n/3的数 最后，一定要注意去重！n1 != n2 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/* * 查找出现次数超过n/3的元素 * Args: * nums -- 数组 * Returns: * res -- 超过n/3的元素 */vector&lt;int&gt; majority_element(const vector&lt;int&gt;&amp; nums) &#123; vector&lt;int&gt; res; if (nums.empty()) &#123; return res; &#125; // 1. 找到出现次数最多的两个数 int n1 = 0, c1 = 0; int n2 = 0, c2 = 0; for (int n : nums) &#123; if (n == n1) &#123; c1++; &#125; else if (n == n2) &#123; c2++; &#125; else if (c1 == 0) &#123; n1 = n; c1 = 1; &#125; else if (c2 == 0) &#123; n2 = n; c2 = 1; &#125; else &#123; c1--; c2--; &#125; &#125; c1 = 0, c2 = 0; // 2. 重新计算出现次数 for (auto n : nums) &#123; if (n1 == n) &#123; c1++; &#125; else if (n2 == n) &#123; c2++; &#125; &#125; // 3. 把出现次数超过n/3的数字放到res里面 if (c1 &gt; nums.size() / 3) &#123; res.push_back(n1); &#125; // 去重 if (n2 != n1 &amp;&amp; c2 &gt; nums.size() / 3) &#123; res.push_back(n2); &#125; return res;&#125;","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"},{"name":"树","slug":"树","permalink":"http://plmsmile.github.io/tags/树/"}]},{"title":"剑指offer算法题(11-20)","date":"2017-12-29T03:57:48.000Z","path":"2017/12/29/aim2offer2/","text":"剑指offer算法题(03-10) 旋转数组中的最小值-11 排序算法 我的排序算法总结 旋转数组最小值 原有序数组：1,2,3,4,5,6,7，旋转一下，把一部分放到后面去：4,5,6,7, 1,2,3。 求：在\\(O(logn)\\)内找到数组中最小的元素 leetcode旋转数组 思路 特点：左边序列全部大于等于右边序列。 最小的值在中间，顺序查找肯定不行，那就只能二分查找了。 设两个指针，left从左边向中间靠拢，right从右边向中间靠拢。 循环条件：a[l] &gt;= a[r] 直到l+1==r 为止，那么a[r]就是我们要的最小值。 计算中间值 a[m] a[m] &gt;= a[l] ：m在左边序列，l = m， l向右走 a[m] &lt;= a[r] ： m在右边序列，r == m， r向左走 陷阱 可能一个数字都不旋转，即a为有序序列，直接返回a[0] 有重复的数字，a[l]==a[m] &amp;&amp; a[m]==[r]， 则只能顺序查找了 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * 查找旋转数组中的最小值。两个指针指向前后两个递增序列，向中间靠拢 */int minum_rotate_array(const vector&lt;int&gt;&amp; a) &#123; if (a.size() == 0) &#123; return 0; &#125; int l = 0; int r = a.size() - 1; // 特殊情况，旋转0个，原数组，小于不能等于 if (a[l] &lt; a[r]) &#123; return a[l]; &#125; int res = -1; while (a[l] &gt;= a[r]) &#123; // 两个指针已经相邻 if (l + 1 == r) &#123; res = a[r]; break; &#125; // 中间指针 int m = (l + r) / 2; // 三个数相等，无法确定中间数在前后那个序列 if (a[l] == a[m] &amp;&amp; a[m] == a[r]) &#123; res = get_min(a, l, r); break; &#125; if (a[m] &gt;= a[l]) &#123; l = m; &#125; else if (a[m] &lt;= a[r]) &#123; r = m; &#125; &#125; return res;&#125;int get_min(const vector&lt;int&gt;&amp;a, int start, int end) &#123; int min = a[start]; for (int i = start + 1; i &lt;= end; i++) if (a[i] &lt; min) &#123; min = a[i]; &#125; return min;&#125; 矩阵中的路径-12 给一个字符数组，手动给行和列进行分割。再给一个字符串，判断该字符串是否在该字符矩阵中。使用回溯法进行搜索。 其实就是遍历所有的点开始，然后依次进行上下左右继续查找。用visited去记录点是否已经走过，用pathlen记录目标字符串的遍历长度。 [关键代码] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/* * 判断str在字符矩阵matrix中是否有路径 * * Args: * matrix -- 字符数组，由rows和cols切分为矩阵 * rows -- 行 * cols -- 列 * str -- 字符串 * Returns: * true -- 包含，false -- 不包含 */bool has_path(const char* matrix, int rows, int cols, const char* str) &#123; if (matrix== nullptr || rows &lt; 1 || cols &lt; 1 || str == nullptr) &#123; return false; &#125; bool *visited = new bool[rows * cols]; memset(visited, 0, rows * cols); int pathlen = 0; // 从每个点开始 for (int row = 0; row &lt; rows; row++) &#123; for (int col = 0; col &lt; cols; col++) &#123; bool has = go_find(matrix, rows, cols, row, col, str, pathlen, visited); if (has) &#123; delete[] visited; return true; &#125; &#125; &#125; delete[] visited; return false;&#125;/* * 上下左右去回溯查询 * Args: * matrix -- 字符矩阵 * rows -- 矩阵的行数 * rows -- 矩阵的列数 * row -- 当前要判断的行 * col -- 当前要判断的列 * str -- 原字符串 * pathlen -- 判断到第几个字符 * visited -- 位置访问与否 * Returns: * true -- * false -- */bool go_find(const char* matrix, int rows, int cols, int row, int col, const char* str, int &amp;pathlen, bool *visited) &#123; if (str[pathlen] == '\\0') &#123; return true; &#125; bool ok = false; int cur = row * cols + col; if (row &gt;= 0 &amp;&amp; row &lt; rows &amp;&amp; col &gt;= 0 &amp;&amp; col &lt; cols &amp;&amp; matrix[cur] == str[pathlen] &amp;&amp; visited[cur] == false) &#123; ++pathlen; visited[cur] = true; bool left = go_find(matrix, rows, cols, row, col - 1, str, pathlen, visited); bool right = go_find(matrix, rows, cols, row, col + 1, str, pathlen, visited); bool down = go_find(matrix, rows, cols, row + 1, col, str, pathlen, visited); bool up = go_find(matrix, rows, cols, row - 1, col, str, pathlen, visited); ok = left || right || down || up; if (!ok) &#123; --pathlen; visited[cur] = false; &#125; &#125; return ok;&#125; 机器人的运动范围-13 给一个\\(\\rm{rows \\times cols}\\)的矩阵，和一个阈值\\(k\\)。 机器人从\\((0, 0)\\)出发，进入矩阵的格子。 能进入格子：(35, 38)，因为3+5+3+8k 思路 回溯法， \\(\\rm{count = 1 + left + right + up + down}\\) [关键代码] 定义Solution 123456789101112131415161718192021class Solution &#123; private: // 阈值 int threshold; // 矩阵行 int rows; // 矩阵列 int cols; // 记录格子有没有被走过 bool *visited; public: // 外部接口 int movingCount(int threshold, int rows, int cols); // 回溯计算 int dfs(int row, int col); // 检查该点是否可以进入 bool check_point(int row, int col); // 把各个位的数字加起来 static int resolve_num(int n);&#125;; movingcount 12345678910111213141516171819202122232425/* * 在这个矩阵和阈值上，从(0, 0)进入统计能进入多少个格子 * * Args: * threshold -- 各个位的阈值 * rows -- 矩阵的行数 * cols -- 矩阵的列数 * Returns: * count -- 可以进入的总的格子数量 */int Solution::movingCount(int threshold, int rows, int cols) &#123; // 参数校验 if (threshold &lt; 0 || rows &lt; 1 || cols &lt; 1) &#123; return 0; &#125; // 变量初始化 this-&gt;threshold = threshold; this-&gt;rows = rows; this-&gt;cols = cols; this-&gt;visited = new bool[rows * cols]; memset(this-&gt;visited, 0, rows * cols); int count = dfs(0, 0); delete[] this-&gt;visited; return count;&#125; 回溯法上下左右格子累加 12345678910111213141516171819202122/* * 回溯查询 * Args: * row -- 当前行，索引 * col -- 当前列，索引 * Returns: * count -- 从当前点(row,col)开始向上下左右走能走的格子之和 */int Solution::dfs(int row, int col) &#123; // 可以进入该点 if (check_point(row, col) == false) &#123; return 0; &#125; int cur = row * cols + col; visited[cur] = true; int left = dfs(row, col - 1); int right = dfs(row, col + 1); int up = dfs(row - 1, col); int down = dfs(row + 1, col); int count = 1 + left + right + up + down; return count;&#125; 检查点是否可进入和分解数 1234567891011121314151617181920212223242526/* * 把n的各个位上的数加起来 */int Solution::resolve_num(int n) &#123; int sum = 0; while (n &gt; 0) &#123; sum += n % 10; n = n / 10; &#125; return sum;&#125;/* * 检查一个点是否可以进入。索引、访问状态、阈值 */bool Solution::check_point(int row, int col) &#123; int cur = row * cols + col; if (row &lt; 0 || row &gt;= rows || col &lt; 0 || col &gt;= cols || visited[cur] == true) &#123; return false; &#125; if (resolve_num(row) + resolve_num(col) &gt; threshold) &#123; return false; &#125; return true;&#125; 剪绳子-14 动态规划 求最优解（最大值or最小值），大问题分解成若干个小问题。使用动态规划四个特点，如下 求最优解 整体问题的最优解依赖于各个子问题的最优解 若干个子问题，之间有相互重叠的更小的子问题。如f(2)是f(4)和f(6)共同的子问题 从上往下分析问题，从下网上求解问题。用数组存下小问题的解嘛。 贪心算法 每一步做一个贪婪的选择，基于这个选择，可以得到最优解。需要数学证明。 剪绳子-动态规划 条件 长度为\\(n\\)的绳子，把绳子剪成\\(m, \\; (m \\ge 2)\\)段， 每段长度为\\(k[0], k[1], \\cdots, k[m]\\)。 m和n都是整数，都大于1。 问题 长度连乘积最大是多少？ 思路 定义： \\(f(n)\\) 为把绳子切成若干段后，各段长度乘积的最大值 第一刀的时候，在长度为\\(i\\)的地方剪，分成\\(i\\)和\\(n-i\\)两段。 递推公式如下： \\[ f(n) = \\max (f(i) \\cdot f(n-i)), \\quad 0 &lt; i &lt; n \\] 1234567891011// 自下而上计算f[i]// 计算单个f[i]int max = 0;for (int j = 1; j &lt;= i / 2; j++) &#123; int t = f[j] * f[i - j]; if (t &gt; max) &#123; max = t; &#125;&#125; f[i] = max; 关键代码 长度在3以内的特殊返回 构建f[i]，f[0,1,2,3]手动赋值处理，意义不同 自下而上计算f[4, ..., length] 单独计算f[i]，找到\\(\\max(f_j \\cdot f_{i-j})\\)， 其中\\(j \\in \\{1, \\cdots, i/2\\}\\) 1234567891011121314151617181920212223242526272829303132333435363738int max_product_dp(int length) &#123; // 1. 特殊处理，长度在3以内，自动返回 if (length &lt;= 1) &#123; return 0; &#125; else if (length == 2) &#123; return 1; &#125; else if (length == 3) &#123; return 2; &#125; // 2. f[i]=k，长度为i的绳子剪成若干段最大乘积为k，i&gt;=4 int * f = new int[length + 1]; // 3. f[i]特殊值处理。比如4切一刀：1-3和2-2，f[1]=1,f[3]=3, f[2]=2。因为2*2&gt;1*3，f[4]=4 f[0] = 0; f[1] = 1; f[2] = 2; f[3] = 3; int max = 0; // 4. 自下而上计算 for (int i = 4; i &lt;= length; i++) &#123; max = 0; // 找最大的分割 for (int j = 1; j &lt;= i / 2; j++) &#123; int t = f[j] * f[i - j]; if (t &gt; max) &#123; max = t; &#125; &#125; f[i] = max; &#125; max = f[length]; delete[] f; return max;&#125; 剪绳子-贪心 贪心策略 当\\(n \\ge 5\\)时， 尽可能多地剪成长度为3的绳子 当剩余长度为4的时候，要剪成2*2的绳子 关键代码 特殊处理，长度在3以内，自动返回 计算3的个数 计算2的个数 计算最终结果 pow(3, t3)*pow(2,t2) 12345678910111213141516171819202122232425int max_product_greedy(int length) &#123; // 1. 特殊处理，长度在3以内，自动返回 if (length &lt;= 1) &#123; return 0; &#125; else if (length == 2) &#123; return 1; &#125; else if (length == 3) &#123; return 2; &#125; else if (length == 4) &#123; return 4; &#125; // 2. 计算3的个数 int t3 = length / 3; // 最后剩1，则补成4 if (length - t3*3 == 1) &#123; t3--; &#125; // 3. 计算2的个数 int t2 = (length - t3*3) / 2; // 4. 计算最终结果 return (int) pow(3, t3) * (int) (pow(2, t2));&#125; 二进制中1的个数-15 基础知识 位运算 操作 意义 与&amp; 都为1，才为1 或| 有一个为1，就为1 异或^ 不同为1，相同为0 移位 操作 意义 注意 左移n位 丢弃左边n位，右边n位补0 右移n位 无符号数&amp;正数：右移n位，左边补n个0 负数：右移n位，左边补n个1 重要结论 操作 意义 n &amp; (n-1) 把n的二进制中，最右边的1变成0 n &amp; 1 检测n的二进制，末尾位是否是1 n &amp; 2 检测n的二进制，倒数第二位是否是1 二进制中1的个数 问题：给一个整数，判断它的二进制中，有多少个1 正数思路 n &amp; 1，n不停右移。每次判断 n &amp; 1，即最末尾位是否是1。 但是负数有问题，负数右移，左边会补1，陷入死循环。 正数负数思路 n不变，n &amp; flag，flag每次向左移。即从右到左依次判断n的各个位是否是1。一共循环n的二进制位数次。 最优思路 n = n &amp; (n - 1)，每次把n的最右边的1变成0。看看能执行多少次这样的操作，就有多少个1。 关键代码 123456789101112131415161718192021222324252627/* * flag不断左移 */int count_1_by_flag(int n) &#123; int count = 0; unsigned int flag = 1; while (flag != 0) &#123; if (n &amp; flag) &#123; count++; &#125; flag = flag &lt;&lt; 1; &#125; return count;&#125;/* * 把n的最左边1变成0，看看能变几次，则有几个1 */int count_1(int n) &#123; int count = 0; while (n) &#123; n = n &amp; (n - 1); count++; &#125; return count;&#125; 数值的整数次方-16 代码风格 代码的规范性：清晰的书写、清晰的布局、合理的命名 代码的完整性：功能测试、边界测试、负面测试 处理错误的方法 方法 优点 缺点 返回值 和系统API一致 不能方便使用计算结果 全局变量 能够方便使用计算结果 用户可能会忘记检查全局变量 异常 不同错误，抛出不同异常，可自定义。逻辑清晰 有的语言不支持；对性能负面影响 数值的整数次方 要求实现 1double power(double base, int exponent); 注意特殊情况条件 指数为0 指数为负数 指数为负数，底数不能为0或者约等于0。用fabs(a-b) &lt; theta 思路1 条件考虑全面，一个一个乘 思路2递归 使用这个公式 \\[ a^n = \\begin{cases} &amp; a^{n/2} \\cdot a^{n/2} &amp;\\text{n为偶数} \\\\ &amp; a^{(n-1)/2} \\cdot a^{(n-1)/2} &amp; \\text{n为奇数} \\\\ \\end{cases} \\] 思路3位运算 例如\\(a^{13}\\)， 13的二进制是1101。 从右向左，依次读1，flag &amp; 1, b = b &gt;&gt; 1 遇到1就累乘， res *= a 每读一位，倍数增加， a *= a 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869double power_normal_binary(double base, int exponent) &#123; if (exponent == 0) &#123; return 1; &#125; else if (exponent == 1) &#123; return base; &#125; double res = 1; while (exponent != 0) &#123; if ((exponent &amp; 1) == 1) &#123; res *= base; &#125; base *= base; exponent = exponent &gt;&gt; 1; &#125; return res;&#125;/* * 递归计算 */double power_normal_recur(double base, int exponent) &#123; if (exponent == 0) &#123; return 1; &#125; else if (exponent == 1) &#123; return base; &#125; double res = power_normal_recur(base, exponent &gt;&gt; 1); // 翻次方倍 res *= res; // 奇数 if ((exponent &amp; 1) == 1) &#123; res *= base; &#125; return res;&#125;double power_normal(double base, int exponent) &#123; double res = 1; for (int i = 0; i &lt; exponent; i++) &#123; res *= base; &#125; return res;&#125;double power(double base, int exponent) &#123; // 指数为0，返回1 if (exponent == 0) &#123; return 1; &#125; // 指数为负数，底数不能为0 if (exponent &lt; 0 &amp;&amp; equal(base, 0.0)) &#123; cout &lt;&lt; \"error. exponent &lt; 0, base should != 0\" &lt;&lt; endl; return 0; &#125; // 分正负计算 double res = 1; if (exponent &gt; 0) &#123; res = power_normal_binary(base, exponent); &#125; else &#123; res = power_normal_binary(base, -exponent); res = 1.0 / res; &#125; return res;&#125; 打印1到最大的N位数-17 问题：输出打印1-999。但是n不确定，非常大，要注意溢出的问题。 思路 开辟n+1位的字符串，来存储数字。末尾位是'' 字符串模拟数字的自增、进位和溢出 长度超过n+1，或者第0位产生进位，则溢出 输出的时候，要人性化点。比如0087，要打印87 关键代码 开始的时候，个位加1。要注意进位， 清楚当前数字存在哪些位置上面的。 12345678910111213141516171819202122232425262728293031323334353637383940/* * 当前数加1，字符串模拟加法、进位、溢出 */void Solution::increment() &#123; // 当前位的值 int now = -1; // 前一位的进位 int take = 0; int i = -1; // 当前位数是n-1~n-real_len for (i = n - 1; i &gt;= n - real_len; i--) &#123; int now = num[i] - '0' + take; if (i == n - 1) &#123; // 实现自增，末尾加1 now = now + 1; &#125; if (now &gt;= 10) &#123; num[i] = '0' + (now - 10); take = 1; &#125; else &#123; num[i] = '0' + now; take = 0; break; &#125; &#125; // 需要新增一位 if (take &gt; 0) &#123; if (i &lt; 0) &#123; cout &lt;&lt; \"num char* over flow\" &lt;&lt; endl; this-&gt;over_flow = true; // 注意释放空间 delete this-&gt;num; return; &#125; num[i] = '0' + 1; real_len++; &#125; return;&#125; 删除链表中的元素-18 删除一个节点 给一个链表和一个节点，删除这个节点i，要求\\(O(1)\\) 思路 从头到尾遍历找到i的前驱节点，时间复杂度为\\(O(n)\\)， 显然不行。 把i的后继节点的值拷贝到i上，然后删除i的后继节点。 但，要注意：头结点、中间节点、末尾节点（顺序遍历）。 关键代码 123456789101112131415161718192021222324252627282930void delete_node(ListNode ** phead, ListNode* pdeleted) &#123; if (!phead || !pdeleted) &#123; return; &#125; // 只有一个节点，删除头节点/尾节点 if (*phead == pdeleted &amp;&amp; pdeleted-&gt;next == nullptr) &#123; delete pdeleted; pdeleted = nullptr; *phead = nullptr; &#125; // 有多个节点，删除尾节点 else if (pdeleted-&gt;next == nullptr) &#123; ListNode* p = *phead; while (p-&gt;next != pdeleted) &#123; p = p-&gt;next; &#125; p-&gt;next = nullptr; delete pdeleted; pdeleted = nullptr; &#125; // 有多个节点，删除中间的节点 else &#123; // 直接把下一个节点的值赋值到当前节点，再删除下一个节点 ListNode* pnext = pdeleted-&gt;next; pdeleted-&gt;val = pnext-&gt;val; pdeleted-&gt;next = pnext-&gt;next; delete pnext; pnext = nullptr; &#125;&#125; 删除有序链表中的重复元素 Remove Duplicates from Sorted List 有序的链表。 留下一个 每个重复的元素，留下一个。 思路 从head开始循环，遍历每一个节点。 对于当前节点c，再进行遍历，直到下一个节点不等于它或者为空。中间删除掉重复的节点。 全部删除 把所有重复的元素，都删除，有序链表。 思路 当前节点cur， 需要保留前一个元素的指针pre。 我的思路 循环遍历到cur，head应该为第一个pre。 cur无重复cur!=next || next==null pre为空，则pre=cur ； pre不为空， 则pre.next=cur ，pre=pre.next 。实际上pre.next指向下一轮的cur cur重复 cur=next 遍历删除所有与cur相同的元素 删除cur 但由于原本pre.next指向当前cur，cur又被删除。所以pre.next=nullptr 其实最重要是：当前不重复，则续接到pre后面，pre后移；当前重复，则删除，pre后面设为空。 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041ListNode* del_dups_nosave(ListNode* head) &#123; if (head == nullptr) &#123; return head; &#125; ListNode* cur = head; ListNode* pre = nullptr; ListNode* next = nullptr; while (cur) &#123; // 1. cur无重复 if (cur-&gt;next == nullptr || cur-&gt;val != cur-&gt;next-&gt;val) &#123; if (pre == nullptr) &#123; pre = cur; head = pre; &#125; else &#123; pre-&gt;next = cur; pre = cur; &#125; cur = cur-&gt;next; &#125; // 2. cur重复，删掉重复的元素 else &#123; next = cur-&gt;next; // 2.1 删除与cur相同的 while (next &amp;&amp; cur-&gt;val == next-&gt;val) &#123; cur-&gt;next = next-&gt;next; delete next; next = cur-&gt;next; &#125; // 2.2 删除cur delete cur; cur = next; // 原本pre-&gt;next=cur，但是当前cur已经被删除，所以重新置为空 if (pre != nullptr) pre-&gt;next = nullptr; &#125; &#125; if (pre == nullptr) &#123; head = nullptr; &#125; return head;&#125; 正则表达式匹配-19 问题 正则表达式：普通字符、. 任意字符 、* 前面的字符出现0次或多次。 给定字符串，去判断该字符串是否符合正则表达式。 例如：aaa 与a.a和ab*ac*a匹配， 与aa.a和ab*a不匹配。 思路 递归去判断。用\\(s_1\\)和\\(p_1\\)去 代表第一个字符和第一个模式。 \\(s_1\\)与\\(p_1\\)匹配成功： s1==p1 || (p1 == '.' &amp;&amp; s1 != '\\0') 必须先判断\\(p2\\)是否是* 如果，第二个模式\\(p_2\\)是* \\(s_1\\)与\\(p_1\\)匹配成功 *代表出现1次，go(s2, p3) *代表出现0次，go(s1, p3) *代表出现多次，go(s2, p1) \\(s_1\\)与\\(p_1\\)匹配失败 *只能代表出现0次，go(s1, p3) 如果\\(s_1\\)与\\(p_1\\)匹配成功 字符串和模式都向后挪一次，go(s2, p2) 总结 p结束，s结束。返回true p结束，s还有。 返回false p未结束，\\(p_2\\) == *。\\(s_1\\)与\\(p_1\\)匹配成功， p2作为0、1、多次，合并返回。匹配失败，作为0次，返回。 p未结束，\\(p_2\\) != *，单独match成功。go(s2, p2) p未结束，\\(p_2\\) != *，单独match失败。返回false 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/* * 单个字符是否能匹配 */bool single_char_match(const string &amp;str, int s, const string &amp;pattern, int p) &#123; // 超出 if (s &gt;= str.size() || p &gt;= pattern.size()) &#123; return false; &#125; // 匹配成功 if (str[s] == pattern[p] || (pattern[p] == '.' &amp;&amp; s &lt; str.size())) &#123; return true; &#125; // 匹配失败 return false;&#125;/* * 递归判断str和pattern是否匹配，从snow和pbgin开始 * Args: * str -- 字符串 * snow -- 从字符串的第几个开始 * pattern -- 正则表达式 * pnow -- 模式的开始 * Returns: * true or false */bool match_core(const string &amp;str, int snow, const string &amp;pattern, int pnow) &#123; // 1. p结束，s结束 if (pnow &gt;= pattern.size() &amp;&amp; snow &gt;= str.size()) &#123; return true; &#125; // 2. p结束，s还有 if (pnow &gt;= pattern.size() &amp;&amp; snow &lt; str.size()) &#123; return false; &#125; // 3. p未结束，p2 == * int p2 = pnow + 1; if (p2 &lt; pattern.size() &amp;&amp; pattern[p2] == '*') &#123; // s1与p1匹配成功 if (single_char_match(str, snow, pattern, pnow) == true) &#123; bool t0 = match_core(str, snow, pattern, pnow + 2); bool t1 = match_core(str, snow+1, pattern, pnow + 2); bool t_many = match_core(str, snow+1, pattern, pnow); return t0 || t1 || t_many; &#125; // s1与p1匹配失败 else &#123; // *只能代表0 return match_core(str, snow, pattern, pnow + 2); &#125; &#125; // 4. p未结束，p2 != *，单独match成功 if (single_char_match(str, snow, pattern, pnow) == true) &#123; return match_core(str, snow+1, pattern, pnow+1); &#125; // 5. p未结束，p2 != *，单独match失败 return false;&#125; 表示数值的字符串-20 数值：+100、5e2、-123、3.1416、-1E-16 、1.5e2 非数值：12e、1a3.14、1.2.3、+-5、12e+5.4 判断字符串是否是一个正确的数值。A[.[B]][e|EC]或者.B[e|EC] （符号）、整数、小数点、（整数）、e、整数 小数点前后，必须有一个整数 e前面是一个数，后面是一个整数 关键代码 12345678910111213141516171819202122232425bool Solution::isNumeric(const string &amp;str) &#123; int start = 0; bool pre_int = scan_integer(str, start); bool numeric = pre_int; // 有小数点 if (start &lt; str.size() &amp;&amp; str[start] == '.') &#123; start++; bool dot_int = scan_unsigned_integer(str, start); // 小数点前面或者后面至少有一个整数 numeric = pre_int || dot_int; &#125; // 有指数符号 if (start &lt; str.size() &amp;&amp; (str[start] == 'E' ||str[start] == 'e')) &#123; start++; bool e_int = scan_integer(str, start); // e前面是一个数值，e后面是一个整数 numeric = numeric &amp;&amp; e_int; &#125; // 没有走完，还剩余字符 if (start &lt; str.size()) &#123; return false; &#125; return numeric;&#125;","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"}]},{"title":"排序算法总结","date":"2017-12-26T08:06:53.000Z","path":"2017/12/26/sort-algorithms/","text":"插入排序 直接插入 前面的已经有序，把后面的插入到前面有序的元素中。 步骤 找到待插入位置 给插入元素腾出空间，边比较边移动 如对4 5 1 2 6 3 12345678910# 5插入44 5 1 2 6 3# 1插入 4 51 4 5 2 6 3# 2插入 1 4 51 2 4 5 6 3# 6插入 1 2 4 51 2 4 5 6 3# 3插入到前面1 2 3 4 5 6 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 直接插入排序，先比较找位置，再移动 **/ void insert_sort(int a[], int n) &#123; for (int i = 1; i &lt; n; i++) &#123; // 最大，追加在末尾即可 if (a[i] &gt; a[i-1]) &#123; continue; &#125; // 找到待插入的位置 int k = -1; for (int j = 0; j &lt; i; j++) &#123; if (a[i] &lt; a[j]) &#123; k = j; break; &#125; &#125; int t = a[i]; // 先挪动元素，向后移动 for (int j = i; j &gt; k; j--) &#123; a[j] = a[j-1]; &#125; a[k] = t; &#125;&#125;/** * 直接插入排序，边比较边移动 **/void insert_sort2(int a[], int n) &#123; for (int i = 1; i &lt; n; i++) &#123; if (a[i] &lt; a[i-1]) &#123; int t = a[i]; int j = i - 1; while (a[j] &gt; t &amp;&amp; j &gt;= 0) &#123; a[j+1] = a[j]; j--; &#125; a[j+1] = t; &#125; &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 适用性 直接插入 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 顺序存储和链式存储的线性表 折半插入 先折半查找出位置，再统一的移动。 若m为1个数字，则\\(a[i]&gt;a[m]\\)，该插入位置为\\(l=m+1\\)。 仅仅减少了元素的比较次数，元素的移动次数依然没有改变。时间复杂度仍然为\\(\\mathrm{O}(n)\\)。 关键代码 123456789101112131415161718192021222324252627282930/** * 插入排序，折半查找出位置，再统一移动 **/ void insert_sort_bisearch(int a[], int n) &#123; for (int i = 1; i &lt; n; i++) &#123; if (a[i] &gt; a[i-1]) &#123; continue; &#125; // 折半查找，a[i]要插入的位置为l int l = 0, r = i - 1; while (l &lt;= r) &#123; int m = (l + r) / 2; if (a[i] &gt; a[m]) &#123; // 查找右边 l = m + 1; &#125; else if (a[i] &lt; a[m])&#123; // 查找左边 r = m - 1; &#125; else &#123; l = m + 1; break; &#125; &#125; int t = a[i]; for (int j = i; j &gt; l; j--) &#123; a[j] = a[j-1]; &#125; a[l] = t; &#125;&#125; 希尔排序 希尔排序又称为缩小增量排序， 把整个列表，分成多个\\(\\rm{L[i, i+d,i+2d,\\cdots, i+kd]}\\)这样的列表，每个进行直接插入排序。每一轮不断缩小d的值，直到全部有序。 实际例子 4 5 1 2 6 3 1234567891011121314151617# d = 34 _ _ 2 _ __ 5 _ _ 6 __ _ 1 _ _ 3# 化为3个列表，分别进行直接插入排序，得到2 5 1 4 6 3# d = 22 _ 1 _ 6 __ 5 _ 4 _ 3# 排序，得到1 3 2 4 6 5# d = 11 3 2 4 6 5# 直接插入排序，得到1 2 3 4 5 6 关键代码 12345678910111213141516171819202122232425262728293031323334/* * 希尔排序，按照步长，去划分为多个组。对这些组分别进行插入排序 */void shell_sort(vector&lt;int&gt;&amp; a) &#123; // 步长gap==组的个数 for (int gap = a.size() / 2; gap &gt; 0; gap = gap / 2) &#123; // 对各个组进行排序 for (int i = 0; i &lt; gap; i++) &#123; group_sort(a, i, gap); &#125; &#125;&#125;/* * 对希尔排序中的单个组进行排序，直接插入 * Args: * a -- 数组 * start -- 该组的起始地址 * gap -- 组的步长，也是组的个数 */void group_sort(vector&lt;int&gt; &amp;a, int start, int gap) &#123; for (int i = start + gap; i &lt; a.size(); i += gap) &#123; if (a[i] &lt; a[i - gap]) &#123; int t = a[i]; int j = i - gap; // 从后向前比较，边比较，边移动 while (a[j] &gt; t &amp;&amp; j &gt;= start) &#123; a[j + gap] = a[j]; j -= gap; &#125; a[j + gap] = t; &#125; &#125;&#125; 总结分析 最好的增量\\(d_1 = \\frac{n}{2}, d_{i+1} = \\lfloor \\frac{d_i}{2}\\rfloor\\) 名称 时间 最好 最差 空间 稳定 适用性 希尔排序 \\({O}(1)\\) 不稳定 线性存储的线性表 交换排序 冒泡排序 执行n-1轮，每一轮把\\(a[0, \\ldots, i]\\)的最大的向下沉。 123456789101112131415162 4 3 1 6 5# 第一趟2 4 _ _ _ _2 3 4 _ _ _ # 4-3 to 3-42 3 1 4 _ _ # 4-1 to 1-4 2 3 1 4 6 _ 2 3 1 4 5 6 # 6-5 to 5-6# 第二趟2 3 _ _ _ 6 2 1 3 _ _ 6 # 3-1 to 1-32 1 3 4 _ 6 2 1 3 4 5 6# 第三趟1 2 _ _ 5 6 # 2-1 to 1-21 2 3 _ 5 61 2 3 4 5 6 关键代码 12345678910111213141516171819202122232425262728293031void bubble_sort1(int* a, int n) &#123; for (int i = n-1; i &gt; 0; i--) &#123; // 每一轮把a[0,...,i]中最大的向下沉 for (int j = 0; j &lt; i; j++) &#123; if (a[j] &gt; a[j+1]) &#123; int t = a[j]; a[j] = a[j+1]; a[j+1] = t; &#125; &#125; &#125; &#125;// 若当前轮，已经没有发生交换，说明已经全部有序void bubble_sort2(int* a, int n) &#123; int swapped = 0; for (int i = n - 1; i &gt; 0; i--) &#123; swapped = 0; for (int j = 0; j &lt; i; j++) &#123; if (a[j] &gt; a[j+1]) &#123; int t = a[j]; a[j] = a[j+1]; a[j+1] = t; swapped = 1; &#125; &#125; if (swapped = 0) &#123; break; &#125; &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 冒泡排序 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 每一轮都有一个元素到最终位置上 快速排序 思想 把一个序列，选一个数（第一个数），进行划分。左边小于x，中间x，右边大于x。再依次递归划分左右两边。 关键代码 划分 123456789101112131415161718192021222324252627282930313233343536/** * 划分，左边小于x，中间x，右边大于x * Args: * a: * l: * r: * Returns: * i: x=a[l]的最终所在位置 **/int partition(int* a, int l, int r) &#123; int x = a[l]; int i = l; int j = r; // 划分 while (i &lt; j) &#123; // 从右到左，找到第一个小于x的a[j]，放到a[i]上 while (a[j] &gt;= x &amp;&amp; j &gt; i) &#123; j--; &#125; // 把a[j]放到左边i上 if (j &gt; i) &#123; a[i++] = a[j]; &#125; // 从左到右，找到一个大于x的[i] while (a[i] &lt;= x &amp;&amp; i &lt; j) &#123; i++; &#125; // 把a[i]放到右边j上 if (i &lt; j) &#123; a[j--] = a[i]; &#125; &#125; // x放在中间 a[i] = x; return i;&#125; 递归快排 1234567891011void quick_sort(int* a, int l, int r) &#123; // 1. 递归终止 if (l &gt;= r) &#123; return; &#125; // 2. 划分，左边小于x，中间x，右边大于x int k = partition(a, l, r); // 3. 递归快排左右两边 quick_sort(a, l, k - 1); quick_sort(a, k + 1, r);&#125; 非递归快排 123456789101112131415161718192021222324252627void quick_sort_stack(int* a, int l, int r) &#123; int i, j; stack&lt;int&gt; st; // 注意进栈和出栈的顺序 st.push(r); st.push(l); while (st.empty() == false) &#123; // 每次出栈一组 i = st.top(); st.pop(); j = st.top(); st.pop(); if (i &lt; j) &#123; int k = partition(a, i, j); // 左边的 if (k &gt; i) &#123; st.push(k - 1); st.push(i); &#125; // 右边的 if (k &lt; j) &#123; st.push(j); st.push(k + 1); &#125; &#125; &#125;&#125; 总结分析 第i趟完成后，最少有i个元素在最终位置。 名称 时间 最好 最差 空间 稳定 备注 快排 \\({O}(n\\log n)\\) \\(O(n\\log n)\\) \\({O}(n^2)\\) \\({O}(\\log_2n)\\)栈的深度 不稳定 基本有序或者逆序，效果最差 选择排序 简单选择 前面已经有序，从后面选择最小的与前面末尾最大的进行交换。 12345678910114 5 1 2 6 3# 选择1与4交换1 5 4 2 6 3# 选择2与5交换1 2 4 5 6 3# 选择3与4交换1 2 3 5 6 4# 选择4与5交换1 2 3 4 6 5# 选择5与6交换1 2 3 4 5 6 总结分析 名称 时间 最好 最差 空间 稳定 备注 简单选择 \\({O}(n^2)\\) \\({O}(n^2)\\) \\({O}(n^2)\\) \\(O(1)\\) 不稳定 关键代码 1234567891011/** * 简单选择排序，选择后面最小的来与当前有序的最后一个（最大的）交换 **/void select_sort(int *a, int n) &#123; for (int i = 1; i &lt; n; i++) &#123; int k = min_index(a, i, n-1); if (a[i-1] &gt; a[k]) &#123; swap(a, i-1, k); &#125; &#125;&#125; 堆 堆 堆是一颗完全二叉树。 有n个节点，堆的索引从0开始，节点i的 左孩子：\\(2\\cdot i+1\\) 右孩子：\\(2\\cdot i +2\\) 父亲：\\(\\lceil \\frac{i-1}{2}\\rceil\\) 最后一个节点是：下标为\\(\\lfloor n/2\\rfloor-1\\) 的节点的孩子，即第\\(\\lfloor n/2\\rfloor\\)个节点的孩子。 大根堆 大根堆：最大元素在根节点。小根堆：最小元素在根节点。 90 70 80 60 10 40 50 30 20 是一个大根堆，10 20 70 30 50 90 80 60 40 是一个小根堆，如下 堆添加 先把元素添加到末尾，再依次向上面调整，若大于父节点，就和父节点交换。 堆删除 删除堆顶元素：把末尾元素，放到堆顶，再依次向下调整，每次选择较大的子节点进行交换。 删除堆中元素：把末尾元素，放入空白处，再调整堆即可。 堆排序 思路 初始化堆。把\\(a_1, \\cdots, a_n\\)构造成为最大堆 取出最大值。每次出堆根最大元素。出\\(a_1\\)，把\\(a_n\\)放到\\(a_1\\)上，再把\\(a_1, \\cdots. a_{n-1}\\)调整为最大堆。再出元素 重复2 建立堆 n个元素，最后一个父亲节点\\(\\lfloor n/2\\rfloor\\)， 下标为\\(k=\\lfloor n/2\\rfloor-1\\)。 对这些节点\\(a_k, \\cdots, a_0\\)， 依次调整它们的子树，使子树成堆。即，若根节点小于左右节点的较大值，则交换。 建立堆实例 对于数据{20,30,90,40,70,110,60,10,100,50,80}，建立为最大堆{110,100,90,40,80,20,60,10,30,50,70} 取出最大值 把根节点（最大值）和当前堆的末尾值进行交换，最大值放到最后。再对剩余的数据进行成堆，再依次取最大值交换。 每一次取出最大值重新恢复堆，要\\(O(\\log n)\\)，有n个数，一共是\\(O(n \\log n)\\)。 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 保证以start为根节点的子树是一个最大堆，末尾元素为end * * Args: * a: 存放堆的数组 * start: 根节点 * end: 子树的末尾元素 * Returns: * None **/ void max_heap_down(vector&lt;int&gt;&amp; a, int start, int end) &#123; // 当前节点 int c = start; // 左孩子 int l = 2 * c + 1; // 当前父亲节点 int t = a[c]; for (; l &lt;= end; c = l, l = 2*c + 1) &#123; // 选择较大的孩子与父亲交换 if (l + 1 &lt;= end &amp;&amp; a[l] &lt; a[l + 1]) &#123; // 有右孩子，并且右孩子比左孩子大，则选择右孩子 l++; &#125; if (t &gt;= a[l]) &#123; // 父亲大于孩子 break; &#125; else &#123; // 交换 a[c] = a[l]; a[l] = t; &#125; &#125;&#125;/** * 堆排序，升序 **/void heap_sort_asc(vector&lt;int&gt;&amp; a) &#123; int n = a.size(); // 初始化一个最大堆 for (int i = n / 2 - 1; i &gt;= 0; i--) &#123; max_heap_down(a, i, n - 1); &#125; // 依次取堆顶元素放到末尾 for (int i = n - 1; i &gt;= 0; i--) &#123; // max放到a[i] int t = a[i]; a[i] = a[0]; a[0] = t; // 保证a[0...i-1]依然是个最大堆 max_heap_down(a, 0, i-1); &#125; return;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 堆排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}( \\log n)\\) \\(O(n)\\) 不稳定 归并排序 从上往下-递归 思路 分解 -- 将区间一分为二，求分裂点\\(\\rm{mid} = \\frac{\\rm{start +end}}{2}\\) 递归求解，sort， sort -- 递归对两个无序子区间 \\(a_s,\\cdots , a_m\\)和\\(a_{m+1}, \\cdots, a_{e}\\)进行归并排序。终结条件是子区间长度为1 合并，merge -- 把两个有序的子区间 \\(a_s,\\cdots , a_m\\)和\\(a_{m+1}, \\cdots, a_{e}\\) 合并为一个完整的有序区间\\(a_s, \\cdots, a_e\\) 分解 分解&amp;递归&amp;合并 关键代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 归并排序，从上到下 **/void merge_sort_up2down(vector&lt;int&gt; &amp;a, int start, int end) &#123; if (start &gt;= end) &#123; return; &#125; int mid = (start + end) / 2; // 递归排序a[start...mid] merge_sort_up2down(a, start, mid); // 递归排序a[mid+1...end] merge_sort_up2down(a, mid + 1, end); // 两个有序序列merge在一起 merge(a, start, mid, end);&#125;/** * 将a中前后两个有序序列合并在一起 **/ void merge(vector&lt;int&gt; &amp;a, int start, int mid, int end) &#123; // 把有序序列临时存放到t中 int * t = new int [end - start + 1]; int i = start; int j = mid + 1; int k = 0; // 依次合并 while (i &lt;= mid &amp;&amp; j &lt;= end) &#123; if (a[i] &lt; a[j]) &#123; t[k++] = a[i++]; &#125; else &#123; t[k++] = a[j++]; &#125; &#125; while (i &lt;= mid) &#123; t[k++] = a[i++]; &#125; while (j &lt;= end) &#123; t[k++] = a[j++]; &#125; // 把新的有序列表复制回a中 for (int i = 0; i &lt; k; i++) &#123; a[start + i] = t[i]; &#125; delete [] t;&#125; 从下往上-非递归 思想 把数组分成若干个长度为1的子数组，再两两合并；得到长度为2的数组，再两两合并；依次反复，直到形成一个数组。 关键代码 123456789101112131415161718192021222324252627282930/** * 归并排序，从下到上 **/void merge_sort_down2up(vector&lt;int&gt; &amp;a) &#123; if (a.size() &lt;= 0) return; for (int i = 1; i &lt; a.size(); i = i * 2) merge_groups(a, i);&#125;/* * 对a做若干次合并，分为若干个gap。对每相邻的两个gap进行合并排序 * Args: * a: 数组 * gap: 一个子数组的长度 */void merge_groups(vector&lt;int&gt; &amp;a, int gap) &#123; int twolen = 2 * gap; int i; for (i = 0; i + twolen - 1 &lt; a.size(); i += twolen) &#123; int start = i; int mid = i + gap - 1; int end = i + twolen - 1; merge(a, start, mid, end); &#125; // 最后还有一个gap if (i + gap - 1 &lt; a.size() - 1) &#123; merge(a, i, i + gap - 1, a.size() - 1); &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 归并排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}(n \\log n)\\) \\(O(n)\\)， merge占用 稳定 归并排序的形式是一颗二叉树，遍历的次数就是二叉树的深度\\(O(\\log n)\\)， 而一共n个数。 桶排序 桶排序很简单。数组a有n个数，最大值为max。则，建立一个长度为max的数组b，初始化为0。 遍历a，遇到\\(a_i = k\\)， 则\\(b_k += 1\\) 。即在对应的桶里计数加1。 基数排序 基数排序分为最高位优先和最低位优先。 基数排序是桶排序的扩展。把所有的数，统一位数。然后，按照每一位进行，从低位到高位跑排序。 关键是找到output和buckets的对应关系。每个bucket存储前面累积的元素的数量。 123456buckets[2] = 1;// 说明排序数是2的元素有1个buckets[3] = 4;// 说明排序数是3的元素有 4-1=3个buckets[4] = 7;// 说明排序数是4的元素有 7-4=3个 后面在进行根据排序数找到当前数的最终所在位置的时候，就会利用这个关系。 1234567// 比如排序数是3的数字，会出现3个// 则id就为 buckets[3]-1// 每出现一个，则buckets[3]--// 举个例子// 初始buckets[2]=1，则这1个数字的最终序号是：0// 初始buckets[3]=4，则这3个数字的最终序号是：3,2,1 关键代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/* * 基数排序 */void radix_sort(vector&lt;int&gt; &amp;a) &#123; if (a.size() &lt;= 1) &#123; return; &#125; int max = *max_element(a.begin(), a.end()); // exp=1, 10, 100, 1000... for (int exp = 1; max / exp &gt; 0; exp *= 10) &#123; count_sort(a, exp); &#125;&#125;/* * 对数组按照某个位数进行排序 * Args: * a -- 数组 * exp -- 指数，1, 10, 100... 分别按照个位、十位、百位排序 * Returns: * None */void count_sort(vector&lt;int&gt;&amp; a, int exp) &#123; // 存储被排序数据的临时数组 int output [a.size()]; // 桶 数据的出现次数 int buckets[10] = &#123;0&#125;; for (int i = 0; i &lt; a.size(); i++) &#123; int t = (a[i] / exp) % 10; buckets[t]++; &#125; // 根据前面的出现次数，推算出当前数字在原数组中的index for (int i = 1; i &lt; 10; i++) buckets[i] += buckets[i - 1]; // 将数据存储到output中 for (int i = a.size() - 1; i &gt;= 0; i--) &#123; int j = (a[i] / exp) % 10; int k = buckets[j]; output[k - 1] = a[i]; buckets[j]--; &#125; // 赋值给a for (int i = 0; i &lt; a.size(); i++) &#123; a[i] = output[i]; &#125;&#125; 总结分析 名称 时间 最好 最差 空间 稳定 备注 基数排序 \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\(O(r)\\)， r个队列 稳定 总结比较 总结 思想总结 名称 一句话描述 直接插入 前面有序，为新来的，在前面找到合适的位置，进行插入 折半插入 前面有序，为新来的，使用折半查找到插入位置，进行插入 希尔排序 gap个间隔为gap的子序列，每个进行直接插入排序；减小gap，依次排序，直至为1 冒泡排序 交换n-1趟，\\(a_{i-1}&gt;a_i\\)，则进行交换，每一趟都有个最大的沉到末尾 快排 第一个数x，先划分，左边小于x，中间x，右边大于x。再依次递归排序左右两边 简单选择 前面有序，从后面选择最小的与前面末尾的（最大的）进行交换 堆排序 初始化大根堆，堆顶和末尾元素交换，再调整使剩下的元素成堆， 重复 归并排序 分解为左右两个序列，对左右两个序列进行递归归并排序，再合并。即sort,sort,merge 基数排序 位数一样，从低位到高位，分别按照每一位进行排序 时空复杂度总结 名称 时间 最好 最差 空间 稳定 直插 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 希尔 \\({O}(1)\\) 不稳定 冒泡 \\({O}(n^2)\\) \\({O}(n)\\) \\({O}(n^2)\\) \\({O}(1)\\) 是 快排 \\({O}(n\\log n)\\) \\(O(n\\log n)\\) \\({O}(n^2)\\) \\({O}(\\log_2n)\\)，栈的深度 不稳定 简选 \\({O}(n^2)\\) \\({O}(n^2)\\) \\({O}(n^2)\\) \\(O(1)\\) 不稳定 堆排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}( \\log n)\\) \\(O(n)\\) 不稳定 归并排序 \\({O}(n \\log n)\\) \\({O}(n\\log n)\\) \\({O}(n \\log n)\\) \\(O(n)\\)， merge占用 稳定 基数排序 \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\({O(d(n+r))}\\) \\(O(r)\\)， r个队列 稳定 稳定的：插、冒、归、基 较快的：快、些、归、堆，插得好、冒得好 比较次数与初始状态无关：选择、归并 排序趟数与初始状态无关：选择、插入、基数 冒、选、堆：每趟都有1个在最终的位置，最大or最小 直接插入：第\\(i\\)趟，前\\(i+1\\)个有序，但不是最终序列 快排： \\(i\\)趟后， 至少有\\(i\\)个在最终位置 时间复杂度：快些归堆\\({O(n\\log n)}\\)，基数排序\\(O(d(n+r))\\) ， 其余\\(O(n^2)\\) ， 空间复杂度：归\\(O(n)\\)， 快\\(O(\\log n)\\)， 基\\(O(r)\\)， 其余\\(O(1)\\) 算法选择 条件 可选算法 n较小， \\(n \\le 50\\) 直接插入、简单选择 基本有序 直接插入、冒泡 n较大，要\\(O(n \\log n)\\) 快排、堆排（不稳定），归并排序（稳定） n较大，要快，要稳定 归并排序，与直插结合的改进的归并排序 n很大，位数很少，可以分解 基数排序 记录本身信息量太大 为了避免移动，可以使用链表作为存储结构","tags":[{"name":"排序","slug":"排序","permalink":"http://plmsmile.github.io/tags/排序/"},{"name":"插入","slug":"插入","permalink":"http://plmsmile.github.io/tags/插入/"},{"name":"快排，快速排序","slug":"快排，快速排序","permalink":"http://plmsmile.github.io/tags/快排，快速排序/"},{"name":"堆排序","slug":"堆排序","permalink":"http://plmsmile.github.io/tags/堆排序/"},{"name":"归并排序","slug":"归并排序","permalink":"http://plmsmile.github.io/tags/归并排序/"},{"name":"基数排序","slug":"基数排序","permalink":"http://plmsmile.github.io/tags/基数排序/"},{"name":"冒泡排序","slug":"冒泡排序","permalink":"http://plmsmile.github.io/tags/冒泡排序/"}]},{"title":"cpp-pointer-object-reference","date":"2017-12-25T13:17:36.000Z","path":"2017/12/25/cpp-pointer-object-reference/","text":"类对象和指针的区别 类对象和指针 代码 类 1234567class Test&#123; public: int a; Test()&#123; a = 1; &#125;&#125;; 类指针 12345678910111213141516171819202122void test1() &#123; // 1. 两个类的指针，使用了new Test* t1 = new Test(); t1-&gt;a = 10; Test* t2 = new Test(); t2-&gt;a = 5; // 2. 两个指针的所指向的地址不一样 cout &lt;&lt; \"&amp;t1:\" &lt;&lt; t1 &lt;&lt; \" a = \" &lt;&lt; t1-&gt;a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; t2 &lt;&lt; \" a = \" &lt;&lt; t2-&gt;a &lt;&lt;endl; // 3. 指针t1的值赋值给了t2，两个指针指向的地址相同，都是t1的地址 t2 = t1; cout &lt;&lt; \"&amp;t1:\" &lt;&lt; t1 &lt;&lt; \" a = \" &lt;&lt; t1-&gt;a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; t2 &lt;&lt; \" a = \" &lt;&lt; t2-&gt;a &lt;&lt;endl; t1-&gt;a = 111; t2-&gt;a = 222; // 4. 修改了同样的地方，输出为222 cout &lt;&lt; \"&amp;t1:\" &lt;&lt; t1 &lt;&lt; \" a = \" &lt;&lt; t1-&gt;a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; t2 &lt;&lt; \" a = \" &lt;&lt; t2-&gt;a &lt;&lt;endl;&#125; 类对象 1234567891011121314151617181920212223void test2() &#123; // 1. 两个类对象 Test t1; t1.a = 10; Test t2; t2.a = 5; // 2. 对象的内容不一样 cout &lt;&lt; \"&amp;t1:\" &lt;&lt; &amp;t1 &lt;&lt; \" a = \" &lt;&lt; t1.a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; &amp;t2 &lt;&lt; \" a = \" &lt;&lt; t2.a &lt;&lt;endl; // 3. 把t1对象的内容赋值给t2。t2的内容和t1相同 t2 = t1; cout &lt;&lt; \"&amp;t1:\" &lt;&lt; &amp;t1 &lt;&lt; \" a = \" &lt;&lt; t1.a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; &amp;t2 &lt;&lt; \" a = \" &lt;&lt; t2.a &lt;&lt;endl; // 4. 再分别修改两个不同的对象，输出分别为111， 222 t1.a = 111; t2.a = 222; cout &lt;&lt; \"&amp;t1:\" &lt;&lt; &amp;t1 &lt;&lt; \" a = \" &lt;&lt; t1.a &lt;&lt; endl; cout &lt;&lt; \"&amp;t2:\" &lt;&lt; &amp;t2 &lt;&lt; \" a = \" &lt;&lt; t2.a &lt;&lt;endl; &#125; 不同点 类指针 类对象 内存分配 定义类指针，不分配内存 定义类对象，分配内存 关系 内存地址，指向类对象。利用构造函数分配一块内存 访问 间接访问 直接访问 多态 实现多态，父类指针调用子类对象 不能实现多态，声明即调用构造函数，已分配内存 堆栈 内存堆，永久变量，手动释放，new-delete搭配 内存栈，局部临时变量 new的对象在堆中 在栈中 使用 new： S* s = new S() , s-&gt;name 声明即可： S s;，s.name 生命期 需要delete 类的析构函数来释放空间 传参 指针4个字节 对象，参数传递资源占用太大 虚函数f 调用分配给它空间时那种类的func 调用自己的func 其它 父类指针指向子类对象 推荐使用const &amp;引用，安全系数高。","tags":[{"name":"cpp","slug":"cpp","permalink":"http://plmsmile.github.io/tags/cpp/"},{"name":"指针，对象，引用","slug":"指针，对象，引用","permalink":"http://plmsmile.github.io/tags/指针，对象，引用/"}]},{"title":"cs224n-assignment-1","date":"2017-12-17T04:47:06.000Z","path":"2017/12/17/cs224n-assignment-1/","text":"Softmax Softmax常数不变性 \\[ \\rm{softmax}(\\mathbf{x})_i = \\frac{e^{\\mathbf x_i}}{\\sum_{j}e^{\\mathbf{x}_j}} \\] 一般在计算softmax的时候，避免太大的数，要加一个常数。 一般是减去最大的数。 \\[ \\rm{softmax}(x) = \\rm{softmax}(x+c) \\] 关键代码 12345678def softmax(x): exp_func = lambda x: np.exp(x - np.max(x)) sum_func = lambda x: 1.0 / np.sum(x) x = np.apply_along_axis(exp_func, -1, x) denom = np.apply_along_axis(sum_func, -1, x) denom = denom[..., np.newaxis] x = x * denom return x 神经网络基础 Sigmoid实现 我的sigmoid笔记 \\[ \\begin{align} &amp; \\sigma (z) = \\frac {1} {1 + \\exp(-z)}, \\; \\sigma(z) \\in (0,1) \\\\ \\\\ &amp; \\sigma^\\prime (z) = \\sigma(z) (1 - \\sigma(z)) \\\\ \\end{align} \\] 关键代码 12345678910def sigmoid(x): s = 1.0 / (1 + np.exp(-x)) return sdef sigmoid_grad(s): \"\"\" 对sigmoid的函数值，求梯度 \"\"\" ds = s * (1 - s) return ds Softmax求梯度 交叉熵和softmax如下，记softmax的输入为\\(\\theta\\) ，\\(y\\)是真实one-hot向量。 \\[ \\begin{align} &amp; \\rm{CE}(y, \\hat y) = - \\sum_{i} y_i \\times \\log (\\hat y_i) \\\\ \\\\ &amp; \\hat y = \\rm{softmax} (\\theta)\\\\ \\end{align} \\] softmax求导 引入记号： \\[ \\begin{align} &amp; f_i = e^{\\theta_i} &amp; \\text{分子} \\\\ &amp; g_i = \\sum_{k=1}^{K}e^{\\theta_k} &amp; \\text{分母，与i无关} \\\\ &amp; \\hat y_i = S_i = \\frac{f_i}{g_i} &amp; \\text{softmax}\\\\ \\end{align} \\] 则有\\(S_i​\\)对其中的一个数据\\(\\theta_j​\\) 求梯度： \\[ \\frac{\\partial S_i}{\\partial \\theta_j} = \\frac{f_i^{\\prime} g_i - f_i g_i^{\\prime}}{g_i^2} \\] 其中两个导数 \\[ f^{\\prime}_i(\\theta_j) = \\begin{cases} &amp; e^{\\theta_j}, &amp; i = j\\\\ &amp; 0, &amp; i \\ne j \\\\ \\end{cases} \\] \\[ g^{\\prime}_i(\\theta_j) = e^{\\theta_j} \\] \\(i=j\\)时 \\[ \\begin{align} \\frac{\\partial S_i}{\\partial \\theta_j} &amp; = \\frac{e^{\\theta_j} \\cdot \\sum_{k}e^{\\theta_k}- e^{\\theta_i} \\cdot e^{\\theta_j}}{\\left( \\sum_ke^{\\theta_k}\\right)^2} \\\\ \\\\ &amp; = \\frac{e^{\\theta_j}}{\\sum_ke^{\\theta_k}} \\cdot \\left( 1 - \\frac{e^{\\theta_j}}{\\sum_k e^{\\theta_k}} \\right) \\\\ \\\\ &amp; = S_i \\cdot (1 - S_i) \\end{align} \\] \\(i \\ne j\\)时 \\[ \\begin{align} \\frac{\\partial S_i}{\\partial \\theta_j} &amp; = \\frac{ - e^{\\theta_i} \\cdot e^{\\theta_j}}{\\left( \\sum_ke^{\\theta_k}\\right)^2} = - S_i \\cdot S_j \\end{align} \\] 交叉熵求梯度 \\[ \\begin{align} &amp; \\rm{CE}(y, \\hat y) = - \\sum_{i} y_i \\times \\log (\\hat y_i) \\\\ \\\\ &amp; \\hat y = \\rm{S} (\\theta)\\\\ \\end{align} \\] 只关注有关系的部分，带入\\(y_i =1\\) ： \\[ \\begin{align} \\frac{\\partial CE}{\\partial \\theta_i} &amp; = -\\frac{\\partial \\log \\hat y_i}{\\partial \\theta_i} = - \\frac{1}{\\hat y_i} \\cdot \\frac{\\partial \\hat y_i}{\\partial \\theta_i} \\\\ \\\\ &amp; = - \\frac{1}{S_i} \\cdot \\frac{\\partial S_i}{\\partial \\theta_i} = S_i - 1 \\\\ \\\\ &amp; = \\hat y_i - y_i \\end{align} \\] 不带入求导 \\[ \\begin{align} \\frac{\\partial CE}{\\partial \\theta_i} &amp; = - \\sum_{k}y_k \\times \\frac{\\partial \\log S_k}{\\partial \\theta_i} \\\\ &amp; = - \\sum_{k}y_k \\times \\frac{1}{S_k}\\times \\frac{\\partial S_k}{\\partial \\theta_i} \\\\ &amp; = - y_i (1 - S_i) - \\sum_{k \\ne i} y_k \\cdot \\frac{1}{S_k} \\cdot (- S_i \\cdot S_k) \\\\ &amp; = - y_i (1 - S_i) + \\sum_{k \\ne i} y_k \\cdot S_i \\\\ &amp; = S_i - y_i \\end{align} \\] 所以，交叉熵的导数是 \\[ \\frac{\\partial CE}{\\partial \\theta_i} = \\hat y_i - y_i, \\quad \\quad \\frac{\\partial CE(y, \\hat y)}{\\partial \\theta} = \\hat y - y \\] 即 \\[ \\frac{\\partial CE(y, \\hat y)}{\\partial \\theta_i} = \\begin{cases} &amp; \\hat y_i - 1, &amp; \\text{i是label} \\\\ &amp;\\hat y_i, &amp; \\text{其它}\\\\ \\end{cases} \\] 简单网络 前向计算 \\[ \\begin{align} &amp; z_1 = xW_1 + b_1 \\\\ \\\\ &amp; h = \\rm{sigmoid}(z1) \\\\ \\\\ &amp; z_2 = hW_2 + b_2 \\\\ \\\\ &amp; \\hat y = \\rm{softmax}(z_2) \\end{align} \\] 关键代码： 123def forward_backward_prop(data, labels, params, dimensions): h = sigmoid(np.dot(data, W1) + b1) yhat = softmax(np.dot(h, W2) + b2) loss函数 \\[ J = \\rm{CE}(y, \\hat y) \\] 关键代码： 123def forward_backward_prop(data, labels, params, dimensions): # yhat[labels==1]实际上是boolean索引，见我的numpy_api.ipynb cost = np.sum(-np.log(yhat[labels == 1])) / data.shape[0] 反向传播 \\[ \\begin {align} &amp; \\delta_2 = \\frac{\\partial J}{\\partial z_2} = \\hat y - y \\\\ \\\\ &amp; \\frac{\\partial J}{\\partial h} = \\delta_2 \\cdot \\frac{\\partial z_2}{\\partial h} = \\delta_2 W_2^T \\\\ \\\\ &amp; \\delta_1 = \\frac{\\partial J}{\\partial z_1} = \\frac{\\partial J}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z_1} = \\delta_2 W_2^T \\circ \\sigma^{\\prime}(z_1) \\\\ \\\\ &amp; \\frac{\\partial J}{\\partial x} = \\delta_1 W_1^T \\end{align} \\] 一共有\\((d_x + 1) \\cdot d_h + (d_h +1) \\cdot d_y\\) 个参数。 关键代码： 123456789101112131415def forward_backward_prop(data, labels, params, dimensions): # 前面推导的softmax梯度公式 gradyhat = (yhat - labels) / data.shape[0] # 链式法则 gradW2 = np.dot(h.T, gradyhat) # 本地导数是1，把第1维的所有加起来 gradb2 = np.sum(gradyhat, axis=0, keepdims=True) gradh = np.dot(gradyhat, W2.T) gradz1 = gradh * sigmoid_grad(h) gradW1 = np.dot(data.T, gradz1) gradb1 = np.sum(gradz1, axis=0, keepdims=True) grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten())) return cost, grad 梯度检查 我的梯度检查 123456789101112131415161718192021222324252627def gradcheck_naive(f, x): fx, grad = f(x) # Evaluate function value at original point h = 1e-4 # Do not change this! # Iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: ix = it.multi_index # 关键代码 x[ix] += h random.setstate(rndstate) new_f1 = f(x)[0] x[ix] -= 2 * h random.setstate(rndstate) new_f2 = f(x)[0] x[ix] += h numgrad = (new_f1 - new_f2) / (2 * h) # Compare gradients reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix])) if reldiff &gt; 1e-5: print (\"Gradient check failed.\") print (\"First gradient error found at index %s\" % str(ix)) print (\"Your gradient: %f \\t Numerical gradient: %f\" % ( grad[ix], numgrad)) return it.iternext() # Step to next dimension Word2Vec 我的word2vec笔记 词向量的梯度 符号定义 \\(v_c\\) 中心词向量，输入词向量，\\(V\\)， \\(\\mathbb{R}^{W\\times d}\\) \\(u_o\\) 上下文词向量，输出词向量，\\(U=[u_1, u_2, \\cdots, u_w]\\) , \\(\\mathbb{R}^{d\\times W}\\) 前向 预测o是c的上下文概率，o为正确单词 \\[ \\hat y_o = p(o \\mid c) = \\rm{softmax}(o) = \\frac{\\exp(u_o^T v_c)}{\\sum_{w} \\exp(u_w^T v_c)} \\] 得分向量： \\[ z=U^T \\cdot v_c, \\quad [W,d] \\times[ d] \\in ,\\mathbb{R}^{W } \\] loss及梯度 \\[ J_{\\rm{softmax-CE}}(v_c, o, U) = CE(y, \\hat y), \\quad \\text{其中} \\; \\frac{\\partial CE(y, \\hat y)}{\\partial \\theta} = \\hat y - y \\] 梯度 中文 计算 维数 \\(\\frac{\\partial J}{\\partial z}\\) softmax \\(\\hat y - y\\) \\(W\\) \\(\\frac{\\partial J}{\\partial v_c}\\) 中心词 \\(\\frac{\\partial J}{\\partial z} \\cdot \\frac{\\partial z}{\\partial v_c} = (\\hat y - y) \\cdot U^T\\) \\(d\\) \\(\\frac{\\partial J}{\\partial U}\\) 上下文 \\(\\frac{\\partial J}{\\partial z} \\cdot \\frac{\\partial z}{\\partial U^T}= (\\hat y - y) \\cdot v_c\\) \\(d \\times W\\) 关键代码 123456789101112131415161718192021222324def softmaxCostAndGradient(predicted, target, outputVectors, dataset): \"\"\" Softmax cost function for word2vec models Args: predicted: 中心词vc target: 上下文uo, index outputVectors: 输出，上下文矩阵U，W*d，未转置 dataset: Returns: cost: 交叉熵loss gradv: 一维向量 gradU: W*d \"\"\" vhat = predicted z = np.dot(outputVectors,vhat) preds = softmax(z) # Calculate the cost: cost = -np.log(preds[target]) # Gradients gradz = preds.copy() gradz[target] -= 1.0 gradU = np.outer(z, vhat) gradv = np.dot(outputVectors.T, z) ### END YOUR CODE return cost, gradv, gradU","tags":[{"name":"cs224n","slug":"cs224n","permalink":"http://plmsmile.github.io/tags/cs224n/"},{"name":"assignment","slug":"assignment","permalink":"http://plmsmile.github.io/tags/assignment/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"},{"name":"cbow","slug":"cbow","permalink":"http://plmsmile.github.io/tags/cbow/"},{"name":"skip-gram","slug":"skip-gram","permalink":"http://plmsmile.github.io/tags/skip-gram/"}]},{"title":"cs224n-notes-5-rnn","date":"2017-12-12T03:29:13.000Z","path":"2017/12/12/cs224n-notes-5-rnn/","text":"公式 \\[ P(s) = P(小猫) \\cdot P(爱 \\mid 小猫) \\cdot P(吃 \\mid 小猫,爱) \\cdot P(\\text{鱼} \\mid 小猫,爱,吃 ) \\] \\[ P(s) = P(小猫) \\cdot P(爱 \\mid 小猫) \\cdot P(吃 \\mid 爱) \\cdot P(\\text{鱼} \\mid 吃 ) \\] 具体推导 \\[ p(爱\\mid 小猫) = \\frac {\\rm{count(小猫, \\,爱)}} {\\rm{count}(小猫)} \\] 设hidden_size是 \\(d_h\\) ，词向量的维度是\\(d\\) 符号 意义 维数 \\(x_t\\) 时刻\\(t\\)的输入词向量 \\(\\mathbb R^{d}\\) \\(W^{(hx)}\\) 调整输入\\(x\\) \\(\\mathbb R^{d_h \\times d}\\) \\(h_{t-1}\\) 时刻t-1的隐状态 \\(\\mathbb R^{d_h}\\) \\(W^{(hh)}\\) 调整上一时刻的隐状态\\(h_{t-1}\\) \\(\\mathbb R^{d_h \\times d_h}\\) \\(W^{(hy)}\\) 把隐状态转h换为输出单词 \\(\\mathbb R^{\\vert V\\vert \\times d_h}\\) \\(y_{t}\\) 结合\\(x_t\\)与历史信息，预测的t+1时刻的单词 \\(\\mathbb R^{\\vert V\\vert}\\) \\(a\\) 激活函数，这里是sigmoid 无 RNN语言模型的计算过程 \\[ \\begin {align} &amp; h_t = a (W^{(hh)} h_{t-1} + W^{(hx)} x_t) \\\\ \\\\ &amp; \\hat y_t = \\rm{softmax} (W^{(hy)} h_t) \\\\ \\\\ &amp; \\hat p(x_{t+1}=w_j \\mid x_1, \\cdots, x_t) = \\hat y_{t,j} \\end{align} \\] 时刻t的损失 \\[ J_t(\\theta) = - \\sum_{j=1}^{\\vert V\\vert} y_{t, j} \\times \\log (\\hat y_{t,j}) \\] 总损失 \\[ J = \\frac{1}{T} \\sum_{t=1}^{T} J_t(\\theta) = - \\frac{1}{T} \\sum_{t=1}^{T}\\sum_{j=1}^{\\vert V\\vert} y_{t, j} \\times \\log (\\hat y_{t,j}) \\] 模型困惑度 \\[ \\rm{Perplexity} = 2 ^{\\text{交叉熵}} = 2^{J} \\] 梯度 简单点 \\[ \\begin {align} &amp; h_t = Wh_{t-1} + W^{(hx)} x_t \\\\ \\\\ &amp; \\hat y_t =W^{(s)} f(h_t) \\\\ \\\\ \\end{align} \\] 总的误差是之前每个时刻的误差之和 \\[ \\frac{\\partial E}{\\partial W} = \\sum_{t=1}^T \\frac{\\partial E_t}{\\partial W} \\] 每一时刻的误差又是之前每个时刻的误差之和，应用链式法则 \\[ \\frac{\\partial E_t}{\\partial W} = \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\sum_{k=1}^t \\frac{\\partial h_t}{\\partial h_k} \\frac{\\partial h_k}{\\partial W} \\] \\[ \\frac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\color{blue} {\\frac{\\partial h_t}{\\partial h_k}} \\frac{\\partial h_k}{\\partial W} \\] \\[ \\frac{\\partial E_t}{\\partial W} = \\sum_{k=1}^t \\frac{\\partial E_k}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k}\\frac{\\partial h_k}{\\partial h_{k-1}} \\frac{\\partial h_{k-1}}{\\partial W} \\] 而中间的 \\[ \\frac{\\partial h_t}{\\partial h_k} = \\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}} = \\prod_{j=k+1}^t W^T \\times \\rm{diag}[f^{\\prime}(j_{j-1})] \\] 而导数矩阵雅克比矩阵 \\[ \\frac{\\partial h_j}{\\partial h_{j-1}} = [ \\frac{\\partial h_{j}}{\\partial h_{j-1,1}}, \\cdots , \\frac{\\partial h_{j}}{\\partial h_{j-1,d_h}}] = \\begin{bmatrix} \\frac{\\partial h_{j,1}}{\\partial h_{j-1,1}} &amp; \\cdots &amp; \\frac{\\partial h_{j,1}}{\\partial h_{j-1,d_h}} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial h_{j,d_h}}{\\partial h_{j-1,1}} &amp; \\cdots &amp; \\frac{\\partial h_{j,d_h}}{\\partial h_{j-1,d_h}} \\\\ \\end{bmatrix} \\] 合并起来，得到最终的 \\[ \\frac{\\partial E}{\\partial W} = \\sum_{t=1}^T\\sum_{k=1}^t \\frac{\\partial E_t}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} (\\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}}) \\frac{\\partial h_k}{\\partial W} \\] 两个不等式 \\[ \\| \\frac{\\partial h_j}{\\partial h_{j-1}}\\| \\le \\| W^T\\| \\cdot \\|\\rm{diag}[f^{\\prime}(h_{j-1})] \\| \\le \\beta_W \\beta_h \\] 所以有 \\[ \\| \\frac{\\partial h_t}{\\partial h_k} \\| =\\| \\prod_{j=k+1}^t \\frac{\\partial h_j}{\\partial h_{j-1}} \\| \\le \\color{blue}{ (\\beta_W \\beta_h)^{t-k}} \\] 防止梯度爆炸 \\[ \\hat g =\\frac{\\partial E}{\\partial W} \\\\ \\\\ \\\\ \\hat g = \\frac{\\text{threshold}}{\\|\\hat g\\|} \\hat g \\] softmax \\[ p(w_t \\mid h_{t-1}) = p(c_t \\mid h_{t-1}) \\cdot p(w_t \\mid c_t) \\] Quasi-RNN 输入单词序列，\\(T\\)个d维的词向量 \\[ X \\in \\mathbb {R}^{T\\times d} \\] m维的filters，产生一个新的序列，产生m维的候选向量\\(\\mathbf{z}_t\\) \\[ Z \\in \\mathbb {R}^{T \\times m} \\] 宽度为k的filter，每个\\(z_t\\)只依赖于\\(\\mathbf{x}_{t-k+1}\\)到 \\(\\mathbf{x}_{t}\\) 。masked convolution \\[ \\mathbf{x}_{t-k+1} \\quad \\mathbf{x}_{t} \\quad \\mathbb{R}^{k \\times d \\times m} \\] 参数维数都是\\(\\mathbb{R}^{k \\times d \\times m}\\) ,*是masked convolution \\[ \\begin{align} &amp; Z = \\tanh (W_z * X) \\\\ \\\\ &amp; F = \\sigma(W_f * X) \\\\ \\\\ &amp; O = \\sigma(W_o * X) \\end{align} \\] fo-pooling \\[ \\begin{align} &amp; c_t = f_t \\cdot c_{t-1} + (1 - f_t) \\cdot z_t \\\\ \\\\ &amp; h_t = o_t \\cdot c_t \\end{align} \\] LSTM梯度消失 本质上维护一个状态\\(h_t\\)， 传统RNN，覆写 \\[ h_t = f(h_{t-1}, x_t) \\] 现代RNN，LSTM&amp;GRU，累加 \\[ h_t = \\sum_{k=1}^t \\Delta h_k \\] 和Residual Net相似，利用一个独立的加法通道，把梯度保持下去。","tags":[{"name":"cs224n","slug":"cs224n","permalink":"http://plmsmile.github.io/tags/cs224n/"}]},{"title":"nlp-labels","date":"2017-12-03T07:31:23.000Z","path":"2017/12/03/nlp-labels/","text":"词性标注和句法依存的符号 词性标注 IBM英语和中文标注集 书上的 斯坦福Stanford coreNLP 中 宾州树库 中的汉语词性标注规范， 如下 词性标记 英文名称 中文名称 示例 AD adverbs 副词 还 AS aspect marker 体表词 了，着，过，的（我是去年来过的） BA in ba-const 把，将 把，将 CC coordinating conjunction 并列连词 和，与，或，或者 CD cardinal conj 数词，基数词 一百 CS subordinating conj 从数连词 若，如果，如 DEC for relative-clause etc 标句词，关系从句的 我买的书 DEG associative 所有格、连接作用的 我的书 DT determiner 限定词 这 ETC tag for words in coordination phrase 等，等等 科技文教等领域，等，等等 IJ Interjection 感叹词 啊 JJ noun-modifier other than nouns 其他名词修饰语 共同的/DEG目的/NN他/PN是/VC男的/DEG LB in long bei-construction 长被 被他打了 LC localizer 方位词 桌子上 M measure word（including classifiers） 量词 一块糖 MSP some particles 其他结构助词 他/PN 所 需要/VV 的/DEC 所，而，以 NN common nouns 普通名词 桌子 NR proper nouns 专有名词 天安门 NT temporal nouns 时间名词 清朝，一月 OD ordinal numbers 序数词 第一 ON onomatopoeia 拟声词 哗啦啦 P prepositions 介词 在 PN pronouns 代词 你，我，他 PU punctuations 标点 ， 。 SB In long bei-consturction 短被 他/PN 被/SB 训了/AS SP Sentence-final particle 句末助词 你好吧、SP吧 呢 啊 吗 VA Predicative adjective 谓词形容词 太阳 红彤彤/VA 雪白 丰富 VC Copula 系动词 是 为 非 VE as the main verb “有”作为主要动词 有，无 VV verbs 普通动词 喜欢，走 自己总结的 标记 英文 中文 NP noun phrase 名词短语 PP prepositional phrase 介词短语 VP verb phrase 动词短语 NNS 名词（复数） NNP 专有名词（单数） NNPS 专有名词（复数） PRP 人称代词 JJ 形容词 JJS 形容词（最高级） JJR 形容词（比较级） MD 情态动词 VB 动词 VBP 动词，现在时，非第三人称单数 VBZ 动词，现在时，第三人称单数 VBG 动词，动名词，现在分词 英语标记 复杂版 词性标记 描述 UNKNOW 未知词 DT 限定词 QT 量词 CD 基数 NN 名词，单数 NNS 名词，复数 NNP 专有名词，单数 NNPS 专有名词，复数 EX 存在性的There PRP 人称代词，PP PRP$ 物主代词，PP$ POS 所有格结束词 RB 副词 RBS 副词，最高级 RBR 副词，比较级 句法依存 中心语为谓语 符号 意义 英语 备注 subj 主语 subject nsubj 名词性主语 nominal subject 同步、建设 top 主题 topic 是，建筑 npsubj 被动型主语 nominal passive subject 被句子 中的主语 csubj 从句主语 clausal subject 中文里无 xsubj x主语 一般一个主语下含多个从句 中心语为谓语或介词 符号 意义 英语 备注 obj 宾语 object dobj 直接宾语 颁布，文件 iobj 间接宾语 indirect object 基本不存在 range 间接宾语为数量词，格 成交，元 pobj 介词宾语 根据，要求 lobj 时间介词 来，今年 中心语为谓词 符号 意义 英语 备注 comp 补语 ccomp 从句补语 一般由两个动词组成， xcomp x从句补语 xclausal complement 不存在 acomp 形容词补语 adjectival complement tcomp 时间补语 temporal complement 遇到，以前 lccomp 位置补语 localizer complement 占，以上 rscomp 结果补语 resultative complement 中心语为名词 符号 意义 英语 备注 mod 修饰语 modifier pass 被动修饰 passive tmod 时间修饰 temporal modifier remod 关系从句修饰 relative clause modifier 问题，遇到 numod 数量修饰 numeric modifier 规定，若干 ornmod 序数修饰 numeric modifier clf 类别修饰 classifier modifier 文件，件 nmod 符合名词修饰 noun compound modifier 浦东，上海 amod 形容词修饰 adjective modifier 情况，新 advmod 副词修饰 adverbial modifier 做到，基本 vmod 动词修饰 verb modifier, participle modifier prnmod 插入词修饰 parenthetical modifier neg 不定修饰 negative modifier 遇到，不 det 限定词修饰 determiner modifier 活动，这些 possm 所属标记 possessive maker NP poss 所属修饰 possessive modifier NP dvpm DVP标记 DVP maker DVP(简单，的) dvpmod DVP修饰 DVP modifier DVP(采取，简单) assm 关联标记 associative marker DNP(开发，的) assmod 关联修饰 associative modifier NP|QP (教训，特区) prep 介词修饰 prepositional modifier NP|VP|IP (采取，对) clmod 从句修饰 clause modifier 因为，开始 plmod 介词性地点修饰 prepositional localizer modifier 在，上 asp 时态修饰 aspect marker 做到，了 partmod 分词修饰 participial modifier 中文不存在 etc 等关系 办法，等 中心语为实词 符号 意义 英语 备注 conj 联合 conjunct cop 系动双指助动词 copula cc 连接 coordination 指中心词与连词 其他 符号 意义 英语 备注 attr 属性关系 是，过程 cordmod 并列联合动词 coordinated verb compound 颁布，实行 mmod 清潭洞次 modal verb 得到，能 ba 把字关系 tclaus 时间从句 以后，积累 补语化成分 complementizer 一般指","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://plmsmile.github.io/tags/词性标注/"},{"name":"句法依存","slug":"句法依存","permalink":"http://plmsmile.github.io/tags/句法依存/"}]},{"title":"cs231n-linear-notes","date":"2017-11-27T03:24:40.000Z","path":"2017/11/27/cs231n-linear-notes/","text":"线性分类器，svm和交叉熵损失函数 线性分类器 得分函数 图片是三维数组，元素在0-255的整数。宽度-高度-3 ，3代表RGB颜色通道。 对图像进行零中心化。 输入图片\\(D=32 \\times 32 \\times 3 = 3072\\)个像素，压缩成1维向量，一共有\\(K=10\\)个类别。 \\[ f(x_i, W, b) = Wx_i + b \\] 分析 \\(W\\)的每一行都是一个类别的分类器，一共10个 得到分为每个类的score 改变\\(W, b\\) ，使得分类准确，正确的score高，错误的score低 为x添加一维，\\(x \\in \\mathbb R^{D+1}\\) ， 写为： \\[ f (x_i, W) = Wx_i \\] 理解 权重 输入4个像素，函数会根据权重对某些位置的某些颜色表现出喜好或者厌恶（正负）。 比如船类别，一般周围有很多蓝色的水，那么蓝色通道的权值就会很大（正）。绿色和红色就比较低（负）。那么如果出现绿色和红色的像素，就会降低是船的概率。 权重解释2 \\(W\\)的每一行对应于一个分类的模板(原型)， 用图像和模板去比较，计算得分（点积），找到最相似的模板。 线性函数 实际上，每个输入\\(x_i\\)就是3072维空间中的一个点，线性函数就是对这些点进行边界决策分类。 与线的距离越大，得分越高。 损失函数 最常用两个分类器：SVM和Softmax。分别使用SVM loss和交叉熵loss。 SVM 多类支持向量积损失（Multiclass Support Vector Machine）。正确分类比错误分类的得分高出一个边界值\\(\\Delta (一般= 1)\\) 。 记\\(x_i\\)分为第j个类别的得分为\\(s_j = f(x_i, W)_j\\) ，单个折叶损失 hinge loss， 也称作max margin loss ，如下： \\[ \\begin {align} loss &amp; = \\max(0, s_j - s_{y_i}+ \\Delta)= \\begin {cases} &amp; 0, &amp; s_{y_i} - s_j &gt; \\Delta \\\\ &amp; s_j - s_{y_i}+ \\Delta, &amp; 其它 \\\\ \\end{cases} \\\\ \\\\ &amp; =\\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\\\ \\end{align} \\] 如果错误分类进入红色区域，就开始计算loss。 第\\(i\\)个数据的loss就是把所有错误类别的loss加起来： \\[ L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i}+ \\Delta) \\] 正则化 plmsmile的L2正则化 使用L2正则化，对\\(W\\)的元素进行平方惩罚， 抑制大数值的权值。正则化loss（正则化惩罚）如下： \\[ R(W) = \\sum_k\\sum_l W_{kl}^2 \\] 最终loss就是数据损失+正则化损失， \\(\\lambda\\) 是正则化强度。 \\[ L = \\underbrace {\\frac{1}{N}\\sum_{i}L_i}_{\\text{data loss}} + \\underbrace {\\lambda R(W)}_{\\text{正则化 loss}} \\] 引入正则化以后，SVM就有了最大边界max margin 这一个良好性质。（不是很懂，后面再解决） Softmax plmsmile的交叉熵 每个类别的得分 \\[ s_j = f(x_i, W)_j = f_j \\] Softmax函数求得\\(x_i\\)分为第\\(j\\) 类的概率，这样会求得所有类别的概率，即预测的结果。 \\[ p(j \\mid x_i) = \\frac {\\exp(f_j)} {\\sum_{k} \\exp(f_k)} \\] 单个数据的loss，就是取其概率的负对数 ： \\[ L_i = - \\log p(y_i \\mid x_i) = - \\log \\left( \\frac{e^{f_{y_i}}}{\\sum e^{f_k}}\\right) = -f_{y_i} + \\log \\sum_{k}e^{f_k} \\] 从直观上看，最小化loss就是要最大化正确的概率（最小化正确分类的负对数概率），最小化其它分类的概率。 交叉熵的体现 程序会预测一个所有类别的概率分布\\(q = (p(1 \\mid x_i), \\cdots, p(K \\mid x_i))\\) 。真实label概率\\(p = (0, \\cdots, 1, 0,\\cdots, 0)\\) ，交叉熵： \\[ \\begin{align} H(p, q) &amp; = - \\sum_{x} p(x) \\log q(x) \\\\ &amp; = - (p(y_i) \\cdot \\log q(y_i)) = - (1 \\cdot \\log p(y_i \\mid x_i) ) = - \\log p(y_i \\mid x_i) \\end{align} \\] 由于\\(H(p) = 0\\)， 唯一确定，熵为0。交叉熵就等于真实和预测的分布的KL距离 。也就是说想要两个概率分布一样，即预测的所有概率密度都在正确类别上面。 \\[ H(p, q) = H(p) + D_{KL}(p || q) = D_{KL}(p || q) \\] 结合正则化 结合正则化 \\[ L = \\underbrace {\\frac{1}{N}\\sum_{i}L_i}_{\\text{data loss}} + \\underbrace {\\lambda R(W)}_{\\text{正则化 loss}} \\] 最小化正确概率分类的负对数概率，就是在进行最大似然估计。正则化部分就是对W的高斯先验，这里进行的是最大后验估计。（不懂） SVM和Softmax比较 SVM loss：希望正确分类比其他分类的得分高出一个边界值。 Softmax 交叉熵loss：希望正确分类概率大，其它分类概率小。","tags":[{"name":"cs231n","slug":"cs231n","permalink":"http://plmsmile.github.io/tags/cs231n/"},{"name":"线性分类","slug":"线性分类","permalink":"http://plmsmile.github.io/tags/线性分类/"},{"name":"svm","slug":"svm","permalink":"http://plmsmile.github.io/tags/svm/"}]},{"title":"cs224n-notes3-神经网络2","date":"2017-11-26T08:21:23.000Z","path":"2017/11/26/cs224n-notes3-neural-networks-2/","text":"过拟合 过拟合 训练数据很少，或者训练次数很多，会导致过拟合。避免过拟合的方法有如下几种： early stop 数据集扩增 正则化 （L1， L2权重衰减） Dropout 现在一般用L2正则化+Dropout。 过拟合时，拟合系数一般都很大。过拟合需要顾及到所有的数据点，意味着拟合函数波动很大。 看到，在某些很小的区间内里，函数值的变化很剧烈。意味着这些小区间的导数值（绝对值）非常大。由于自变量值可大可小，所以只有系数足够大，才能保证导数值足够大。 所以：过拟合时，参数一般都很大。参数较小时，意味着模型复杂度更低，对数据的拟合刚刚好， 这也是奥卡姆剃刀法则。 范数 向量范数 \\(x \\in \\mathbb {R}^d\\) 范数 定义 1-范数 \\(\\left \\| x\\right\\|_1 = \\sum_i^d \\|x_i\\|\\)， 绝对值之和 2-范数 \\(\\left \\| x\\right\\|_2 = \\left(\\sum_i^d \\|x_i\\|^2\\right)^{\\frac{1}{2}}\\)， 绝对值之和再开方 p-范数 \\(\\left \\| x\\right\\|_p = \\left(\\sum_i^d \\|x_i\\|^p\\right)^{\\frac{1}{p}}\\)， 绝对值的p次方之和的\\(\\frac{1}{p}\\)次幂 \\(\\infty\\)-范数 \\(\\left \\| x\\right\\|_\\infty = \\max_\\limits i \\|x_i\\|\\) ，绝对值的最大值 -\\(\\infty\\)-范数 \\(\\left \\| x\\right\\|_{-\\infty} = \\min_\\limits i \\|x_i\\|\\) ，绝对值的最小值 矩阵范数 \\(A \\in \\mathbb R^{m \\times n}\\) 范数 定义 1-范数 \\(\\left \\| A\\right\\|_1 = \\max \\limits_{j}\\sum_i^m \\|a_{ij}\\|\\)，列和范数，矩阵列向量绝对值之和的最大值。 \\(\\infty\\)-范数 \\(\\left \\| A\\right\\|_\\infty = \\max_\\limits i \\sum_{j}^{n}\\|a_{ij}\\|\\) ，行和范数，所有行向量绝对值之和的最大值。 2-范数 \\(\\left \\| A\\right\\|_2 = \\sqrt{\\lambda_{m}}\\) ， 其中\\(\\lambda_m\\)是\\(A^TA\\)的最大特征值。 F-范数 \\(\\left \\| A\\right\\|_F = \\left(\\sum_i^m \\sum_j^n a_{ij}^2\\right)^{\\frac{1}{2}}\\)，所有元素的平方之和，再开方。或者不开方， L2正则化就直接平方，不开方。 L2正则化权重衰减 为了避免过拟合，使用L2正则化参数。\\(\\lambda\\)是正则项系数，用来权衡正则项和默认损失的比重。\\(\\lambda\\) 的选取很重要。 \\[ J_R = J + \\lambda \\sum_{i=1}^L \\left \\| W^{(i)}\\right \\|_F \\] L2惩罚更倾向于更小更分散的权重向量，鼓励使用所有维度的特征，而不是只依赖其中的几个，这也避免了过拟合。 标准L2正则化 \\(\\lambda\\) 是正则项系数，\\(n\\)是数据数量，\\(w\\)是模型的参数。 \\[ C = C_0 + \\frac {\\lambda} {2n} \\sum_w w^2 \\] \\(C\\)对参数\\(w\\)和\\(b\\)的偏导： \\[ \\begin {align} &amp; \\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\\\ &amp; \\frac{\\partial C}{\\partial b} = \\frac{\\partial C_0}{\\partial b} \\\\ \\end{align} \\] 更新参数 ：可以看出，正则化\\(C\\)对\\(w\\)有影响，对\\(b\\)无影响。 \\[ \\begin{align} w &amp;= w - \\alpha \\cdot \\frac{\\partial C}{\\partial w} \\\\ &amp;= (1 - \\frac{\\alpha \\lambda}{n})w - \\alpha \\frac{\\partial C_0}{\\partial w} \\\\ \\end{align} \\] 从上式可以看出： 不使用正则化时，\\(w\\)的系数是1 使用正则化时 \\(w\\)的系数是\\(1 - \\frac{\\alpha \\lambda}{n} &lt; 1\\) ，效果是减小\\(w\\)， 所以是权重衰减 weight decay 当然，\\(w\\)具体增大或减小，还取决于后面的导数项 mini-batch随机梯度下降 设\\(m\\) 是这个batch的样本个数，有更新参数如下，即求batch个C对w的平均偏导值 \\[ \\begin {align} &amp; w = (1 - \\frac{\\alpha \\lambda}{n})w - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}\\frac{\\partial C_i}{\\partial w} \\\\ &amp; b = b - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m}\\frac{\\partial C_i}{\\partial b} \\\\ \\end{align} \\] 所以，权重衰减后一般可以减小过拟合。 L2正则化比L1正则化更加发散，权值也会被限制的更小。 一般使用L2正则化。 还有一种方法是最大范数限制：给范数一个上界\\(\\left \\| w \\right \\| &lt; c\\) ， 可以在学习率太高的时候网络不会爆炸，因为更新总是有界的。 实例说明 增加网络的层的数量和尺寸时，网络的容量上升，多个神经元一起合作，可以表达各种复杂的函数。 如下图，2分类问题，有噪声数据。 一个隐藏层。神经元数量分别是3、6、20。很明显20过拟合了，拟合了所有的数据。正则化就是处理过拟合的非常好的办法。 对20个神经元的网络，使用正则化，解决过拟合问题。正则化强度\\(\\lambda\\)很重要。 L1正则化 正则化loss如下： \\[ C = C_0 + \\frac {\\lambda} {n} \\sum_w |w| \\] 对\\(w\\)的偏导， 其中\\(\\rm{sgn}(w)\\)是符号函数： \\[ \\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} \\cdot \\rm{sgn}(w) \\] 更新参数： \\[ w = w - \\frac{\\alpha \\lambda}{n} \\cdot \\rm{sgn}(w) - \\alpha \\frac{\\partial C_0}{\\partial w} \\] 分析：\\(w\\)为正，减小；\\(w\\)为负，增大。所以L1正则化就是使参数向0靠近，是权重尽可能为0，减小网络复杂度，防止过拟合。 特别地：当\\(w=0\\)时，不可导，就不要正则化项了。L1正则化更加稀疏。 随机失活Dropout Dropout是非常有用的正则化的办法，它改变了网络结构。一般采用L2正则化+Dropout来防止过拟合。 训练的时候，输出不变，随机以概率\\(p\\)保留神经元，\\(1-p\\)删除神经元（置位0）。每次迭代删除的神经元都不一样。 BP的时候，置位0的神经元的参数就不再更新， 只更新前向时alive的神经元。 预测的时候，要保留所有的神经元，即不使用Dropout。 相当于训练了很多个（指数级数量）小网络（半数网络），在预测的时候综合它们的结果。随着训练的进行，大部分的半数网络都可以给出正确的分类结果。 数据预处理 用的很多的是0中心化。CNN中很少用PCA和白化。 应该：线划分训练、验证、测试集，只是从训练集中求平均值！然后各个集再减去这个平均值。 中心化 也称作均值减法， 把数据所有维度变成0均值，其实就是减去均值。就是将数据迁移到原点。 \\[ x = x - \\rm{avg}(x) = x - \\bar x \\] 标准化 也称作归一化， 数据所有维度都归一化，使其数值变化范围都近似相等。 除以标准差 最大值和最小值按照比例缩放到\\((-1 ,1)\\) 之间 方差\\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar x)^2\\) ，标准差就是\\(s\\) 。数据除以标准差，接近标准高斯分布。 \\[ x = \\frac{x}{s} \\] PCA 斯坦福PCA ，CSDNPCA和SVD的区别和联系 协方差 协方差就是乘积的期望-期望的乘积。 \\[ \\rm{Cov}(X, Y) = E(XY) - E(X)E(Y) \\] 协方差的性质如下： \\[ \\begin{align} &amp; \\rm{Cov}(X, Y) = \\rm{Conv}(Y, X) \\\\ \\\\ &amp; \\rm{Cov}(aX, bY) = ab \\cdot \\rm{Conv}(Y, X) \\\\ \\\\ &amp; \\rm{Cov}(X, X) = E(X^2) - E^2(X) = D(X) , \\quad \\text{三方公式}\\\\ \\\\ &amp; \\rm{Cov}(X, C) = 0 \\\\ \\\\ &amp; \\rm{Cov}(X, Y) = 0 \\leftrightarrow X与Y独立 \\end{align} \\] 还有别的性质就看考研笔记吧。 奇异值分解 \\[ A_{m \\times n} = U_{m \\times m} \\Sigma_{m \\times n} V^T_{n \\times n} \\] \\(V_{n \\times n}\\) ：\\(V\\)的列，一组对A正交输入或分析的基向量（线性无关）。这些向量是\\(M^TM\\) 的特征向量。 \\(U_{m \\times m}\\) ：\\(U\\)的列，一组对A正交输出的基向量 。是\\(MM^T\\)的特征向量。 \\(\\Sigma_{m \\times n}\\)：对角矩阵。对角元素按照从小到大排列，这些对角元素称为奇异值。 是\\(M^TM, MM^T\\) 的特征值的非负平方根，并且与U和V的行向量对应。 记\\(r\\)是非0奇异值的个数，则A中仅有\\(r\\)个重要特征，其余特征都是噪声和冗余特征。 奇异值的物理意义 利用SVD进行PCA 先将数据中心化。输入是\\(X \\in \\mathbb R^ {N \\times D}\\) ，则协方差矩阵 如下： \\[ \\mathrm{Cov}(X) = \\frac{X^TX}{N} \\; \\in \\mathbb R^{D \\times D} \\] 比如X有a和b两维，均值均是0。那么\\(\\rm{Cov}(ab)=E(ab)-0=(a_0b_0+a_1b_1+\\cdots + a_nb_n) /n\\) ，就得到了协方差值。 中心化 计算\\(x\\)的协方差矩阵cov 对协方差矩阵cov进行svd分解，得到u, s, v 去除x的相关性，旋转，\\(xrot = x \\cdot u\\) ，此时xrot的协方差矩阵只有对角线才有值，其余均为0 选出大于0的奇异值 数据降维 12345678910111213141516171819202122def test_pca(): x = np.random.randn(5, 10) # 中心化 x -= np.mean(x, axis=0) print (x.shape) # 协方差 conv = np.dot(x.T, x) / x.shape[0] print (conv.shape) print (conv) u, s, v = np.linalg.svd(conv) print (s) print (u.shape, s.shape, v.shape) # 大于0的奇异值 n_sv = np.where(s &gt; 1e-5)[0].shape[0] print(n_sv) # 对数据去除相关性 xrot = np.dot(x, u) print (xrot.shape) # 数据降维 xrot_reduced = np.dot(x, u[:, :n_sv]) # 降到了4维 print (xrot_reduced.shape) 白化 斯坦福白化 白化希望特征之间的相关性较低，所有特征具有相同的协方差。白化后，得到均值为0，协方差相等的矩阵。对\\(xrot\\)除以特征值。 \\[ x_{white} = \\frac{x_{rot}}{\\sqrt{\\lambda + \\epsilon}} \\] 1x_white = xrot / np.sqrt(s + 1e-5) 缺陷是：可能会夸大数据中的早上，因为把所有维度都拉伸到了相同的数值范围。可能有一些极少差异性（方差小）但大多数是噪声的维度。可以使用平滑来解决。 权重初始化 如果数据恰当归一化以后，可以假设所有权重数值中大约一半为正数，一半为负数。所以期望参数值是0。 千万不能够全零初始化。因为每个神经元的输出相同，BP时梯度也相同，参数更新也相同。神经元之间就失去了不对称性的源头。 小随机数初始化 如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并成为网络的不同部分。 参数接近于0单不等于0。使用零均值和标准差的高斯分布来生成随机数初始化参数，这样就打破了对称性。 1W = 0.01 * np.random.randn(D,H) 注意：不是参数值初始越小就一定好。参数小，意味着会减小BP中的梯度信号，在深度网络中，就会有问题。 校准方差 随着数据集的增长，随机初始化的神经元的输出数据分布的方差也会增大。可以使用\\(\\frac{1}{\\sqrt{n}}\\) 校准方差。n是数据的数量。这样就能保证网络中所有神经元起始时有近似同样的输出分布。这样也能够提高收敛的速度。 感觉实际上就是做了一个归一化。 数学详细推导见cs231n ， \\(s = \\sum_{i}^nw_ix_i\\) ，假设w和x都服从同样的分布。想要输出s和输入x有同样的方差。 \\[ \\begin {align} &amp; \\because D(s) = n \\cdot D(w)D(x), \\; D(s) = D(x) \\\\ &amp; \\therefore D(w) = \\frac{1}{n} \\\\ &amp; \\because D(w_{old}) = 1, \\; D(aX) = a^2 D(X) \\\\ &amp; \\therefore D(w) = \\frac{1}{n}D(w_{old}) = D(\\frac{1}{\\sqrt n} w_{old}) \\\\ &amp; \\therefore w = \\frac{1}{\\sqrt n} w_{old} \\end{align} \\] 所以要使用\\(\\frac{1}{\\sqrt{n}}\\)来标准化参数： 1W = 0.01 * np.random.randn(D,H)/ sqrt(n) 经验公式 对于某一层的方差，应该取决于两层的输入和输出神经元的数量，如下： \\[ \\rm{D}(w) = \\frac{2}{n_{in} + n_{out}} \\] ReLU来说，方差应该是\\(\\frac{2}{n}\\) 1W = 0.01 * np.random.randn(D,H) * sqrt(2.0 / n) 稀疏和偏置初始化 一般稀疏初始化用的比较少。一般偏置都初始化为0。 Batch Normalization 莫凡python BN讲解 和 CSDN-BN论文介绍 。Batch Normalization和普通数据标准化类似，是将分散的数据标准化。 Batch Normalization在神经网络非常流行，已经成为一个标准了。 训练速度分析 网络训练的时候，每一层网络参数更新，会导致下一层输入数据分布的变化。这个称为Internal Convariate Shift。 需要对数据归一化的原因 ： 神经网络的本质是学习数据分布。如果训练数据与测试数据的分布不同，那么泛化能力也大大降低 如果每个batch数据分布不同（batch 梯度下降），每次迭代都要去学习适应不同的分布，会大大降低训练速度 深度网络，前几层数据微小变化，后面几层数据差距会积累放大。 一旦某一层网络输入数据发生改变，这层网络就需要去适应学习这个新的数据分布。如果训练数据的分布一直变化，那么就会影响网络的训练速度。 敏感度问题 神经网络中，如果使用tanh激活函数，初始权值是0.1。 输入\\(x=1\\)， 正常更新： \\[ z = wx = 0.1, \\quad a(z_1) = 0.1 \\quad \\to \\quad a^\\prime(z) = 0.99 \\] 但是如果一开始输入 \\(x=20\\) ，会导致梯度消失，不更新参数。 \\[ z = wx = 2 ,\\quad a(z) \\approx 1 \\quad \\to \\quad a^\\prime(z) = 0 \\] 同样地，如果再输入\\(x=100\\) ，神经元的输出依然是接近于1，不更新参数。 \\[ z = wx = 10 ,\\quad a(z) \\approx 1 \\quad \\to \\quad a^\\prime(z) = 0 \\] 对于一个变化范围比较大特征维度，神经网络在初始阶段对它已经不敏感没有区分度了！ 这样的问题，在神经网络的输入层和中间层都存在。 BN算法 BN算法在每一次迭代中，对每一层的输入都进行归一化。把数据转换为均值为0、方差为1的高斯分布。 \\[ \\hat x = \\frac{x - E(x)} {\\sqrt{D(x) + \\epsilon}} \\] 非常大的缺陷：强行归一化会破坏掉刚刚学习到的特征。 把每层的数据分布都固定了，但不一定是前面一层学习到的数据分布。 牛逼的地方 ：设置两个可以学习的变量扩展参数\\(\\gamma\\) ，和平移参数 \\(\\beta\\) ，用这两个变量去还原上一层应该学习到的数据分布。（但是芳芳说，这一步其实可能没那么重要，要不要都行，CNN的本身会处理得更好）。 \\[ y = \\gamma \\hat x+ \\beta \\] 这样理解：用这两个参数，让神经网络自己去学习琢磨是前面的标准化是否有优化作用，如果没有优化效果，就用\\(\\gamma, \\beta\\)来抵消标准化的操作。 这样，BN就把原来不固定的数据分布，全部转换为固定的数据分布，而这种数据分布恰恰就是要学习到的分布。从而加速了网络的训练。 对一个mini-batch进行更新， 输入一个\\(batchsize=m\\)的数据，学习两个参数，输出\\(y\\) \\[ \\begin{align} &amp; \\mu = \\frac{1}{m} \\sum_{i=1}^m x_i &amp; \\text{求均值} \\\\ &amp; \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2 &amp; \\text{求方差} \\\\ &amp; \\hat x = \\frac{x - E(x)} {\\sqrt{\\sigma^2 + \\epsilon}} &amp; \\text{标准化} \\\\ &amp; y = \\gamma \\hat x+ \\beta &amp; \\text{scale and shfit} \\end{align} \\] 其实就是对输入数据做个归一化： \\[ z = wx+b \\to z = \\rm{BN}(wx + b) \\to a = f(z) \\] 一般在全连接层和激活函数之间添加BN层。 在测试的时候，由于是没有batch，所以使用固定的均值和标准差，也就是对训练的各个batch的均值和标准差做批处理。 \\[ E(x) = E(\\mu), \\quad D(x) = \\frac{b}{b-1} E(\\sigma^2) \\] BN的优点 1 训练速度快 2 选择大的初始学习率 初始大学习率，学习率的衰减也很快。快速训练收敛。小的学习率也可以。 3 不再需要Dropout BN本身就可以提高网络泛化能力，可以不需要Dropout和L2正则化。源神说，现在主流的网络都没有dropout了。但是会使用L2正则化，比较小的正则化。 4 不再需要局部相应归一化 5 可以把训练数据彻底打乱 效果图片展示 对所有数据标准化到一个范围，这样大部分的激活值都不会饱和，都不是-1或者1。 大部分的激活值在各个分布区间都有值。再传递到后面，数据更有价值。","tags":[{"name":"CS224N","slug":"CS224N","permalink":"http://plmsmile.github.io/tags/CS224N/"},{"name":"正则化","slug":"正则化","permalink":"http://plmsmile.github.io/tags/正则化/"},{"name":"范数","slug":"范数","permalink":"http://plmsmile.github.io/tags/范数/"},{"name":"Dropout","slug":"Dropout","permalink":"http://plmsmile.github.io/tags/Dropout/"},{"name":"数据预处理","slug":"数据预处理","permalink":"http://plmsmile.github.io/tags/数据预处理/"},{"name":"PCA","slug":"PCA","permalink":"http://plmsmile.github.io/tags/PCA/"},{"name":"白化","slug":"白化","permalink":"http://plmsmile.github.io/tags/白化/"},{"name":"BatchNorm","slug":"BatchNorm","permalink":"http://plmsmile.github.io/tags/BatchNorm/"}]},{"title":"cs224n-notes3-神经网络","date":"2017-11-23T04:01:08.000Z","path":"2017/11/23/cs224n-notes3-neural-networks/","text":"神经网络基础 很多数据都是非线性分割的，所以需要一种非线性non-linear决策边界 来分类。神经网络包含很多这样的非线性的决策函数。 神经元 神经元其实就是一个计算单元。 输入向量 \\(x \\in \\mathbb R^n\\) \\(z = w^T x + b\\) \\(a = f(z)\\) 激活函数，sigmoid, relu等，后文有讲。 Sigmoid神经元 传统用sigmoid多，但是现在一定不要使用啦。大多使用Relu作为激活函数。 \\[ z = \\mathbf{w}^T \\mathbf{x} + b , \\; a = \\frac {1}{1 + \\exp (-z)} \\] 网络层 一个网络层有很多个神经元。输入\\(\\mathbf x\\)向量，会传递到多个神经元。如 输入是\\(n\\)维，隐层是\\(m\\)维，有\\(m\\)个神经元。则有 \\[ \\begin{align} &amp; z = W x + b , &amp; W \\in \\mathbb{R}^{m \\times n}, x \\in \\mathbb R^n, b \\in \\mathbb R^m\\\\ &amp; a = f (z) &amp; a \\in \\mathbb R^m\\\\ &amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\\\ \\end{align} \\] 激活函数的意义 每个神经元 输入\\(z = w^Tx+b\\) ：对特征进行加权组合的结果 激活\\(a = f(z)\\)： 对\\(z\\)是否继续保留 最后会把所有的神经元的所有\\(z\\)的激活信息\\(a\\)综合起来，得到最终的分类结果。比如\\(s = U^T a\\)。 前向计算 输入\\(x \\in \\mathbb R^n\\)， 激活信息\\(a \\in \\mathbb R^m\\)。一般前向计算如下： \\[ \\begin{align} &amp; z = W x + b , &amp; W \\in \\mathbb{R}^{m \\times n}, x \\in \\mathbb R^n, b \\in \\mathbb R^m\\\\\\\\ &amp; a = f (z) &amp; a \\in \\mathbb R^m\\\\\\\\ &amp; s = U^T a &amp; 一般会对a进行变换得到最终结果s\\\\ \\end{align} \\] 下面是一个简单的全连接，最后的圆圈里的1代表等价输出。 NER例子 NER(named-entity recognition)，命名实体识别。对于一个句子Museums in Paris are amazing。 要判断中心单词Paris是否是个命名实体。 既要看window里的所有词向量，也要看这些词的交互关系。比如：Paris出现在in的后面。 因为可能有Paris和Paris Hilton。这就需要non-linear decisions。 如果直接把input给到softmax，是很难获取到非线性决策的。所以需要添加中间层使用神经网络。如上图所示。 维数分析 每个单词4维，输入整个窗口就是20维。在隐层使用8个神经元。计算过程如下，最终得到一个分类的得分。 \\[ \\begin {align} &amp; z = Wx + b \\\\ &amp; a = f(z) \\\\ &amp; s = U^T a \\\\ \\end{align} \\] 维数如下： \\[ x \\in \\mathbb R^{20}, \\; W \\in \\mathbb R^{8\\times20}, \\; U \\in \\mathbb R^{8\\times1}, s \\in R \\] Max magin目标函数 正样本\\(s\\) ：Museums in Paris are amazing ，负样本\\(s_c\\)： Not all museums in Paris 。 只关心：正样本的得分高于负样本的得分， 其它的不关注。即要\\(s - s_c &gt; 0\\)： \\[ \\mathrm{maxmize}(s -s_c) \\leftrightarrow \\mathrm{minmize}(s_c - s) \\] 优化目标函数如下： \\[ \\rm{minimize} \\; J = \\max(s_c - s, 0) \\; = \\begin{cases} &amp; s_c - s, &amp; s &lt; s_c \\\\ &amp; 0, &amp; s \\ge s_c \\end{cases} \\] 上式其实有风险，更需要\\(s - s_c &gt; \\Delta\\)， 即\\(s\\)比\\(s_c\\)得分大于\\(\\Delta\\)，来保证一个安全的间距。 \\[ \\rm{minimize} \\; J = \\max(\\Delta + s_c - s, 0) \\] 给具体间距\\(\\Delta=1\\)， 所以优化目标函数：详情见SVM。 \\[ \\rm{minimize} \\; J = \\max(1 + s_c - s, 0) \\] 其中\\(s_c = U^T f(Wx_c + b), \\; s = U^T f(Wx+b)\\) 。 反向传播训练 梯度下降 ，或者SGD： \\[ \\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\cdot \\Delta_{\\theta^{(t)}} J \\] 反向传播 使用链式法则 来计算前向计算中用到的参数的梯度。 符号定义 如下图，一个简单的网络： 网络在输入层和输出层是等价输入和等价输出，只有中间层会使用激活函数进行非线性变换。 符号 意义 \\(x\\) 网络输入，这里是4维 \\(s\\) 网络输出，这里是1维，即一个数字 \\(W^{(k)}\\) 第\\(k \\to k+1\\)层的转移矩阵。\\(W \\in \\mathbb R^{n \\times m}\\)。 k层m个神经元，k+1层n个神经元 \\(W_{ij}^{(k)}\\) k+1层的\\(i\\) 神经元 到 到\\(k\\)层\\(j\\)神经元的 的权值 \\(b_i^{(k)}\\) \\(k \\to k+1\\) 转移， k+1层的\\(i\\) 神经元的接收偏置 \\(z^{(k)}_j\\) 第\\(k\\)层的第\\(j\\)个神经元的输入 计算输入 \\(z_j^{(k+1)} = \\sum_i W_{ji}^{(k)} \\cdot a^{(k)}_i + b^{(k)}_j\\) \\(a_j^{(k)}\\) 第\\(k\\)层的第\\(j\\)个神经元的输入。\\(a = f(z)\\) \\(\\delta_j^{(k)}\\) BP时，在\\(z_j^{(k)}\\)处的梯度。即\\(f^\\prime(z_j^{(k)}) \\cdot g\\) ，\\(g\\)是传递来的梯度 W梯度推导 误差函数\\(J = \\max (1 + s_c - s, 0)\\) ，当\\(J &gt; 0\\)的时候，\\(J = 1 + s_c - s\\)要去更新参数W和b。 \\[ \\frac{\\partial J} {\\partial s} = - \\frac{\\partial J} {\\partial s_c} = -1 \\] 反向传播时，必须知道参数在前向时所贡献所关联的对象，即知道路径。 这里是等价输出： \\[ s = a_1^{(3)} = z_1^{(3)} = W_1^{(2)}a_1^{(2) } + W_2^{(2)}a_2^{(2) } \\] 这里对\\(W_{ij}^{(1)}\\)的偏导进行反向传播推导： \\[ \\begin{align} \\frac{\\partial s}{\\partial W_{ij}^{(1)}} &amp; = \\frac{\\partial W^{(2)} a^{(2)}}{\\partial W_{ij}^{(1)}} \\\\ &amp;= \\frac{ \\color{blue} {\\partial W_i^{(2)} a_i^{(2)}}} {\\partial W_{ij}^{(1)}} = \\color{blue}{W_i^{(2)}} \\cdot \\frac{\\partial a_i^{(2)}}{\\partial W_{ij}^{(1)}} \\\\ &amp; = W_i^{(2)} \\cdot \\color{blue} {\\frac{\\partial a_i^{(2)}}{\\partial z_i^{(2)}} \\cdot \\frac{\\partial z_i^{(2)}}{\\partial W_{ij}^{(1)}}} \\\\ &amp; = W_i^{(2)} \\cdot \\color{blue}{f^\\prime(z_i^{(2)})} \\cdot \\frac{\\partial }{\\partial W_{ij}^{(1)}} \\left(\\color{blue}{b_i^{(2)} + \\sum_k^4 a_k^{(1)}W_{ik}^{(1)}}\\right) \\\\ &amp; = W_i^{(2)}f^\\prime(z_i^{(2)}) \\color{blue}{a_j^{(1)}} \\\\ &amp; = \\color{blue}{\\delta^{(2)}_i} \\cdot a_j^{(1)} \\end{align} \\] 结果分析 我们知道\\(z_i^{(2)} = \\sum_k^4 a_k^{(1)}W_{ik}^{(1)} + b_i^{(2)}\\)。 单纯\\(z_i^{(2)}\\)对\\(W_{ij}^{(2)}\\)的导数是\\(a_j^{(1)}\\)。反向时，在\\(z_i^{(2)}\\)处的梯度是\\(\\delta_i^{(2)}\\)。 反向时，\\(\\frac{\\partial s}{\\partial W_{ij}^{(1)}} = \\delta^{(2)}_i \\cdot a_j^{(1)}\\)，是传来的梯度和当前梯度的乘积。这正好应证了反向传播。 传来的梯度也作error signal。 反向过程也是error sharing/distribution。 W元素实例 \\(W_{14}^{(1)}\\) 只直接贡献于\\(z_1^{(2)}\\)和\\(a_1^{(2)}\\) 步骤 梯度 \\(s \\to a_1^{(3)}\\) 梯度\\(g=1\\)。开始为1。 \\(a_1^{(3)} \\to z_1^{(3)}\\) 在\\(z_1^{(3)}\\)处的梯度\\(g = 1 \\cdot 1 = \\delta_1^{(3)}\\) 。\\(local \\; g= 1\\) ，等价变换 \\(z_1^{(3)} \\to a_1^{(2)}\\) \\(g = \\delta_1^{(3)} \\cdot W_1^{(2)} = W_1^{(2)}\\) 。\\(lg = w\\), \\(z=wa+b\\) \\(a_1^{(2)} \\to z_1^{(2)}\\) \\(g = W_1^{(2)} \\cdot f^\\prime(z_1^{(2)}) = \\delta_1^{(2)}\\)。 \\(lg=f^\\prime(z_1^{(2)})\\) \\(z_1^{(2)} \\to W_{14}^{(1)}\\) \\(g =W_1^{(2)} \\cdot f^\\prime(z_1^{(2)}) \\cdot a_4^{(1)} = \\delta_1^{(2)} \\cdot a_4^{(1)}\\)。 \\(lg = a_4^{(1)}\\) ， 因为\\(z =wa+b\\) \\(z_1^{(2)} \\to b_1^{(1)}\\) \\(g = W_1^{(2)} \\cdot f^\\prime(z_1^{(2)}) \\cdot 1 = \\delta_1^{(2)} \\cdot a_4^{(1)}\\)。 \\(lg = 1\\) ， 因为\\(z =wa+b\\) 对于上式的梯度计算，有两种理解方法，通过这两种思路去思考能更深入了解。 链式法则 error sharing and distributed flow approach 梯度反向传播 \\(\\delta_i^{(k)} \\to \\delta_j^{(k-1)}\\) 传播图如下： 但是更多时候，当前层的某个神经元的信息会传播到下一层的多个节点上，如下图： 梯度推导公式如下： \\[ \\begin{align} &amp; g_w = \\delta_i^{(k)} \\cdot a_j^{(k-1)} &amp; W_{ij}^{(k-1)}的梯度\\\\\\\\ &amp; g_a = \\sum_i \\delta_i^{(k)}W_{ij}^{(k-1)} &amp; a_j^{(k-1)}的梯度 \\\\\\\\ &amp; g_z = \\delta_j^{(k-1)} = f^\\prime(z_j^{(k-1)}) \\cdot \\sum_i \\delta_i^{(k)}W_{ij}^{(k-1)} &amp; z_j^{(k-1)}的梯度 \\\\\\\\ \\end{align} \\] BP向量化 很明显，不能一个一个参数地去更新element-wise。所以需要用矩阵和向量去表达，去一次性全部更新matrix-vector level。 梯度计算， \\(W_{ij}^{(k)}\\)的梯度是\\(\\delta_i^{(k+1)} \\cdot a_j^{(k)}\\) 。向量表达如下： \\[ \\Delta _{W^{(k)}} = \\begin{bmatrix} \\delta_1^{(k+1)} \\cdot a_1^{(k)} &amp; \\delta_1^{(k+1)} \\cdot a_2^{(k)} &amp; \\cdots\\\\ \\delta_2^{(k+1)} \\cdot a_1^{(k)} &amp; \\delta_2^{(k+1)} \\cdot a_2^{(k)} &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix} = \\delta^{(k+1)} a^{(k)T} \\] 梯度传播，\\(\\delta_j^{(k)} = f^\\prime(z_j^{(k)}) \\cdot \\sum_i \\delta_i^{(k+1)}W_{ij}^{(k)}\\)。向量表达如下： \\[ \\delta^{(k)} = f^\\prime(z^{(k)}) \\circ (\\delta^{(k+1)}W^{(k)}) \\] 其中\\(\\circ\\)是叉积向量积element-wise，是各个位置相乘， 即\\(\\mathbb R^N \\times \\mathbb R^N \\to \\mathbb R^N\\)。 点积和数量积是各个位置相乘求和。 计算效率 很明显，在计算的时候要把上一层的\\(\\delta^{(k+1)}\\)存起来，去计算\\(\\delta^{(k)}\\) ，这样可以减少大量的多余的计算。 神经网络常识 梯度检查 使用导数的定义来估计导数，去和BP算出来的梯度做对比。 \\[ f ^\\prime (\\theta) \\approx \\frac{J(\\theta^{(i+)}) - J(\\theta^{(i-)})} {2 \\epsilon } \\] 由于这样计算非常，效率特别低，所以只用这种办法来检查梯度。具体实现代码见原notes。 激活函数 激活函数有很多，现在主要用ReLu，不要用sigmoid。 用ReLU学习率一定不要设置太大！同一个网络中都使用同一种类型的激活函数。 Sigmoid 数学形式和导数如下： \\[ \\begin{align} &amp; \\sigma (z) = \\frac {1} {1 + \\exp(-z)}, \\; \\sigma(z) \\in (0,1) \\\\ \\\\ &amp; \\sigma^\\prime (z) = \\sigma(z) (1 - \\sigma(z)) \\\\ \\end{align} \\] 图像 优点是具有好的解释性，将实数挤压到\\((0,1)\\)中，很大的负数变成0，很大的正数变成1 。但现在用的已经越来越少了。有下面2个缺点。 Sigmoid会造成梯度消失 靠近0和1两端时，梯度会变成0。 BP链式法则，\\(0 \\times g_{from} = 0\\) ，后面的梯度接近0， 将没有信息去更新参数。 初始化权重过大，大部分神经元会饱和，无法更新参数。因为输入值很大，靠近1了。\\(f^\\prime(z) = 0\\)， 没法传播了。 Sigmoid输出不是以0为均值 如果输出\\(x\\)全是正的，\\(z=wx+b\\)， 那么\\(\\frac{\\partial z}{\\partial w} = x\\) 梯度就全是正的 不过一般是batch训练，其实问题也还好 Sigmoid梯度消失的问题最严重。 Tanh 数学公式和导数如下： \\[ \\begin{align} &amp; \\tanh (z) = \\frac{\\exp(z) - \\exp(-z)}{\\exp(z) + \\exp(-z)} = 2 \\sigma(2z) - 1, \\; \\tanh(z) \\in(-1, 1) \\\\ \\\\ &amp; \\tanh^\\prime (z) = 1 - \\tanh^2 (z) \\end{align} \\] 图像： Tanh是Sigmoid的代替，它是0均值的，但是依然存在梯度消失的问题。 ReLU ReLURectified Linear Unit 最近越来越流行，不会对于大值\\(z\\)就导致神经元饱和的问题。在CV取得了很大的成功。 \\[ \\begin{align} &amp; \\rm{rect}(z) = \\max(z, 0) \\\\ \\\\ &amp; \\rm{rect}^\\prime (z) = \\begin{cases} &amp;1, &amp;z &gt; 0 \\\\&amp; 0, &amp; z \\le 0 \\end{cases} \\\\ \\end{align} \\] 其实ReLU是一个关于0的阈值，现在一般都用ReLU： ReLU的优点 加速收敛（6倍）。线性的，不存在梯度消失的问题。一直是1。 计算简单 ReLU的缺点 训练的时候很脆弱 BP时，如果有大梯度经过ReLU，当前在z处的梯度\\(\\delta^{(k+1)} = 1 \\times g_m\\) 就很大 对参数\\(w\\)的梯度 \\(\\Delta_{W^{(k)}} =\\delta^{(k)} a^{(k)T}\\) 也就很大 参数\\(w\\)会更新的特别小 \\(W^{(k)} = W^{(k)} - \\alpha \\cdot \\Delta_{W^{(k)}}\\) 前向时，\\(z =wx+b \\le 0\\) 也就特别小，激活函数就不会激活 不激活，梯度就为0。 再BP的时候，就无法更新参数了 总结也就是：大梯度\\(\\to\\)小参数\\(w\\) ，新小$z = wx+b $ ReLU不激活， 不激活梯度为0 \\(\\to\\) 不更新参数w了。 当然可以使用比较小的学习率来解决这个问题。 Maxout maxout 有ReLU的优点，同时避免了它的缺点。但是maxout加倍了模型的参数，导致了模型的存储变大。 \\[ \\begin{align} &amp; \\rm{mo}(x) = \\max(w_1x+b_1, w_2x+b_2) \\\\ \\\\ &amp; \\rm{mo}^\\prime (x) = \\begin{cases} &amp;w_1, &amp;w_1x+b_1 大 \\\\&amp; w_2, &amp; 其它 \\\\\\end{cases} \\\\ \\end{align} \\]","tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://plmsmile.github.io/tags/神经网络/"},{"name":"CS224N","slug":"CS224N","permalink":"http://plmsmile.github.io/tags/CS224N/"},{"name":"反向传播","slug":"反向传播","permalink":"http://plmsmile.github.io/tags/反向传播/"},{"name":"激活函数","slug":"激活函数","permalink":"http://plmsmile.github.io/tags/激活函数/"}]},{"title":"CS224n笔记1-Word2Vec","date":"2017-11-12T12:54:37.000Z","path":"2017/11/12/cs224n-notes1-word2vec/","text":"Word2vec 简介 把词汇变成词向量。 类别1 类别2 算法 CBOW，上下文预测中心词汇 Skip-gram，中心词汇预测上下文 训练方法 负采样 哈夫曼树 语言模型 两种句子： 正常的句子：The cat jumped over the puddle。 概率高，有意义。 没意义的句子：stock boil fish is toy 。概率低，没意义。 二元模型 一个句子，有\\(n\\)个单词。每个词出现的概率由上一个词语来决定。则整体句子的概率如下表示： \\[ P(w_1, w_2, \\cdots, w_n) = \\prod_{i=2}^n P(w_i \\mid w_{i-1}) \\] 缺点 只考虑单词相邻传递概率，而忽略句子整体的可能性。 context size=1，只学了相邻单词对的概率 会计算整个大数据集的全局信息 CBOW 给上下文The cat _ over the puddle，预测jump 。对于每个单词，学习两个向量： \\(v\\) ：输入向量 ，（上下文单词） \\(u\\)： 输出向量 ， （中心单词） 符号说明 \\(V\\) ：词汇表，后面用\\(V\\)代替词汇表单词个数 \\(w_i\\) ：词汇表中第\\(i\\)个单词 \\(d\\) ：向量的维数 \\(\\mathcal V_{d \\times |V|}\\)：输入矩阵，也可以用\\(W\\)来表达 \\(v_i\\) ：\\(\\mathcal{V}\\)的第\\(i\\)列，\\(w_i\\)的输入向量表达 \\(\\mathcal {U}_{|V| \\times d}\\) ：输出矩阵，可以用\\(W ^ \\prime\\)来表达 \\(u_i\\) ：\\(\\mathcal U\\)的第i行， \\(w_i\\)的输出向量表达 输入与输出 \\(x^{(c)}\\)， 输入\\(2m\\)个上下文单词，上下文词汇的one-hot向量 \\(y_c\\)： 真实标签 \\(\\hat y^{(c)}\\)， 输出一个中心单词，中心词汇的one-hot向量 步骤 1 上下文单词onehot向量 one-hot向量的表达：\\((x^{(c-m)}, \\cdots, x^{(c-1)}, x^{(c+1)}, x^{(c+m)} \\in \\mathbb R^V)\\) 2 上下文单词向量 \\((v_{c-m}, v_{c-m+1}, \\cdots. v_{c+m} \\in \\mathbb{R}^d)\\)， 其中，\\(v_{c-m}=\\mathcal V x^{(c-m)}\\)， 即输入矩阵乘以one-hot向量就找到所在的列 3 平均上下文词向量 \\(\\hat v = \\frac {v_{c-m} + \\cdots + v_{c+m}}{2m} \\in \\mathbb R^d\\) 4 输出单词与上下文计算得分向量 \\(z = \\mathcal U \\hat v \\in \\mathbb R ^V\\) 。点积，单词越相似，得分越高 5 得分向量转为概率 $y = (z) R^V $ 6 真实预测概率对比 预测的概率向量\\(\\hat y\\)与唯一真实中心单词one-hot向量\\(y\\)，进行交叉熵比较算出loss。 目标函数 使用交叉熵计算loss，损失函数如下： \\[ H(\\hat y, y) = - \\sum_{j=1}^{|V|} y_j \\log (\\hat y_j) \\] 由于中心单词\\(y\\)是one-hot编码，只有正确位置才为1，其余均为0，所以只需计算中心单词对应的位置概率的loss即可： \\[ H(\\hat y, y) = - y_c \\log (\\hat y_c) = - \\log (\\hat y_c) \\] 交叉熵很好是因为 \\(-1 \\cdot \\log (1) = 0\\)，预测得好 \\(-1 \\cdot \\log (0.01) = 4.605\\)， 预测得不好 最终损失函数： \\[ \\begin{align} \\rm{minimize} \\; J &amp; = - \\log P(w_c \\mid w_{c-m}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+m}) \\\\ &amp; = - \\log P(u_c \\mid \\hat v) \\\\ &amp; = - \\log \\frac {\\exp(u_c^T \\hat v)}{\\sum_{j=1}^{|V|} \\exp(u_j^T \\hat v)} \\\\ &amp; = -u_c^T \\hat v + \\log \\sum_{j=1}^{|V|} \\exp(u_j^T \\hat v) \\end{align} \\] 再使用SGD方法去更新相关的两种向量\\(u_c, v_j\\) 。 Skip-gram 给中心单词jump，预测上下文The cat _ over the puddle 。 输入中心单词\\(x\\)， 输出上下文单词\\(y\\) 。与CBOW正好输入输出相反，但同样有两个矩阵\\(\\mathcal {U, V}\\) 。符号说明同CBOW。 步骤 1 中心单词onehot向量 \\(x \\in \\mathbb {R}^{|V|}\\) 2 中心单词词向量 \\(v_c = \\mathcal V x \\in \\mathbb R^d\\) 3 中心词与其他词的得分向量 \\(z = \\mathcal U v_c \\in \\mathbb R ^{|V|}\\) 4 得分向量转为概率 概率 \\(\\hat y = \\rm {softmax} (z)\\)， \\(\\hat y_{c-m}, \\ldots, \\hat y_{c+m}\\) 是目标上下文单词是中心单词的上下文的预测概率。 5 预测真实概率对比 预测概率\\(\\hat y\\) 与\\(2m\\) 个真实上下文onehot向量\\(y_{c-m}, \\ldots, y_{c+m}\\)进行交叉熵对比，算出loss 目标函数 与CBOW不同的是，Skip-gram做了一个朴素贝叶斯条件假设，所有的输出上下文单词都是独立的。 \\[ \\begin {align} \\rm{minimize} \\; J &amp; = - \\log P(w_{c-m}, \\ldots, w_{c-1}, w_{c+1}, \\ldots, w_{c+m} \\mid w_c) \\\\ &amp; = - \\log \\prod_{j=0, j \\neq m}^{2m} P(w_{c-m+j} \\mid w_c) \\\\ &amp; = -\\log \\prod_{j=0, j \\neq m}^{2m} \\frac {\\exp (u_{c-m+j}^T \\cdot v_c)} {\\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c)} \\\\ &amp; = - \\sum_{j=0, j \\neq m}^{2m} \\left ( \\log \\exp (u_{c-m+j}^T \\cdot v_c) - \\log \\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c) \\right) \\\\ &amp; = - \\sum_{j=0, j \\neq m}^{2m} u_{c-m+j}^T v_c + 2m \\cdot \\log \\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c) \\end {align} \\] 一样，使用SGD去优化U和V。 损失函数实际上是\\(2m\\)个交叉熵求和，求出的向量\\(\\hat y\\)与\\(2m\\)个onehot向量\\(y_{c-m+j}\\) 计算交叉熵： \\[ \\begin {align} J &amp; = - \\sum_{j=0, j \\neq m}^{2m} \\log P(u_{c-m+j} \\mid v_c) \\\\ &amp; = \\sum_{j=0, j \\neq m}^{2m} H(\\hat y, y_{c-m+j}) \\\\ \\end{align} \\] 负采样训练 每次计算都会算整个\\(|V|\\)词表，太耗时了。 可以从噪声分布\\(P_n(w)\\)中进行负采样，来代替整个词表。当然单词采样概率与其词频相关。只需关心：目标函数、梯度、更新规则。 标签函数 对于一对中心词和上下文单词\\((w, c)\\) ，设标签如下： \\(P(l = 1 \\mid w, c)\\)， \\((w, c)\\) 来自于真实语料 \\(P(l = 0 \\mid w, c)\\) ，\\((w, c)\\)来自于负样本，即不在语料中 用sigmoid表示标签函数： \\[ \\begin {align} &amp; P(l = 1 \\mid w, c; \\theta) = \\sigma (u^T_w v_c) = \\frac {1}{ 1 + e^{-u^T_w v_c}} \\\\ &amp; P(l = 0 \\mid w, c; \\theta) = 1 - \\sigma (u^T_w v_c) = \\frac {1}{ 1 + e^{u^T_w v_c} } \\\\ \\end {align} \\] 目标函数 选取合适的\\(\\theta= \\mathcal {U, V}\\) ，去增大正样本的概率，减小负样本的概率。设\\(D\\)为正样本集合，\\(\\bar D\\)为负样本集合。 \\[ \\begin {align} \\theta &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\prod_{(w, c) \\in D} P(l=1 \\mid w, c, \\theta) \\prod_{(w, c) \\in \\bar D} P(l=0 \\mid w, c, \\theta) \\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\prod_{(w, c) \\in D} P(l=1 \\mid w, c, \\theta) \\prod_{(w, c) \\in \\bar D} (1 - P(l=1 \\mid w, c, \\theta) )\\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\sum_{(w, c) \\in D} \\log P(l=1 \\mid w, c, \\theta) + \\sum_{(w, c) \\in \\bar D} \\log (1 - P(l=1 \\mid w, c, \\theta) )\\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\sum_{(w, c) \\in D} \\log \\frac {1}{ 1 + \\exp (-u^T_w v_c)}+ \\sum_{(w, c) \\in \\bar D} \\log \\frac {1}{ 1 + \\exp (u^T_w v_c) } \\\\ &amp; = \\mathop{\\rm{argmax}}_\\limits{\\theta} \\sum_{(w, c) \\in D} \\log \\sigma(u^T_w v_c) + \\sum_{(w, c) \\in \\bar D} \\log \\sigma (-u^T_w v_c) \\end {align} \\] 最大化概率也就是最小化负对数似然 \\[ J = - \\sum_{(w, c) \\in D} \\log \\sigma(u^T_w v_c) - \\sum_{(w, c) \\in \\bar D} \\log \\sigma (-u^T_w v_c) \\] 负采样集合选择 为中心单词\\(w_c\\) 从\\(P_n(w)\\) 采样\\(K\\)个假的上下文单词。表示为\\(\\{ \\bar u_k \\mid k=1\\ldots K\\}\\) CBOW 给上下文向量\\(\\hat{v}=\\frac {v_{c-m} + \\cdots + v_{c+m}}{2m}\\) 和真实中心词\\(u_c\\) 原始loss \\[ J = -u_c^T \\hat v + \\log \\sum_{j=1}^{|V|} \\exp(u_j^T \\hat v) \\] 负采样loss \\[ J = - \\log \\sigma (u_c^T \\cdot \\hat v) - \\sum_{k=1}^K \\log \\sigma (- \\bar u_k^T \\cdot \\hat v) \\] Skip-gram 给中心单词\\(v_c\\)， 和\\(2m\\)个真实上下文单词\\(u_{c-m+j}\\) 原始loss \\[ J = - \\sum_{j=0, j \\neq m}^{2m} u_{c-m+j}^T v_c + 2m \\cdot \\log \\sum_{k=1}^{|V|} \\exp (u_k^T \\cdot v_c) \\] 负采样loss \\[ J = - \\sum_{j=0, j \\neq m}^{2m} \\log \\sigma (u_{c-m+j}^T \\cdot v_c) - \\sum_{k=1}^K \\log \\sigma (-\\bar u_{k}^T \\cdot v_c) \\]","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"},{"name":"CS224N","slug":"CS224N","permalink":"http://plmsmile.github.io/tags/CS224N/"}]},{"title":"word2vec中的数学模型","date":"2017-11-02T13:53:49.000Z","path":"2017/11/02/word2vec-math/","text":"word2vec有两种模型：CBOW和Skip-gram，有两种训练方法：Hierarchical Softmax和Negative Sampling 背景介绍 符号 \\(C\\) ：语料Corpus，所有的文本内容，包含重复的词。 D：词典，D是从C中取出来的，不重复。 \\(w\\)：一个词语 \\(m\\)：窗口大小，词语\\(w\\)的前后\\(m\\)个词语 \\(\\rm{Context(w)} = C_w\\)： 词\\(w\\)的上下文词汇，取决于\\(m\\) \\(v(w)\\)： 词典\\(D\\)中单词\\(w\\)的词向量 \\(k\\)：词向量的长度 \\(i_w\\)：词语\\(w\\)在词典\\(D\\)中的下标 \\(NEG(w)\\) ： 词\\(w\\)的负样本子集 常用公式： \\[ \\begin {align} &amp; \\log (a^n b^m) = \\log a^n + \\log b^m = n \\log a + m \\log b \\\\ \\end{align} \\] \\[ \\log \\prod_{i=1}a^i b^{1-i} = \\sum_{i=1} \\log a^i + \\log b^{1-i} = \\sum_{i=1} i \\cdot \\log a + (i-1) \\cdot \\log b \\] 目标函数 n-gram模型。当然，我们使用神经概率语言模型。 \\(P(w \\mid C_w)\\) 表示上下文词汇推出中心单词\\(w\\)的概率。 对于统计语言模型来说，一般利用最大似然，把目标函数设为： \\[ \\prod_{w \\in C} p(w \\mid C_w) \\] 一般使用最大对数似然，则目标函数为： \\[ L = \\sum_{w \\in C} \\log p(w \\mid C_w) \\] 其实概率\\(P(w \\mid C_w)\\)是关于\\(w\\)和\\(C_w\\)的函数，其中\\(\\theta\\)是待定参数集，就是要求最优 \\(\\theta^*\\)，来确定函数\\(F\\)： \\[ p(w \\mid C_w) = F(w, C_w; \\; \\theta) \\] 有了函数\\(F\\)以后，就能够直接算出所需要的概率。 而F的构造，就是通过神经网络去实现的。 神经概率语言模型 一个二元对\\((C_w, w)\\)就是一个训练样本。神经网络结构如下，\\(W, U\\)是权值矩阵，\\(p, q\\)是对应的偏置。 但是一般会减少一层，如下图：（其实是去掉了隐藏层，保留了投影层，是一样的） 窗口大小是\\(m\\)，\\(\\rm{Context}(w)\\)包含\\(2m\\)个词汇，词向量长度是\\(k\\)。可以做拼接或者求和（下文是）。拼接得到长向量\\(2mk\\)， 在投影层得到\\(\\mathbf{x_w}\\)，然后给到隐藏层和输出层进行计算。 \\[ \\mathbf{z}_w = \\rm{tanh}(W\\mathbf{z}_w + \\mathbf{p}) \\;\\to \\;\\mathbf{y}_w = U \\mathbf{z}_w + \\mathbf{q} \\] 再对\\(\\mathbf{y}_w = (y_1, y_2, \\cdots, y_K)\\) 向量进行softmax即可得到所求得中心词汇的概率： \\[ p(w \\mid C_w) = \\frac{e^{y_{i_w}}}{\\sum_{i=1}^K e^{y_i}} \\] 优点 词语的相似性可以通过词向量来体现 自带平滑功能。N-Gram需要自己进行平滑。 词向量的理解 有两种词向量，一种是one-hot representation，另一种是Distributed Representation。one-hot太长了，所以DR中把词映射成为相对短的向量。不再是只有1个1（孤注一掷），而是向量分布于每一维中（风险平摊）。再利用欧式距离就可以算出词向量之间的相似度。 传统可以通过LSA（Latent Semantic Analysis）和LDA（Latent Dirichlet Allocation）来获得词向量，现在也可以用神经网络算法来获得。 可以把一个词向量空间向另一个词向量空间进行映射，就可以实现翻译。 Hierarchical Softmax 两种模型都是基于下面三层模式（无隐藏层），输入层、投影层和输出层。没有hidden的原因是据说是因为计算太多了。 CBOW和Skip-gram模型： CBOW模型 一共有\\(\\left| C \\right|\\)个单词。CBOW是基于上下文\\(context(w) = c_w\\)去预测目标单词\\(w\\)，求条件概率\\(p(w \\mid c_w)\\)，语言模型一般取目标函数为对数似然函数： \\[ L = \\sum_{w \\in C} \\log p(w \\mid c_w) \\] 窗口大小设为\\(m\\)，则\\(c_w\\)是\\(w\\)的前后m个单词。 输入层 是上下文单词的词向量。（初始随机，训练过程中逐渐更新） 投影层 就是对上下文词向量进行求和，向量加法。得到单词\\(w\\)的所有上下文词\\(c_w\\)的词向量的和\\(\\mathbf{x}_w\\)，待会儿参数更新的时候再依次更新回来。 输出层 从\\(C\\)中选择一个词语，实际上是多分类。这里是哈夫曼树层次softmax。 因为词语太多，用softmax太慢了。多分类实际上是多个二分类组成的，比如SVM二叉树分类： 这是一种二叉树结构，应用到word2vec中，被称为Hierarchical Softmax。CBOW完整结构如下： 每个叶子节点代表一个词语\\(w\\)，每个词语被01唯一编码。 哈夫曼编码 哈夫曼树很简单。每次从许多节点中，选择权值最小的两个合并，根节点为合并值；依次循环，直到只剩一棵树。 比如“我 喜欢 看 巴西 足球 世界杯”，这6个词语，出现的次数依次是15, 8, 6, 5, 3, 1。建立得到哈夫曼树，并且得到哈夫曼编码，如下： CBOW足球例子 引入一些符号： \\(p^w\\) ：从根节点到达\\(w\\)叶子节点的路径 \\(l^w\\) ： 路径\\(p^w\\)中节点的个数 \\(p^w_1, \\cdots, p^w_{l_w}\\) ：依次代表路径中的节点，根节点-中间节点-叶子节点 \\(d^w_2, \\cdots, d^w_{l^w} \\in \\{0, 1\\}\\)：词\\(w\\)的哈夫曼编码，由\\(l^w-1\\)位构成， 根节点无需编码 \\(\\theta_1^w, \\cdots, \\theta^w_{l^w -1}\\)：路径中非叶子节点对应的向量， 用于辅助计算。 单词\\(w\\)是足球，对应的所有上下文词汇是\\(c_w\\)， 上下文词向量的和是\\(\\mathbf{x}_w\\) 看一个例子： 约定编码为1是负类，为0是正类。即左边是负类，右边是正类。 每一个节点就是一个二分类器，是逻辑回归(sigmoid)。其中\\(\\theta\\)是对应的非叶子节点的向量，一个节点被分为正类和负类的概率分别如下： \\[ \\sigma(\\mathbf{x}_w^T \\theta) = \\frac {1}{ 1 + e^{-\\mathbf{x}_w^T \\theta}}, \\quad 1 - \\sigma(\\mathbf{x}_w^T \\theta) \\] 那么从根节点到达足球的概率是： \\[ p (足球 \\mid c_{足球}) = \\prod_{j=2}^5 p(d_j^w \\mid \\mathbf{x}_w, \\theta_{j-1}^w) \\] CBOW总结 目标函数 从根节点到每一个单词\\(w\\)都存在一条路径\\(p^w\\)，路径上有\\(l^w-1\\)个分支节点，每个节点就是一个二分类，每次产生一个概率 \\(p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1})\\)， 把这些概率乘起来就得到了\\(p(w \\mid c_w)\\)。 其中每个节点的概率是，与各个节点的参数和传入的上下文向量和\\(\\mathbf{x}_w\\)相关。 \\[ p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1}) = \\begin{cases} &amp; \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}), &amp; d_j^w = 0 \\\\ &amp; 1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}), &amp; d_j^w = 1\\\\ \\end{cases} \\] 写成指数形式是 \\[ p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1}) = [\\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{1-d_j^w} \\cdot [1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{d_j^w} \\] 则上下文推中间单词的概率，即目标函数： \\[ p(w \\mid c_w) = \\prod_{j=2}^{l^w} p(d_j^w \\mid \\mathbf{x}_w, \\theta^w_{j-1}) \\] 对数似然函数 对目标函数取对数似然函数是： \\[ \\begin{align} L &amp; = \\sum_{w \\in C} \\log p(w \\mid c_w) = \\sum_{w \\in C} \\log \\prod_{j=2}^{l^w} [\\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{1-d_j^w} \\cdot[1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})]^{d_j^w} \\\\ &amp; = \\sum_{w \\in C} \\sum_{j=2}^{l^w} \\left( (1-d_j^w) \\cdot \\log \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}) + d_j^w \\cdot \\log (1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})) \\right) \\\\ &amp; = \\sum_{w \\in C} \\sum_{j=2}^{l^w} \\left( (1-d_j^w) \\cdot \\log A + d_j^w \\cdot \\log (1 -A)) \\right) \\end{align} \\] 简写： \\[ \\begin{align} &amp; L(w, j) = (1-d_j^w) \\cdot \\log \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1}) + d_j^w \\cdot \\log (1 - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})) \\\\ &amp; L = \\sum_{w,j} L(w, j) \\end{align} \\] 怎样最大化对数似然函数呢，可以最大化每一项，或者使整体最大化。尽管最大化每一项不一定使整体最大化，但是这里还是使用最大化每一项\\(L(w, j)\\)。 sigmoid函数的求导： \\[ \\sigma ^{\\prime}(x) = \\sigma(x)(1 - \\sigma(x)) \\] \\(L(w, j)\\)有两个参数：输入层的\\(\\mathbf{x}_w\\) 和 每个节点的参数向量\\(\\theta_{j-1}^w\\) 。 分别求偏导并且进行更新参数： \\[ \\begin{align} &amp; \\frac{\\partial}{\\theta_{j-1}^w} L(w, j) = [1 - d_j^w - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})] \\cdot \\mathbf{x}_w \\quad \\to \\quad \\theta_{j-1}^w = \\theta_{j-1}^w + \\alpha \\cdot \\frac{\\partial}{\\theta_{j-1}^w} L(w, j) \\\\ &amp; \\frac{\\partial}{\\mathbf{x}_w} L(w, j) = [1 - d_j^w - \\sigma(\\mathbf{x}^T_w \\theta^w_{j-1})] \\cdot \\theta_{j-1}^w \\quad \\to \\quad v(\\hat w)+= v(\\hat w) + \\alpha \\cdot \\sum_{j=2}^{l^w} \\frac{\\partial}{\\mathbf{x}_w} L(w, j), \\hat w \\in c_w \\\\ \\end{align} \\] 注意：\\(\\mathbf{x}_w\\)是所有上下文词向量的和，应该把它的更新平均更新到每个上下文词汇中去。\\(\\hat w\\) 代表\\(c_w\\)中的一个词汇。 Skip-Gram模型 Skip-gram模型是根据当前词语，预测上下文。网络结构依然是输入层、投影层(其实无用)、输出层。如下： 输入一个中心单词的词向量\\(v(w)\\)，简记为\\(v_w\\)，输出是一个哈夫曼树。单词\\(u\\)是\\(w\\)的上下文单词\\(c_w\\)中的一个。这是一个词袋模型，每个\\(u\\)是互相独立的。 目标函数 所以\\(c_w\\)是\\(w\\)的上下文词汇的概率是： \\[ p(c_w \\mid w) = \\prod_{u \\in c_w} p(u \\mid w) \\] 与上面同理，\\(p(u \\mid w)\\) 与传入的中心单词向量\\(v(w)\\)和路径上的各个节点相关： \\[ \\begin{align} &amp; p(u \\mid w) = \\prod_{j=2}^{l^w} p(d_j^u \\mid v_w,\\; \\theta^u_{j-1}) \\\\ &amp; p(d_j^u \\mid v_w ,\\; \\theta^u_{j-1} ) = [\\sigma(v_w^T \\theta^u_{j-1})]^{1-d_j^u} \\cdot [1 - \\sigma(v_w^T \\theta^u_{j-1})]^{d_j^u} \\\\ \\end{align} \\] 下文\\(v_w^T \\theta^w_{j-1}\\)简记为\\(v_w \\theta_{j-1}^w\\)，要记得转置向量相乘就可以了。 对数似然函数 \\[ \\begin{align} L &amp; = \\sum_{w \\in C} \\log p(c_w \\mid w) \\\\ &amp; = \\sum_{w \\in C} \\log \\prod_{u \\in c_w} \\prod _{j=2}^{l^w} [\\sigma(v_w^T \\theta^u_{j-1})]^{1-d_j^u} \\cdot [1 - \\sigma(v_w^T \\theta^u_{j-1})]^{d_j^u} \\\\ &amp; = \\sum_{w \\in C} \\sum_{u \\in c_w} \\sum_{j=2}^{l^w} \\left( (1-d_j^u) \\cdot \\log \\sigma(v_w^T \\theta^u_{j-1}) + d_j^u \\cdot \\log (1 - \\sigma(v_w^T \\theta^u_{j-1})) \\right) \\\\ \\end{align} \\] 同样，简写每一项为\\(L(w, u, j)\\) \\[ L(w, u, j) = (1-d_j^u) \\cdot \\log \\sigma(v_w^T \\theta^u_{j-1}) + d_j^u \\cdot \\log (1 - \\sigma(v_w^T \\theta^u_{j-1})) \\] 然后就是，分别对\\(v_w\\)和\\(\\theta_{j-1}^u\\)求梯度更新即可，同上面的类似。得到下面的更新公式 \\[ \\begin{align} &amp; \\theta_{j-1}^u = \\theta_{j-1}^u + \\alpha \\cdot [1 - d_j^u - \\sigma(v_w^t \\cdot \\theta_{j-1}^u)] \\cdot v(w) \\\\ &amp; v_w = v_w + \\alpha \\cdot \\sum_{u \\in c_w} \\sum_{j=2}^{l^w} \\frac{\\partial L(w, u, j)}{\\partial v_w} \\\\ \\end{align} \\] Negative Sampling 背景知识介绍 Negative Sampling简称NEG，是Noise Contrastive Estimation(NCE)的一个简化版本，目的是用来提高训练速度和改善所得词向量的质量。 NEG不使用复杂的哈夫曼树，而是使用随机负采样，大幅度提高性能，是Hierarchical Softmax的一个替代。 NCE 细节有点复杂，本质上是利用已知的概率密度函数来估计未知的概率密度函数。简单来说，如果已知概率密度X，未知Y，如果知道X和Y的关系，Y也就求出来了。 在训练的时候，需要给正例和负例。Hierarchical Softmax是把负例放在二叉树的根节点上，而NEG，是随机挑选一些负例。 CBOW 对于一个单词\\(w\\)，输入上下文\\(\\rm{Context}(w) = C_w\\)，输出单词\\(w\\)。那么词\\(w\\)是正样本，其他词都是负样本。 负样本很多，该怎么选择呢？后面再说。 定义\\(\\rm{Context}(w)\\)的负样本子集\\(\\rm{NEG}(w)\\)。对于样本\\((C_w, w)\\)，\\(\\mathbf{x}_w\\)依然是\\(C_w\\)的词向量之和。\\(\\theta_u\\)为词\\(u\\)的一个（辅助）向量，待训练参数。 设集合\\(S_w = w \\bigcup NEG(w)\\) ，对所有的单词\\(u \\in S_w\\)，有标签函数： \\[ b^w(u) = \\begin{cases} &amp; 1, &amp; u = w \\\\ &amp; 0, &amp; u \\neq w \\\\ \\end{cases} \\] 单词\\(u\\)是\\(C_w\\) 的中心词的概率是： \\[ p(u \\mid C_w) = \\begin{cases} &amp; \\sigma(\\mathbf x_w^T \\theta^u), &amp; u=w \\; \\text{正样本} \\\\ &amp; 1 - \\sigma(\\mathbf x_w^T \\theta^u), &amp; u \\neq w \\; \\text{负样本} \\\\ \\end{cases} \\] 简写为： \\[ \\color{blue} {p(u \\mid C_w)} = [ \\sigma(\\mathbf x_w^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(\\mathbf x_w^T \\theta^u)]^{1 - b^w(u)} \\] 要最大化目标函数\\(g(w) = \\sum_{u \\in S_w} p(u \\mid C_w)\\)： \\[ \\begin{align} \\color{blue}{g(w) } &amp; = \\prod_{u \\in S_w} [ \\sigma(\\mathbf x_w^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(\\mathbf x_w^T \\theta^u)]^{1 - b^w(u)} \\\\ &amp;= \\color{blue} {\\sigma(\\mathbf x_w^T \\theta^u) \\prod_{u \\in NEG(w)} (1 - \\sigma(\\mathbf x_w^T \\theta^u)) } \\\\ \\end{align} \\] 观察\\(g(w)\\)可知，最大化就是要：增大正样本概率和减小化负样本概率。 每个词都是这样，对于整个语料库的所有词汇，将\\(g\\)累计得到优化目标，目标函数如下： \\[ \\begin {align} L &amp; = \\log \\prod_{w \\in C}g(w) = \\sum_{w \\in C} \\log g(w) \\\\ &amp; = \\sum_{w \\in C} \\log \\left( \\prod_{u \\in S_w} [ \\sigma(\\mathbf x_w^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(\\mathbf x_w^T \\theta^u)]^{1 - b^w(u)} \\right) \\\\ &amp;= \\sum_{w \\in C} \\sum_{u \\in S_w} \\left[ b^w_u \\cdot \\sigma(\\mathbf x_w^T \\theta^u) + (1-b^w_u) \\cdot (1 - \\sigma(\\mathbf x_w^T \\theta^u)) \\right] \\end {align} \\] 简写每一步\\(L(w, u)\\)： \\[ L(w, u) = b^w_u \\cdot \\sigma(\\mathbf x_w^T \\theta^u) + (1-b^w_u) \\cdot (1 - \\sigma(\\mathbf x_w^T \\theta^u)) \\] 计算\\(L(w, u)\\)对\\(\\theta^u\\)和\\(\\mathbf{x}_w\\)的梯度进行更新，得到梯度(对称性)： \\[ \\frac{\\partial L(w, u) }{ \\partial \\theta^u} = [b^w(u) - \\sigma(\\mathbf x_w^T \\theta^u)] \\cdot \\mathbf{x}_w, \\quad \\frac{\\partial L(w, u) }{ \\partial \\mathbf{x}_w} = [b^w(u) - \\sigma(\\mathbf x_w^T \\theta^u)] \\cdot \\theta^u \\] 更新每个单词的训练参数\\(\\theta^u\\) ： \\[ \\theta^u = \\theta^u + \\alpha \\cdot \\frac{\\partial L(w, u) }{ \\partial \\theta^u} \\] 对每个单词更新词向量\\(v(u)\\) ： \\[ v(u) = v(u) + \\alpha \\cdot \\sum_{u \\in S_w} \\frac{\\partial L(w, u) }{ \\partial \\mathbf{x}_w} \\] Skip-gram H给单词\\(w\\)，预测上下文向量\\(\\rm{Context}(w) = C_w\\)。 输入样本\\((w, C_w)\\)。 中心单词是\\(w\\)，遍历样本中的上下文单词\\(w_o \\in C_w\\)，为每个上下文单词\\(w_o\\)生成一个包含负采样的集合\\(S_o = w \\bigcup \\rm{NEG}(o)\\) 。即\\(S_o\\)里面只有\\(w\\)才是\\(o\\)的中心单词。 下面\\(w_o\\)简写为\\(o\\)，要注意实际上是当前中心单词\\(w\\)的上下文单词。 \\(S_o\\)中的\\(u\\)是实际的w就为1，否则为0。标签函数如下： \\[ b^w(u) = \\begin{cases} &amp; 1, &amp; u = w \\\\ &amp; 0, &amp; u \\neq w \\\\ \\end{cases} \\] \\(S_o​\\)中的\\(u​\\)是\\(o​\\)的中心词的概率是 \\[ p(u \\mid o) = \\begin{cases} &amp; \\sigma (v_o^T \\theta^u ), &amp; u=w \\; \\leftrightarrow \\; b^w(u) = 1 \\\\ &amp; 1 - \\sigma (v_o^T \\theta^u ), &amp;u \\neq w \\; \\leftrightarrow \\; b^w(u) = 0 \\\\ \\end{cases} \\] 简写为 \\[ \\color{blue} {p(u \\mid o)} = [ \\sigma(v_o^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(v_o^T \\theta^u)]^{1 - b^w(u)} \\] 对于\\(w\\)的一个上下文单词\\(o\\)来说，要最大化这个概率： \\[ \\prod_{u \\in S_o} p(u \\mid o ) \\] 对于\\(w\\)的所有上下文单词\\(C_w\\)来说，要最大化： \\[ g(w) = \\prod_{o \\in C_w} \\prod_{u \\in S_o} p(u \\mid o ) \\] 那么，对于整个预料，要最大化： \\[ G = \\prod_{w \\in C} g(w) =\\prod_{w \\in C} \\prod_{o \\in C_w} \\prod_{u \\in S_o} p(u \\mid o ) \\] 对G取对数，最终的目标函数就是： \\[ \\begin {align} L &amp; = \\log G = \\sum_{w \\in C} \\sum_{o \\in C_w} \\log \\prod_{u \\in S_o} p(u \\mid o ) \\\\ &amp;= \\sum_{w \\in C} \\sum_{o \\in C_w} \\log \\prod_{u \\in S_o} [ \\sigma(v_o^T \\theta^u)]^{b^w(u)} \\cdot [1 - \\sigma(v_o^T \\theta^u)]^{1 - b^w(u)} \\\\ &amp; = \\sum_{w \\in C} \\sum_{o \\in C_w} \\sum_{u \\in S_o} \\left ( b^w_u \\cdot \\sigma(v_o^T \\theta^u) + (1 - b^w_u) \\cdot (1-\\sigma(v_o^T \\theta^u)) \\right) \\end{align} \\] 取\\(w, o, u\\)简写L(w, o, u)： \\[ L(w, o, u) = b^w_u \\cdot \\sigma(v_o^T \\theta^u) + (1 - b^w_u) \\cdot (1-\\sigma(v_o^T \\theta^u)) \\] 分别对\\(\\theta^u、v_o\\)求梯度 \\[ \\frac{\\partial L(w, o, u) }{ \\partial \\theta^u} = [b^w_u - \\sigma(v_o^T \\theta^u)] \\cdot v_o, \\quad \\frac{\\partial L(w,o, u) }{ \\partial v_o} = [b^w_u - \\sigma(v_o^T \\theta^u)] \\cdot \\theta^u \\] 更新每个单词的训练参数\\(\\theta^u\\) ： \\[ \\theta^u = \\theta^u + \\alpha \\cdot \\frac{\\partial L(w, o,u) }{ \\partial \\theta^u} \\] 对每个单词更新词向量\\(v(o)\\) ： \\[ v(o) = v(o) + \\alpha \\cdot \\sum_{u \\in S_o} \\frac{\\partial L(w, u) }{ \\partial v_o} \\]","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"}]},{"title":"CS224N教程2-Word2Vec","date":"2017-11-02T01:33:45.000Z","path":"2017/11/02/cs224n-lecture2-word2vec/","text":"Word meaning 词意 词的意思就是idea，如下： 词汇本身表达的意义 人通过词汇传达的想法 在写作、艺术中表达的意思 signifier - signified(idea or thing) - denotation 传统离散表达 传统使用分类学去建立一个WordNet，其中包含许多上位词is-a和同义词集等。如下： 上义词 同义词 entity, physical_entity,object, organism, animal full, good; estimable, good, honorable, respectable 离散表达的问题： 丢失了细微差别，比如同义词：adept, expert, good, practiced, proficient, skillful 不能处理新词汇 分类太主观 需要人力去构建和修改 很难去计算词汇相似度 每个单词使用one-hot编码，比如hotel=\\([0, 1, 0, 0, 0]\\)，motel=\\([0, 0, 1, 0, 0]\\)。 当我搜索settle hotel的时候也应该去匹配包含settle motel的文章。 但是我们的查询hotel向量和文章里面的motel向量却是正交的，算不出相似度。 分布相似表达 通过一个单词的上下文去表达这个单词。 You shall know a word by the company it keeps. --- JR. Firth 例如，下面用周围的单词去表达banking ： government debt problems turning into banking crises as has happened in ​ saying that Europe needs unified banking regulation to replace the hodgepodge 稠密词向量 一个单词的意义应该是由它本身的词向量来决定的。这个词向量可以预测出的上下文单词。 比如lingustics的词向量是\\([0.286, 0.792, -0.177, -0.107, 0.109, -0.542, 0.349]\\) 词嵌入思想 构建一个模型，根据中心单词\\(w_t\\)，通过自身词向量，去预测出它的上下文单词。 \\[ p (context \\mid w_t) = \\cdots \\] 损失函数如下，\\(w_{-t}\\)表示\\(w_t\\)的上下文（负号通常表示除了某某之外），如果完美预测，损失函数为0。 \\[ J = 1 - p(w_{-t} \\mid w_t) \\] Word2Vec 在每个单词和其上下文之间进行预测。 有两种算法： Skip-grams(SG)： 给目标单词，预测上下文 Continuous Bag of Words(CBOW)：给上下文，预测目标单词 两个稍微高效的训练方法： 分层softmax 负采样 课上只是Naive softmax。两个模型，两种方法，一共有4种实现。这里是word2vec详细信息。 Skip-gram 对于每个单词\\(w_t\\)，会选择一个上下文窗口\\(m\\)。 然后要预测出范围内的上下文单词，使概率\\(P(w_{t+i} \\mid w_t)\\)最大。 目标函数 \\(\\theta\\)是我们要训练的参数，目标函数就是所有位置预测结果的乘积，最大化目标函数： \\[ J^\\prime (\\theta) = \\prod_{t=1}^T \\prod_{-m\\le j \\le m} p(w_{t+j} \\mid w_t ;\\; \\theta), \\quad t \\neq j \\] 一般使用negative log likelihood ：负采样教程。 要最大化目标函数，就得得到损失函数。对于对数似然函数，取其负对数就可以得到损失函数，再最小化损失函数，其中\\(T\\)是文本长度，\\(m\\)是窗口大小： \\[ J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T \\sum_{-m\\le j \\le m} \\log P(w_{t+j} \\mid w_t) \\] Loss 函数 = Cost 函数 = Objective 函数 对于softmax概率分布，一般使用交叉熵作为损失函数 单词\\(w_{t+j}\\)是one-hot编码 negative log probability Word2vec细节 词汇和词向量符号说明： \\(u\\) 上下文词向量，向量是\\(d\\)维的 \\(v\\) 词向量 中心词汇\\(t\\)，对应的向量是\\(v_t\\) 上下文词汇\\(j\\) ，对应的词向量是\\(u_j\\) 一共有\\(V\\)个词汇 计算\\(p(w_{t+j} \\mid w_t)\\)， 即： \\[ p(w_{j} \\mid w_t) = \\mathrm{softmax} (u_j^T \\cdot v_t) = \\frac{\\exp(u_j^T \\cdot v_t)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_t)} \\] 两个单词越相似，点积越大，向量点积如下： \\[ u^T \\cdot v = \\sum_{i=1}^M u_i \\times v_i \\] softmax之所以叫softmax，是因为指数会让大的数越大，小的数越小。类似于max函数。下面是计算的详细信息： 一些理解和解释： \\(w_t\\)是one-hot编码的中心词汇，维数是\\((V, 1)\\) \\(W\\)是词汇表达矩阵，维数是\\((d, V)\\)，一列就是一个单词 \\(Ww_t = v_t\\) 相乘得到词向量\\(v_t\\) ，\\((d, V) \\cdot (V, 1) \\to (d, 1)\\)， 用\\(d\\)维向量去表达了词汇t \\(W^\\prime\\)， \\(W^{\\prime}\\cdot v_t = s\\)，\\((V, d) \\cdot (d, 1) \\to (V, 1)\\) ， 得到 语义相似度向量\\(s\\) 再对\\(s\\)进行softmax即可求得上下文词汇 每个单词有两个向量，作为center单词向量和context单词向量 偏导计算 设\\(o\\)是上下文单词，\\(c\\)是中心单词，条件概率如下： \\[ P(o \\mid c) = \\frac{\\exp(u_o^T \\cdot v_c)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} \\] 这里只计算\\(\\log P\\)对\\(v_c\\)向量的偏导。 用\\(\\mathbf{\\theta}\\)向量表示所有的参数，有\\(V\\)个单词，\\(d\\)维向量。每个单词有2个向量。参数个数一共是\\(2dV\\)个。 向量偏导计算公式，\\(\\mathbf{x, a}\\) 均是向量 \\[ \\frac {\\partial \\mathbf{x}^T \\mathbf{a}} { \\partial \\mathbf{x}} = \\frac {\\partial \\mathbf{a}^T \\mathbf{x}} { \\partial \\mathbf{x}} = \\mathbf{a} \\] 函数偏导计算，链式法则，\\(y=f(u), u=g(x)\\) \\[ \\frac{\\mathrm{d}y}{\\mathrm{d} x} = \\frac{\\mathrm{d}y}{\\mathrm{d} u} \\frac{\\mathrm{d}u}{\\mathrm{d} x} \\] 最小化损失函数： \\[ J(\\theta) = - \\frac{1}{T} \\sum_{t=1}^T \\sum_{-m\\le j \\le m} \\log P(w_{t+j} \\mid w_t), \\quad j \\neq m \\] 这里只计算\\(v_c\\)的偏导，先进行分解原式为2个部分： \\[ \\frac { \\partial} {\\partial v_c} \\log P(o \\mid c) = \\frac { \\partial} {\\partial v_c} \\log \\frac{\\exp(u_o^T \\cdot v_c)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} = \\underbrace { \\frac { \\partial} {\\partial v_c} \\log \\exp (u_o^T \\cdot v_c) }_{1} - \\underbrace { \\frac { \\partial} {\\partial v_c} \\log \\sum_{i=1}^V \\exp(u_i^T \\cdot v_c) }_{2} \\] 部分1推导 \\[ \\begin{align} \\frac { \\partial} {\\partial v_c} \\color{red}{\\log \\exp (u_o^T \\cdot v_c) } &amp; = \\frac { \\partial} {\\partial v_c} \\color{red}{u_o^T \\cdot v_c} = \\mathbf{u_o} \\end{align} \\] 部分2推导 \\[ \\begin{align} \\frac { \\partial} {\\partial v_c} \\log \\sum_{i=1}^V \\exp(u_i^T \\cdot v_c) &amp; = \\frac{1}{\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} \\cdot \\color{red}{ \\frac { \\partial} {\\partial v_c} \\sum_{x=1}^V \\exp(u_x^T \\cdot v_c)} \\\\ &amp; = \\frac{1}{A} \\cdot \\sum_{x=1}^V \\color{red} {\\frac { \\partial} {\\partial v_c} \\exp(u_x^T \\cdot v_c)} \\\\ &amp; = \\frac{1}{A} \\cdot \\sum_{x=1}^V \\exp (u_x^T \\cdot v_c) \\color{red} {\\frac { \\partial} {\\partial v_c} u_x^T \\cdot v_c} \\\\ &amp; = \\frac{1}{\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)} \\cdot \\sum_{x=1}^V \\exp (u_x^T \\cdot v_c) \\color{red} {u_x} \\\\ &amp; = \\sum_{x=1}^V \\color{red} { \\frac{\\exp (u_x^T \\cdot v_c)} {\\sum_{i=1}^V \\exp(u_i^T \\cdot v_c)}} \\cdot u_x \\\\ &amp; = \\sum_{x=1}^V \\color{red} {P(x \\mid c) }\\cdot u_x \\end{align} \\] 所以，综合起来可以求得，单词o是单词c的上下文概率\\(\\log P(o \\mid c)\\) 对center向量\\(v_c\\)的偏导： \\[ \\frac { \\partial} {\\partial v_c} \\log P(o \\mid c) = u_o -\\sum_{x=1}^V P(x \\mid c) \\cdot u_x = \\color{blue} {\\text{观察到的} - \\text{期望的}} \\] 实际上偏导是，单词\\(o\\)的上下文词向量，减去，所有单词\\(x\\)的上下文向量乘以x作为\\(c\\)的上下文向量的概率。 总体梯度计算 在一个window里面，对中间词汇\\(v_c\\)求了梯度， 然后再对各个上下文词汇\\(u_o\\)求梯度。 然后更新这个window里面用到的参数。 比如句子We like learning NLP。设\\(m=1\\)： 中间词汇求梯度 \\(v_{like}\\) 上下文词汇求梯度 \\(u_{we}\\) 和 \\(u_{learning}\\) 更新参数 梯度下降 有了梯度之后，参数减去梯度，就可以朝着最小的方向走了。机器学习梯度下降 \\[ \\theta^{new} = \\theta^{old} - \\alpha \\frac{\\partial}{\\partial \\theta^{old}} J(\\theta), \\quad \\quad \\theta^{new} = \\theta^{old} - \\alpha \\Delta_{\\theta} J(\\theta) \\] 随机梯度下降 预料会有很多个window，因此每次不能更新所有的。只更新每个window的，对于window t： \\[ \\theta^{new} = \\theta^{old} - \\alpha \\Delta_{\\theta} J_t(\\theta) \\]","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"},{"name":"CS224N","slug":"CS224N","permalink":"http://plmsmile.github.io/tags/CS224N/"}]},{"title":"subword-units","date":"2017-10-19T14:16:07.000Z","path":"2017/10/19/subword-units/","text":"Neural Machine Translation of Rare Words with Subword Units 背景 摘要 NMT处理的词汇表是定长的，但是实际翻译却是OOV(out of vocabulary)的。以前是把 新词汇加到词典里去。本文是提出一种subword单元的策略，会把稀有和未知词汇以subword units序列来进行编码，更简单更有效。 会介绍不同分词技术的适用性，包括简单的字符n元模型和基于字节对编码压缩算法的分词技术。也会以经验说明subword模型比传统back-off词典的方法好。 简介 NMT模型的词汇一般是30000-5000，但是翻译却是open-vocabulary的问题。很多语言富有信息创造力，比如凝聚组合等等，翻译系统就需要一种低于word-level的机制。 Word-level NMT的缺点 对于word-level的NMT模型，翻译out-of-vocabulary的单词会回退到dictionary里面去查找。有下面几个缺点 种技术在实际上使这种假设并不成立。比如源单词和目标单词并不是一对一的，你怎么找呢 不能够翻译或者产生未见单词 把unknown单词直接copy到目标句子中，对于人名有时候可以。但是有时候却需要改变形态或者直译。 Subword-level NMT 我们的目标是建立open-vocabulary的翻译模型，不用针对稀有词汇去查字典。事实证明，subword模型效果比传统大词汇表方法更好、更精确。Subword神经网络模型可以从subword表达中学习到组合和直译等能力，也可以有效的产生不在训练数据集中的词汇。本文主要有下面两个贡献 open-vocabulary的问题可以通过对稀有词汇使用subword units单元来编码解决 采用Byte pair encoding (BPE) 算法来进行分割。BPE通过一个固定大小的词汇表来表示开放词汇，这个词汇表里面的是变长的字符串序列。这是一种对于神经网络模型非常合适的词分割策略。 神经机器翻译 NMT是使用的Bahdanau的Attention模型。Encoder是双向RNN，输入\\(X=(x_1, x_2, \\cdots, x_m)\\)，会把两个方向的隐状态串联起来得到annotation向量\\(\\mathbf x\\)。实际上是一个矩阵，对于单个\\(x_j\\)来说，对应的注释向量是\\(\\mathbf{x}_j\\)。 Decoder是一个单向的RNN，预测\\(Y=(y_1, y_2, \\cdots, y_n)\\)。 预测\\(y_i\\) 时，需要： 当前的隐状态\\(s_i\\) 上一时刻的输出\\(y_{i-1}\\)作为当前的输入 语义向量\\(\\mathbf c_i\\) 。语义向量是由所有的注释向量\\(x_j\\) 加权求和得到的。权就是对齐概率\\(\\alpha_{ij}\\)。 即\\(\\mathbf c_i = \\sum_{j=1} ^ m \\alpha_{ij} \\mathbf x_j\\) 详情请看谷歌论文里面的介绍或者Bahdanau的论文。 Subword 翻译 下面词汇的翻译是透明的(transparent，明显的) 命名实体。如果两个语言的字母表相同，可以直接copy到目标句子中去，也可以抄写音译直译等。 同源词和外来词。有着共同的起源，但是不同的语言表达形式不同，所以character-level翻译规则就可以了。 形态复杂的词语。包含多个语素的单词，可以通过单独翻译语素来翻译。 总之，通过subword单元表示稀有词汇对于NMT来说可以学到transparent翻译，并且可以翻译和产生未见词汇。 相关工作 对于SMT(Statistical Machine Translation)来说，翻译未见单词一直是研究的主题。 很多未见单词都是人名，如果两种语言的字母表一样，那么可以直接复制过去。如果不一样，那么就得音译过去。基于字符（character-based）的翻译是比较成功的。 形态上很复杂的单词往往需要分割，这里有很多的分割算法。基于短语的SMT的分割算法是比较保守的。而我们需要积极的细分，让网络可以处理open-vocabulary，而不是去求助于背字典。 怎么选择subword units要看具体的任务。 提出了很多这样的技术：生成基于字符或者基于语素的定长的连续的词向量。于此同时，word-based的方法并没有重大发现。现在的注意力机制还是基于word-level的。我们希望，注意力机制能从我们变长表达中收益：网络可以把注意力放在不同的subword units中。 这可以突破定长表达的信息传达瓶颈。 NMT减少词汇表可以大大节省时间和增加空间效率。我们也想要对一个句子更紧凑的表达。因为文本长度增加了，会减少效率，也会增加模型传递信息的距离。(hidden size？) 权衡词汇表大小和文本长度，可以用未分割单词列表，subword 单元表达的稀有词汇。作为一个代替，Byte pair encoding就是这样的一种分割算法，可以学到一个词汇表，同时对文本有很好的压缩率。 Byte Pair Encoding Byte pair encoding是一种简单的数据压缩技术，它把句子中经常出现的字节pairs用一个没有出现的字节去替代。我们使用这种算法去分割单词，但我们合并字符或者字符序列。 算法步骤 算法步骤如下： 初始化符号词表。用所有的字符加入到符号词表中。对所有单词的末尾加入特殊标记，如-。翻译后恢复原始的标记。 迭代对所有符号进行计数，找出次数最多的(A, B)，用AB代替。 每次合并，会产生一个新的符号，代表着n-gram字符 常见的n-grams字符(或者whole words)，最终会被合并到一个符号 最终符号词表大小=初始大小+合并操作次数。操作次数是算法唯一的超参数。 不用考虑不在训练集里面的pair，为每个word根据出现频率设置权重。 和传统的压缩算法(哈夫曼编码)相比，我们的以subword 单元堆积的符号序列依然是可以解释的，网络也可以翻译和产生新的词汇（训练集没有见过的）。 下面是代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def process_raw_words(words, endtag='-'): '''把单词分割成最小的符号，并且加上结尾符号''' vocabs = &#123;&#125; for word, count in words.items(): # 加上空格 word = re.sub(r'([a-zA-Z])', r' \\1', word) word += ' ' + endtag vocabs[word] = count return vocabsdef get_symbol_pairs(vocabs): ''' 获得词汇中所有的字符pair，连续长度为2，并统计出现次数 Args: vocabs: 单词dict，(word, count)单词的出现次数。单词已经分割为最小的字符 Returns: pairs: ((符号1, 符号2), count) ''' #pairs = collections.defaultdict(int) pairs = dict() for word, freq in vocabs.items(): # 单词里的符号 symbols = word.split() for i in range(len(symbols) - 1): p = (symbols[i], symbols[i + 1]) pairs[p] = pairs.get(p, 0) + freq return pairsdef merge_symbols(symbol_pair, vocabs): '''把vocabs中的所有单词中的'a b'字符串用'ab'替换 Args: symbol_pair: (a, b) 两个符号 vocabs: 用subword(symbol)表示的单词，(word, count)。其中word使用subword空格分割 Returns: vocabs_new: 替换'a b'为'ab'的新词汇表 ''' vocabs_new = &#123;&#125; raw = ' '.join(symbol_pair) merged = ''.join(symbol_pair) # 非字母和数字字符做转义 bigram = re.escape(raw) p = re.compile(r'(?&lt;!\\S)' + bigram + r'(?!\\S)') for word, count in vocabs.items(): word_new = p.sub(merged, word) vocabs_new[word_new] = count return vocabs_newraw_words = &#123;\"low\":5, \"lower\":2, \"newest\":6, \"widest\":3&#125;vocabs = process_raw_words(raw_words)num_merges = 10print (vocabs)for i in range(num_merges): pairs = get_symbol_pairs(vocabs) # 选择出现频率最高的pair symbol_pair = max(pairs, key=pairs.get) vocabs = merge_symbols(symbol_pair, vocabs)print (vocabs) 单独BPE 为目标语言和原语言分别使用BPE去计算词典。从文本和词汇表大小来说更加紧凑，能保证每个subword单元在各自的训练数据上都有。同样的名字在不同的语言中可能切割的不一样，神经网络很难去学习subword units之间的映射。 Joint BPE 为目标语言和原语言一起使用BPE，即联合两种语言的词典去做BPE。提高了源语言和目标语言的分割一致性。训练中一般concat两种语言。 评估 有两个重要问题 subword units表达稀有词汇，是否真的对翻译有效果？ 根据词汇表大小，文本长度，翻译质量，怎样分割才是最好的？ 我们的使用WMT2015的数据，使用BLEU来评判结果。英语-德语： 420万句子对 1亿个token 英语-俄罗斯语： 260万句子对 5000万个token minibatch-size是80，每个epoch都会reshuffle训练数据。训练了7天，每12个小时存一次模型，取最后4个模型再单独训练。分别选择clip 梯度是5.0和1.0，1.0效果好一些。最终是融合了8个模型。 Beam search的大小是12，使用双语词典进行快速对齐，类似于对稀有词汇查找词典，也会用词典去加速训练。 Subword统计 我们的目标是通过一个紧凑的固定大小的subword词典去代表一个open-vocabulary，并且能够有效的训练和解码。 一个简单的基准就是把单词分割成字符n-grams 。n的选择很重要，可以在序列长度(tokens)和词汇表大小(size)之间做一个权衡。序列的长度会增加许多，一个比较好得减少长度的方法就是使用k个最常见的未被分割的词列表。只有unigram(n=1，一元模型)表达才能真正实现open-vocabulary，但是实际上效果却并不好。Bigram效果好，但是不能产生测试集中的tokens。 BPE符合open-vocabulary的目标，并且合并操作可以应用于测试集，去发现未知符号的分割。与字符集模型的主要区别在于，BPE更紧凑的表示较短序列，注意力模型可以应对变长的单元。 分割方法 tokens types unk merge次数 BPE 112 m 63000 0 59500 BPE(joint) 111 m 82000 32 89500 实际上，NMT词汇表中，并不会包含不常见的subword单元，因为里面有很多噪声。 name seg shotlist s-v t-v S-BLEU BLEU CHAR-F3 CHAR-F3 F1 F1 F1 BPE-60k BPE 无 60000 60000 21.5 24.5 52.0 53.9 58.4 40.9 29.3 BPE-J60k BPE(joint) 无 90000 90000 22.8 24.7 51.7 54.1 58.5 41.8 33.6 翻译评估 所有的subword系统都不会去查字典。使用UNK表示模型词典以外的单词，OOV表示训练集里面没有的单词。","tags":[{"name":"NMT","slug":"NMT","permalink":"http://plmsmile.github.io/tags/NMT/"},{"name":"subword","slug":"subword","permalink":"http://plmsmile.github.io/tags/subword/"}]},{"title":"Wordpiece模型","date":"2017-10-19T06:41:00.000Z","path":"2017/10/19/26_wordpieacemodel/","text":"Japanese and Korean Voice Search 看了半天才发现不稳啊。 背景知识 摘要 这篇文章主要讲了构建基于日语和法语的语音搜索系统遇到的困难，并且提出了一些解决的方法。主要是下面几个方面： 处理无限词汇表的技术 在语言模型和词典的书面语中，完全建模并且避免系统复杂度 如何去构建词典、语言和声学模型 展示了由于模糊不清，多个script语言的打分结果的困难性。这些语言语音搜索的发展，大大简化了构建一门新的语言的语音搜索系统的最初的处理过程，这些很多都成为了语言搜索国际化的默认过程。 简介 语音搜索通过手机就可以访问到互联网，这对于一些不好输入字符的语言来说，非常有用。尽管从基础技术来讲，语音识别的技术是在不同的语言之间是非常相似的，但是许多亚洲语言面临的问题，如果只是用传统的英语的方法去对待，这根本很难解决嘛。许多亚洲语言都有非常大的字符库。这让发音词典就很复杂。在解码的时候，由于很多同音异义词汇，解码也会很复杂。基本字符集里面的很多字符都会以多种形式存在，还要数字也会有多种形式，在某些情况下，这都需要适当的标准化。 很多亚洲语言句子中没有空格去分割单词。需要使用segmenters去产生一些词单元。 这些词单元会在词典和语言模型中使用，词单元之间可能需要添加或者删除空白字符。我们开发了一个纯数据驱动的sementers，可以使用任何语言，不需要修改。 还有就是如何去处理英文中的许多词汇，比如URL、数字、日期、姓名、邮件、缩写词汇、标点符号和其它特殊词汇等等。 语音数据收集 公告开放的数据集很难用作商用，有很多限制，所以自己收集数据集。通过手机，从不同的地区、年龄、方言等等，收集数据。一般是尽可能使用这些原始的数据并且建模，而不是转化为书面的数据或者有利于英语的数据。 分词和词库 提出一种WordPieceModel去解决OOV(out-of-vocabulary)的问题。WordPieaceModel通过一种贪心算法，自动地、增量地从大量文本中学得单词单元（word units），一般数量是200k。算法可以，不关注语义，而去最大化训练数据语言模型的可能性，这也是解码过程中的度量标准。该算法可以有效地自动学习词库。 WordPieceModel算法步骤 1 初始化词库 给词库添加基本的所有的unicode字符和ascii字符。日语是22000，韩语是11000。 2 建立模型 基于训练数据，建立模型，使用初始化好的词库。 3 生成新单元 从词库中选择两个词单元组成新的词单元，加入到词库中。组成的新词要使模型的似然函数likelyhood最大。 4 继续加或者停止 如果达到词库数量的上限，或者似然函数增加很小，那么就停止，否则就继续2步，继续合并添加。 算法优化 你也发现了，计算所有可能的Pair这样会非常非常耗费时间。如果当前词库数量是\\(K\\)，那么每次迭代计算的复杂度是\\(O(K^2)\\) 。有下面3个步骤可以进行优化 选择组合新的单元时，只测试训练数据中有的单元。 只测试有很大机会成为最好的Pair，例如high priors 把一些不会影响到彼此的group pairs组合到一起，作为一个单一的迭代过程 only modify the language model counts for the affected entries （不懂什么意思） 使用这些加速算法，我们可以在一个机器上，几个小时以内，从频率加权查询列表中，构建一个200k的词库。 得到wordpiece词库之后，可以用来语言建模，做词典和解码。分割算法，构建了以基础字符开始的Pairs的逆二叉树。本身已经不需要动态规划或者其他的搜索方法。因此在计算上非常有效。分开基本的字符，基于树从上到下，会在线性时间给出一个确定的分割信息，线性时间取决于句子的长度。大约只有4%的单词具有多个发音。如果添加太多的发音会影响性能，可能是因为在训练和解码时对齐过程期间的可能数太多了 继续说明 一般是句子没有空格的，但是有的时候却有空格，比如韩文，搜索关键字。线上系统没有办法去把这些有空格的word pieces组合在一起。这对于常见的词汇和短查询是没有影响的，因为它们已经组合成一个完整的word unit。但是对于一些例如空格出现在不该出现的地方等不常见的查询，就很烦恼了。 在解码的时候，加空格效率更高，采用下面的技术： 1 原始语言模型数据被用来&quot;as written&quot;，表示一些有空格一些没有空格。 2 WPM模型分割LM数据时，每个单元在前面或者后面遇到一个空格，那么就添加一个空格标记。单元有4种情况：两边都有空格，左边有，右边有，两边都没有。使用下划线标记 3 基于这个新词库构建LM和词典 4 解码时，根据模型会选择一个最佳路径，之前在哪些地方放了空格或者没有。为了输出显示，需要把空格全部移除。有3种情况，移除所有空格；移除两个空格用一个空格表示；移除一个空格。","tags":[{"name":"WPM","slug":"WPM","permalink":"http://plmsmile.github.io/tags/WPM/"},{"name":"语音搜索","slug":"语音搜索","permalink":"http://plmsmile.github.io/tags/语音搜索/"},{"name":"语音识别","slug":"语音识别","permalink":"http://plmsmile.github.io/tags/语音识别/"}]},{"title":"循环神经网络","date":"2017-10-18T12:35:29.000Z","path":"2017/10/18/rnn/","text":"LSTM经典描述 经典RNN模型 模型 人类在思考的时候，会从上下文、从过去推断出现在的结果。传统的神经网络无法记住过去的历史信息。 循环神经网络是指随着时间推移，重复发生的结构。它可以记住之前发生的事情，并且推断出后面发生的事情。用于处理时间序列很好。所有的神经元共享权值。如下图所示。 记住短期信息 比如预测“天空中有__”，如果过去的信息“鸟”离当前位置比较近，则RNN可以利用这个信息预测出下一个词为“鸟” 不能长期依赖 如果需要的历史信息距离当前位置很远，则RNN无法学习到过去的信息。这就是不能长期依赖的问题。 LSTM总览与核心结构 总览 所有的RNN有着重复的结构，如下图，比如内部是一个简单的tanh 层。 LSTM也是一样的，只不过内部复杂一些。 单元状态 单元状态像一个传送带，通过整个链向下运行，只有一些小的线性作用。信息就沿着箭头方向流动。 LSTM的门结构 LSTM的门结构 可以添加或者删除单元状态的信息，去有选择地让信息通过。它由sigmoid网络层 和 点乘操作组成。输出属于\\([0, 1]\\)之间，代表着信息通过的比例。 LSTM细节解剖 一些符号说明，都是\\(t\\)时刻的信息 ： \\(C_{t-1}\\) : 的单元状态 \\(h_{t}\\) : 隐状态信息 （也作单个神经元的输出信息） \\(x_t\\) : 输入信息 \\(o_t\\) ：输出信息 （输出特别的信息） 1 遗忘旧信息 对于\\(C_{t-1}\\)中的每一个数字， \\(h_{t-1}\\)和\\(x_t\\)会输出0-1之间的数来决定遗忘\\(C_{t-1}\\)中的多少信息。 2 生成候选状态和它的更新比例 生成新的状态：tanh层创建新的候选状态\\(\\hat{C}_t\\) 输入门：决定新的状态哪些信息会被更新\\(i_t\\)，即候选状态\\(\\hat{C}_t\\)的保留比例。 3 新旧状态合并更新 生成新状态\\(C_t\\)：旧状态\\(C_{t-1}\\) + 候选状态\\(\\hat{C}_t\\)。 旧状态\\(C_{t-1}\\)遗忘不需要的， 候选状态\\(\\hat{C}_{t-1}\\)保留需要更新的，都是以乘积比例形式去遗忘或者更新。 4 输出特别的值 sigmoid：决定单元状态\\(C_t\\)的哪些信息要输出。 tanh: 把单元状态\\(C_t\\)的值变到\\([-1, 1]\\)之间。 LSTM总结 核心结构如下图所示 要忘掉部分旧信息，旧信息\\(C_{t-1}\\)的遗忘比例\\(f_t\\) \\[ f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f) \\] 新的信息来了，生成一个新的候选\\(\\hat{C}_t\\) \\[ \\hat{C}_t = \\tanh (W_C \\cdot [h_{t-1}, x_t] + b_C) \\] 新信息留多少呢，新候选\\(\\hat C_t\\)的保留比例\\(i_t\\) \\[ i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i) \\] 合并旧信息和新信息，生成新的状态信息\\(C_t\\) \\[ C_t = f_t * C_{t-1} + i_t * \\hat C_t \\] 输出多少呢，单元状态\\(C_t\\)的输出比例\\(o_t\\) \\[ o_t = \\sigma (W_o \\cdot [h_{t-1}, x_t] + b_o) \\] 把\\(C_t\\)化到\\([-1, 1]\\)再根据比例输出 \\[ h_t = o_t * \\tanh(C_t) \\] 图文简介描述LSTM 总体架构 单元架构 流水线架构 数据流动 圆圈叉叉代表着遗忘\\(C_{t-1}\\)的信息。乘以向量来实现，向量各个值在\\([0, 1]\\)之间。 靠近0就代表着遗忘很多，靠近1就代表着保留很多。 框框加号代表着数据的合并。旧信息\\(C_{t-1}\\)和新候选信息\\(\\hat C_t\\)的合并。 合并之后就得到新信息\\(C_t\\)。 遗忘门 上一个LSTM的输出\\(h_{t-1}\\) 和 当前的输入\\(x_t\\)，一起作为遗忘门的输入。 0是偏置\\(b_0\\)， 一起做个合并，再经过sigmoid生成遗忘权值\\(f_t\\)信息， 去遗忘\\(C_{t-1}\\)。 新信息门 新信息门决定着新信息对旧信息的影响力。和遗忘门一样\\(h_{t-1}\\)和\\(x_t\\)作为输入。 sigmoid：生成新信息的保留比例。tanh：生成新的信息。 新旧信息合并 旧信息\\(C_{t-1}\\)和新信息\\(\\hat{C}_t\\)合并，当然分别先过遗忘阀门和更新阀门。 输出特别的值 把新生成的状态信息\\(C_t\\)使用tanh变成\\((-1, 1)\\)之间，然后经过输出阀门进行输出。 LSTM变体 观察口连接 传统LSTM阀门值比例的计算，即更新、遗忘、输出的比例只和\\(h_{t-1}, x_t\\)有关。 观察口连接，把观察到的单元状态也连接sigmoid上，来计算。即遗忘、更新比例和\\(C_{t-1}, h_{t-1}, x_t\\)有关，输出的比例和\\(C_t, h_{t-1}, x_t\\)有关。 组队遗忘 如下图所示，计算好\\(C_{t-1}\\)的遗忘概率\\(i_t\\)后，就不再单独计算新候选\\(\\hat C_t\\)的保留概率\\(i_t\\)。而是直接由1减去遗忘概率得到更新概率。即\\(i_t = 1 - f_t\\)，再去更新。 GRU LSTM有隐状态\\(h_t\\)和输出状态\\(o_t\\)，而GRU只有\\(h_t\\)，即GRU的隐状态和输出状态是一样的，都用\\(h_t\\)表示。 更新门\\(z_t\\)负责候选隐层层\\(\\hat h_t\\)保留的比例， \\(1-z_t\\)负责遗忘旧状态信息\\(h_{t-1}\\)的比例 \\[ z_t = \\sigma (W_z \\cdot [h_{t-1}, x_t]) \\] 候选隐藏层\\(\\hat h_t\\)的计算由\\(h_{t-1}\\)和\\(x_t\\)一起计算得到。所以计算\\(\\hat h_t\\)之前，要先计算\\(h_{t-1}\\)的重置比例。 重置门\\(r_t\\)负责\\(h_{t-1}\\)对于新的候选\\(\\hat h_t\\)的重置比例 \\[ r_t = \\sigma (W_r \\cdot [h_{t-1}, x_t]) \\] 新候选状态\\(\\hat h_t\\)的计算 \\[ \\hat h_t = \\tanh (W \\cdot [r_t * h_{t-1}, x_t]) \\] 最终新状态\\(h_t\\)由\\(h_{t-1}\\)和\\(\\hat h_t\\)计算得到，分别的保留比例是\\(1-z_t\\)和\\(z_t\\) \\[ h_t = (1 - z_t) * h_{t-1} + z_t * \\hat h_t \\]","tags":[{"name":"RNN","slug":"RNN","permalink":"http://plmsmile.github.io/tags/RNN/"},{"name":"LSTM","slug":"LSTM","permalink":"http://plmsmile.github.io/tags/LSTM/"},{"name":"GRU","slug":"GRU","permalink":"http://plmsmile.github.io/tags/GRU/"}]},{"title":"谷歌翻译论文笔记","date":"2017-10-17T05:25:38.000Z","path":"2017/10/17/谷歌翻译论文笔记/","text":"谷歌神经机器翻译系统 简介 神经机器翻译是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点 训练和翻译都太慢了，花费代价很大 缺乏鲁棒性，特别是输入句子包含生僻词汇 精确度和速度也不行 传统NMT缺点 神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。 NMT避开了传统基于短语的翻译模型的很多缺点。但是，在实际中，NMT的准确度要比基于短语的翻译模型更差些。 NMT有3个主要的缺点：训练和推理速度太慢，不能有效处理稀有词汇，有时不能完全翻译原句子。 训练和推理速度太慢 训练大数据集，需要大量时间和资源；反馈太慢周期太长。加了一个小技巧，看结果要等很长时间。推理翻译的时候，要使用大量的参数去计算，也很慢。 不能有效处理稀有词汇 有两个方法去复制稀有单词： 模仿传统对齐模型去训练1个copy model 使用注意力机制去复制 但是效果都不是很好，都不可靠，不同语言的对齐效果差；在网络很深的时候，注意力机制的对齐向量也不稳定。而且，简单的复制过去也不是最好的办法，比如需要直译的时候。 不能完整翻译整个句子 不能覆盖整个输入句子的内容，然后会导致一些奇怪的翻译结果。 GNMT的模型优点 采用的模型：深层LSTM 、Encoder8层、Decoder8层 。我的LSTM笔记。 各层之间使用残差连接促进梯度流，顶层Enocder到底层Decoder使用注意力连接，提高并行性。 进行翻译推断的时候，使用低精度算法，去加速翻译。 处理稀有词汇：使用sub-word单元，也称作wordpieces方法。把单词划分到有限的sub-word (wordpieces)单元集合，输入输出都这样。sub-word结合了字符分割模型的弹性和单词分割模型的效率。 Beam Search 使用长度规范化和覆盖惩罚。覆盖惩罚就是说，希望，翻译的结果句子，尽量多地包含输入句子中的所有词汇。 使用强化学习去优化模型， 优化翻译的BLEU 分数。 先进技术 有很多先进的技术来提高NMT，下面这些都有论文的。 利用attention去处理稀有词汇 建立翻译覆盖的机制 多任务和半监督训练，去合并使用更多数据 字符分割的encoder和decoder 使用subword单元处理稀疏的输出 系统架构 系统总览 架构 有3个模块：Encoder，Decoder，Attention。 Encdoer：把句子转换成一系列的向量，每一个向量代表一个输入词汇（符号）。 Decoder：根据这些向量，每一时刻会生成一个目标词汇，直到EOS。 Attention：连接Encoder和Decoder，在解码的过程中，可以让Decoder有权重的有选择的关注输入句子的部分区域。 符号说明 加粗小写代表向量，如\\(\\boldsymbol {v, o_i}\\) 加粗大写，矩阵，如\\(\\boldsymbol {U, W}\\) 和\\(\\mathbf{U, W}\\) 手写体，集合，如\\(\\mathcal{V, F}\\) 大写字母，句子，如\\(X, Y\\) 小写字母，单个符号，如\\(x_1, x_2\\) Encoder 输入句子和目标句子组成一个Pair \\((X, Y)\\)，其中输入句子\\(X = x_1, x_2, \\cdots, x_M\\) ，\\(M\\) 个单词，翻译的输出目标句子\\(Y = y_1, y_2, \\cdots, y_N\\) ，有\\(N\\)个单词。 Encoder其实就是一个转换函数，得到\\(M\\)个长度固定的向量，也就是其中Encoder对各个\\(x_i\\)的编码向量 \\(\\mathbf{x_i}\\) ： \\[ \\mathbf {x_1, x_2, \\cdots, x_M} = \\mathit{EncoderRNN} (x_1, x_2, \\cdots, x_n) \\] 使用链式条件概率可得到翻译概率\\(\\color{blue} {P (Y \\mid X)}\\) ，其中\\(y_0\\)是起始符号\\(SOS\\) 。 \\[ \\begin{align} P(Y \\mid X) &amp; = P(Y \\mid \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ &amp; =\\prod_{i=1}^N P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ \\end{align} \\] Decoder 在翻译\\(y_i\\)的时候， 利用Encoder得到的编码向量\\(\\mathbf{x_i}\\) 和 \\(y_0 \\sim y_{i-1}\\) 来进行计算概率翻译 \\[ P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\] Decoder是由RNN+Softmax构成的。会得到一个隐状态\\(\\mathbf{y_i}\\) 向量，有2个作用： 作为下一个RNN的输入 \\(\\mathbf{y_i}\\)经过softmax得到概率分布， 选出\\(y_i\\) 输出符号 Attention 在之前的文章里有介绍论文 和 通俗理解，其实就是影响力模型。原句子的各个单词对翻译当前单词分别有多少的影响力，也叫作对齐概率吧。使用decoder-RNN的输出\\(\\mathbf{y_{t-1}}\\) 向量作为时刻\\(t\\)的输入。 时刻\\(t\\)，给定\\(\\mathbf{y_{t-1}}\\) 有3个符号定义： \\(s_i\\) ： \\(y_t\\)与\\(x_i\\)的得分，在luong论文里面有3种计算方式，分别是dot, general和concat。 \\(p_i\\) ：\\(y_t\\)与\\(x_i\\)的对齐概率，\\((p_1, p_2, \\cdots, p_M)\\) 联合起来就是\\(y_t\\)与\\(X\\)的对齐向量。其实就是对得分softmax。 \\(\\mathbf{a_t}\\) ：带注意力的语义向量。对于所有的\\(x_i\\)，使用\\(y_t\\)与它的对齐概率\\(p_i\\)乘以本身的编码向量\\(\\mathbf{x_i}\\)，得到\\(x_i\\)传达的语义，再对所有的语义求和，即得到总体的带有注意力的语义。 整体详细计算的流程，如下面的公式： \\[ \\begin {align} &amp; s_i = \\mathit{AttentionFunction} (\\mathbf{y_{t-1}}, \\mathbf{x_i}), \\quad i \\in [1, M] \\\\ &amp; p_i = \\frac {\\exp (s_i)}{\\sum_{j=1}^M \\exp(s_j)} \\quad i \\in [1, M] \\\\ &amp; \\mathbf{a_t} = \\sum_{i=1}^M p_i \\cdot \\mathbf{x_i} \\quad \\color{blue}{对所有带注意力的x_i的语义求和得总体的语义} \\end{align} \\] 计算打分的函数即\\(\\mathit{AttentionFunction}\\)是一个有隐藏层的前馈网络！实现是Badh这个人的，不是Luong的。 系统架构图说明 架构图如下 Encoder是8层的LSTM：最底层是双向的LSTM，得到两个方向的信息；上面7层都是单向的。Encoder和Decoder的残差连接都是从第3层开始的。 训练时，会让Encoder最底层的双向的LSTM开始训练，完成之后，再训练别的层，每层都用单独的GPU。 为了提高并行性，Decoder最底层，只是为了用来计算Attention Context。带注意力的语义计算好之后，会单独发给其它的各个层。 经验说明 实验结果得到，要想NMT有好效果，Encoder和Decoder的网络层数一定要够深，才能发现2种语言之间的细微异常规则。和这个同理，深层LSTM比浅层LSTM明显效果好。每加一层，会大约减少10%的perplexity。所以使用deep stacked LSTM。 残差连接 残差网络讲解 。 虽然深层LSTM比浅层LSTM效果好，但是如果只是简单堆积的话，只在几个少数层效果才可以。经过试验，4层的话估计效果还可以，6层大部分都不好，8层的话，效果就相当差了。这是因为网络会变得很慢和很难训练，很大程度是因为梯度爆炸和梯度消失的问题。 根据在中间层和目标之间建立差别的思想，引入残差连接，如下图右边所示。其实就是把之前层的输入和当前的输出合并起来，作为下一层的输入。 一些参数和符号说明，一下均是时刻\\(t\\) \\(\\mathbf{x^i_t}\\) : 第\\(i+1\\)层 \\(\\mathit{LSTM}_{i+1}\\)的输入。 即上标代表LSTM的层数，下标代表时间。 \\(\\mathbf{W} ^i\\) : 第\\(i\\)层LSTM的参数 \\(\\mathbf {h} _t^i\\) : 第\\(i\\)层输出隐状态 \\(\\mathbf {c} ^i_t\\) : 第\\(i\\)层输出单元状态 那么\\(LSTM_i\\)和\\(LSTM_{i+1}\\)是这样交互的。即层层纵向传递输入，时间横向传递隐状态和单元状态。 \\[ \\begin{align} &amp; \\mathbf{c}_t^i, \\mathbf{h}_t^i = LSTM_i(\\mathbf{c}_{t-1}^i, \\mathbf{h}_{t-1}^i, \\mathbf x_{t}^{i-1} ; \\; \\mathbf W^i ) \\\\ &amp; \\mathbf x_t^i = \\mathbf h_t^i \\quad\\quad\\quad\\quad \\color{blue}{普通连接：i+1层输入=i层隐层输出} \\\\ &amp; \\mathbf{x}_t^i = \\mathbf h_t^i + \\mathbf{x}_t^{i-1} \\quad \\color{blue} {残差连接：第i+1层的输入=i层输入+i层隐层输出} \\\\ &amp; \\mathbf{c}_t^{i+1}, \\mathbf{h}_t^{i+1} = LSTM_{i+1} (\\mathbf c_{t-1}^{i+1}, \\mathbf {h} _{t-1}^{i+1}, x_{t}^i ; \\; \\mathbf W ^{i+1}) \\\\ \\end{align} \\] 残差连接可以在反向传播的时候大幅度提升梯度流，这样就可以训练很深的网络。 双向Encoder 一般输入的句子是从左到右，输出也是。但是由于语言的复杂性，有助于翻译的关键信息可能在原句子的不同地方。为了在Encoder中的每一个点都有最好的上下文语义，所以需要使用双向LSTM。 这里只在Encoder的最底层使用双向LSTM，其余各层均使用单向的LSTM。双向LSTM训练完成之后，再训练别的层。 \\(LSTM_f\\)从左到右处理句子，\\(LSTM_b\\)从右到左处理句子。把两个方向的信息\\(\\mathbf{x^f_i}\\)和\\(\\mathbf{x}^b_i\\)concat起来，传递给下一层。 模型并行性 模型很复杂，所以使用模型并行和数据并行，来加速。 数据并行 数据并行很简单，使用大规模分布式深度网络(Downpour SGD) 同时训练\\(n\\)个模型副本，它们都使用相同的模型参数，但是每个副本会使用Adam和SGD去异步地更新参数。每个模型副本一次处理m个句子。一般实验中，\\(n=10, m=128\\)。 模型并行 除了数据并行以外，模型并行也会加速每个副本的梯度计算。Encoder和Decoder会进行深度去划分，一般每一层会放在一个单独的GPU上。除了第一层的Encoder之外，所有的层都是单向的，所以第\\(i+1\\)层可以提前运行，不必等到第\\(i\\)层完全训练好了才进行训练。Softmax也会进行划分，每个处理一部分的单词。 并行带来的约束 由于要并行计算，所以我们不能够在Encoder的所有层上使用双向LSTM。因为如果使用了双向的，上面层必须等到下面层前向后向完全训练好之后才能开始训练，就不能并行计算。在Attention上，我们也只能使用最顶层的Encoder和最底层的Decoder进行对齐计算。如果使用顶层Encoder和顶层Decoder，那么整个Decoder将没有任何并行性，也就享受不到多个GPU的快乐了。 分割技巧 一般NMT都是的词汇表都是定长的，但是实际上词汇表却是开放的。比如人名、地名和日期等等。一般有两种方法去处理OOV(out-of-vocabulary)单词，复制策略和sub-word单元策略。GNMT是使用sub-word单元策略，也称为wordpiece模型。 复制策略 有下面几种复制策略 把稀有词汇直接复制到目标句子中，因为大部分都是人名和地名 使用注意力模型，添加特别的注意力 使用一个外部的对齐模型，去处理稀有词汇 使用一个复杂的带有特殊目的的指出网络，去指出稀有词汇 sub-word单元 比如字符，混合单词和字符，更加智能的sub-words。 Wordpiece 模型","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"神经机器翻译","slug":"神经机器翻译","permalink":"http://plmsmile.github.io/tags/神经机器翻译/"}]},{"title":"谷歌翻译论文笔记","date":"2017-10-17T05:25:38.000Z","path":"2017/10/17/25_谷歌翻译论文笔记/","text":"谷歌神经机器翻译系统 简介 神经机器翻译是自动翻译的端到端的学习方法，克服了传统的基于词典翻译的许多缺点。但仍然有以下的缺点 训练和翻译都太慢了，花费代价很大 缺乏鲁棒性，特别是输入句子包含生僻词汇 精确度和速度也不行 传统NMT缺点 神经机器翻译(NMT)是自动翻译的端到端的学习方法。NMT一般由两个RNN组成，分别处理输入句子和生成目标句子。一般会使用注意力机制，会有效地去处理长句子。 NMT避开了传统基于短语的翻译模型的很多缺点。但是，在实际中，NMT的准确度要比基于短语的翻译模型更差些。 NMT有3个主要的缺点：训练和推理速度太慢，不能有效处理稀有词汇，有时不能完全翻译原句子。 训练和推理速度太慢 训练大数据集，需要大量时间和资源；反馈太慢周期太长。加了一个小技巧，看结果要等很长时间。推理翻译的时候，要使用大量的参数去计算，也很慢。 不能有效处理稀有词汇 有两个方法去复制稀有单词： 模仿传统对齐模型去训练1个copy model 使用注意力机制去复制 但是效果都不是很好，都不可靠，不同语言的对齐效果差；在网络很深的时候，注意力机制的对齐向量也不稳定。而且，简单的复制过去也不是最好的办法，比如需要直译的时候。 不能完整翻译整个句子 不能覆盖整个输入句子的内容，然后会导致一些奇怪的翻译结果。 GNMT的模型优点 采用的模型：深层LSTM 、Encoder8层、Decoder8层 。我的LSTM笔记。 各层之间使用残差连接促进梯度流，顶层Enocder到底层Decoder使用注意力连接，提高并行性。 进行翻译推断的时候，使用低精度算法，去加速翻译。 处理稀有词汇：使用sub-word单元，也称作wordpieces方法。把单词划分到有限的sub-word (wordpieces)单元集合，输入输出都这样。sub-word结合了字符分割模型的弹性和单词分割模型的效率。 Beam Search 使用长度规范化和覆盖惩罚。覆盖惩罚就是说，希望，翻译的结果句子，尽量多地包含输入句子中的所有词汇。 使用强化学习去优化模型， 优化翻译的BLEU 分数。 先进技术 有很多先进的技术来提高NMT，下面这些都有论文的。 利用attention去处理稀有词汇 建立翻译覆盖的机制 多任务和半监督训练，去合并使用更多数据 字符分割的encoder和decoder 使用subword单元处理稀疏的输出 系统架构 系统总览 架构 有3个模块：Encoder，Decoder，Attention。 Encdoer：把句子转换成一系列的向量，每一个向量代表一个输入词汇（符号）。 Decoder：根据这些向量，每一时刻会生成一个目标词汇，直到EOS。 Attention：连接Encoder和Decoder，在解码的过程中，可以让Decoder有权重的有选择的关注输入句子的部分区域。 符号说明 加粗小写代表向量，如\\(\\boldsymbol {v, o_i}\\) 加粗大写，矩阵，如\\(\\boldsymbol {U, W}\\) 和\\(\\mathbf{U, W}\\) 手写体，集合，如\\(\\mathcal{V, F}\\) 大写字母，句子，如\\(X, Y\\) 小写字母，单个符号，如\\(x_1, x_2\\) Encoder 输入句子和目标句子组成一个Pair \\((X, Y)\\)，其中输入句子\\(X = x_1, x_2, \\cdots, x_M\\) ，\\(M\\) 个单词，翻译的输出目标句子\\(Y = y_1, y_2, \\cdots, y_N\\) ，有\\(N\\)个单词。 Encoder其实就是一个转换函数，得到\\(M\\)个长度固定的向量，也就是其中Encoder对各个\\(x_i\\)的编码向量 \\(\\mathbf{x_i}\\) ： \\[ \\mathbf {x_1, x_2, \\cdots, x_M} = \\mathit{EncoderRNN} (x_1, x_2, \\cdots, x_n) \\] 使用链式条件概率可得到翻译概率\\(\\color{blue} {P (Y \\mid X)}\\) ，其中\\(y_0\\)是起始符号\\(SOS\\) 。 \\[ \\begin{align} P(Y \\mid X) &amp; = P(Y \\mid \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ &amp; =\\prod_{i=1}^N P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\\\ \\end{align} \\] Decoder 在翻译\\(y_i\\)的时候， 利用Encoder得到的编码向量\\(\\mathbf{x_i}\\) 和 \\(y_0 \\sim y_{i-1}\\) 来进行计算概率翻译 \\[ P(y_i \\mid y_0, \\cdots, y_{i-1}; \\mathbf{x_1, x_2, \\cdots, x_M}) \\] Decoder是由RNN+Softmax构成的。会得到一个隐状态\\(\\mathbf{y_i}\\) 向量，有2个作用： 作为下一个RNN的输入 \\(\\mathbf{y_i}\\)经过softmax得到概率分布， 选出\\(y_i\\) 输出符号 Attention 在之前的文章里有介绍论文 和 通俗理解，其实就是影响力模型。原句子的各个单词对翻译当前单词分别有多少的影响力，也叫作对齐概率吧。使用decoder-RNN的输出\\(\\mathbf{y_{t-1}}\\) 向量作为时刻\\(t\\)的输入。 时刻\\(t\\)，给定\\(\\mathbf{y_{t-1}}\\) 有3个符号定义： \\(s_i\\) ： \\(y_t\\)与\\(x_i\\)的得分，在luong论文里面有3种计算方式，分别是dot, general和concat。 \\(p_i\\) ：\\(y_t\\)与\\(x_i\\)的对齐概率，\\((p_1, p_2, \\cdots, p_M)\\) 联合起来就是\\(y_t\\)与\\(X\\)的对齐向量。其实就是对得分softmax。 \\(\\mathbf{a_t}\\) ：带注意力的语义向量。对于所有的\\(x_i\\)，使用\\(y_t\\)与它的对齐概率\\(p_i\\)乘以本身的编码向量\\(\\mathbf{x_i}\\)，得到\\(x_i\\)传达的语义，再对所有的语义求和，即得到总体的带有注意力的语义。 整体详细计算的流程，如下面的公式： \\[ \\begin {align} &amp; s_i = \\mathit{AttentionFunction} (\\mathbf{y_{t-1}}, \\mathbf{x_i}), \\quad i \\in [1, M] \\\\ &amp; p_i = \\frac {\\exp (s_i)}{\\sum_{j=1}^M \\exp(s_j)} \\quad i \\in [1, M] \\\\ &amp; \\mathbf{a_t} = \\sum_{i=1}^M p_i \\cdot \\mathbf{x_i} \\quad \\color{blue}{对所有带注意力的x_i的语义求和得总体的语义} \\end{align} \\] 计算打分的函数即\\(\\mathit{AttentionFunction}\\)是一个有隐藏层的前馈网络！实现是Badh这个人的，不是Luong的。 系统架构图说明 架构图如下 Encoder是8层的LSTM：最底层是双向的LSTM，得到两个方向的信息；上面7层都是单向的。Encoder和Decoder的残差连接都是从第3层开始的。 训练时，会让Encoder最底层的双向的LSTM开始训练，完成之后，再训练别的层，每层都用单独的GPU。 为了提高并行性，Decoder最底层，只是为了用来计算Attention Context。带注意力的语义计算好之后，会单独发给其它的各个层。 经验说明 实验结果得到，要想NMT有好效果，Encoder和Decoder的网络层数一定要够深，才能发现2种语言之间的细微异常规则。和这个同理，深层LSTM比浅层LSTM明显效果好。每加一层，会大约减少10%的perplexity。所以使用deep stacked LSTM。 残差连接 残差网络讲解 。 虽然深层LSTM比浅层LSTM效果好，但是如果只是简单堆积的话，只在几个少数层效果才可以。经过试验，4层的话估计效果还可以，6层大部分都不好，8层的话，效果就相当差了。这是因为网络会变得很慢和很难训练，很大程度是因为梯度爆炸和梯度消失的问题。 根据在中间层和目标之间建立差别的思想，引入残差连接，如下图右边所示。其实就是把之前层的输入和当前的输出合并起来，作为下一层的输入。 一些参数和符号说明，一下均是时刻\\(t\\) \\(\\mathbf{x^i_t}\\) : 第\\(i+1\\)层 \\(\\mathit{LSTM}_{i+1}\\)的输入。 即上标代表LSTM的层数，下标代表时间。 \\(\\mathbf{W} ^i\\) : 第\\(i\\)层LSTM的参数 \\(\\mathbf {h} _t^i\\) : 第\\(i\\)层输出隐状态 \\(\\mathbf {c} ^i_t\\) : 第\\(i\\)层输出单元状态 那么\\(LSTM_i\\)和\\(LSTM_{i+1}\\)是这样交互的。即层层纵向传递输入，时间横向传递隐状态和单元状态。 \\[ \\begin{align} &amp; \\mathbf{c}_t^i, \\mathbf{h}_t^i = LSTM_i(\\mathbf{c}_{t-1}^i, \\mathbf{h}_{t-1}^i, \\mathbf x_{t}^{i-1} ; \\; \\mathbf W^i ) \\\\ &amp; \\mathbf x_t^i = \\mathbf h_t^i \\quad\\quad\\quad\\quad \\color{blue}{普通连接：i+1层输入=i层隐层输出} \\\\ &amp; \\mathbf{x}_t^i = \\mathbf h_t^i + \\mathbf{x}_t^{i-1} \\quad \\color{blue} {残差连接：第i+1层的输入=i层输入+i层隐层输出} \\\\ &amp; \\mathbf{c}_t^{i+1}, \\mathbf{h}_t^{i+1} = LSTM_{i+1} (\\mathbf c_{t-1}^{i+1}, \\mathbf {h} _{t-1}^{i+1}, x_{t}^i ; \\; \\mathbf W ^{i+1}) \\\\ \\end{align} \\] 残差连接可以在反向传播的时候大幅度提升梯度流，这样就可以训练很深的网络。 双向Encoder 一般输入的句子是从左到右，输出也是。但是由于语言的复杂性，有助于翻译的关键信息可能在原句子的不同地方。为了在Encoder中的每一个点都有最好的上下文语义，所以需要使用双向LSTM。 这里只在Encoder的最底层使用双向LSTM，其余各层均使用单向的LSTM。双向LSTM训练完成之后，再训练别的层。 \\(LSTM_f\\)从左到右处理句子，\\(LSTM_b\\)从右到左处理句子。把两个方向的信息\\(\\mathbf{x^f_i}\\)和\\(\\mathbf{x}^b_i\\)concat起来，传递给下一层。 模型并行性 模型很复杂，所以使用模型并行和数据并行，来加速。 数据并行 数据并行很简单，使用大规模分布式深度网络(Downpour SGD) 同时训练\\(n\\)个模型副本，它们都使用相同的模型参数，但是每个副本会使用Adam和SGD去异步地更新参数。每个模型副本一次处理m个句子。一般实验中，\\(n=10, m=128\\)。 模型并行 除了数据并行以外，模型并行也会加速每个副本的梯度计算。Encoder和Decoder会进行深度去划分，一般每一层会放在一个单独的GPU上。除了第一层的Encoder之外，所有的层都是单向的，所以第\\(i+1\\)层可以提前运行，不必等到第\\(i\\)层完全训练好了才进行训练。Softmax也会进行划分，每个处理一部分的单词。 并行带来的约束 由于要并行计算，所以我们不能够在Encoder的所有层上使用双向LSTM。因为如果使用了双向的，上面层必须等到下面层前向后向完全训练好之后才能开始训练，就不能并行计算。在Attention上，我们也只能使用最顶层的Encoder和最底层的Decoder进行对齐计算。如果使用顶层Encoder和顶层Decoder，那么整个Decoder将没有任何并行性，也就享受不到多个GPU的快乐了。 分割技巧 一般NMT都是的词汇表都是定长的，但是实际上词汇表却是开放的。比如人名、地名和日期等等。一般有两种方法去处理OOV(out-of-vocabulary)单词，复制策略和sub-word单元策略。GNMT是使用sub-word单元策略，也称为wordpiece模型。 复制策略 有下面几种复制策略 把稀有词汇直接复制到目标句子中，因为大部分都是人名和地名 使用注意力模型，添加特别的注意力 使用一个外部的对齐模型，去处理稀有词汇 使用一个复杂的带有特殊目的的指出网络，去指出稀有词汇 sub-word单元 比如字符，混合单词和字符，更加智能的sub-words。 Wordpiece 模型","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"神经机器翻译","slug":"神经机器翻译","permalink":"http://plmsmile.github.io/tags/神经机器翻译/"}]},{"title":"那些年折磨过的问题","date":"2017-10-16T14:47:46.000Z","path":"2017/10/16/tips/","text":"搭建博客 搭建博客 12345678910111213141516171819202122232425mkdir PLMBlogscd PLMBlogs# install hexonpm install hexo-cli -g# inithexo init npm installhexo server# install pluginsnpm install hexo-deployer-git --savenpm install hexo-renderer-scss --save# 在默认_config.yml中添加需要的插件plugins: hexo-generator-feed #RSS订阅插件 hexo-generator-sitemap #sitemap插件git clone https://github.com/ahonn/hexo-theme-even themes/even# 替换配置文件 or 一步一步地去配置# 生成，再替换文件hexo new page tagshexo new page categories indigo主题 123456789101112131415161718192021222324hexo init# 配置.yml文件，复制旧的过来即可npm install hexo-deployer-git --savegit clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigogit checkout -b card origin/cardnpm install hexo-renderer-less --savenpm install hexo-generator-feed --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --savehexo new page tagshexo new page categorieshexo new page about# 再去配置各个目录下的index文件，也可以直接copy# 配置主题中的yml，直接copy# 修改图片等# 后续 修改宽度# source/css/_partial/variable.css 中第28行，修改为80%的宽度contentWidth: 80%# 主题配置文件中 cdn改为falsecdn: false 搭建indigo博客 安装博客 1234567891011121314hexo initnpm install hexo-deployer-git --save''' 安装 '''git clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigonpm install hexo-renderer-less --savenpm install hexo-generator-feed --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --save''' 配置标签和类别页面，去配置index.md中的数据 '''hexo new page tagshexo new page categories# 主要是配置 layout: tags layout: categories comment: false# 新建关于我的页面，并填上相应的信息 layout: abouthexo new page about 配置hexo/_config.yml中的主题是indigo 12# 配置indigo主题theme: indigo 配置数学公式 1234567891011# 在主题_config.yml 中配置 mathjax: true# 要先卸载已有的渲染器npm uninstall hexo-renderer-marked --save# 潜在的npm uninstall hexo-renderer-kramed --savenpm uninstall hexo-math --save# 只需要安装pandoc就可以了# 先在本地下载pandoc，安装好，再执行如下命令npm install hexo-renderer-pandoc --save 做到这里，要先启动，放两篇带数学公式的文档，去测试一下。 博客配置 编辑hexo/_config.yml ，添加如下项目 1234567891011121314151617181920212223242526272829303132333435363738# Sitetitle: PLM's Blogsubtitle: 好好学习，天天向上description: 菜鸟程序员author: 蒲黎明language: zh-CN# url url: https://plmsmile.github.io/# Deploymentdeploy: type: git repo: git@github.com:plmsmile/plmsmile.github.io.git branch: master# indigo的配置项feed: type: atom path: atom.xml limit: 0jsonContent: meta: false pages: false posts: title: true date: true path: true text: true raw: false content: false slug: false updated: false comments: false link: false permalink: false excerpt: false categories: false tags: true indigo主题配置 参考官方配置说明 编辑themes/indigo/_config.yml 配置左侧菜单 12345678910111213141516171819menu: home: text: 主页 url: / archives: text: 归档 url: /archives tags: text: 标签 url: /tags th-list: text: 类别 url: /categories user: url: /about text: 关于我 github: url: https://github.com/plmsmile target: _blank 配置自我介绍 1about: 自然语言处理，机器学习，深度学习，Spark，Leetcode，Java，C++，数据结构。都不会呢，赶紧快学吧！ 设置图片，需要配置站点图片、头像图片，也可以换头像背景图片。 1234567891011# 你的头像urlavatar: /img/avatar.jpg# avatar linkavatar_link: https://plmsmile.github.io/about# 头像背景图brand: /img/brand.jpg# faviconfavicon: /img/favicon.png# emailemail: plmsmile@126.com 配置页面宽度 修改source/css/_partial/variable.css 中的28行，设置为80%。设置主题配置文件中，cdn: false。 、 配置支付宝和微信图片，默认就有打赏功能。可以关掉。 12345''' 是否开启打赏，关闭 reward: false'''reward: title: 谢谢大爷~ wechat: /img/wechat.png '微信，关闭设为 false alipay: /img/alipay.png '支付宝，关闭设为 false 每张文章最后的备注 1postMessage: &lt;br&gt;原始链接：&lt;a href=\"&lt;%- url_for(page.path).replace(/index\\.html$/, '') %&gt;\" target=\"_blank\" rel=\"external\"&gt;&lt;%- page.permalink.replace(/index\\.html$/, '') %&gt;&lt;/a&gt; 关闭动态title 1234# 动态定义title# title_change:# normal: (つェ⊂)咦!又好了!# leave: 死鬼去哪里了！ 配置几个页面的标题 1234# 页面标题tags_title: 标签archives_title: 归档categories_title: 类别 总结 12345678910111213141516hexo init# 1. 运行下面的脚本npm install hexo-deployer-git --savegit clone git@github.com:yscoder/hexo-theme-indigo.git themes/indigonpm install hexo-renderer-less --savenpm install hexo-generator-feed --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --savehexo new page tagshexo new page categorieshexo new page about# 2. 替换source/中3个文件夹，about, categories, tags# 3. 替换hexo和主题中的配置文件 _config.yml# 4. 替换\\indigo\\source\\img 文件夹# 5. 修改source/css/_partial/variable.css的28行 宽度为80%# 6. 把博客文件复制过来，运行查看。 PyPlot使用中文 参考文档 12345678910111213# 下载字体放到下面的目录# 下载simhei.tff/home/plm/app/anaconda2/lib/python2.7/site-packages/matplotlib/mpl-data/fonts/ttf# 编辑文件/home/plm/app/anaconda2/lib/python2.7/site-packages/matplotlib/mpl-data/matplotlibrc# 打开下面的注释# font.family : sans-serif# 打开注释，加上SimHei# font.sans-serif : SimHei,Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif# 删除缓存rm -rf ~/.cache/matplotlib 简单测试例子 123456789101112131415import matplotlibmatplotlib.matplotlib_fname()import matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['simhei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号matplotlib.rcParams['axes.unicode_minus'] = False #-号为方块问题plt.plot((1,2,3),(4,3,-1))s = \"横坐标\"plt.xlabel(unicode(s))plt.ylabel(u'纵坐标')plt.show()print (s) 有时候依然不好使，那么就 123import sys reload(sys) sys.setdefaultencoding('utf8') 应该就可以了。","tags":[{"name":"心得","slug":"心得","permalink":"http://plmsmile.github.io/tags/心得/"},{"name":"hexo","slug":"hexo","permalink":"http://plmsmile.github.io/tags/hexo/"},{"name":"latex","slug":"latex","permalink":"http://plmsmile.github.io/tags/latex/"},{"name":"中文","slug":"中文","permalink":"http://plmsmile.github.io/tags/中文/"},{"name":"pyplot","slug":"pyplot","permalink":"http://plmsmile.github.io/tags/pyplot/"}]},{"title":"Attention-based NMT 阅读笔记","date":"2017-10-12T08:12:39.000Z","path":"2017/10/12/Attention-based-NMT/","text":"Effective Approaches to Attention-based Neural Machine Translation 简介 Attention介绍 在翻译的时候，选择性的选择一些重要信息。详情看这篇文章 。 本着简单和有效的原则，本论文提出了两种注意力机制。 Global 每次翻译时，都选择关注所有的单词。和Bahdanau的方式 有点相似，但是更简单些。简单原理介绍。 Local 每次翻译时，只选择关注一部分的单词。介于soft和hard注意力之间。(soft和hard见别的论文)。 优点有下面几个 比Global和Soft更好计算 局部注意力 随处可见、可微，更好实现和训练。 应用范围 在训练神经网络的时候，注意力机制应用十分广泛。让模型在不同的形式之间，学习对齐等等。有下面一些领域： 机器翻译 语音识别 图片描述 between image objects and agent actions in the dynamic control problem (不懂，以后再说吧) 神经机器翻译 思想 输入句子\\(x = (x_1, x_2, \\cdots, x_n)\\) ，输出目标句子\\(y = (y_1, y_2, \\cdots, y_m)\\) 。 神经机器翻译(Neural machine translation, NMT)，利用神经网络，直接对\\(\\color{blue} {p(y \\mid x)}\\) 进行建模。一般由Encoder和Decoder构成。Encoder-Decoder介绍文章链接 。 Encoder把输入句子\\(x\\) 编码成一个语义向量\\(s\\) (c表示也可以)，然后由Decoder 一个一个产生目标单词 \\(y_i\\) \\[ \\log p(y \\mid x) = \\sum_{j=1}^m \\log \\color{red} {p(y_j \\mid y _{&lt;j}, s) } = \\sum_{j=1}^m \\log p(y_j \\mid y_1, \\cdots, y_{j-1}, s) \\] 但是怎么选择Encoder和Decoder（RNN, CNN, GRU, LSTM），怎么去生成语义\\(s\\)却有很多选择。 概率计算 结合Dncoder上一时刻的隐状态\\(\\color{blue}{h_{j-1}}\\)和encoder给的语义向量\\(\\color{blue}{s}\\)，通过函数\\(\\color{blue}{f}\\) ，就可以计算出当前的隐状态\\(\\color{blue}{h_j}\\) ： \\[ h_j = f(h_{j-1}, s) \\] 通过函数\\(\\color{blue}{g}\\)对当前隐状态\\(h_j\\)进行转换，再softmax，就可以得到翻译的目标单词\\(y_i\\)了（选概率最大的那个）。 \\(g\\)一般是线性变换，维数变化是\\([1, h] \\to [1, vocab\\_size]\\)。 \\[ p(y_j \\mid y _{&lt;j}, s) = \\mathrm{softmax} \\; g(h_j) \\] 语义向量\\(s​\\) 会贯穿整个翻译的过程，每一步翻译都会使用到语义向量的内容，这就是注意力机制。 本论文的模型 本论文采用stack LSTM的构建NMT系统。如下所示： 训练目标是 \\[ J_t = \\sum_{(x, y)} - \\log p(y \\mid x) \\] 注意力模型 注意力模型广义上分为global和local。Global的attention来自于整个序列，而local的只来自于序列的一部分。 解码总体流程 Decoder时，在时刻\\(t\\)，要翻译出单词\\(y_t\\) ，如下步骤： 最顶层LSTM的隐状态 \\(h_t\\) 计算带有原句子信息语义向量\\(c_t\\)。Global和Local的区别在于\\(c_t\\)的计算方式不同 串联\\(h_t, c_t\\)，计算得到带有注意力的隐状态 \\(\\hat {h}_t = \\tanh (W_c [c_t; h_t])\\) 通过注意力隐状态得到预测概率 \\(p(y_t \\mid y_{&lt;t}, x) = \\rm {softmax} (W_s \\hat h _t)\\) Global Attention 总体思路 在计算\\(c_t\\) 的时候，会考虑整个encoder的隐状态。Decoder当前隐状态\\(h_t\\)， Encoder时刻s的隐状态\\(\\bar h _s\\)。 对齐向量\\(\\color{blue}{\\alpha_t}\\)代表时刻\\(t\\) 输入序列中的单词对当前单词\\(y_t\\) 的对齐概率，长度是\\(T_x\\)， 随着输入句子的长度而改变 。\\(x_s\\)与\\(y_t\\) 的对齐概率如下： \\[ \\alpha_t(s) = \\mathrm {align} (h_t, \\bar h_s) = \\frac {score(h_t, \\bar h_s)}{ \\sum_{i=1}^{T_x} score(h_t, \\bar h_i)}, \\quad 实际上\\mathrm{softmax} \\] 结合上面的解码总体流程，有下面的流程 \\[ all (\\bar h_s) , h_t \\to \\alpha_t \\to c_t . \\quad c_t , h_t \\to \\hat h_t .\\quad \\hat h_t \\to y_t \\quad \\] 简单来说是\\(h_t \\to \\alpha_t \\to c_t \\to \\hat h_t \\to y_t\\) 。 score计算 \\(score(h_t, \\bar h_s)\\) 是一种基于内容content-based的函数，有3种实现方式 \\[ \\color{blue}{score(h_t, \\bar h_s)} = \\begin{cases} h_t^T \\bar h_s &amp; dot \\\\ h_t^T W_a \\bar h_s &amp; general \\\\ v_a^T \\tanh (W_a [h_t; \\bar h_s]) &amp; concat \\\\ \\end{cases} \\] 缺点 生成每个目标单词的时候，都必须注意所有的原单词， 这样计算量很大，翻译长序列可能很难，比如段落或者文章。 Local Attention 在生成目标单词的时候，Local会选择性地关注一小部分原单词去计算\\(\\alpha_t, c_t\\)，这样就解决了Global的问题。如下图 Soft和Hard注意 Soft 注意 ：类似global注意，权值会放在图片的所有patches中。计算复杂。 Hard 注意： 不同时刻，会选择不同的patch。虽然计算少，但是non-differentiable，并且需要复杂的技术去训练模型，比如方差减少和强化学习。 Local注意 类似于滑动窗口，计算一个对齐位置\\(\\color{blue}{p_t}\\)，根据经验设置窗口大小\\(D\\)，那么需要注意的源单词序列是 ： \\[ [p_t -D, p_t + D] \\] \\(\\alpha_t\\) 的长度就是\\(2D\\)，只需要选择这\\(2D\\)个单词进行注意力计算，而不是Global的整个序列。 对齐位置选择 对齐位置的选择就很重要，主要有两种办法。 local-m (monotonic) 设置位置， 即以当前单词位置作为对齐位置 \\[ p_t = t \\] local-p (predictive) 预测位置 \\(S\\) 是输入句子的长度，预测对齐位置如下 \\[ p_t = S \\cdot \\mathrm{sigmoid} \\left(v_p^T \\tanh (W_p h_t) \\right), \\quad p_t \\in [0, S] \\] 对齐向量计算 \\(\\alpha_t\\)的长度就是\\(2D\\)，对于每一个\\(s \\in [p_t -D, p_t + D]\\)， 为了更好地对齐，添加一个正态分布\\(N(\\mu, \\sigma ^2)\\)，其中 \\(\\mu = p_t, \\sigma = \\frac{D}{2}\\)。 计算对齐概率： \\[ \\alpha_t(s) = \\mathrm{align} (h_t, \\bar h_s) \\exp \\left( - \\frac{(s - \\mu)^2}{2\\sigma^2}\\right) = \\mathrm{align} (h_t, \\bar h_s) \\exp \\left( - \\frac{2(s - p_t)^2}{D^2}\\right) \\] Input-feeding 前面的Global和Local两种方式中，在每一步的时候，计算每一个attention (实际上是指 \\(\\hat h_t\\))，都是独立的，这样只是次最优的。 在每一步的计算中，这些attention应该有所关联，当前知道之前的attention才对。实际是应该有个coverage set去追踪之前的信息。 我们会把当前的注意\\(\\hat h_t\\) 作为下一次的输入，并且做一个串联，来计算新的attention，如下图所示 这样有两重意义： 模型会知道之前的对齐选择 会建立一个水平和垂直都很深的网络 PyTorch实现Global 计算对齐向量 初始化参数 score_type: 计算score的方法 hidden_size: Encoder和Decoder的hidden_size 12345678910111213141516def __init__(self, score_type, hidden_size): ''' Args: score_type: 计算score的方法，'dot', 'general', 'concat' hidden_size: Encoder和Decoder的hidden_size ''' super(Attn, self).__init__() self.score_type = score_type self.hidden_size = hidden_size self.max_length = max_length if score_type == 'general': self.attn = nn.Linear(hidden_size, hidden_size) elif score_type == 'concat': self.attn = nn.Linear(hidden_size * 2, hidden_size) self.other = nn.Parameter(torch.FloatTensor(1, hidden_size)) 计算score 123456789101112131415161718def score(self, hidden, encoder_output): ''' 计算Decoder中LSTM的ht与Encoder中的hs的得分，便于后面算对齐概率 Args: hidden: Decoder中最顶层LSTM的隐状态，h_de_t，[1, h_size] encoder_output: Encoder某时刻的隐状态，h_en_s，[1, h_size] Returns: energy: d_ht与e_hs的得分，即Yt与Xs的得分 ''' if self.score_type == 'dot': energy = hidden.dot(encoder_output) elif self.score_type == 'general': energy = self.attn(encoder_output) energy = hidden.dot(energy) elif self.score_type == 'concat': h_o = torch.cat((hidden, encoder_output), 1) energy = self.attn(h_o) energy = self.other.dot(energy) return energy 前向计算 实际就是对所有的打分结果进行softmax 得到对齐向量，也成为注意力权值，attn_weights 12345678910111213141516def forward(self, hidden, encoder_outputs): ''' 时刻t，计算对齐向量 Args: hidden: Top LSTM的隐状态 encoder_outputs: Encoder的所有隐状态, len=Tx输入句子长度 Returns: align_vec: 当前ht与所有encoder_outputs的对齐向量，alpha_t，len=Tx，返回[1, 1, seq_len]格式 ''' seq_len = len(encoder_outputs) attn_energies = get_variable(torch.zeros(seq_len)) for i in range(seq_len): attn_energies[i] = self.score(hidden, encoder_outputs[i]) # normalize [0, 1], resize to [1, 1, seq_len] align_vec = F.softmax(attn_energies) align_vec = align_vec.unsqueeze(0).unsqueeze(0) return align_vec Decoder网络 参数 hidden_size: Embedding和GRU的hidden_size output_size: 输出的size，是目标语言的词典大小 score_type: &#39;dot&#39;, &#39;general&#39;, &#39;concat&#39; n_layers: GRU的层数 dropout_p: dropout 1234''' initArgs: '''","tags":[{"name":"论文笔记","slug":"论文笔记","permalink":"http://plmsmile.github.io/tags/论文笔记/"},{"name":"Attention","slug":"Attention","permalink":"http://plmsmile.github.io/tags/Attention/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"}]},{"title":"自然语言处理中的Attention Model","date":"2017-10-10T03:40:01.000Z","path":"2017/10/10/attention-model/","text":"Encoder-Decoder 基本介绍 举个翻译的例子，原始句子\\(X = (x_1, x_2, \\cdots, x_m)\\) ，翻译成目标句子\\(Y = (y_1, y_2, \\cdots, y_n)\\) 。 现在采用Encoder-Decoder架构模型，如下图 Encoder会利用整个原始句子生成一个语义向量，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句子的意思、句法结构、性别信息等等。具体框架可以参考Encoder-Decoder框架。 Encoder对\\(X\\) 进行非线性变换得到中间语义向量c ： \\[ c = G(x_1, x_2, \\cdots, x_n) \\] Decoder根据语义\\(c\\) 和生成的历史单词\\((y_1, y_2, \\cdots, y_{i-1})\\) 来生成第\\(i\\) 个单词 \\(y_i\\)： \\[ y_i = f(c, y_1, y_2, \\cdots, y_{i-1}) \\] Encoder-Decoder是个创新大杀器，是个通用的计算框架。Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN，RNN，BiRNN，GRU，LSTM， Deep LSTM。上面的内容任意组合，只要得到的效果好，就是一个创新，就可以毕业了。（当然别人没有提出过） 缺点 在生成目标句子\\(Y\\)的单词时，所有的单词\\(y_i\\)使用的语义编码\\(c\\) 都是一样的。而语义编码\\(c\\)是由句子\\(X\\) 的每个单词经过Encoder编码产生，也就是说每个\\(x_i\\)对所有\\(y_j\\)的影响力都是相同的，没有任何区别的。所以上面的是注意力不集中的分心模型。 句子较短时问题不大，但是较长时，所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，会丢失更多的细节信息。 例子 比如输入\\(X\\)是Tom chase Jerry，模型翻译出 汤姆 追逐 杰瑞。在翻译“杰瑞”的时候，“Jerry”对“杰瑞”的贡献更重要。但是显然普通的Encoder-Decoder模型中，三个单词对于翻译“Jerry-杰瑞”的贡献是一样的。 解决方案应该是，每个单词对于翻译“杰瑞”的贡献应该不一样，如翻译“杰瑞”时： \\[ (Tom, 0.3), \\; (Chase, 0.2), \\; (Jerry, 0.5) \\] Attention Model 基本架构 Attention Model的架构如下： 如图所示，生成每个单词\\(y_i\\)时，都有各自的语义向量\\(C_i\\)，不再是统一的\\(C\\) 。 \\[ y_i = f(C_i, y_1, \\cdots, y_{i-1}) \\] 例如，前3个单词的生成： \\[ \\begin{align} &amp; y_1 = f(C_1) \\\\ &amp; y_2 = f(C_2, y_1) \\\\ &amp; y_3 = f(C_3, y_1, y_2) \\\\ \\end{align} \\] 语义向量的计算 注意力分配概率 \\(a_{ij}\\) 表示 \\(y_i\\)收到\\(x_j\\) 的注意力概率。 例如\\(X=(Tom, Chase, Jerry)\\)，\\(Y = (汤姆, 追逐, 杰瑞)\\) 。\\(a_{12}=0.2\\)表示汤姆 收到来自Chase的注意力概率是0.2。 有下面的注意力分配矩阵： \\[ A = [a_{ij}] = \\begin {bmatrix} 0.6 &amp; 0.2 &amp; 0.2 \\\\ 0.2 &amp; 0.7 &amp; 0.1 \\\\ 0.3 &amp; 0.1 &amp; 0.5 \\\\ \\end {bmatrix} \\] 第\\(i\\)行表示\\(y_i\\) 收到的所有来自输入单词的注意力分配概率。\\(y_i\\) 的语义向量\\(C_i\\) 由这些注意力分配概率和Encoder对单词\\(x_j\\)的转换函数相乘，计算而成，例如： \\[ \\begin {align} &amp; C_1 = C_{汤姆} = g(0.6 \\cdot h(Tom),\\; 0.2 \\cdot h(Chase),\\; 0.2 \\cdot h(Jerry)) \\\\ &amp; C_2 = C_{追逐} = g(0.2 \\cdot h(Tom) ,\\;0.7 \\cdot h(Chase) ,\\;0.1 \\cdot h(Jerry)) \\\\ &amp; C_3 = C_{汤姆} = g(0.3 \\cdot h(Tom),\\; 0.2 \\cdot h(Chase) ,\\;0.5 \\cdot h(Jerry)) \\\\ \\end {align} \\] \\(\\color{blue}{h(x_j)}\\) 就表示Encoder对输入英文单词的某种变换函数。比如Encoder使用RNN的话，\\(h(x_j)\\)往往都是某个时刻输入\\(x_j\\) 后隐层节点的状态值。 g函数 表示注意力分配后的整个句子的语义转换信息，一般都是加权求和，则有语义向量计算公式： \\[ C_i = \\sum_{j=1}^{T_x} a_{ij} \\cdot h_j, \\quad h_j = h(x_j) \\] 其中\\(\\color{blue}{T_x}\\) 代表输入句子的长度。形象来看计算过程如下图： 注意力分配概率计算 语义向量需要注意力分配概率和Encoder输入单词变换函数来共同计算得到。 但是比如汤姆收到的分配概率\\(a_1 = (0.6, 0.2, 0.2)\\)是怎么计算得到的呢？ 这里采用RNN作为Encoder和Decoder来说明。 注意力分配概率如下图计算 对于\\(a_{ij}\\) 其实是通过一个对齐函数F来进行计算的，两个参数：输入节点\\(j\\)，和输出节点\\(i\\)，当然一般是取隐层状态。 \\[ a_i = F(i, j), \\quad j \\in [1, T_x], \\quad h(j)\\,Encoder, \\; H(i)\\,Decoder \\] \\(\\color{blue}{F(i, j)}\\)代表\\(y_i\\)和\\(x_j\\)的对齐可能性。一般F输出后，再经过softmax就得到了注意力分配概率。 AM模型的意义 一般地，会把AM模型看成单词对齐模型，输入句子单词和这个目标生句子成单词的对齐概率。 其实，理解为影响力模型也是合理的。就是在生成目标单词的时候，输入句子中的每个单词，对于生成当前目标单词有多大的影响程度。 AM模型有很多的应用，思想大都如此。 文本摘要例子 比如文本摘要的例子，输入一个长句，提取出重要的信息。 输入&quot;russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism&quot;。 输出&quot;russia calls for joint front against terrorism&quot;。 下图代表着输入单词对输出单词的影响力，颜色越深，影响力越大，注意力分配概率也越大。 PyTorch翻译AM实现 思想 参考这篇论文 。 生成目标单词\\(y_i\\) 的计算概率是 \\[ p(y_i \\mid (y_1,\\cdots, y_{i-1}), x) = g(y_{i-1}, s_i, c_i) \\] 符号意义说明 \\(y_i\\) 当前应该生成的目标单词，\\(y_{i-1}\\) 上一个节点的输出单词 \\(s_i\\) 当前节点的隐藏状态 \\(c_i\\) 生成当前单词应该有的语义向量 \\(g\\) 全连接层的函数 隐层状态\\(s_i\\) 求当前Decoder隐层状态\\(s_i\\)：由上一层的隐状态\\(s_{i-1}\\)，输出单词\\(y_{i-1}\\) ，语义向量\\(c_i\\) \\[ s_i = f(s_{i-1}, y_{i-1}, c_i) \\] 语义向量\\(c_i\\) 语义向量：分配权值\\(a_{ij}\\)，Encoder的输出 \\[ c_i = \\sum_{j=1}^{T_x} a_{ij} \\cdot h_j, \\quad h_j = h(x_j) \\] 分配概率\\(a_{ij}\\) 注意力分配概率\\(a_{ij} ，\\) \\(y_i\\) 收到\\(x_j\\) 的注意力：分配能量\\(e_{ij}\\) \\[ a_{ij} = \\frac{\\exp(e_{ij})} {\\sum_{k=1}^{T_x} \\exp (e_{ik})} \\] 分配能量\\(e_{ij}\\) \\(x_j\\) 注意\\(y_i\\) 的能量，由encoder的隐状态\\(h_j\\) 和 decoder的上一层的隐状态\\(s_{i-1}\\) 计算而成。a函数就是一个线性层。也就是上面的F函数。 \\[ e_{ij} = a(s_{i-1}, h_j) \\] 实现 Decoder由4层组成 embedding : word2vec attention layer: 为每个encoder的output计算Attention RNN layer: output layer: Decoder输入 \\(s_{i-1}\\) , \\(y_{i-1}\\) 和encoder的所有outputs \\(h_*\\) Embedding Layer 输入\\(y_{i-1}\\)，对其进行编码 12# y(i-1)embedded = embedding(last_rnn_output) Attention Layer 输入\\(s_{i-1}, h_j\\)，输出分配能量\\(e_{ij}\\)， 计算出\\(a_{ij}\\) 12attn_weights[j] = attn_layer(last_hidden, encoder_outputs[j])attn_weights = normalize(attn_weights) 计算语义向量 求语义向量\\(c_i\\)， 一般是加权求和 1context = sum(attn_weights * encoder_outputs) RNN Layer 输入\\(s_{i-1}, y_{i-1}, c_i\\) ，内部隐层状态，输出\\(s_i\\) 12rnn_input = concat(embeded, context)rnn_output, rnn_hidden = rnn(rnn_input, last_hidden) 输出层 输入\\(y_{i-1}, s_i, c_i\\) ，输出\\(y_i\\) 1output = out(embedded, rnn_output, context)","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"Attention Model","slug":"Attention-Model","permalink":"http://plmsmile.github.io/tags/Attention-Model/"},{"name":"Encoder-Decoder","slug":"Encoder-Decoder","permalink":"http://plmsmile.github.io/tags/Encoder-Decoder/"}]},{"title":"PyTorch快速上手","date":"2017-10-05T05:30:54.000Z","path":"2017/10/05/pytorch-start/","text":"PyTorch介绍 PyTorch Torch 是一个使用Lua 语言的神经网络库，而PyTorch是Torch在Python的衍生。 PyTorch是一个基于python的科学计算包。本质上是Numpy的代替者，支持GPU、带有高级功能，可以用来搭建和训练深度神经网络；是一个深度学习框架，速度更快，弹性更好。 PyTorch和Tensorflow Tensorflow 类似一个嵌入Python的编程语言。写的Tensorflow代码会被Python编译成一张计算图，然后由TensorFlow执行引擎运行。Tensorflow有一些额外的概念需要学习，上手时间慢。 对比参考这篇文章PyTorch还是Tensorflow 。下面是结论。后续再补充详细内容。 PyTorch更有利于研究人员、爱好者、小规模项目等快速搞出原型，易于理解。而TensorFlow更适合大规模部署，特别是需要跨平台和嵌入式部署时。 PyTorch和Tensorflow对比如下 PyTORCH Tensorflow 动静态 建立的神经网络是动态的 建立静态计算图 代码难度 易于理解，好看一些。有弹性 底层代码难以看懂 工业化 好上手 高度工业化 数据操作 Tensor 创建Tensor Tensor实际上是一个数据矩阵，PyTorch处理的单位就是一个一个的Tensor。下面是一些创建方法 12345678910111213import torch# 1. 未初始化，都是0x = torch.Tensor(5, 3)# 2. 随机初始化x = torch.rand(5, 3)# 3. 传递参数初始化x = torch.Tensor([1, 2])# 4. 通过Numpy初始化a = np.ones(5)b = torch.from_numpy(a)# 5. 获取size，返回一个tuple [5, 3]print x.size() Tensor也可以通过Numpy来进行创建，或者从Tensor得到一个Numpy。 1234import numpy as npa = np.ones(5)b = torch.from_numpy(a)c = b.numpy() Tensor操作 Tensor的运算也很简单，一般的四则运算都是支持的。 123456789101112x = torch.rand(5, 3)y = torch.rand(5, 3)# 1. 直接相加z = x + y# 2. torch相加z = torch.add(x, y)# 3. 传递参数返回结果result = torch.rand(5, 3)torch.add(x, y, out = result)# 4. 加到自身去，自身y会改变y.add_(x) 其中所有类似于x.add_(y)的操作都会改变自己x，如x.copy_(y) 、x.t_() 。 对于Tensor可以像Numpy那样索引和切片。 改变Tensor和Numpy 改变Tensor后，对应的Numpy也会发生改变 1234567a = torch.ones(5)b = a.numpy()# 改变aa.add_(1)# b也会改变print a # 22222print b # 22222 CUDA Tensors 使用GPU很简单，只需使用.cuda就可以了 1234if torch.cuda.is_available(): x = x.cuda() y = y.cuda() x + y Variable 在神经网络中，最重要的是torch.autograd这个包，而其中最重要的一个类就是Variable。 本质上Variable和Tensor没有区别，不过Variable会放入一个计算图，然后进行前向传播，反向传播和自动求导。这也是PyTorch和Numpy不同的地方。 Variable由data, grad, creator 三部分组成。 data: 包装的Tensor，即数据 grad: 方向传播的梯度缓冲区 creator: 得到这个Variable的操作，如乘法加法等等。 用一个Variable进行计算，返回的也是一个同类型的Variable。 梯度计算例子 线性计算\\(z= 2 \\cdot x + 3 \\cdot y + 4\\) ，求\\(\\frac{ \\partial z}{\\partial x}\\) 和\\(\\frac{ \\partial z}{\\partial y}\\) 。 123456789101112import torchfrom torch.autograd import Variable# 1. 准备式子# 默认求导是falsex = Variable(torch.Tensor([2]), requires_grad = True)y = Variable(torch.Tensor([3]), requires_grad = True)z = 2 * x + 3 * y + 4# 2. z对x和y进行求导z.backward()# 3. 获得z对x和y的导数print x.grad.data # 2print y.grad.data # 3 复杂计算$ y = x + 2$， \\(z = y * y * 3\\)， \\(o = avg(z)\\) ，求\\(\\frac{dz}{dx}\\) 1234567x = Variable(torch.ones(2, 2), requires_grad = True)y = x + 2z = y * y * 3out = z.mean()out.backward()# d(out)/dxprint x.grad 传递梯度 12345x = Variable(torch.Tensor([2]), requires_grad = True)y = x + 2gradients = torch.FloatTensor([0.01, 0.1, 1])y.backward(gradients)print x.grad.data 一些常用的API总结 12345678910111213x = torch.Tensor([2, 2])# 随机创建数字x = torch.randn(3)x = torch.randn(3, 3)# 求平均值x.mean()# 范数x.norm()# torch view。数据相同，改变形状。得到一个Tensorx = torch.randn(4, 4)y = x.view(16)z = x.view(2, 2, 4)z = x.view(2, 2, -1) # 最后-1，会自己适配 神经网络","tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"神经网络","slug":"神经网络","permalink":"http://plmsmile.github.io/tags/神经网络/"},{"name":"PyTorch","slug":"PyTorch","permalink":"http://plmsmile.github.io/tags/PyTorch/"}]},{"title":"神经网络机器翻译","date":"2017-10-02T02:03:31.000Z","path":"2017/10/02/NMT/","text":"在机器翻译、语音识别、文本摘要等领域中，Sequence-to-sequence 模型都取得了了非常好的效果。神经机器翻译(Neural Machine Translation, NMT) 使用seq2seq模型取得了巨大的成功。 本文参考谷歌NMT教程。 Basic 背景知识 传统翻译是以词为核心一词一词翻译的，这样会切断句子本身的意思，翻译出来也很死板，不像我们人类说的话。 Encoder-Decoder 现在采用Encoder-Decoder架构模型。如下图 Encoder会利用整个原始句子生成一个语义向量，Decoder再利用这个向量翻译成其它语言的句子。这样可以把握整个句子的意思、句法结构、性别信息等等。具体框架可以参考Encoder-Decoder框架。 举个翻译的例子，原始句子\\(X = (x_1, x_2, \\cdots, x_m)\\) ，翻译成目标句子\\(Y = (y_1, y_2, \\cdots, y_m)\\) 。 Encoder对\\(X\\) 进行非线性变换得到中间语义\\(C\\) \\[ C = \\Gamma(x_1, x_2, \\cdots, x_n) \\] Decoder根据语义\\(C\\) 和生成的历史信息\\(y_1, y_2, \\cdots, y_{i-1}\\) 来生成第\\(i\\) 个单词 \\(y_i\\) \\[ y_i = \\Psi(C, y_1, y_2, \\cdots, y_{i-1}) \\] 当然，在Attention Model 中，Decoder生成Y的时候每个单词对应的\\(C\\)不一样，记作\\(C_j, j \\in [1, n]\\) 。\\(C_j\\) 就是体现了源语句子中不同的单词对目标句子中不同的单词的注意力概率分布。即各个单词的对齐的概率，也就是student对&quot;学生&quot;更重要，而对&quot;我&quot;不那么重要。这个在后续会用到。 Encoder-Decoder是个创新大杀器，是个通用的计算框架。Encoder和Decoder具体使用什么模型，都可以自己选择。通常有CNN,RNN,BiRNN,GRU,LSTM, Deep LSTM。比如编码CNN-解码RNN, 编码BiRNN-解码Deep LSTM等等。 上面的内容任意组合，只要得到的效果好，就是一个创新，就可以毕业了。（当然别人没有提出过） NMT模型选择 有3个维度需要选择。 方向性。是单向还是双向 深度：是一层还是多层 网络选择：encoder和decoder具体分别选什么 在本文的实现中，我们选择单向的、多层的、LSTM。基于这篇论文。如下图。","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"机器翻译","slug":"机器翻译","permalink":"http://plmsmile.github.io/tags/机器翻译/"}]},{"title":"条件随机场","date":"2017-09-28T03:17:59.000Z","path":"2017/09/28/crf/","text":"条件随机场(Conditional Random Field, CRF)是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型。常常用于标注问题。隐马尔科夫模型和条件随机场是自然语言处理中最重要的算法。CRF最重要的就是根据观测序列，把标记序列给推测出来。 概率无向图模型 概率无向图模型又称为马尔科夫随机场，是一个可以由无向图表示的联合概率分布。一些类似内容。 有一组随机变量\\(Y \\in \\Gamma\\)，联合概率分布为\\(P(Y)\\)，由图\\(G=(V,E)\\)表示。节点v代表变量\\(Y_v\\)，节点之间的边代表两个变量的概率依赖关系。 定义 马尔可夫性就是说，给定一些条件下，没有连接的节点之间是条件独立的。 成对马尔可夫性 设\\(u\\)和\\(v\\)是两个没有边连接的节点，其它所有节点为\\(O\\)。成对马尔可夫性是说，给定随机变量组\\(Y_O\\)的条件下，随机变量\\(Y_u\\)和\\(Y_v\\)是独立的。即有如下： \\[ P(Y_u, Y_v \\mid Y_O) = P(Y_u \\mid Y_O)P(Y_v \\mid Y_O) \\] 局部马尔可夫性 节点\\(v\\)，\\(W\\)是与\\(v\\)连接的所有节点，\\(O\\)是与\\(v\\)没有连接的节点。局部马尔可夫性认为，给定\\(Y_w\\)的条件下，\\(Y_v\\)和\\(Y_O\\)独立。即有： \\[ P(Y_v, Y_O \\mid Y_W) = P(Y_v \\mid Y_W) P(Y_O \\mid Y_W) \\] 全局马尔可夫性 节点集合\\(A\\)，\\(B\\)被中间节点集合\\(C\\)分隔开，即不相连。全局马尔可夫性认为，给定\\(Y_C\\)的条件下，\\(Y_A\\)和\\(Y_B\\)是独立的。即有： \\[ P(Y_A, Y_B \\mid Y_C) = P(Y_A \\mid Y_C) P(Y_B \\mid Y_C) \\] 上面的3个马尔可夫性的定义是等价的。 概率无向图模型 设有联合概率密度\\(P(Y)\\)，由无向图\\(G=(V,E)\\)表示。节点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率密度\\(P(Y)\\)满足马尔可夫性，那么就称此联合概率分布为概率图模型，或马尔可夫随机场。 实际上我们更关心怎么求联合概率密度，一般是把整体的联合概率写成若干个子联合概率的乘积，即进行因子分解。概率无向图模型最大的优点就是易于因子分解。 概率无向图因子分解 团与最大团 团：无向图中的一个子集，任何两个节点均有边连接。 最大团：无向图中的一个子集，任何两个节点均有边连接。不能再加入一个节点组成更大的团了。 如\\(\\{Y_1, Y_2\\}\\)，\\(\\{Y_1, Y_2, Y_3\\}\\) 都是团，其中后者是最大团。而\\(\\{Y_1, Y_2, Y_3, Y_4\\}\\) 不是团，因为\\(Y_1\\)和\\(Y_4\\)没有边连接。 因子分解 有无向图模型\\(G\\), \\(C\\) 是 \\(G\\) 上的最大团，有很多个。\\(Y_C\\) 是\\(C\\) 对应的随机变量。则联合概率分布\\(P(Y)\\) 可以写成多个最大团\\(C\\) 上的势函数的乘积。 \\[ \\color{blue}{P(Y)} = \\frac {1} {Z} \\prod_C \\Psi_C(Y_C), \\quad Z = \\sum_Y \\prod_C \\Psi_C(Y_C) \\] 其中\\(Z\\)是规范化因子。\\(\\Psi_C(Y_C)\\)是 势函数，是一个严格正函数。等式左右两端都取条件概率也是可以的。下文就是。 \\[ \\color{blue}{\\Psi_C(Y_C)} = \\exp \\left(-E(Y_C) \\right) \\] 其中\\(\\color{blue}{E(Y_C) }\\) 是能量函数。 条件随机场的定义与形式 HMM的问题 这里是HMM的讲解 。HMM有下面几个问题 需要给出隐状态和观察符号的联合概率分布，即发射概率 \\(b_j(k)\\)，是生成式模型，也是它们的通病。 观察符号需要是离散的，可以枚举的，要遍历所有观察符号。如果是一个连续序列，则不行。 观察符号是独立的，没有观察相互之间的依赖关系。如一个句子的前后，都有关联才是。即输出独立性假设问题。 无法考虑除了字词顺序以外的其它特征。比如字母为大小写，包含数字等。 标注偏置问题。 标注偏置问题，举例，是说有两个单词&quot;rib-123&quot;和&quot;rob-456&quot;，&quot;ri&quot;应该标记为&quot;12&quot;，&quot;ro&quot;应该标记为&quot;45&quot;。 \\[ \\begin {align} &amp; P(12 \\mid ri) = P(1 \\mid r)P(2 \\mid i, r=1) = P(1 \\mid r) \\cdot 1 = P(1 \\mid r) \\\\ &amp; P(45 \\mid ro) = P(4 \\mid r)P(5 \\mid o, r=4) = P(4 \\mid r) \\cdot 1 = P(4 \\mid r) \\\\ \\end {align} \\] 由上面计算概率可知，ri标为12和 ro标为45的概率最终变成r标为1和4的概率。但是由于语料库中&quot;rob&quot;的出现次数很多，所以\\(P(4 \\mid r) &gt; P(1 \\mid r)\\) ，所以可能会一直把&quot;rib&quot;中的&quot;i&quot;标记为1，会导致标记出错。这就是标记偏置问题。 定义 条件随机场是给定随机变量\\(X\\)条件下，随机变量\\(Y\\) 的马尔可夫随机场。我们主要关心线性链随机场，它可以用于标注问题。 条件随机场 \\(X\\)与\\(Y\\)是随机变量，条件概率分布\\(P(Y \\mid X)\\)。随机变量\\(Y\\)可以构成一个无向图表示的马尔可夫随机场。任意一节点\\(Y_v\\)，\\(Y_A\\)是与\\(v\\)相连接的节点，\\(Y_B\\)是除了\\(v\\)以外的所有节点。若都有 \\[ P(Y_v \\mid X, Y_B) = P(Y_v \\mid X, Y_A) \\] 则称\\(P(Y \\mid X)\\) 为条件随机场。并不要求\\(X\\)和\\(Y\\) 具有相同的结构。 线性链条件随机场 \\(X\\)和\\(Y\\) 有相同的线性结构。设\\(X = (X_1, X_2, \\cdots, X_n)\\)，\\(Y = (Y_1, Y_2, \\cdots, Y_n)\\)均为线性链表示的随机变量序列。每个最大团包含2个节点。 \\(P(Y \\mid X)\\) 构成条件随机场，即满足马尔可夫性 \\[ P(Y_i \\mid X, Y_1, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_n) = P(Y_i \\mid X, Y_{i-1}, Y_{i+1}), \\quad i=1,\\cdots, n。 \\; (1和n时只考虑单边) \\] 则称\\(P(Y \\mid X)\\)为线性链条件随机场。 HMM，每个观察状态只与当前的隐状态有关系，分离了关系。就像1个字1个字地向后讲。输出观察符号还需要条件独立。 线性链条件随机场， 每个状态都与整个序列有关系。即先想好了整句话，再依照相应的次序去说出来。更加直击语言模型的核心。\\(X_1, X_2\\) 不需要条件独立。 基本形式 两种特征函数 状态转移特征函数t，只依赖与当前和前一个位置，即\\(y_i\\)和\\(y_{i-1}\\)。一般是01函数。 \\[ t(y_{i-1}, y_i, x, i) = \\begin {cases} 1, \\quad &amp; 满足某种条件, i \\in [2, n]. \\;例如y_{i-1}+y_{i}=3 \\\\ 0, \\quad &amp; 其他 \\end {cases} \\] 状态特征函数s，只依赖与当前位置\\(y_i\\) \\[ s(y_i, x, i) = \\begin {cases} 1, \\quad &amp; 满足某种条件, i \\in [1, n]. \\;例如y_{i}是偶数 \\\\ 0, \\quad &amp; 其他 \\end {cases} \\] 基础形式 设有\\(K_1\\)个状态特征转移函数，\\(K_2\\)个状态特征函数。分别对应的权值是\\(\\lambda_{k_1}\\)和\\(\\mu_{k_2}\\)。则线性链条件随机场参数化形式\\(P(y \\mid x)\\) 如下： \\[ P(y \\mid x) = \\frac {1}{Z(x)} \\exp \\left( \\sum_k^{K_1}\\lambda_k \\sum_{i=2}^n t_k(y_{i-1}, y_i, x, i) + \\sum_k^{K_2}\\mu_k \\sum_{i=1}^n s_k(y_i, x, i) \\right) \\] 其中\\(Z(x)\\)是规范化因子，如下 \\[ Z(x) = \\sum_x \\exp \\left( \\sum_k^{K_1}\\lambda_k \\sum_{i=2}^nt_k(y_{i-1}, y_i, x, i) + \\sum_k^{K_2}\\mu_k \\sum_{i=1}^n s_k(y_i, x, i) \\right) \\] 条件随机场完全由特征函数\\(t_{k_1}\\) 、\\(s_{k_2}\\)，和对应的权值\\(\\lambda_{k_1}\\) 和\\(\\mu_{k_2}\\) 决定的。 特征函数实际上也是势函数。 简化形式 有\\(K=K_1 + K_2\\)个特征，特征函数如下： \\[ f_k(y_{i-1}, y_i, i) = \\begin {cases} t_k(y_{i-1}, y_i, x, i) \\quad &amp; k = 1, \\cdots, K_1 \\\\ s_k(y_i, x, i) \\quad &amp; k = K_1 + l; \\; l = 1, \\cdots, K_2 \\\\ \\end {cases} \\] 同一个特征函数，要在整个\\(Y​\\)序列的各个位置进行计算，可以进行求和，即转化为全局特征函数， 新的特征函数\\(f_k (y, x)​\\)如下： \\[ \\color{blue} {f_k (y, x)} = \\sum _{i=1} ^n f_k(y_{i-1}, y_i, i), \\quad k = 1, \\cdots, K \\] \\(f_k (y, x)\\) 对应的新的权值\\(w_k\\)如下 \\[ \\color{blue} {w_k} = \\begin {cases} \\lambda_{k}, \\quad &amp; k = 1, \\cdots, K_1 \\\\ \\mu_{k - K_1}, \\quad &amp; k = K_1 + 1, K_1 + 2, \\cdots, K \\\\ \\end {cases} \\] 所以新的条件随机场形式如下： \\[ P(y \\mid x) = \\frac {1} {Z(x)} \\exp \\sum_{k=1} ^K w_k f_k(y, x) , \\quad Z(x) = \\sum_y \\exp \\sum_{k=1} ^K w_k f_k(y, x) \\] 可以看出，格式和最大熵模型很像。条件随机场最重要的就是，根据观察序列，把标记序列给推测出来。 向量形式 向量化特征函数和权值 \\[ F(y, x) = (f_1(y, x), \\cdots, f_K(y, x))^T, \\quad w = (w_1, \\cdots, w_K)^T \\] 可以写成向量内积的形式 \\[ P_w (y \\mid x) = \\frac{1}{Z(x)} \\exp (w \\cdot F(y, x)) , \\quad Z(x) = \\sum_y \\exp (w \\cdot F(y, x)) \\] 矩阵形式 为状态序列\\(Y\\)设置起点和终点标记，\\(y_0 = start\\) 和\\(y_{n+1} = stop\\)。从\\(0 \\to n+1\\)中，有\\(n+1\\)次的状态转移。我们可以用\\(n+1\\) 个状态转移矩阵来表示状态转移的概率。 设\\(\\color{blue}{M_i(x)}\\) 是\\(i-1 \\to i\\)的转移矩阵，是\\(m\\)阶，\\(m\\) 是\\(y_i\\) 取值的个数。表示了各种取值情况互相转化的概率。 \\[ M_1(x) = \\begin{bmatrix} a_{01} &amp; a_{02} \\\\ 0 &amp; 0 \\\\ \\end{bmatrix} , \\; M_2(x) = \\begin{bmatrix} b_{11} &amp; b_{12} \\\\ b_{21} &amp; b_{22} \\\\ \\end{bmatrix} , \\; M_3(x) = \\begin{bmatrix} c_{11} &amp; c_{12} \\\\ c_{21} &amp; c_{22} \\\\ \\end{bmatrix} , \\; M_4(x) = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\\\ \\end{bmatrix} \\] 要求什么样的路径概率，则相乘相应的数值概率即可。那么这些矩阵里面的数值是怎么来的呢。有下面的矩阵定义 ： \\[ \\color{blue} {g(y_{i-1}, y_i, x, i)} = M_i(y_{i-1}, y_i \\mid x) = \\exp \\sum_{k=1}^K f_k(y_{i-1}, y_i, i, x) ,\\quad \\color{blue} {M_i(x)} = [g(y_{i-1}, y_i, x, i) ] \\] 每一步转移的时候，由于\\((y_{i-1}, y_i)\\) 有\\(m^2\\) 种情况，计算\\(g\\)时会得到多个值，即可得到一个矩阵。 其中\\(g(y_{i-1}, y_i, x, i)\\) 在计算的时候，会根据\\(i\\) 的不同，而选择不同的特征函数进行计算。不要忘记了\\(f_k\\)函数的定义。 \\[ \\color{blue} {f_k(y_{i-1}, y_i, i)} = \\begin {cases} t_k(y_{i-1}, y_i, x, i) \\quad &amp; k = 1, \\cdots, K_1 \\\\ s_k(y_i, x, i) \\quad &amp; k = K_1 + l; \\; l = 1, \\cdots, K_2 \\\\ \\end {cases} \\] 所以非规范化条件概率可以通过矩阵的某些元素的乘积表示，有 \\[ P_w( y \\mid x) = \\frac {1}{Z_w(x)} \\prod_{i=1}^{n+1} M_i(y_{i-1}, y_i \\mid x) \\] 其中规范化因子 是\\(n+1\\)的矩阵相乘的结果矩阵中，第\\((start, stop)\\) 元素。例如第\\((0, 0)\\)。其中是\\((start, end)\\) 是矩阵下标对应。 条件随机场的概率计算问题 主要问题是给定条件随机场\\(P(Y \\mid X)\\) ，给定输入序列\\(x\\) 和输出序列\\(y\\)，求\\(P(Y_i = y_i \\mid x)\\) 和\\(P(Y_{i-1} = y_{i-1}, Y_i = y_{i} \\mid X)\\)， 以及相应的期望问题。 关键是求这些特征函数期望值，当模型训练好之后，去验证我们的模型。 前向后向算法 \\(y_i\\) 确定后， \\(\\color{blue} {\\alpha_i(y_i \\mid x) }\\) 表示，从\\(start \\to i\\)，就是$y = (start, y_1, y_2, , y_i) $ 的概率，也就是从前面到位置\\(y_i\\) 的概率。特别地 \\[ \\alpha_0(y \\mid x) = \\begin{cases} 1, \\quad &amp; y=start \\\\ 0, \\quad &amp; 其他 \\\\ \\end{cases} \\] 而\\(y_i​\\) 的取值有\\(m​\\) 种， 所以前向变量 \\(\\color{blue}{\\alpha_i(x)}​\\) 到\\(y​\\) 到第 \\(i​\\) 个位置 的所有概率取值向量。 \\[ \\color{blue} {\\alpha_i^T(x)} = \\alpha_{i-1}^T(x) \\cdot M_i(x) ,\\quad i = 1,2,\\cdots, n+1 \\] \\(y_i\\) 确定后， \\(\\color{blue} {\\beta_i (y_i \\mid x) }\\) 表示，位置\\(i\\)的标记为\\(y_i\\) ，并且后面为\\(y_{i+1}, \\cdots, y_n, stop\\) 的概率。同理一个是概率值。特别地 \\[ \\beta_{n+1}(y \\mid x) = \\begin{cases} 1, \\quad &amp; y=stop \\\\ 0, \\quad &amp; 其他 \\\\ \\end{cases} \\] 后向变量 \\(\\color{blue} {\\beta_i (y \\mid x) }​\\)，是一个m维向量 \\[ \\color{blue} {\\beta_i (x) } = M_{i+1}(x) \\cdot \\beta_{i+1}(x) \\] 可以得到\\(Z(x)\\)： \\[ Z(x) = \\alpha_n^T(x) \\cdot 1 = 1^T \\cdot \\beta_{i+1}(x) \\] 条件概率计算 位置是\\(i\\) 标记\\(y_i\\) 的条件概率\\(P(Y_i = y_i \\mid x)\\)是 \\[ P(Y_i = y_i \\mid x) = \\frac {1}{Z(x)} \\cdot \\alpha_i(y_i \\mid x) \\beta_i(y_i \\mid x) \\] 位置\\(i-1, i\\) 分别标记为\\(y_{i-1}, y_i\\) 的概率是 \\[ P(Y_{i-1} = y_{i-1}, Y_i = y_i \\mid x) = \\frac{1}{Z(x)} \\cdot \\alpha_{i-1}(y_{i-1} \\mid x) M_i(y_{i-1}, y_i \\mid x) \\beta_i (y_i \\mid x) \\] 特征期望值计算 两个期望值和最大熵模型的约束条件等式 有点像。 特征函数\\(f_k\\) 关于条件概率分布\\(P(Y \\mid X)\\) 的概率 \\[ E_{P(Y \\mid X)}(f_k) = \\sum_y P(y \\mid x) f_k(y, x) = \\sum_{i=1}^{n+1}\\sum_{y_{i-1}y_i} f_k(y_{i-1}, y_i, x, i) P(y_{i-1}, y_i \\mid x) \\] 特征函数\\(f_k\\) 关于条件概率分布\\(P(X, Y)\\) 的概率，\\(\\hat P(x)\\) 是经验分布 \\[ E_{P(X, Y)}(f_k) = \\sum_{x, y} P(x, y) \\sum_{i=1}^{n+1} f_k(y_{i-1}, y_i, x, i) = \\sum_{x} \\hat P(x) \\sum_y P(y \\mid x) \\sum_{i=1}^{n+1} f_k(y_{i-1}, y_i, x, i) \\] 通过前向和后向向量可以计算出两个概率，然后可以计算出相应的期望值。就可以与我们训练出的模型进行比较。 学习算法 条件随机场模型实际上是定义在时序数据上的对数线性模型，学习方法有极大似然估计和正则化的极大似然估计。具体的优化实现算法有：改进的迭代尺度法IIS、梯度下降法和拟牛顿法。 改进的迭代尺度法 这里是最大熵模型中的改进的迭代尺度算法。每次更新一个\\(\\delta_i\\) 使得似然函数的该变量的下界增大，即似然函数增大。 已知经验分布\\(\\hat P(X, Y)\\)，和模型如下 \\[ P(y \\mid x) = \\frac {1} {Z(x)} \\exp \\sum_{k=1} ^K w_k f_k(y, x) , \\quad Z(x) = \\sum_y \\exp \\sum_{k=1} ^K w_k f_k(y, x) \\] 对数似然函数和最大熵算法的极大似然函数很相似，如下： \\[ L(w) = L_{\\hat P}(P_w) = \\log \\prod_{x,y} P_w(y \\mid x) ^ {\\widetilde P(x, y)} = \\sum_{x,y} \\widetilde P(x, y) \\log P_w(y \\mid x) \\] 对数似然函数\\(L(w)\\) \\[ L(w) = \\sum_{j=1}^{N} \\sum_{k=1}^K w_k f_k(y_j, x_j) - \\sum_{j=1}^N \\log Z_w(x_j) \\] 数据\\((x, y)\\) 中出现的特征总数\\(T(x, y)\\) ： \\[ T(x, y) = \\sum_k f_k(y, x) = \\sum_{k=1}^K \\sum_{i=1}^{n+1}f_k(y_{i-1}, y_i, x) \\] 输入：特征函数\\(t_1, t_2, \\cdots, t_{K_1}\\)和\\(s_1, s_2, \\cdots, s_{K_2}\\) ；经验分布\\(\\hat P(x, y)\\) 输出：模型参数\\(\\hat w\\)，模型\\(P_{\\hat w}\\) 步骤： 1 赋初值 \\(w_k = 0\\) 2 对所有\\(k\\)，求解方程，解为\\(\\delta_k\\) \\[ \\begin{align} &amp; \\sum_{x, y} \\hat P(x) P(y \\mid x) \\sum_{i=1}^{n+1} t_k(y_{i-1}, y_i, x, i) \\exp (\\delta_k T(x, y)) = E_{\\hat p}[t_k] , \\quad k = 1, 2, \\cdots, K_1 时 \\\\ &amp; \\sum_{x, y} \\hat P(x) P(y \\mid x) \\sum_{i=1}^{n+1} s_l(y_{i-1}, y_i, x, i) \\exp (\\delta_k T(x, y)) = E_{\\hat p}[s_l] , \\quad k = K_1 + l, l = 1, 2, \\cdots, K_2 时 \\\\ \\end{align} \\] 3 更新\\(w_k + \\delta_k \\to w_k\\) ，如果还有\\(w_k\\)未收敛，则继续2 算法S 对于不同的数据\\((x, y)\\)的特征出现次数\\(T(x, y)\\) 可能不同，可以选取一个尽量大的数\\(S\\)作为特征总数，使得所有松弛特征\\(s(x, y) \\ge 0\\) ： \\[ s(x, y) = S - \\sum_{i=1}^{n+1}\\sum_{k=1}^K f_k(y_{i-1}, y_i, x, i) \\] 所以可以直接解得\\(\\delta_k\\) ，当然\\(f_k\\) 要分为\\(t_k\\)和\\(s_k\\)，对应的期望值计算也不一样。具体见书上。 \\[ \\delta_k = \\frac{1}{S} \\log \\frac{E_{ \\hat P}[f_k] } {E_P[f_k]} \\] 算法T 算法S中\\(S\\)会选择很大，导致每一步的迭代增量会加大，算法收敛会变慢，算法T重新选择一个特征总数 T(x) \\[ T(x) = \\max \\limits_y T(x, y) \\] 使用前后向递推公式，可以算得\\(T(x)=t\\) 。 对于\\(k \\in [1, K_1]\\)的\\(t_k\\)关于经验分布的期望： \\[ E_{\\hat P}[t_k] = \\sum_{t=0}^{T_{max}} a_{k,t} \\beta_{k}^t \\] 其中，\\(a_{k,t}\\)是\\(t_k\\)的期待值， \\(\\delta_k = \\log \\beta_k\\) 对于\\(k \\in [1+K_1, K]\\)的\\(s_k\\)关于经验分布的期望： \\[ E_{\\hat P}[s_k] = \\sum_{t=0}^{T_{max}} b_{k,t} \\gamma_{k}^t \\] 其中\\(\\gamma_k^t\\)是特征\\(s_k\\)的期望值，\\(\\delta_k = \\log \\gamma_k\\)。当然，求根也可以使用牛顿法去求解。 拟牛顿法 预测算法 预测问题 给定条件随机场\\(P(Y \\mid X)\\)和输入序列\\(x\\)，求条件概率最大的输出序列（标记序列）\\(y^*\\)，即对观测序列进行标注。 \\[ \\begin{align} y^* &amp; = \\arg \\max \\limits_y P_w(y \\mid x) = \\arg \\max \\limits_y \\frac{\\exp (w \\cdot F(y, x))}{Z_w(x)} \\\\ &amp; = \\arg \\max \\limits_y ( w \\cdot F(y, x)) \\\\ \\end {align} \\] 其中路径\\(y\\)表示标记序列，下面是参数说明 \\[ \\begin {align} &amp; w = (w_1, w_2, \\cdots, w_k)^T \\\\ &amp; F(y, x) = (f_1(y, x), \\cdots, f_K(y, x))^T, \\quad w = (w_1, \\cdots, w_K)^T \\\\ &amp; f_k (y, x) = \\sum _{i=1} ^n f_k(y_{i-1}, y_i, x, i) \\\\ &amp; F_i(y_{i-1}, y_i, x) = \\left(f_1(y_{i-1}, y_i, x, i), f_2(y_{i-1}, y_i, x, i),\\cdots, f_k(y_{i-1}, y_i, x, i) \\right)^T \\end {align} \\] 所以，为了求解最优路径，只需计算非规范化概率，即转换为下面的问题： \\[ \\max \\limits_y \\quad \\sum_{i=1}^n w \\cdot F_i(y_{i-1}, y_i, x) \\] 维特比算法 HMM的维特比算法。 维特比变量\\(\\delta_i(l)\\)，到达位置\\(i\\)， 标记为\\(l \\in [1, m]\\) 的概率 \\[ \\delta_i(l) = \\max \\limits_{1 \\le j \\le m} \\{ \\delta_{i-1}(j) + w \\cdot F_i(y_{i-1} = j, y_i = l, x) \\}, \\quad j = 1, 2, \\cdots, m \\] 记忆路径\\(\\psi_i(l) = a\\) 当前时刻\\(t\\)标记为l, \\(t-1\\)时刻标记为a \\[ \\psi_i(l) = = \\arg \\max \\limits_{1 \\le j \\le m} \\{ \\delta_{i-1}(j) + w \\cdot F_i(y_{i-1} = j, y_i = l, x) \\} \\] 算法主体 输入：特征向量\\(F(y, x)\\)和权值向量\\(\\mathbf{w}\\)，观测向量\\(x = (x_1, x_2, \\cdots. x_n)\\) 输出：最优路径\\(y^* = (y_1^*, y_2^*, \\cdots, y_n^*)\\) 步骤如下 初始化 \\[ \\delta_1(j) = w \\cdot F_1(y_0 = start, y_1 = j, x), \\quad j = 1, \\cdots, m \\] 递推 \\[ \\begin{align} &amp; \\delta_i(l) = \\max \\limits_{1 \\le j \\le m} \\{ \\delta_{i-1}(j) + w \\cdot F_i(y_{i-1} = j, y_i = l, x) \\}, \\quad j = 1, 2, \\cdots, m \\\\ &amp; \\psi_i(l) = \\arg \\max \\limits_{1 \\le j \\le m} \\delta_i(j), \\quad \\text{即上式的参数j} \\\\ \\end{align} \\] 终止 \\[ \\begin{align} &amp; \\max \\limits_y (w \\cdot F(y, x)) = \\max \\limits_{1 \\le j \\le m} \\delta_n(j) \\\\ &amp; y_n^* = \\arg \\max \\limits_{1 \\le j \\le m} \\delta_n(j) \\\\ \\end{align} \\] 返回路径 \\[ y_i^* = \\psi_{i+1} (y_{i+1}^*), \\quad i = n-1, n-2, \\cdots, 1 \\] 求得最优路径\\(y^* = (y_1^*, y_2^*, \\cdots, y_n^*)\\)","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"条件随机场","slug":"条件随机场","permalink":"http://plmsmile.github.io/tags/条件随机场/"}]},{"title":"最大熵模型","date":"2017-09-20T09:39:12.000Z","path":"2017/09/20/maxentmodel/","text":"最大熵原理 预备知识 离散型变量\\(X\\)的概率分布是\\(\\color{blue}{P(X)}\\)。它的熵\\(\\color{blue}{H(X) \\; or \\; H(P)}\\)越大，代表越均匀、越混乱、越不确定。各种熵点这里 \\[ \\color{blue}{H(P)} = \\color{red} {- \\sum_{x \\in X}P(x) \\log P(x)} \\] 熵满足下面不等式 \\[ 0 \\le H(P) \\le \\log |X|, \\quad 其中|X|是X的取值个数 \\] 当前仅当\\(X\\)的分布是均匀分布的时候等号成立。当\\(X\\)服从均匀分布时，熵最大。 最大熵的思想 最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。 事情分为两个部分：确定的部分（约束条件）和不确定的部分。选择模型时要 要满足所有的约束条件，即满足已有的确定的事实 要均分不确定的部分 \\(X\\)有5个取值\\(\\{A, B,C,D,E\\}\\)，取值概率分别为\\(P(A), P(B), P(C), P(D), P(E)\\)。满足以下约束条件 \\[ P(A)+ P(B)+ P(C)+ P(D)+ P(E) = 1 \\] 满足这个条件的模型有很多。再加一个约束条件 \\[ P(A) + P(B) = \\frac{3}{10} \\] 则，满足约束条件，不确定的平分（熵最大）：这样的模型是最好的模型 \\[ P(A) = P(B) = \\frac{3}{20}, \\quad P(C)=P(D)=P(E) = \\frac{7}{30} \\] 即：约束条件，熵最大 最大熵模型 假设分类模型是一个条件概率分布\\(\\color{blue}{P(Y \\mid X)}\\)。(有的不是选择条件模型，如论文里面)。训练数据集\\(N\\)个样本 \\(T = \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}\\) 基本概念 联合分布：\\(\\color{blue}{P(X, Y)}\\) 边缘分布：\\(\\color{blue}{P(X)}\\) 联合经验分布：\\(\\color{blue}{\\widetilde{P}(X, Y)} = \\color{red}{\\frac {v(X=x, Y=y)}{N}}\\)，其中\\(v(x, y)\\)为频数 联合边缘分布：\\(\\color{blue}{\\widetilde P(X) = \\color{red} {\\frac{v(X=x)}{N}}}\\) 特征函数\\(\\color{blue}{f(x, y)}\\)用来描述\\(x\\)和\\(y\\)满足的一个事实约束条件： \\[ f(x, y) = \\begin{cases} 1, \\quad &amp; x与y满足一个事实，即约束条件 \\\\ 0, \\quad &amp; 否则 \\end{cases} \\] 如果有\\(n\\)个特征函数\\(\\color{blue}{f_i(x, y), i = 1, 2, \\cdots, n}\\), 就有\\(n\\)个约束条件。 概率期望的计算 \\(X\\)的期望 \\(X\\) 是随机变量，概率分布是\\(P(X)\\) ，或概率密度函数是\\(f(x)\\) \\[ E(X) = \\begin{cases} &amp;\\sum_{i} x_i P(x_i), \\quad &amp; \\text{离散} \\\\ &amp; \\int_{-\\infty}^{+\\infty} {x \\cdot f(x)} \\, {\\rm d}x , \\quad &amp; \\text{连续} \\\\ \\end{cases} \\] 下面只考虑离散型的期望，连续型同理，求积分即可。 一元函数的期望 \\(Y = g(X)\\)，期望是 \\[ E[Y] = E[g(X)] = \\sum_{i}^{\\infty} g(x_i) \\cdot P(x_i) \\] 二元函数的期望 \\(Z = g(X, Y)\\) ，期望是 \\[ E(Z) = \\sum_{x, y} g(x, y) \\cdot p(x, y) = \\sum_{i=1} \\sum_{j=1} g(x_i, y_j) p(x_i, y_j) \\] 期望其实就是\\(E 狗 = \\sum 狗 \\cdot 老概率\\) 。可离散，可连续。 约束条件等式 实际分布期望 特征函数\\(f(x, y)\\)关于经验分布\\(\\color{blue}{\\widetilde P(x, y)}\\)的期望\\(\\color{blue}{E_{\\widetilde P}(f)}\\)，即实际应该有的特征 ，也就是一个给模型加的约束条件 ： \\[ \\color{blue}{E_{\\widetilde P}(f)} = \\sum_{x, y} \\color{red} {\\widetilde P(x,y)} f(x, y) \\] 理论模型期望 特征函数\\(f(x, y)\\) 关于模型\\(\\color{blue}{P(Y\\mid X)}\\)和经验分布\\(\\color{blue}{\\widetilde P(X)}\\)的期望\\(\\color{blue}{E_{P}(f)}\\) ，即理论上模型学得后的期望： \\[ \\color{blue}{E_{ P}(f)} =\\sum_{x, y} \\color{red} { \\widetilde{P}(x) P(y \\mid x)} f(x, y) \\] 要从训练数据中获取信息，特征函数关于实际经验分布和理论模型的两个期望就得相等，即理论模型要满足实际约束条件 \\[ E_{\\widetilde P}(f) = E_{ P}(f) \\] 最大熵模型思想 条件概率分布\\(P(Y \\mid X)\\)的条件熵为\\(\\color{blue}{H(P)}\\)如下，条件熵： \\[ \\color{blue}{H(P)} = \\color{red} {- \\sum_{x, y} \\widetilde P(x) P(y \\mid x) \\log P(y \\mid x)} \\] 则满足约束条件\\(\\color{blue}{E_{\\widetilde P}(f) = E_{ P}(f)}\\)的模型中，条件熵\\(\\color{blue}{H(P)}\\)最大的模型就是最大熵模型。 最大熵模型的学习 学习问题 最大熵模型的学习等价于约束最优化问题，这类问题可以用拉格朗日对偶性去求解。 给定数据集\\(T = \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_N, y_N)\\}\\)和特征函数\\(\\color{blue}{f_i(x, y), i = 1, 2, \\cdots, n}\\)。 要满足2个约束条件 \\[ \\color{red} {E_{\\widetilde P}(f) = E_{ P}(f), \\quad \\sum_{x, y}P(y \\mid x) = 1 } \\] 要得到最大化熵 \\[ \\max \\limits_{P \\in C} \\; \\color{blue}{H(P)} = \\color{red} {- \\sum_{x, y} \\widetilde P(x) P(y \\mid x) \\log P(y \\mid x)} \\] 按照最优化问题的习惯，将求最大值问题改写为等价的求最小值问题 ，如下 \\[ \\min \\limits_{P \\in C} \\; \\color{blue}{ - H(P)} = \\color{red} { \\sum_{x, y} \\widetilde P(x) P(y \\mid x) \\log P(y \\mid x)} \\] 推导最大熵模型 一般使用拉格朗日对偶性去求解，可以见李航书附录。 引入拉格朗日乘子\\(\\color{blue}{w = (w_0, w_1, \\cdots, w_n)}\\)，即参数向量，构造拉格朗日函数\\(\\color{blue}{L(P, w)}\\) ： \\[ \\color{blue}{L(P, w)} = \\color{red}{ -H(P) + w_0 \\cdot \\left( 1 - \\sum_{x, y} P(y \\mid x)\\right) + \\sum_{i=1}^n w_i \\cdot \\left(E_{\\widetilde P}(f) - E_{ P}(f) \\right) } \\] 由于是凸函数，根据相关性质，所以原始问题和对偶问题同解，原始问题如下： \\[ \\min \\limits_{P \\in C} \\max \\limits_{w} L(P, w) \\] 对应的对偶问题如下： \\[ \\max \\limits_{w} \\min \\limits_{P \\in C} L(P, w) \\] 主要思路是：先固定\\(\\color{blue}{w}\\)，去计算\\(\\color{blue} {\\min \\limits_{P \\in C} L(P, w)}\\)，即去找到一个合适的\\(\\color{blue}{P(Y \\mid X)}\\)。再去找到一个合适的\\(\\color{blue}{w}\\)。 第一步：求解\\(\\color{blue}{P}\\) 。设对偶函数\\(\\color{blue} { \\Psi (w)}\\)如下： \\[ \\color{blue} { \\Psi (w)} = \\color{red} {\\min \\limits_{P \\in C} L(P, w) = L(P_w, w)} \\] 对偶函数的解，即我们找到的\\(P(Y \\mid X)\\)，记作\\(\\color{blue}{P_w}\\)，如下： \\[ \\color{blue}{P_w} = \\arg \\min \\limits_{P \\in C} L(P, w) =\\color{red} {P_w(y \\mid x)} \\] 用\\({L(P, w)}\\)对\\(P\\)进行求偏导，令偏导为0，可以解得\\({P_w}\\)，即最大熵模型 如下： \\[ \\color{blue}{P_w(y \\mid x)} = \\color{red} {\\frac{1}{Z_w(x)} \\cdot \\exp \\left({\\sum_{i=1}^nw_if_i(x, y)}\\right) }, \\; \\color{blue}{Z_w(x)} = \\color{red} {\\sum_{y} \\exp \\left( \\sum_{i=1}^{n} w_i f_i(x, y)\\right) } \\] 其中\\(\\color{blue}{Z_w(x)}\\)是归一化因子，\\(\\color{blue} {f_i(x, y)}\\)是特征函数，\\(\\color{blue}{w_i}\\)是特征的权值，\\(\\color{blue}{P_w(y \\mid x)}\\) 就是最大熵模型， \\(\\color{blue}{w}\\)是最大熵模型中的参数向量。 第二步：求解\\(\\color{blue}{w}\\)。即求\\(w\\)去最大化对偶函数，设解为\\(\\color{blue} {w^*}\\) 。可以使用最优化算法去求极大化。 \\[ \\color{blue}{w^*} = \\color{red} {\\arg \\max \\limits_{w} \\Psi(w)} \\] 最终，求到的\\(\\color{blue} {P^* = P_{w^*} = P_{w^*}(y \\mid x)}\\)就是学习得到的最大熵模型。 最大熵模型 最大熵模型如下，其中\\(\\color{blue}{Z_w(x)}\\)是归一化因子，\\(\\color{blue} {f_i(x, y)}\\)是特征函数，\\(\\color{blue}{w_i}\\)是特征的权值 。 \\[ \\color{blue}{P_w(y \\mid x)} = \\color{red} {\\frac{1}{Z_w(x)} \\cdot \\exp \\left({\\sum_{i=1}^nw_if_i(x, y)}\\right) }, \\; \\color{blue}{Z_w(x)} = \\color{red} {\\sum_{y} \\exp \\left( \\sum_{i=1}^{n} w_i f_i(x, y)\\right) } \\] 极大似然估计 其实对偶函数\\(\\color{blue}{\\Psi(w)}\\)的极大化等价于最大熵模型的极大似然估计。 已知训练数据的经验分布\\(\\widetilde{P}(X, Y)\\)，条件概率分布的\\(P(Y \\mid X)\\)的对数似然函数是： \\[ \\begin{align*} \\color{blue} {L_{\\widetilde P}(P_w)} &amp; = \\log \\prod_{x, y} P(y \\mid x) ^ {\\widetilde{P}(X, Y)} = \\sum_{x, y} \\widetilde P(x, y) \\log P(y \\mid x) \\\\ &amp; = \\color{red}{\\sum_{x, y}\\widetilde{P}(X, Y) \\sum_{i=1}^{n} w_i f_i(x, y) - \\sum_{x} \\widetilde{P}(X)\\log Z_w(x) } \\end{align*} \\] 可以证明得到，$L_{P}(P_w) = (w) $，极大似然函数等于对偶函数。 模型学习的最优化算法 逻辑回归、最大熵模型的学习都是以似然函数为目标函数的最优化问题，可以通过迭代算法求解。这个目标函数是个光滑的凸函数。通过很多方法都可以保证找到全局最优解，常用的有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法，其中牛顿法和拟牛顿法一般收敛速度更快。 改进的迭代尺度法 改进的迭代尺度法(improved iterative scaling, IIS)是一种最大熵模型学习的最优化算法。 已知最大熵模型如下： \\[ \\color{blue}{P_w(y \\mid x)} = \\color{red} {\\frac{1}{Z_w(x)} \\cdot \\exp \\left({\\sum_{i=1}^nw_if_i(x, y)}\\right) }, \\; \\color{blue}{Z_w(x)} = \\color{red} {\\sum_{y} \\exp \\left( \\sum_{i=1}^{n} w_i f_i(x, y)\\right) } \\] 对数似然函数如下： \\[ \\color{blue} {L_{\\widetilde P}(P_w)} = \\color{red}{\\sum_{x, y}\\widetilde{P}(X, Y) \\sum_{i=1}^{n} w_i f_i(x, y) - \\sum_{x} \\widetilde{P}(X)\\log Z_w(x) } \\] 目标是：通过极大似然估计学习模型参数，即求对数似然函数的极大值\\(\\color{blue} {\\hat w}\\)。 基本思想 当前参数向量\\(w = (w_1, w_2, \\cdots, w_n)^T\\)，找到一个新的参数向量\\(w+\\delta = (w_1 + \\delta_1, w_2 + \\delta_2, \\cdots, w_n + \\delta_n)\\)，使得每次更新都使似然函数值增大。 由于\\(\\delta\\)是一个向量，含有多个变量，不易同时优化。所以IIS 每次只优化其中一个变量\\(\\delta_i\\)，而固定其他变量\\(\\delta_j\\)。 设所有特征在\\((x, y)\\)中的出现次数\\(f^\\#(x, y) = M\\) ： \\[ \\color{blue}{f^\\# (x, y)} = \\sum_i f_i(x, y) \\] 计算每次的改变量： \\[ L(w+\\delta) - L(w) \\ge \\color{blue}{B(\\delta \\mid w)}, \\; 改变量的下界限 \\] 如果找到适当的\\(\\delta\\)使得改变量的下界\\(B(\\delta \\mid w)\\)提高，则对数似然函数也能提高。 计算\\(B(\\delta \\mid w)\\)对\\(\\delta_i\\)求偏导，令偏导等于0，得如下方程： \\[ \\color{red} { \\sum_{x, y} \\widetilde P(x) P_w(y \\mid x) f_i(x, y) \\exp \\left(\\delta_i f^\\#(x, y)\\right) = E_{\\widetilde p}(f_i) }, \\quad 其中\\, E_{\\widetilde p}(f_i) = \\sum_{x, y} \\widetilde P(x, y)f_i(x, y) \\] 然后，依次对\\(\\delta_i​\\)求解该方程，就可以求得\\(\\delta​\\)，也就能够更新\\(w​\\)，即\\(w \\to w+\\delta​\\) 算法步骤 输入：特征函数\\(f_1, \\cdots, f_n\\)；经验分布\\(\\widetilde P(x, y)\\)，模型\\(P_w(y \\mid x)\\) 输出：最优参数值\\(w_i^*\\)；最优模型\\(P_{w^*}\\) 初始化参数，取初值 \\(w_i = 0\\) 求解方程 \\(\\delta_i\\) \\[ \\sum_{x, y} \\widetilde P(x) P_w(y \\mid x) f_i(x, y) \\exp \\left(\\delta_i f^\\#(x, y)\\right) = E_{\\widetilde p}(f_i), \\] 更新参数 \\(w_i + \\delta_i \\to w_i\\) 其中解方程的时候，如果特征出现次数\\(f^\\#(x, y)\\) 是常数\\(M\\)，则可以直接计算\\(\\delta_i\\) ： \\[ \\color{blue}{\\delta_i } = \\color{red} {\\frac{1}{M} \\log \\frac{E_{\\widetilde p}(f_i)}{E_{p}(f_i)} } \\] 如果\\(f^\\#(x, y)\\)不是常数，则必须通过数值计算\\(\\delta_i\\)。最简单就是通过牛顿迭代法去迭代求解\\(\\delta_i^*\\)。以\\(g(\\delta_i) = 0\\) 表示该方程，进行如下迭代： \\[ \\color{blue} {\\delta_i^{(k+1)}} = \\color{red} {\\delta_i^{(k)} - \\frac{g(\\delta_i^{(k)})}{g^\\prime (\\delta_i^{(k)})} } \\]","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"各种熵","slug":"各种熵","permalink":"http://plmsmile.github.io/tags/各种熵/"},{"name":"最大熵模型","slug":"最大熵模型","permalink":"http://plmsmile.github.io/tags/最大熵模型/"},{"name":"IIS","slug":"IIS","permalink":"http://plmsmile.github.io/tags/IIS/"},{"name":"期望","slug":"期望","permalink":"http://plmsmile.github.io/tags/期望/"}]},{"title":"机器学习笔记","date":"2017-08-20T13:38:54.000Z","path":"2017/08/20/ml-ng-notes/","text":"线性回归 有\\(m\\)个样本\\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\cdots, (x^{(m)}, y^{(m)})\\)，假设函数有2个参数\\(\\theta_0, \\theta_1\\)，形式如下： \\[ h_\\theta(x) = \\theta_0 + \\theta_1x \\] 代价函数 代价函数 \\[ \\color{red} {J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2} \\] 目标是要找到合适的参数，去最小化代价函数\\(min\\, J(\\theta_0, \\theta_1)\\)。 假设\\(\\theta_0 = 0\\)，去描绘出\\(J(\\theta_1)\\)和\\(\\theta_1\\)的关系，如下面右图所示。 假设有3个样本\\((1,1), (2,2), (3,3)\\)，图中选取了3个\\(\\theta_1 = 1, 0.5, 0\\)，其中\\(J(\\theta_1)\\)在\\(\\theta_1=1\\)时最小。 那么回到最初的两个参数\\(h_\\theta(x) = \\theta_0 + \\theta_1x\\)，如何去找\\(min\\, J(\\theta_0, \\theta_1)\\)呢？这里绘制一个等高图去表示代价函数，如下面右图所示，其中中间点是代价最小的。 梯度下降 基础说明 上文已经定义了代价函数\\(J(\\theta_0, \\theta_1)\\)，这里要使用梯度下降算法去最小化\\(J(\\theta_0, \\theta_1)\\)，自动寻找出最合适的\\(\\theta\\)。梯度下降算法应用很广泛，很重要。大体步骤如下： 设置初始值\\(\\theta_0, \\theta_1\\) 不停改变\\(\\theta_0, \\theta_1\\)去减少\\(J(\\theta_0, \\theta_1)\\) 当然选择不同的初始值，可能会得到不同的结果，得到局部最优解。 对于所有的参数\\(\\theta_j\\)进行同步更新，式子如下 \\[ \\color{red}{\\theta_j = \\theta_j - \\underbrace{\\alpha \\cdot \\frac{\\partial}{\\partial_{\\theta_j}} J(\\theta_0, \\theta_1)}_{学习率 \\times 偏导}} \\] 上面公式中\\(\\color{blue}{\\alpha}\\)是学习率(learning rate)，是指一次迈多大的步子，一次更新的幅度大小。 例如上面的两个参数，对于一次同步更新(梯度下降) \\[ t_0 = \\theta_0 - \\alpha \\frac{\\partial}{\\partial_{\\theta_0}} J(\\theta_0, \\theta_1), t_1 = \\theta_1 - \\alpha \\frac{\\partial}{\\partial_{\\theta_1}} J(\\theta_0, \\theta_1) \\quad \\to\\quad \\theta_0 = t_0, \\theta_1 = t_1 \\] 也有异步更新(一般指别的算法) \\[ t_0 = \\theta_0 - \\alpha \\frac{\\partial}{\\partial_{\\theta_0}} J(\\theta_0, \\theta_1),\\theta_0 = t_0 \\quad \\to\\quad t_1 = \\theta_1 - \\alpha \\frac{\\partial}{\\partial_{\\theta_1}} J(\\theta_0, \\theta_1), \\theta_1 = t_1 \\] 偏导和学习率 这里先看一个参数的例子，即\\(J(\\theta_1)\\)。\\(\\theta_1 = \\theta_1 - \\alpha \\frac{d}{dx}J(\\theta_1)\\)。当\\(\\theta\\)从左右靠近中间值，导数值(偏导/斜率)分别是负、正，所以从左右两端都会靠近中间值。 当学习率\\(\\alpha\\)太小，梯度下降会很缓慢；\\(\\alpha\\)太大，可能会错过最低点，导致无法收敛。 当已经处于局部最优的时候，导数为0，并不会改变参数的值，如下图 当逐渐靠近局部最优的时候，梯度下降会自动采取小步子到达局部最优点。是因为越接近，导数会越来越小。 在线性回归上使用梯度下降 代价函数 \\[ \\color{blue} {J(\\theta_0, \\theta_1)} = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2 = \\color{red}{\\frac{1}{2m} \\sum_{i=1}^m (\\theta_0 + \\theta_1x^{(i)} -y^{(i)})^2} \\] 分别对\\(\\theta_0\\)和\\(\\theta_1\\)求偏导有 \\[ \\color{blue}{\\frac{\\partial}{\\partial_{\\theta_0}}J(\\theta_0, \\theta_1) }= \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right), \\quad \\color{blue}{\\frac{\\partial}{\\partial_{\\theta_1}} J(\\theta_0, \\theta_1)}= \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)\\cdot x^{(i)} \\] 那么使用梯度下降对\\(\\theta_0 和 \\theta_1\\)进行更新，如下 \\[ \\theta_0 = \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right), \\quad \\theta_1 = \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta(x^{(i)}) - y^{(i)}\\right)\\cdot x^{(i)} \\] 当前代价函数实际上是一个凸函数，如下图所示。它只有全局最优，没有局部最优。 通过不断地改变参数减小代价函数\\(\\color{blue} {J(\\theta_0, \\theta_1)}\\)，逼近最优解，最终会得到一组比较好的参数，正好拟合了我们的训练数据，就可以进行新的值预测。 梯度下降技巧 特征缩放 不同的特征的单位的数值变化范围不一样，比如\\(x_1 \\in (0,2000), x_2 \\in (1,5)\\)，这样会导致代价函数\\(J(\\theta)\\)特别的偏，椭圆。这样来进行梯度下降会特别的慢，会来回震荡。 所以特征缩放是把所有的特征缩放到相同的规模上。得到的\\(J(\\theta)\\)就会比较圆，梯度下降能很快地找到一条通往全局最小的捷径。 特征缩放的数据规模不能太小或者太大，如下面可以的规模是 \\[ [-1, 1], [0, 3], [-2, 0.5], [-3, 3], [-\\frac{1}{3}, \\frac{1}{3}] 都是可以的。而[-100, 100], [-0.0001, 0.0001]是不可以的 \\] 有一些常见的缩放方法 \\(x_i = \\frac{x_i - \\mu}{max - min}\\), \\(x_i = \\frac{x_i - \\mu}{s}\\)，其中\\(\\mu\\)是均值，\\(s\\)是标准差 \\(x_i = \\frac{x_i - min} {max - min}\\) \\(x_i = \\frac{x_i}{max}\\) 学习率的选择 当梯度下降正确运行的时候，每一次迭代\\(J(\\theta)\\)都会减少，但是减少到什么时候合适呢？当然最好的办法就是画图去观察，当然也可以设定减小的最小值来判断。下图中， 迭代次数到达400的时候就已经收敛。不同的算法，收敛次数不一样。 当图像呈现如下的形状，就需要使用更小的学习率。理论上讲，只要使用足够小的学习率，\\(J(\\theta)\\)每次都会减少。但是太小的话，梯度下降会太慢，难以收敛。 学习率总结 学习率太小，慢收敛 学习率太大，\\(J(\\theta)\\)可能不会每次迭代都减小，甚至不会收敛 这样去选择学习率调试： \\(\\ldots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \\ldots\\) 多变量线性回归 数据有\\(n\\)个特征，如\\(x^{(i)} = (1, x_1, x_2, \\cdots, x_n)\\)，其中\\(x_0 = 1\\)。则假设函数有\\(n+1\\)个参数，形式如下 \\[ \\color{blue}{h_\\theta(x)} = \\theta^Tx = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n \\] 代价函数 \\[ \\color{blue}{J(\\theta)} = \\frac{1}{2m} \\sum_{i=1}^m\\left( h_\\theta(x^{(i)}) - y^{(i)}\\right) ^ 2 \\] 梯度下降，更新每个参数\\(\\theta_j\\) \\[ \\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial_{\\theta_j}} =\\color{red}{ \\theta_j -\\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left(h_\\theta(x^{(i)})-y^{(i)} \\right) \\cdot x_j^{(i)}} \\] 多项式回归 有时候，线性回归并不能很好地拟合数据，所以我们需要曲线来适应我们的数据。比如一个二次方模型 \\[ h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x^2_2 \\] 当然可以用\\(x_2 = x^2_2, x_3 = x_3^3\\)来转化为多变量线性回归。如果使用多项式回归，那么在梯度下降之前，就必须要使用特征缩放。 正规方程 对于一些线性回归问题，使用正规方程方法求解参数\\(\\theta\\)，比用梯度下降更好些。代价函数如下 \\[ \\color{red} {J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2} \\] 正规方程的思想是函数\\(J(\\theta)\\)对每个\\(\\theta_j\\)求偏导令其等于0，就能得到所有的参数。即\\(\\frac{\\partial J}{\\partial \\theta_j} = 0\\)。 那么设\\(X_{m\\times(n+1)}\\)为数据矩阵（其中包括\\(x_0=1\\)），\\(y\\)为标签向量。则通过如下方程可以求得\\(\\theta\\) \\[ \\theta = (X^TX)^{-1}X^Ty \\] 正规方程和梯度下降的比较 梯度下降 正规方程 需要特征缩放 不需要特征缩放 需要选择学习率\\(\\alpha\\) 不虚选择学习率 需要多次迭代计算 一次运算出结果 特征数量\\(n\\)很大时，依然适用 \\(n\\)太大，求矩阵逆运算代价太大，复杂度为\\(O(n^3)\\)。\\(n\\leq10000\\)可以接受 适用于各种模型 只适用于线性模型，不适合逻辑回归和其他模型 逻辑回归 1&lt;img src=\"\" style=\"display:block; margin:auto\" width=\"60%\"&gt; 线性回归有2个不好的问题：直线难以拟合很多数据；数据标签一般是\\(0, 1\\)，但是\\(h_\\theta(x)\\)却可能远大于1或者远小于0。如下图。 基本模型 逻辑回归是一种分类算法，使得输出预测值永远在0和1之间，是使用最广泛的分类算法。模型如下 \\[ h_\\theta(x) = g(\\theta^Tx), \\quad g(z) = \\frac{1}{1+e^{-z}} \\] \\(g(z)\\)的图像如下，也称作Sigmoid函数或者Logistic函数，是S形函数。 将上面的公式整理后得到逻辑回归的模型 \\[ \\color{red}{h_\\theta(x) = \\frac {1}{1+e^{-\\theta^Tx}}}, \\quad 其中\\; \\color{red}{0 \\le h_\\theta(x) \\le 1} \\] 模型的意义是给出分类为1的概率，即\\(h_\\theta(x) = P(y=1\\mid x; \\theta)\\)。例如\\(h_\\theta(x)=0.7\\)，则分类为1的概率是0.7，分类为0的概率是\\(1-0.7=0.3\\)。 \\[ x的分类预测, y = \\begin{cases} 1, \\; &amp; h_\\theta(x) \\ge 0.5, \\;即\\; \\theta^Tx \\ge 0\\\\ 0, \\; &amp; h_\\theta(x) &lt; 0.5, \\; 即 \\; \\theta^Tx &lt; 0 \\\\ \\end{cases} \\] 决策边界 线性边界 假设我们有一个模型\\(h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2)\\)，已经确定参数\\(\\theta = (-3, 1, 1)\\)，即模型\\(h_\\theta(x) = g(-3+x_1+x_2)\\)，数据和模型如下图所示 由上可知，分类结果如下 \\[ y = \\begin{cases} 1, \\, &amp; x_1+ x_2 \\ge 3 \\\\ 0, \\, &amp; x_1 + x_2 &lt; 3 \\\\ \\end{cases} \\] 那么直线\\(x_1+x_2=3\\)就称作模型的决策边界，将预测为1的区域和预测为0的区域分隔开来。gg 非线性边界 先看下面的数据 使用这样的模型去拟合数据 \\[ h_\\theta(x) = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_1^2 + \\theta_4x_2^2) , \\; \\theta = (-1, 0, 0, 1, 1), \\; 即\\, \\color{red}{h_\\theta(x) = g(-1+x_1^2 + x_2^2)} \\] 对于更复杂的情况，可以用更复杂的模型去拟合，如\\(x_1x_2, x_1x_2^2\\)等 代价函数和梯度下降 我们知道线性回归中的代价函数是\\(J(\\theta_0, \\theta_1) = \\frac {1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)^2\\)，但是由于逻辑回归的模型是\\(\\color{red}{h_\\theta(x) = \\frac {1}{1+e^{-\\theta^Tx}}}\\)，所以代价函数关于\\(\\theta\\)的图像就是一个非凸函数，容易达到局部收敛，如下图左边所示。而右边，则是一个凸函数，有全局最小值。 代价函数 逻辑回归的代价函数 \\[ \\color{red}{ Cost(h_\\theta(x), y) = \\begin{cases} -\\log(h_\\theta(x)),\\; &amp; y=1 \\\\ -\\log(1-h_\\theta(x)), \\; &amp; y=0 \\\\ \\end{cases} } \\] 当实际上\\(y=1\\)时，若预测为0，则代价会无穷大。当实际上\\(y=0\\)时，若预测为1，则代价会无穷大。 整理代价函数如下 \\[ \\color{red}{Cost(h_\\theta(x), y) = -y \\cdot \\log(h_\\theta(x)) - (1-y) \\cdot \\log(1- h_\\theta(x))} \\] 得到所有的\\(\\color{red}{J(\\theta)}\\) \\[ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left( y^{(i)}\\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log(1-h_\\theta(x^{(i)})) \\right) \\] 梯度下降 逻辑回归的假设函数估计\\(y=1\\)的概率 \\(\\color{red}{h_\\theta(x) = \\frac {1}{1+e^{-\\theta^Tx}}}\\)。 代价函数\\(\\color{red}{J(\\theta)}\\)，求参数\\(\\theta\\)去\\(\\color{red}{\\min \\limits_{\\theta} J(\\theta)}\\) 对每个参数\\(\\theta_j\\)，依次更新参数 \\[ \\color{red} {\\theta_j = \\theta_j -\\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left(h_\\theta(x^{(i)})-y^{(i)} \\right) \\cdot x_j^{(i)}} \\] 逻辑回归虽然梯度下降的式子和线性回归看起来一样，但是实际上\\(h_\\theta(x)\\)和\\(J(\\theta)\\)都不一样，所以是不一样的。 gg","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://plmsmile.github.io/tags/逻辑回归/"},{"name":"梯度下降","slug":"梯度下降","permalink":"http://plmsmile.github.io/tags/梯度下降/"}]},{"title":"最大期望算法","date":"2017-08-13T10:37:48.000Z","path":"2017/08/13/em/","text":"EM算法定义 背景 如果概率模型的变量都是观测变量，那么可以直接使用极大似然估计法或贝叶斯估计法去估计模型参数。 如果模型既有观测变量又有隐变量，就不能简单使用上述方法。 EM算法，期望极大算法，就是含有隐变量的概率模型参数的极大似然估计法或极大后验概率估计法，是一种迭代算法。每次迭代分为如下两步 E步：求期望(expectation) M步：求极大(maximization) 三硬币模型 有3枚硬币，记做ABC，每次出现正面的概率分别是\\(\\pi, p, q\\)。先掷A，正面选B，反面选C。再掷B/C，得到正面1或反面0作为一次的结果。问：去估计参数\\(\\theta = (\\pi, p, q)\\) 观察变量：一次 实验得到的结果1或0，记做\\(y\\) 隐变量：A的结果，即掷的是B还是C，记做\\(z\\) 对于一次实验，求出\\(y\\)的概率分布 \\[ \\begin{align*} \\color{blue}{P(y \\mid \\theta)} &amp;= \\underbrace{\\sum_zP(y, z \\mid \\theta)}_{\\color{red}{把所有z的y加起来}} = \\sum_z \\underbrace{P(z \\mid \\theta)P(y \\mid z, \\theta)}_{\\color{red}{贝叶斯公式}} = \\underbrace{\\pi p^y(1-p)^{1-y}}_{\\color{red}{z=1时}}+\\underbrace{(1-\\pi)q^y(1-q)^{1-y}}_{\\color{red}{z=0时}} \\end{align*} \\] 设观察序列\\(Y=(y_1, y_2, \\cdots, y_n)^T\\)，隐藏数据\\(Z=(z_1, z_2, \\cdots, z_n)^T\\)，则观测数据的似然函数为 \\[ \\begin{align*} \\color{blue}{P(Y \\mid \\theta)} &amp;= \\sum_Z \\color{red}{P(Z \\mid \\theta)P(Y \\mid Z, \\theta)} \\\\ &amp;= \\prod_{i=1}^n[\\pi p^{y_i}(1-p)^{1-y_i} + (1-\\pi)q^{y_i}(1-q)^{1-y_i}] \\end{align*} \\] 求模型参数\\(\\theta = (\\pi, p, q)\\)的最大似然估计，即 \\[ \\hat\\theta = arg max_\\theta \\log P(Y \\mid \\theta) \\] 这个问题不能直接求解，只有通过迭代的方法求解。EM算法就是解决这种问题的一种迭代算法。先给\\(\\theta^{(0)}\\) 选择初始值，然后去迭代。每次迭代分为E步和M步。 EM算法 基本概念 一些概念如下 \\(Y\\) 观测变量，\\(Z\\) 隐变量 不完全数据：\\(Y\\)；概率分布：\\(P(Y \\mid \\theta)\\)；对数似然函数：\\(\\color{red} {L(\\theta) = \\log P(Y \\mid \\theta)}\\) 完全数据：\\(Y\\)和\\(Z\\)合在一起；概率分布：\\(P(Y, Z \\mid \\theta)\\)；对数似然函数：\\(\\log P(Y, Z \\mid \\theta)\\) EM算法通过迭代求\\(L(\\theta) = \\log P(Y \\mid \\theta)\\)的极大似然估计。 概率论函数的期望 设\\(Y\\)是随机变量\\(X\\)的函数，\\(Y = g(X)\\)，\\(g\\)是连续函数，那么 \\(X\\)是离散型变量，\\(X\\)的分布律为\\(P(X = x_i) = p_i, \\; i=1,2,3\\cdots\\)，则有 \\[ E(Y) = E(g(X)) = \\sum_{i=1}^{\\infty}g(x_i)p_i, \\quad 左式收敛时成立 \\] \\(X\\)是 连续型变量，\\(X\\)的概率密度为\\(f(x)\\)，则有 \\[ E(Y) = E(g(X)) = \\int_{-\\infty}^{+\\infty} {g(x)f(x)} \\, {\\rm d}x, \\quad 左式绝对收敛成立 \\] Q函数 \\(\\color{blue}{Q(\\theta, \\theta^{(i)})}\\)是EM算法的核心。它是完全数据的对数似然函数\\(\\log P(Y,Z \\mid \\theta)\\)的期望，是关于未观测数据\\(Z\\)的条件概率分布\\(P(Z \\mid Y, \\theta^{(i)})\\)，而\\(Z\\)的条件是在给定观测数据\\(Y\\)和当前参数\\(\\theta^{(i)}\\)。（都是后置定语，不通顺） \\[ \\begin {align*} \\color{blue}{Q(\\theta, \\theta^{(i)})} &amp;= E_Z[\\log P(Y, Z \\mid \\theta) \\mid Y, \\theta^{(i)}] = \\sum_Z \\color{red}{\\log P(Y, Z \\mid \\theta)P(Z \\mid Y, \\theta^{(i)})} \\end {align*} \\] 下面是我具体的理解 \\(\\color{blue}{\\theta^{(i)}}\\)是第\\(i\\)次迭代参数\\(\\theta\\)的估计值 \\(P(Z \\mid Y, \\theta^{(i)})\\)是以\\(Y\\)和当前参数\\(\\theta^{(i)}\\)的条件下的分布律，简写为\\(P(Z)\\)。类似于上面的\\(X\\) \\(P(Y, Z \\mid \\theta)\\) 是在以\\(\\theta\\)为参数的分布的联合概率密度，简写为\\(P(Y, Z)\\)。类似于上面的\\(Y=g(X)\\) 求对数似然函数\\(\\log P(Y, Z)\\)的期望，转移到隐变量\\(Z\\)上 把目标函数映射到\\(Z\\)上，\\(g(z) = \\log P(Y, Z)\\)，\\(E(g(z)) = \\sum_z \\log P(Y, Z) P(Z)\\) \\(\\color{blue}{Q(\\theta, \\theta^{(i)})}\\) 是因为要找到一个新的\\(\\theta\\)优于之前的\\(\\theta^{(i)}\\)，是代表的分布优于 EM算法步骤 输入：\\(\\color{blue}{Y}\\) 观测变量，\\(\\color{blue}{Z}\\) 隐藏变量，\\(\\color{blue}{P(Y, Z \\mid \\theta)}\\) 联合分布，条件分布 \\(\\color{blue}{P(Z \\mid Y, \\theta)}\\) 输出：模型参数\\(\\color{blue}{\\theta}\\) 步骤 选择参数初始值\\(\\color{blue}{\\theta^{(0)}}\\)，开始迭代 E步：第\\(i+1\\)次迭代， 求 \\(\\color{blue}{Q(\\theta, \\theta^{(i)})} = \\sum_Z \\color{red}{\\log P(Y, Z \\mid \\theta)P(Z \\mid Y, \\theta^{(i)})}\\) M步：求使\\(Q(\\theta, \\theta^{(i)})\\)极大化的\\(\\theta\\)，得到\\(i+1\\)次迭代新的估计值\\(\\color{blue}{\\theta^{(i+1)}} = \\color{red}{arg \\, max_\\theta (Q(\\theta, \\theta^{(i)}))}\\) 重复E和M步，直到收敛 Jensen不等式 凸函数与凹函数 从图像上讲，在函数上两点连接一条直线。直线完全在图像上面，就是凸函数 convex；完全在下面，就是凹函数 concave。 \\(f^{\\prime\\prime}(x) \\ge 0 \\implies f(x) 是凸函数; \\quad f^{\\prime\\prime}(x) \\le 0 \\implies f(x)是凹函数\\) Jensen不等式 函数\\(f(x)​\\)，上有两点\\(x_1, x_2​\\)，对于任意\\(\\lambda \\in [0,1]​\\) 如果\\(f(x)\\)是凸函数，\\(f(\\lambda x_1+(1-\\lambda)x_2) \\le \\lambda f(x_1) + (1-\\lambda)f(x_2)\\) 如果\\(f(x)\\)是凹函数，\\(f(\\lambda x_1+(1-\\lambda)x_2) \\ge \\lambda f(x_1) + (1-\\lambda)f(x_2)\\) 一般地，\\(n\\)个点\\(x_1, x_2, \\cdots, x_n\\)和参数\\(\\lambda_1 + \\lambda_2 + \\cdots + \\lambda_n = 1\\)，对于凸函数，则有 \\[ f(\\underbrace{\\lambda_1x_1 + \\lambda_2x_2 + \\cdots + \\lambda_nx_n}_{\\color{red}{E[X], 总体是f(E[X])}}) \\le \\underbrace{f(\\lambda_1x_1) + f(\\lambda_2x_2) + \\cdots + f(\\lambda_nx_n)}_{\\color{red}{E[f(X)]}}, \\;即\\color{red}{f(\\sum_{i=1}^n\\lambda_ix_i) \\leq \\sum_{i=1}^nf(\\lambda_ix_i)} \\] 琴声不等式 凸函数 \\(f(E[X]) \\le E[f(X)]\\)，即\\(\\color{red}{f(\\sum_{i=1}^n\\lambda_ix_i) \\leq \\sum_{i=1}^nf(\\lambda_ix_i)}\\) 凹函数 \\(f(E[X]) \\ge E[f(X)]\\)，即\\(\\color{red}{f(\\sum_{i=1}^n\\lambda_ix_i) \\geq \\sum_{i=1}^nf(\\lambda_ix_i)}\\)","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"最大期望算法","slug":"最大期望算法","permalink":"http://plmsmile.github.io/tags/最大期望算法/"},{"name":"Jesen不等式","slug":"Jesen不等式","permalink":"http://plmsmile.github.io/tags/Jesen不等式/"}]},{"title":"概率图模型(一)","date":"2017-08-04T03:06:41.000Z","path":"2017/08/04/pgm-01/","text":"概述 产生式和判别式 判别方法 由数据直接去学习决策函数\\(Y=f(X)\\) 或者\\(P(Y \\mid X)\\)作为预测模型 ，即判别模型 生成方法 先求出联合概率密度\\(P(X, Y)\\)，然后求出条件概率密度\\(P(Y \\mid X)\\)。即生成模型\\(P(Y \\mid X) = \\frac {P(X, Y)} {P(X)}\\) 判别式 生成式 原理 直接求\\(Y=f(X)\\) 或\\(P(Y \\mid X)\\) 先求\\(P(X,Y)\\)，然后 \\(P(Y \\mid X) = \\frac {P(X, Y)} {P(X)}\\) 差别 只关心差别，根据差别分类 关心数据怎么生成的，然后进行分类 应用 k近邻、感知机、决策树、LR、SVM 朴素贝叶斯、隐马尔可夫模型 概率图模型 概率图模型(probabilistic graphical models) 在概率模型的基础上，使用了基于图的方法来表示概率分布。节点表示变量，边表示变量之间的概率关系 概率图模型便于理解、降低参数、简化计算，在下文的贝叶斯网络中会进行说明。 贝叶斯网络 贝叶斯网络 又称为信度网络或者信念网络（belief networks），实际上就是一个有向无环图。 节点表示随机变量；边表示条件依存关系。没有边说明两个变量在某些情况下条件独立或者说是计算独立，有边说明任何条件下都不条件独立。 如上图所示，要表示上述情况的概率只需要求出\\(4*2*2*2*2-1=63\\)个参数的联合概率密度就行了，实际上这个太难以求得。我们可以考虑一下独立关系\\((F \\perp H \\mid S) \\,\\,\\, 表示在S确定的情况下，F和H独立\\)，所以有以下独立关系： \\[ (F \\perp H \\mid S)、\\, (C \\perp S \\mid F,H)、\\, (M \\perp H, C \\mid F)、 \\, (M \\perp C | F) \\] 所以我们得到如下的计算独立假设： \\[ P(C \\mid FHS) = P(C \\mid FH)，即假设C只与FH有关，而与S无关 \\] 又由\\(P(AB)=P(A|B)P(B)\\)，所以得到联合概率分布： \\[ \\begin{align*} P(SFHMC) &amp;= P(M \\mid SHFC) \\cdot P(SHFC) = \\underbrace {P(M \\mid F)}_{\\color {red}{计算独立性}} \\cdot \\underbrace {P(C \\mid SHF) \\cdot P(SHF)}_{\\color{red}{继续分解}} \\\\ &amp;= P(M \\mid F) \\cdot P(C \\mid FH) \\cdot P(F \\mid S) \\cdot P(H \\mid S) \\cdot P(S) \\end{align*} \\] \\(P(S)\\) 4个季节，需要3个参数；\\(P(H \\mid S)\\)时，\\(P(Y \\mid Spring)\\) 和 \\(P(N \\mid Spring)\\)只需要一个参数，所以\\(P(H \\mid S)\\)只需要4个参数即可，其他同理。 所以联合概率密度就转化成了上述公式中的5个乘积项，其中每一项需要的参数个数分别是2、4、4、4、3，所以一共只需要17个参数，这就大大降低了参数的个数。 马尔可夫模型 简介 马尔可夫模型(Markov Model) 描述了一类重要的随机过程，未来只依赖于现在，不依赖于过去。这样的特性的称为马尔可夫性，具有这样特性的过程称为马尔可夫过程。 时间和状态都是离散的马尔可夫过程称为马尔可夫链，简称马氏链，关键定义如下 系统有\\(N\\)个状态\\(S = \\{ s_1, s_2, \\cdots, s_N\\}\\)，随着时间的推移，系统将从某一状态转移到另一状态 设\\(q_t \\in S\\)是系统在\\(t\\)时刻的状态，\\(Q = \\{q_q, q_2, \\cdots, q_T \\}\\)系统时间的随机变量序列 一般地，系统在时间\\(t\\)时的状态\\(s_j\\)取决于\\([1, t-1]\\)的所有状态\\(\\{q_1, q_2, \\cdots, q_{t-1}\\}\\)，则当前时间的概率是 \\[ P(q_t = s_j \\mid q_{t-1} = s_i, q_{t-2} = s_k, \\cdots) \\] 在时刻\\(m\\)处于\\(s_i\\)状态，那么在时刻\\(m+n\\)转移到状态\\(s_j\\)的概率称为转移概率，即从时刻\\(m \\to m+n\\)： \\[ \\color{blue} {P_{ij}(m, m+n)} = P(q_{m+n} = s_j \\mid q_m = s_i) \\] 如果\\(P_{ij}(m, m+n)\\)只与状态\\(i, j\\)和步长\\(n\\)有关，而与起始时间\\(m\\)无关，则记为\\(\\color {blue} {P_{ij}(n)}\\),称为n步转移概率。 并且称此转移概率具有平稳性，且称此链是齐次的，称为齐次马氏链，我们重点研究齐次马氏链。\\(P(n) = [P_{ij}(n)]\\)称为n步转移矩阵。 \\[ P_{ij}(m, m+n) =\\color {blue} {P_{ij}(n)} = P(q_{m+n} = s_j \\mid q_m = s_i) \\] 特别地，\\(n = 1\\)时，有一步转移概率如下 \\[ p_{ij} = P_{ij}(1) = P(q_{m+1} \\mid q_{m}) = a_{ij} \\] 一阶马尔可夫 特别地，如果\\(t\\)时刻状态只与\\(t-1\\)时刻状态有关，那么下有离散的一阶马尔可夫链如下： \\[ P(q_t = s_j \\mid q_{t-1} = s_i, q_{t-2} = s_k, \\cdots) = P(q_t = s_j\\mid q_{t-1} = s_i) \\] 其中\\(t-1​\\)的状态\\(s_i​\\)转移到\\(t​\\)的状态\\(s_j​\\)的概率定义如下： \\[ P(q_t = s_j\\mid q_{t-1} = s_i) = \\color{blue} {a_{ij}}，其中i, j \\in [1, N]，a_{ij} \\ge 0，\\sum_{j=1}^Na_{ij} = 1 \\] 显然，\\(N​\\)个状态的一阶马尔可夫链有\\(N^2​\\)次状态转移，这些概率\\(a_{ij}​\\)构成了状态转移矩阵。 \\[ A = [a_{ij}] = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\\\ \\end{bmatrix} \\] 设系统在初始状态的概率向量是 \\(\\color{blue} {\\pi_i} \\ge 0\\) ，其中，\\(\\sum_{i=1}^{N}\\pi_i = 1\\) 那么时间序列\\(Q = \\{q_1, q_2, \\cdots, q_T \\}\\)出现的概率是 \\[ \\color{blue} {P(q_1, q_2, \\cdots, q_T) } = P(q_1) P(q2 \\mid q_1) P(q_3 \\mid q_2) \\cdots P(q_T \\mid q_{T-1}) = \\color{red} {\\underbrace {\\pi_{q_1}}_{初态概率} \\prod_{t=1}^{T-1} a_{q_tq_{t+1}}} \\] 下图是一个例子 多步转移概率 对于齐次马氏链，多步转移概率就是\\(u+v\\)时间段的状态转移，可以分解为先转移\\(u\\)步，再转移\\(v\\)步。则有CK方程的矩阵形式 \\[ P(u+v) = P(u)P(v) \\] 由此得到\\(n\\)步转移概率矩阵是一次转移概率矩阵的\\(n\\)次方 \\[ P(n) = P(1) P(n-1) = PP(n-1) \\implies P(n) = P^n \\] 对于求矩阵的幂\\(A^n\\)，则最好使用相似对角化来进行矩阵连乘。 存在一个可逆矩阵P，使得\\(P^{-1}AP = \\Lambda，A = P \\Lambda P^{-1}\\)，其中\\(\\Lambda\\)是矩阵\\(A\\)的特征值矩阵 \\[ \\Lambda = \\begin{bmatrix} \\lambda_1 &amp; &amp; &amp; \\\\ &amp;\\lambda_2 &amp; &amp; \\\\ &amp;&amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\lambda_n \\\\ \\end{bmatrix} ，其中\\lambda是矩阵A的特征值 \\] 则有\\(A^n = P\\Lambda ^ {n}P^{-1}\\) 遍历性 齐次马氏链，状态\\(i\\)向状态\\(j\\)转移，经过无穷步，任何状态\\(s_i\\)经过无穷步转移到状态\\(s_j\\)的概率收敛于一个定值\\(\\pi_j\\)，即\\(\\lim_{n \\to \\infty} P_{ij}(n) = \\pi_j \\; (与i无关)\\) 则称此链具有遍历性。若\\(\\sum_{j=1}^N \\pi_j = 1\\)，则称\\(\\vec{\\pi} = (\\pi_1, \\pi_2, \\cdots)\\)为链的极限分布。 遍历性的充分条件：如果存在正整数\\(m\\)(步数)，使得对于任意的，都有如下（转移概率大于0），则该马氏链具有遍历性 \\[ P_{ij}(m) &gt; 0, \\quad i, j =1, 2, \\cdots, N, \\quad s_i,s_j \\in S \\; \\] 那么它的极限分布\\(\\vec{\\pi} = (\\pi_1, \\pi_2, \\cdots, \\pi_N)​\\)，它是下面方程组的唯一解 \\[ \\pi = \\pi P, \\quad 即\\pi_j = \\sum_{i=1}^{N} \\pi_i p_{ij}, \\quad 其中\\pi_j &gt; 0, \\sum_{j=1}^N \\pi_j = 1 \\] PageRank应用 有很多应用，压缩算法、排队论等统计建模、语音识别、基因预测、搜索引擎鉴别网页质量-PR值。 Page Rank算法 这是Google最核心的算法，用于给每个网页价值评分，是Google“在垃圾中找黄金”的关键算法。 大致思想是要为搜索引擎返回最相关的页面。页面相关度是由和当前网页相关的一些页面决定的。 当前页面会把自己的importance平均传递给它所指向的页面，若有\\(k\\)个，则为每个传递\\(\\frac 1 k\\) 如果有很多页面都指向当前页面，则当前页面很重要，相关度高 当前页面有一些来自官方页面的backlink，当前页面很重要 例如有4个页面，分别如下 矩阵\\(\\color {blue }A\\)是页面跳转的一次转移矩阵，\\(\\color {blue }q\\)是当前时间每个页面的相关度向量，即PageRank vector。 \\[ A = \\begin{bmatrix} 0 &amp; 0 &amp;1 &amp; \\frac {1}{2} \\\\ \\frac {1}{3}&amp; 0 &amp; 0 &amp; 0 \\\\ \\frac {1}{3}&amp; \\frac {1}{2} &amp; 0 &amp; \\frac {1}{2} \\\\ \\frac {1}{3} &amp; \\frac {1}{2} &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\quad 初始时刻，q = \\begin {bmatrix} \\frac1 4 \\\\ \\frac1 4 \\\\ \\frac1 4 \\\\ \\frac1 4 \\\\ \\end {bmatrix} \\] \\(A\\)的一列是当前页面出去的所有页面，一行是进入当前页面的所有页面。设\\(u\\)表示第\\(A\\)的第\\(i\\)行，那么\\(u*q\\)就表示当页面\\(i\\)接受当前\\(q\\)的更新后的rank值。 定义矩阵\\(\\color {blue} {G} = \\color{red} {\\alpha A + (1-\\alpha) \\frac {1} {n} U}\\)，对\\(A\\)进行修正，\\(G\\)所有元素大于0，具有遍历性 \\(\\alpha \\in[0, 1] \\; (\\alpha = 0.85)\\) 阻尼因子 \\(A\\) 一步转移矩阵 \\(n\\) 页面数量 \\(U\\) 元素全为\\(1\\)的矩阵 使用\\(G\\)进行迭代的好处 解决了很多\\(A\\)元素为0导致的问题，如没有超链接的节点，不连接的图等 \\(A\\)所有元素大于0，具有遍历性，具有极限分布，即它的极限分布\\(q\\)会收敛 那么通过迭代就可以求出PR向量\\(\\color {red} {q^{next} = G q^{cur}}\\)，实际上\\(q\\)是\\(G\\)的特征值为1的特征向量。 迭代具体计算如下图(下图没有使用G，是使用A去算的，这是网上找的图[捂脸]) 随着迭代，\\(q\\)会收敛，那么称为\\(q\\)就是PageRank vector。 我们知道节点1有2个backlink，3有3个backlink。但是节点1却比3更加相关，这是为什么呢？因为节点3虽然有3个backlink，但是却只有1个outgoing，只指向了页面1。这样的话它就把它所有的importance都传递给了1，所以页面1也就比页面3的相关度高。 隐马尔可夫模型 定义 隐马尔可夫模型（Hidden Markov Model， HMM）是统计模型，它用来描述含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数，然后利用这些参数来做进一步的分析。大概形状如下 一个HMM由以下5个部分构成。 隐藏状态 模型的状态，隐蔽不可观察 有\\(N\\)种，隐状态种类集合\\(\\color {blue} {S = \\{s_1, s_2, \\cdots, s_N\\}}\\)会相 隐藏状态互相互转换，一步转移。\\(s_i\\)转移到\\(s_j\\)的概率 \\(\\color {red} {a_{ij} = P(q_t= s_j \\mid q_{t-1}=s_i)}\\) \\(q_t = s_i\\) 代表在\\(t\\)时刻，系统隐藏状态\\(q_t\\)是\\(s_i\\) 隐状态时间序列 \\(\\color{blue}{Q = \\{q_1, q_2, \\cdots, q_t, q_{t+1}\\cdots \\}}\\) 观察状态 模型可以显示观察到的状态 有\\(M\\)种，显状态种类集合\\(\\color{blue} {K = \\{v_1, v_2, \\cdots, v_M\\}}\\)。不能相互转换，只能由隐状态产生(发射) \\(o_t = v_k​\\) 代表在\\(t​\\)时刻，系统的观察状态\\(o_t​\\)是\\(v_k​\\) 每一个隐藏状态会发射一个观察状态。\\(s_j\\)发射符号\\(v_k\\)的概率\\(\\color {red} {b_j (k) = P(o_t = v_k \\mid s_j)}\\) 显状态时间序列 \\(\\color{blue} {O = \\{o_1, o_2, \\cdots, o_ t\\}}\\) 状态转移矩阵A (隐--隐) 从一个隐状\\(s_i\\)转移到另一个隐状\\(s_j\\)的概率。\\(A = \\{a_{ij}\\}\\) \\(\\color {red} {a_{ij} = P(q_t= s_j \\mid q_{t-1}=s_i)}\\)，其中 \\(1 \\leq i, j \\leq N, \\; a_{ij} \\geq 0, \\; \\sum_{j=1}^N a_{ij}=1\\) 发射概率矩阵B (隐--显) 一个隐状\\(s_j\\)发射出一个显状\\(v_k\\)的概率。\\(B = \\{b_j(k)\\}\\) \\(\\color {red} {b_j(k) = P(o_t = v_k \\mid s_j)}\\)，其中\\(1 \\leq j \\leq N; \\; 1 \\leq k \\leq M; \\; b_{jk} \\ge 0; \\; \\sum_{k=1}^Mb_{jk}=1\\) 初始状态概率分布 \\(\\pi\\) 最初的隐状态\\(q_1=s_i\\)的概率是\\(\\pi_i = P(q_1 = s_i)\\) 其中\\(1 \\leq i \\leq N, \\; \\pi_i \\ge 0, \\; \\sum_{i=1}^N \\pi_i = 1\\) 一般地，一个HMM记作一个五元组\\(\\mu = (S, K, A, B, \\pi)\\)，有时也简单记作\\(\\mu = (A, B, \\pi)\\)。一般，当考虑潜在事件随机生成表面事件的时候，HMM是非常有用的。 HMM中的三个问题 观察序列概率 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)和模型\\(\\mu = (A, B, \\pi)\\)，求当前观察序列\\(O\\)的出现概率\\(P(O \\mid \\mu)\\) 状态序列概率 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)和模型\\(\\mu = (A, B, \\pi)\\)，求一个最优的状态序列\\(Q=\\{q_1, q_2, \\cdots, q_T\\}\\)的出现概率，使得最好解释当前观察序列\\(O\\) 训练问题或参数估计问题 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)，调节模型\\(\\mu = (A, B, \\pi)\\)参数，使得\\(P(O \\mid u)\\)最大 前后向算法 给定观察序列\\(O=\\{o_1, o_2, \\cdots, o_T\\}\\)和模型\\(\\mu = (A, B, \\pi)\\)，求给定模型\\(\\mu\\)的情况下观察序列\\(O\\)的出现概率。这是解码问题。如果直接去求，计算量会出现指数爆炸，那么会很不好求。我们这里使用前向算法和后向算法进行求解。 前向算法 前向变量\\(\\color {blue} {\\alpha_t(i)}\\)是系统在\\(t\\)时刻，观察序列为\\(O=o_1o_2\\cdots o_t\\)并且隐状态为\\(q_t = s_i\\)的概率，即 \\[ \\color {red} {\\alpha_t(i) = P(o_1o_2\\cdots o_t, q_t = s_i \\mid \\mu)} \\] \\(\\color {blue} {P(O \\mid \\mu)}\\) 是在\\(t\\)时刻，状态\\(q_t=\\) 所有隐状态的情况下，输出序列\\(O\\)的概率之和 \\[ \\color {blue} {P(O \\mid \\mu)} = \\sum_{i=1}^N P(O, q_t = s_i \\mid \\mu) = \\color {red} {\\sum_{i=1}^{N}\\alpha_t(i)} \\] 接下来就是计算\\(\\color {blue} {\\alpha_t(i)}\\)，其实是有动态规划的思想，有如下递推公式 \\[ \\color {blue} {\\alpha_{t+1}(j)} = \\color{red}{\\underbrace{\\left( \\sum_{i=1}^N \\alpha_t(i)a_{ij} \\right)}_{所有状态i转为j的概率} \\underbrace {b_j(o_{ t+1})}_{状态j发射o_{t+1}}} \\] 上述计算，其实是分为了下面3步 从1到达时间\\(t\\)，状态为\\(s_i\\)，输出\\(o_1o_2 \\cdots o_t\\)。\\(\\color{blue}{\\alpha_t(i)}\\) 从\\(t\\)到达\\(t+1\\)，状态变化\\(s_i \\to s_j \\text{。} \\quad\\color{blue}{a_{ij}}\\) 在\\(t+1\\)时刻，输出\\(o_{t+1}\\)。\\(\\color{blue}{b_j(o_{ t+1})}\\) 算法的步骤如下 初始化 \\(\\color {blue} {\\alpha_1(i)} = \\color {red} {\\pi_ib_i(o_1)}, \\; 1 \\leq i \\leq N\\) 归纳计算 \\(\\color {blue} {\\alpha_{t+1}(j)} = \\color{red} {\\left( \\sum_{i=1}^N \\alpha_t(i)a_{ij} \\right) b_j(o_{ t+1})}, \\; 1 \\leq t \\leq T-1\\) 求和终结 \\(\\color {blue} {P(O \\mid \\mu)} = \\color {red} {\\sum_{i=1}^{N}\\alpha_T(i)}\\) 在每个时刻\\(t\\)，需要考虑\\(N\\)个状态转移到\\(s_{j}\\)的可能性，同时也需要计算\\(\\alpha_t(1), \\cdots , \\alpha_t(N)\\)，所以时间复杂度为\\(O(N^2)\\)。同时在系统中有\\(T\\)个时间，所以总的复杂度为\\(O(N^2T)\\)。 后向算法 后向变量 \\(\\color {blue} {\\beta_{t}(i)}\\) 是系统在\\(t\\)时刻，状态为\\(s_i\\)的条件下，输出为\\(o_{t+1}o_{t+2}\\cdots o_T\\)的概率，即 \\[ \\color {red} {\\beta_t(i) = P(o_{t+1}o_{t+2}\\cdots o_T \\mid q_t = s_i , \\mu)} \\] 递推 \\(\\color {blue} {\\beta_{t}(i)}\\)的思路及公式如下 从\\(t \\to t+1\\)，状态变化\\(s_i \\to s_j\\)，并从\\(s_j \\implies o_{t+1}\\)，发射\\(o_{t+1}\\) 在\\(q_{t+1}=s_j\\)的条件下，输出序列\\(o_{t+2}\\cdots o_T\\) \\[ \\color {blue} {\\beta_{t}(i)} = \\sum_{j=1}^N\\color{red}{\\underbrace {a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}_{s_i转s_j \\; s_j发o_{t+1} \\; t+1时s_j后面\\{o_{t+2}, \\cdots\\}} } \\] 上面的公式个人的思路解释如下(不明白公式再看) 其实要从\\(\\beta_{t+1}(j) \\to \\beta_{t}(i)\\) \\(\\beta_{t+1}(j)\\)是\\(t+1\\)时刻状态为\\(s_j\\)，后面的观察序列为\\(o_{t+2}, \\cdots, o_{T}\\) \\(\\beta_{t}(i)\\)是\\(t\\)时刻状态为\\(s_i\\)，后面的观察序列为\\(\\color{red}{o_{t+1}}, o_{t+2}, \\cdots, o_{T}\\) \\(t \\to t+1\\) \\(s_i\\)会变成各种\\(s_j\\)，\\(\\beta_t(i)\\)只关心t+1时刻的显示状态为\\(o_{t+1}\\)，而不关心隐状态，所以是所有隐状态发射\\(o_{t+1}\\)的概率和 \\(\\color{red} {a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}\\)，\\(s_i\\)转为\\(s_j\\)的概率，在t+1时刻\\(s_j\\)发射\\(o_{t+1}\\)的概率，t+1时刻状态为\\(s_j\\) 观察序列为\\(o_{t+2}, \\cdots, o_{T}\\)的概率 把上述概率加起来，就得到了t时刻为\\(s_i\\),后面的观察为\\(o_{t+1}, o_{t+2}, \\cdots, o_{T}\\)的概率\\(\\beta_{t}(i)\\) 上式是把所有从\\(t+1 \\to t\\)的概率加起来，得到\\(t\\)的概率。算法步骤如下 初始化 \\(\\color {blue} {\\beta_T(i) = 1}, \\; 1 \\leq i \\leq N\\) 归纳计算 \\(\\color {blue} {\\beta_{t}(i)} = \\sum_{j=1}^N\\color{red}{a_{ij}b_j(o_{t+1})\\beta_{t+1}(j) }, \\quad 1 \\leq t \\leq T-1; \\; 1 \\leq i \\leq N\\) 求和终结 \\(\\color {blue} {P(O \\mid \\mu)} = \\sum_{i=1}^{N} \\color{red} {\\pi_i b_i(o_1)\\beta_1(i)}\\) 前后向算法结合 模型\\(\\mu\\)，观察序列\\(O=\\{o_1, o_2, \\cdots, o_t, o_{t+1}\\cdots, o_T\\}\\)，\\(t\\)时刻状态为\\(q_t=s_i\\)的概率如下 \\[ \\color {blue} {P(O, q_t = s_i \\mid \\mu)} = \\color{red} {\\alpha_t(i) \\times \\beta_t(i)} \\] 推导过程如下 \\[ \\begin{align*} P(O, q_t = s_i \\mid \\mu) &amp;= P(o_1\\cdots o_T, q_t=s_i \\mid \\mu) =P(o_1 \\cdots o_t, q_t=s_i, o_{t+1} \\cdots o_T \\mid \\mu) \\\\ &amp;= P(o_1 \\cdots o_t, q_t=s_i \\mid \\mu) \\times P(o_{t+1} \\cdots o_T \\mid o_1 \\cdots o_t, q_t=s_i, \\mu) \\\\ &amp;= \\alpha_t(i) \\times P((o_{t+1} \\cdots o_T \\mid q_t=s_i, \\mu) \\quad (显然o_1 \\cdots o_t是显然成立的，概率为1，条件忽略)\\\\ &amp;= \\alpha_t(i) \\times \\beta_t(i) \\end{align*} \\] 所以，把\\(q_t\\)等于所有\\(s_i\\)的概率加起来就可以得到观察概率\\(\\color{blue} {P(O \\mid \\mu)}\\) \\[ \\color{blue} {P(O \\mid \\mu)} = \\sum_{i=1}^N\\ \\color{red} {\\alpha_t(i) \\times \\beta_t(i)}, \\quad 1 \\leq t \\leq T \\] 维特比算法 维特比(Viterbi)算法用于求解HMM的第二个问题状态序列问题。即给定观察序列\\(O=o_1o_2\\cdots o_T\\)和模型\\(\\mu = (A, B, \\pi)\\)，求一个最优的状态序列\\(Q=q_1q_2 \\cdots q_T\\)。 有两种理解最优的思路。 使该状态序列中每一个状态都单独地具有最大概率，即\\(\\gamma_t(i) = P(q_t = s_i \\mid O,\\mu)\\)最大。但可能出现\\(a_{q_tq_{t+1}}=0\\)的情况 另一种是，使整个状态序列概率最大，即\\(P(Q \\mid O, \\mu)\\)最大。\\(\\hat{Q} = arg \\max \\limits_Q P(Q \\mid O, \\mu)\\) 维特比变量 \\(\\color{blue}{\\delta_t(i)}\\)是，在\\(t\\)时刻，\\(q_t = s_i\\) ，HMM沿着某一条路径到达状态\\(s_i\\)，并输出观察序列\\(o_1o_2 \\cdots o_t\\)的概率。 \\[ \\color{blue}{\\delta_t(i)} = \\arg \\max \\limits_{q_1\\cdots q_{t-1}} P(q_1 \\cdots q_{t-1}, q_t = s_i, o_1 \\cdots o_t \\mid \\mu) \\] 递推关系 \\[ \\color{blue}{\\delta_{t+1}(i)} = \\max \\limits_j [\\delta_t(j) \\cdot a_{ji}] \\cdot b_i(o_{t+1}) \\] 路径记忆变量 \\(\\color{blue}{\\psi_t(i) = k}\\) 表示\\(q_t = s_i, q_{t-1} = s_k\\)，即表示在该路径上状态\\(q_t=s_i\\)的前一个状态\\(q_{t-1} = s_k\\)。 维特比算法步骤 初始化 \\(\\delta_1(i) = \\pi_ib_i(o_1), \\; 1 \\le i \\le N\\)，路径变量\\(\\psi_1(i) = 0\\) 归纳计算 维特比变量 \\(\\delta_t(j) = \\max \\limits_{1 \\le i \\le N} [\\delta_{t-1}(i) \\cdot a_{ij}] \\cdot b_j(o_t), \\quad 2 \\le t \\le T; 1 \\le j \\le N\\) 记忆路径(记住参数\\(i\\)就行) \\(\\psi_t(j) = \\arg \\max \\limits_{1 \\le i \\le N} [\\delta_{t-1}(i) \\cdot a_{ij}] \\cdot b_j(o_t), \\quad 2 \\le t \\le T; 1 \\le j \\le N\\) 终结 \\[ \\hat{Q_T} = \\arg \\max \\limits_{1 \\le i \\le N} [\\delta_T(i)], \\quad \\hat P(\\hat{Q_T}) = \\max \\limits_{1 \\le i \\le N} [\\delta_T(i)] \\] 路径（状态序列）回溯 \\(\\hat{q_t} = \\psi_{t+1}(\\hat{q}_{t+1}), \\quad t = T-1, T-2, \\cdots, 1\\) Baum-Welch算法 Baum-Welch算法用于解决HMM的第3个问题，参数估计问题，给定一个观察序列\\(O= o_1 o_2 \\cdots o_T\\)，去调节模型\\(\\mu = (A, B, \\pi)\\)的参数使得\\(P(O\\mid \\mu)\\)最大化，即\\(\\mathop{argmax} \\limits_{\\mu} P(O_{training} \\mid \\mu)\\)。模型参数主要是\\(a_{ij}, b_j(k) \\text{和}\\pi_i\\)，详细信息见上文。 有完整语料库 如果我们知道观察序列\\(\\color{blue}{O= o_1 o_2 \\cdots o_T}\\)和状态序列\\(\\color{blue}{Q = q_1 q_2 \\cdots q_T}\\)，那么我们可以根据最大似然估计去计算HMM的参数。 设\\(\\delta(x, y)\\)是克罗耐克函数，当\\(x==y\\)时为1，否则为0。计算步骤如下 \\[ \\begin{align*} &amp; 初始概率\\quad \\color{blue}{\\bar\\pi_i} = \\delta(q_1, s_1) \\\\ &amp; 转移概率\\quad \\color{blue}{\\bar {a}_{ij}} = \\frac{s_i \\to s_j的次数}{s_i \\to all的次数} = \\frac {\\sum_{t=1}^{T-1} \\delta(q_t, s_i) \\times \\delta(q_{t+1}, s_j)} { \\sum_{t=1}^{T-1}\\delta(q_t, s_i)} \\\\ &amp; 发射概率 \\quad \\color{blue}{\\bar{b}_j(k)} = \\frac{s_j \\to v_k 的次数}{Q到达q_j的次数} = \\frac {\\sum_{t=1}^T\\delta(q_t, s_i) \\times \\delta(o_t, v_k)}{ \\sum_{t=1}^{T}\\delta(q_t, s_j)} \\end{align*} \\] 但是一般情况下是不知道隐藏状态序列\\(Q​\\)的，还好我们可以使用期望最大算法去进行含有隐变量的参数估计。主要思路如下。 我们可以给定初始值模型\\(\\mu_0\\)，然后通过EM算法去估计隐变量\\(Q\\)的期望来代替实际出现的次数，再通过上式去进行计算新的参数得到新的模型\\(\\mu_1\\)，再如此迭代直到参数收敛。 这种迭代爬山算法可以局部地使\\(P(O \\mid \\mu)\\)最大化，BW算法就是具体实现这种EM算法。 Baum-Welch算法 给定HMM的参数\\(\\mu\\)和观察序列\\(O= o_1 o_2 \\cdots o_T\\)。 定义t时刻状态为\\(s_i\\)和t+1时刻状态为\\(s_j\\)的概率是\\(\\color{blue}{\\xi_t(i, j)} = P(q_t =s_i, q_{t+1}=s_j \\mid O, \\mu)\\) \\[ \\begin{align} \\color{blue}{\\xi_t(i, j)} &amp;= \\frac{P(q_t =s_i, q_{t+1}=s_j, O \\mid \\mu)}{P(O \\mid \\mu)} = \\color{red}{\\frac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{P(O \\mid \\mu)}} = \\frac{\\overbrace{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}^{o_1\\cdots o_t, \\; o_{t+1}, \\; o_{t+2}\\cdots o_T}} {\\underbrace{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}_{\\xi_t(i, j)对ij求和，只留下P(O\\mid \\mu)}} \\\\ \\end{align} \\] 定义\\(t\\)时刻状态为\\(s_i\\)的概率是\\(\\color{blue}{\\gamma_t(i)} = P(q_t = s_i \\mid O, \\mu)\\) \\[ \\color{blue}{\\gamma_t(i)} = \\color{red}{\\sum_{j=1}^N \\xi_t(i, j)} \\] 那么有算法步骤如下（也称作前向后向算法） 1初始化 随机地给参数\\(\\color{blue}{a_{ij}, b_j(k), \\pi_i}\\)赋值，当然要满足一些基本条件，各个概率和为1。得到模型\\(\\mu_0\\)，令\\(i=0\\)，执行下面步骤 2EM步骤 2.1E步骤 使用模型\\(\\mu_i\\)计算\\(\\color{blue}{\\xi_t(i, j)}和\\color{blue}{\\gamma_t(i)}\\) \\[ \\color{blue}{\\xi_t(i, j)} = \\color{red}{\\frac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}} , \\; \\color{blue}{\\gamma_t(i)} = \\color{red}{\\sum_{j=1}^N \\xi_t(i, j)} \\] 2.2M步骤 用上面算得的期望去估计参数 \\[ \\begin{align*} &amp; 初始概率\\quad \\color{blue}{\\bar\\pi_i} = P(q_1=s_i \\mid O, \\mu) = \\gamma_1(i) \\\\ &amp; 转移概率\\quad \\color{blue}{\\bar {a}_{ij}} = \\frac{\\sum_{t=1}^{T-1}\\xi_t(i, j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)} \\\\ &amp; 发射概率 \\quad \\color{blue}{\\bar{b}_j(k)} = \\frac{\\sum_{t=1}^T \\gamma_t(j) \\times \\delta(o_t, v_k)}{\\sum_{t=1}^T \\gamma_t(j)} \\end{align*} \\] 3循环计算 令\\(i=i+1\\)，直到参数收敛","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"概率图模型","slug":"概率图模型","permalink":"http://plmsmile.github.io/tags/概率图模型/"},{"name":"马尔可夫链","slug":"马尔可夫链","permalink":"http://plmsmile.github.io/tags/马尔可夫链/"},{"name":"隐马尔科夫模型","slug":"隐马尔科夫模型","permalink":"http://plmsmile.github.io/tags/隐马尔科夫模型/"},{"name":"维特比算法","slug":"维特比算法","permalink":"http://plmsmile.github.io/tags/维特比算法/"},{"name":"前向算法","slug":"前向算法","permalink":"http://plmsmile.github.io/tags/前向算法/"},{"name":"后向算法，BW算法","slug":"后向算法，BW算法","permalink":"http://plmsmile.github.io/tags/后向算法，BW算法/"}]},{"title":"语言模型和平滑方法","date":"2017-07-31T00:57:52.000Z","path":"2017/07/31/nlp-notes/","text":"语言模型 二元语法$ $ 对于一个句子\\(s=w_1 \\cdots w_n\\)，近似认为一个词的概率只依赖于它前面的1个词。即一个状态只跟上一个状态有关，也称为一阶马尔科夫链。 \\[ \\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_2) \\cdots p(w_n|w_{l-1})= \\color {red} {\\prod_{i=1}^l {p(w_i|w_{i-1})}} \\] 设\\(\\color {blue} {c(w_{i-1}w_i)}\\) 表示二元语法\\(\\color {green} {w_{i-1}w_i}\\)在给定文本中的出现次数，则上一个词是\\(w_{i-1}\\)下一个词是\\(w_i\\)的概率\\(\\color {blue} {p(w_i \\mid w_{i-1})}\\)是当前语法\\(\\color {green} {w_{i-1}w_i}\\)出现的次数比上所有形似\\(\\color {green} {w_{i-1}}w\\)的二元语法的出现次数 \\[ \\color {blue} {p(w_i \\mid w_{i-1})} =\\color {red} {\\frac {c(w_{i-1}w_i)} {\\sum_{w} {c(w_{i-1}w)}}}，w是变量 \\] \\(n\\)元语法 认为一个词出现的概率和它前面的n个词有关系。则对于句子\\(s=w_1w_2 \\cdots w_l\\)，其概率计算公式为如下： \\[ \\color{blue}{p(s)}=p(w_1)p(w_2|w_1)p(w_3|w_1w_2) \\cdots p(w_n|w_1w_2 \\cdots w_{l-1})=\\color {red} {\\prod_{i=1}^n{p(w_i|w_1 \\cdots w_{i-1})}} \\] 上述公式需要大量的概率计算，太理想了。一般取\\(n=2\\)或者\\(n=3\\)。 对于\\(n&gt;2\\)的\\(n\\)元语法模型，条件概率要考虑前面\\(n-1\\)个词的概率，设\\(w_i^j\\)表示\\(w_i\\cdots w_j\\)，则有 \\[ \\color{blue} {p(s)} = \\prod_{i=1}^{l+1}p(w_i \\mid w_{i-n+1}^{i})，\\color {blue} {p(w_i \\mid w_{i-n+1}^{i})}=\\frac { \\overbrace {c(w_{i-n+1}^i)}^{\\color{red}{具体以w_i结尾的词串w[i-n+1, i]}}} { \\underbrace{\\sum_{w_i}{c(w_{i-n+1}^i)}}_{\\color{red}{所有以w_i结尾的词串w[i-n+1, i]}}} \\] 实际例子 假设语料库\\(S\\)是由下面3个句子组成，所求的句子t在其后： 12s = ['brown read holy bible', 'plm see a text book', 'he read a book by david']t = 'brown read a book' 那么求句子\\(t\\)出现的概率是 \\[\\color{blue}{p(t)}=p(\\color{green}{brown\\,read\\, a\\, book})=p(brown|BOS)p(read|brown)p(a|read)p(book|a)p(eos|book)\\approx0.06\\] \\(n\\)元文法的一些应用如下 语音识别歧义消除 如给了一个拼音 \\(\\color{green}{ta\\,shi \\,yan \\,jiu \\,sheng\\, wu\\, de}\\)，得到了很多可能的汉字串：踏实研究生物的，他实验救生物的，他是研究生物的 ，那么求出\\(arg_{str}maxP(str|pinyin)\\)，即返回最大概率的句子 汉语分词问题 给定汉字串他是研究生物的。可能的汉字串 他 是 研究生 物 的和他 是 研究 生物 的，这也是求最大句子的概率 开发自然语言处理的统计方法的一般步骤 收集大量语料（基础工作，工作量最大，很重要） 对语料进行统计分析，得出知识（如n元文法，一堆概率） 针对场景建立算法，如计算概率可能也用很多复杂的算法或者直接标注 解释或者应用结果 模型评估参数 基础 评价目标：语言模型计 算出的概率分布与“真实的”理想模型是否接近 难点：无法知道“真实的”理想模型的分布 常用指标：交叉熵，困惑度 信息量和信息熵 \\(X\\)是一个离散随机变量，取值空间为\\(R\\)，其概率分布是\\(p(x)=P(X=x), x \\in R\\)。 信息量 概率是对事件确定性的度量，那么信息就是对不确定性的度量。信息量 \\(\\color {blue} {I(X)}\\)代表特征的不确定性，定义如下 \\[ \\color {blue} {I(X)}= \\color {red} {-\\log {p(x)}} \\] 信息熵 信息熵\\(\\color{blue}{H(x)}\\)是特征不确定性的平均值，用表示，定义如下 \\[ \\color{blue}{H(X)}=\\sum_{x \\in R}{p(x)log\\frac 1 {p(x)}}=\\color {red} {-\\sum_{x \\in R} {p(x) \\log p(x)}} \\] 一般是\\(log_2{p(x)}\\)，单位是比特。若是\\(\\ln {p(x)}\\)，单位是奈特。 信息熵的本质是信息量的期望 信息熵是对不确定性的度量 随机变量\\(X\\)的熵越大，说明不确定性也大；若\\(X\\)为定值，则熵为0 平均分布是&quot;最不确定”的分布 联合熵和条件熵 \\(X, Y\\)是一对离散型随机变量，并且\\(\\color{blue}{X,Y \\sim p(x,y)}\\)。 联合熵 联合熵实际上描述的是一对随机变量平均所需要的信息量，定义如下。 \\[ \\color{blue}{H(X, Y)} = \\color{red} {- \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(x, y)} \\] 条件熵 给定\\(X\\)的情况下，\\(Y\\)的条件熵为 \\[ \\color{blue}{H(Y \\mid X)} = \\color{red}{ - \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log p(y \\mid x)} \\] 其中可以推导出：\\(H(X, Y) = H(X) + H(Y \\mid X)\\)。 相对熵和交叉熵 相对熵 随机变量\\(X\\)的状态空间\\(\\Omega {x}\\)上有两个概率分布\\(p(x)\\)和\\(q(x)\\)。一般p是真实分布，q是预测分布。 相对熵也称为KL距离，用来衡量相同事件空间里两个概率分布的差异。 \\(p\\)和\\(q\\)的相对熵\\(\\color{blue}{D(p\\mid\\mid q)}\\)用来度量它们之间的差异，如下 \\[ \\color{blue}{D(p\\mid\\mid q)} =\\color{red}{\\sum_{x\\in X} {p(x)\\log{\\frac {p(x)}{q(x)}}}} = E_p(\\log \\frac{p(X)}{q(X)}) \\; (期望) \\] 特别地，若\\(p==q\\)，则相对熵为0；若差别增加，相对熵的值也增加。简单理解“相对”如下： \\[ D(p \\mid\\mid q)=\\sum_{x \\in X}{p(x)(\\log p(x) - \\log q(x))} = \\underbrace{\\left(-\\sum_{x \\in X}{ \\color{red}{p(x)\\log q(x)}}\\right)}_{\\color {red}{以q去近似p的熵=交叉熵}} - \\underbrace{\\left(-\\sum_{x \\in X} {\\color{red}{p(x)\\log p(x)}}\\right)}_{\\color{red} {p本身的熵}} \\] 交叉熵 交叉熵用来衡量估计模型与真实概率分布之间的差异。 随机变量\\(X \\sim p(x)\\)，\\(q(x)\\)近似于\\(p(x)\\)。 则随机变量\\(X\\)和模型\\(q\\)之间的交叉熵\\(\\color {blue} {H(X, q)}\\)如下：以\\(q\\)去近似\\(p\\)的熵 \\[ \\color {blue} {H(p, q)} = H(X) + D(p \\mid\\mid q) = \\color {red} {-\\sum_{x \\in X}{p(x)\\log q(x)}} \\] 实际应用 交叉熵的实际应用，设\\(y\\)是预测的概率分布，\\(y^\\prime\\)为真实的概率分布。则用交叉熵去判断估计的准确程度 \\[ H(y^{\\prime}, y)= - \\sum_i y_i^{\\prime}\\log y_i = \\color {red} {-\\sum_i y_{真实} \\log y_{预测}} \\] n元文法模型的交叉熵 设测试集\\(T=(t_1, t_2, \\ldots, t_l)\\)包含\\(l\\)个句子，则定义测试集的概率\\(\\color {blue} {p(T)}\\)为多个句子概率的乘积 \\[ \\color {blue} {p(T)} = \\prod_{i=1}^{l} p(t_i)， \\, \\text{其中}\\color{blue}{p(t_i)}=\\color{red}{\\prod_{i=1}^{l_w} {p(w_i|w_{i-n+1}^{i-1})}}, \\text{见上面} \\] 其中\\(w_i^j\\)表示词\\(w_i\\cdots w_j\\)，\\(\\sum_{w}{c(read \\, w)}\\)是查找出所有以\\(read\\)开头的二元组的出现次数。 则在数据\\(T\\)上n元模型\\(\\color {green} {p(w_i|w_{i-n+1}^{i-1})}\\)的交叉熵\\(\\color {blue} {H_p(T)}\\)定义如下 \\[ \\color {blue} {H_p(T)} = \\color {red} {-\\frac {1} {W_T} \\log _2 p(T)}，其中W_T是文本T中基元(词或字)的长度 \\] 公式的推导过程如下 \\[ -\\sum_{x \\in X}{p(x)\\log q(x)} \\implies \\underbrace { -{\\frac{1} {W_T}}\\sum \\log q(x)}_{\\color{red}{使用均匀分布代替p(x)}} \\implies -{\\frac{1} {W_T}} \\log {\\prod r(w_i|w_{i-n+1}^{i-1})} \\implies -{\\frac{1} {W_T}} \\log_2p(T) \\] 可以这么理解：利用模型\\(p\\)对\\(W_T\\)个词进行编码，每一个编码所需要的平均比特位数。 困惑度 困惑度是评估语言的基本准则人，也是对测试集T中每一个词汇的概率的几何平均值的倒数。 \\[ \\color{blue}{PP_T(T)} =\\color{red}{ 2^{H_p(T)}= \\frac {1} {\\sqrt [W_T]{p(T)}}} = 2 ^{\\text{交叉熵}} \\] 当然，交叉熵和困惑度越小越好。语言模型设计的任务就是要找出困惑度最小的模型。 在英语中，n元语法模型的困惑度是\\(50 \\sim 1000\\)，交叉熵是\\(6 \\sim 10\\)个比特位。 数据平滑 问题的提出 按照上面提出的语言模型，有的句子就没有概率，但是这是不合理的，因为总有出现的可能，概率应该大于0。设\\(\\color {blue}{c(w)}\\)是\\(w\\)在语料库中的出现次数。 \\[ p(\\color{green} {read \\mid plm}) = \\frac {c(plm \\mid read)} {\\sum_{w_i}{c(plm | w_i})} = \\frac {\\color{red}{0}} {1}=\\color{red}{0， \\, 这是不对的} \\] 因此，必须分配给所有可能出现的字符串一个非0的概率值来避免这种错误的发送。 平滑技术就是用来解决这种零概率问题的。平滑指的是为了产生更准确的概率来调整最大似然估计的一种技术，也称作数据平滑。思想是劫富济贫，即提高低概率、降低高概率，尽量是概率分布趋于均匀。 数据平滑是语言模型中的核心问题 加法平滑 其实为了避免0概率，最简单的就是给统计次数加1。这里我们可以为每个单词的出现次数加上\\(\\delta，\\delta \\in [0, 1]\\)，设\\(V\\)是所有词汇的单词表，\\(|V|\\)是单词表的词汇个数，则有概率： \\[ p_{add}(w_i \\mid w_{i-n+1}^{i-1}) = \\frac {\\delta + c(w_{i-n+1}^i)} {\\sum_{w_i}{(\\delta*|V| + c(w_{i-n+1}^i)})}=\\frac {\\delta + \\overbrace {c(w_{i-n+1}^i)}^{\\color{red}{具体词串[i-n+1, i]}}} {\\delta*|V| + \\underbrace{\\sum_{w_i}{c(w_{i-n+1}^i)}}_{\\color{red}{所有以w_i结尾的词串[i-n+1, i]}}} \\] 注：这个方法很原始。 Good-Turing Good-Turing也称作古德-图灵方法，这是很多平滑技术的核心。 主要思想是重新分配概率，会得到一个剩余概率量\\(\\color {blue} {p_0}= \\color {red} {\\frac {n_1} N}\\)，设\\(n_0\\)为未出现的单词的个数，然后由这\\(n_0\\)个单词去平均分配得到\\(p_0\\)，即每个未出现的单词的概率为\\(\\frac {p_0} {n_0}\\)。 对于一个\\(n\\)元语法，设\\(\\color {blue} n_r\\)恰好出现\\(r\\)次的\\(n\\)元语法的数目，下面是一些新的定义 出现次数为\\(r\\)的\\(n\\)元语法 新的出现次数\\(\\color {blue} {r^*} = \\color {red} {(r+1)\\frac{n_{r+1}}{n_r}}\\) 设\\(N = \\sum_{r=0}^{\\infty}n_r r^* = \\sum_{r=1}^{\\infty} n_r r\\)，即\\(N\\)是这个分布中最初的所有文法出现的次数，例如所有以\\(read\\)开始的总次数 出现次数为\\(r\\)的修正概率 \\(\\color {blue}p_r = \\color {red} {\\frac {r^*} {N}}\\) 剩余概率量\\(\\color {blue} {p_0}= \\color {red} {\\frac {n_1} N}\\)的推导 \\[ 总的概率 = \\sum_{r&gt;0}{n_r p_r} = \\sum_{r&gt;0}{n_r (r+1)\\frac{n_{r+1}}{n_r N}} = \\frac {1}{N} (\\sum_{r&gt;0} (r+1)n_{r+1} = \\frac {1}{N} (\\sum_{r&gt;0} (r n_r - n_1) = 1 - \\frac {n_1} N &lt; 1 \\] 然后把\\(p_0\\)平均分配给所有未见事件(r=0的事件)。 缺点 若出现次数最大为\\(k\\)，则无法计算\\(r=k\\)的新的次数\\(r^*\\)和修正概率\\(p_r\\) 高低阶模型的结合通常能获得较好的平滑效果，但是Good-Turing不能高低阶模型结合 Jelinek-Mercer 问题引入 假如\\(c(send \\, the)=c(send \\, thou)=0\\)，则通过GT方法有\\(p(the \\mid send)=p(thou \\mid send)\\)，但是实际上却应该是\\(p(the \\mid send)&gt;p(thou \\mid send)\\)。 所以我们需要在二元语法模型中加入一个一元模型 \\[ p_{ML}(w_i) = \\frac {c(w_i)}{\\sum_w{c(w)}} \\] 二元线性插值 使用\\(r\\)将二元文法模型和一元文法模型进行线性插值 \\[ p(w_i \\mid w_{i-1}) = \\lambda p_{ML}(w_i | w_{i-1}) + (1-\\lambda)p_{ML}(w_i)，\\lambda \\in [0, 1] \\] 所以可以得到\\(p(the \\mid send)&gt;p(thou \\mid send)\\)","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"各种熵","slug":"各种熵","permalink":"http://plmsmile.github.io/tags/各种熵/"},{"name":"语言模型","slug":"语言模型","permalink":"http://plmsmile.github.io/tags/语言模型/"},{"name":"数据平滑","slug":"数据平滑","permalink":"http://plmsmile.github.io/tags/数据平滑/"}]},{"title":"剑指Offer算法题","date":"2017-07-29T03:42:07.000Z","path":"2017/07/29/aim2offer/","text":"数组中重复的数字-03 题目1 找到数组中重复的数字 一个数组存放n个数字，所有数字在[0, n-1]范围内。某些数字是随机重复的。请找出任意一个重复的数字。例如[2,3,1,0,2,5,3]，输出2或3 思路1 对数组进行排序，然后可以找出重复的数字。但是排序的时间复杂度是O(nlogn) 思路2 使用哈希表，每次存放的时候检查是否在哈希表中，如果已经存在，那么就重复了。时间复杂度O(n)，空间复杂度O(n) 最优思路 利用下标和值的关系，从头到尾依次扫描这个数组。扫描到下标为i，值为m。尽量把m放到a[m]的位置上。修改了原来的数组。 12345678910while (a[i] != i) if m == i: # 扫描下一个数字 else: # 把m和a[m]进行比较 if m == a[m]: # 找到一个相同的数字 else: # 把m放到a[m]的位置，交换 m &lt;--&gt; a[m] 尽管有两重循环，但每个数字最多交换两次就能找到自己的位置，所以总的时间复杂度是O(n)，空间复杂度为O(1) 关键代码 123456789101112131415161718192021222324252627// 找到数组中重复的值// Args:// a: 数组// alen: 数组长度// dup: 用于返回重复的数值// Returns:// True：数据合法(长度和值)并且有重复的数字，否则返回False// bool duplicate(int a[], int alen, int *dup) &#123; // 遍历数组，把i都放到a[i]上 for (int i = 0; i &lt; alen; i++) &#123; while (a[i] != i) &#123; int m = a[i]; if (a[m] == m) &#123; // a[m]已经有m *dup = m; return true; &#125; else &#123; // 把m放到a[m]上 int t = a[m]; a[m] = m; a[i] = t; &#125; &#125; &#125; return false;&#125; 题目2 不修改数组找出重复的数字 数组，长度为n+1，数字范围[1, n]，数组中至少有一个是重复的，找出任意一个重复的数字，但是不能修改数组 思路1 创建一个新数组b存放原数组a。遍历原数组，当前是m，如果b[m]已经没有值，则存放；如果有值，则重复。但是需要O(n)的辅助空间 最优思路 见二分查重描述清晰版 把\\(a[1, n]\\)的个数字，分为两部分。\\(a[1, m]\\)和\\(a[m+1, n]\\)。 在\\(a[1, m]\\)中，统计数字\\(1,2\\cdots, m\\)在\\(a[1,m]\\)中出现的次数count 如果是m次，则\\(a[1,m]\\)每个数字独一无二，重复的区间在a[m+1, n]中。则\\(\\rm{start}=m+1\\)，继续查找。 否则不独一无二，则重复在\\(a[1, m]\\)中 。则\\(\\rm{end}=m\\)， 继续查找。 直到\\(\\rm{start} == \\rm{end}\\) 。count &gt; 1，则start重复，否则没有重复的。 关键代码 1234567891011121314151617181920212223242526272829303132// 二分查找数组中重复的值// Args:// a: 数组// alen: 数组长度// Returns:// dup: 重复的数值; 没有重复时返回-1int get_duplication(const int *a, int alen) &#123; if (a == nullptr || alen &lt;= 0) &#123; return -1; &#125; int start = 1; int end = alen - 1; while (start &lt;= end) &#123; int m = ((end - start) &gt;&gt; 1) + start; int count = count_range(a, alen, start, m); // last if (start == end) &#123; if (count &gt; 1) &#123; return start; &#125; else &#123; break; &#125; &#125; // continue if (count == m - start + 1) &#123; start = m + 1; &#125; else &#123; end = m; &#125; &#125; return 0;&#125; 二维数组查找-04 ​ 一个二维数组，每一行从左到右递增，每一列，从上到下递增。输入一个整数，判断二维数组中是否有这个数字 错误思路 全盘扫描肯定不行，从左上角最小的开始也不行，应该从最大角的地方开始 思路 一行的最大元素在最右边，一列的最小元素在上边。所以从右上角开始查找最好。即向左查、向下查，这样每次都能够剔除一行或者一列。 1234567891011while# 当期右上角值是a[i, j]=m，查找的值是tif t == a[i,j]: done # 查找成功else if t &gt; a[i,j]: # 删除当前行 m = a[i+1, j]else if t &lt; a[i, j]: # 删除当前列 m = a[i, j-1]# 继续查找 关键代码 12345678910111213141516171819202122232425262728// 查找一个数，是否在一个矩阵中// Args:// target: 要查找的数字// array: 矩阵// Returns:// exists: true or falsebool find(int target, std::vector&lt;std::vector&lt;int&gt;&gt; array) &#123; int col = array.size(); int row = array[0].size(); bool exist = false; int i = 0; int j = col - 1; // 注意i,j的范围 while (exist == false &amp;&amp; (i &lt; row &amp;&amp; i &gt;= 0 &amp;&amp; j &lt; col &amp;&amp; j &gt;= 0)) &#123; int t = array[i][j]; if (target == t) &#123; exist = true; break; &#125; else if (target &lt; t) &#123; // to left --j; &#125; else if(target &gt; t) &#123; // to down ++i; &#125; &#125; return exist;&#125; 字符串替换空格-05 把字符串中的每个空格替换成&quot;%20&quot; 如果在原来的字符串上修改，则会覆盖原来字符串后面的内存 如果创建新的字符串，则要分配足够的内存 C/C++中字符串最后一个字符是\\0 不好思路 从前向后扫描，遇到一个空格替换一个。但是每次都需要大量移动后面的元素，所以时间复杂度是\\(O(n^2)\\) 最优思路 从后向前替换。使用两个指针p1和p2。先计算出替换后的长度，p2指向替换后的长度的末尾指针。p1指向之前的字符串的指针。 从p1开始向前移动 当前是普通字符，则复制到p2，p2向前移动 当前是空格，则在p2加入“%20”，p2向前移动 如果p1==p2，那么移动完毕 总的来说，先找到最终的长度，从后向后拉。时间复杂度是\\(O(n)\\) 技巧 合并两个数组/字符串，如果从前往后，则需要移动多次。从后向前，能够减少移动次数，提高效率 关键代码 1234567891011121314151617181920212223242526272829303132// 替换字符串中的空格字符，每个空格用'02%'替换// 直接修改原字符串// Args:// str: 字符串// len: 长度// Returns:// Nonevoid replace_space(char *str, int len) &#123; // 统计空格的个数 int count = 0; for (int i = 0; i &lt; len; i++) if (str[i] == ' ') ++count; int newlen = (len - count) + count * 3; int i = len - 1, j = newlen - 1; while (i &gt;= 0 &amp;&amp; j &gt;= 0) &#123; if (str[i] == ' ') &#123; // 在j处添加替换字符 str[j--] = '0'; str[j--] = '2'; str[j--] = '%'; // 向前移动 --i; &#125; else &#123; // 字符复制到后面 str[j--] = str[i--]; &#125; &#125; // 字符串结尾 str[newlen] = '0';&#125; 逆序打印链表-06 链表基础考点 链表是面试中最频繁的数据结构。动态结构很灵活，考指针、考编程功底。 链表创建 ： 链表插入： 为新节点分配内存，调整指针的指向。 删除链表中的节点 从尾到头打印链表 链表中倒数第k个节点 反转链表 合并两个排序的链表 两个链表的第一个公共节点 环形链表 ：尾节点指针指向头结点。题目62：圆圈中最后剩下的数字 双向链表 ：题目36，二叉搜索树与双向链表 复杂链表 ：指向下一个，指向任意节点的指针 从尾到头打印链表 本质上是先进后出， 可以用栈和递归。显然，栈的效率高。 关键代码 1234567891011121314151617181920212223// 使用栈逆序打印链表// Args:// head: 头指针// Returns:// res: vector&lt;int&gt;，逆序值vector&lt;int&gt; get_reverse_by_stack(ListNode *head) &#123; ListNode* pnode = head; stack&lt;int&gt; st; int count = 0; while (pnode != nullptr) &#123; st.push(pnode-&gt;val); pnode = pnode-&gt;next; count++; &#125; // 分配定长的vector，不用 vector&lt;int&gt; res(count); for (int i = 0; i &lt; count &amp;&amp; st.empty() == false; i++) &#123; res[i] = st.top(); st.pop(); &#125; return res;&#125; 重建二叉树-07 树的考点 树的遍历 叉树涉及指针，比较难。最常问遍历。需要对下面7种了如指掌。 前序 中序 后序 层次遍历 递归 无递归 循环 考题 题26，树的子结构 题34，二叉树中和为某一值的路径 题55，二叉树的深度 题7，重建二叉树 题33，二叉搜索树的后序遍历序列 题32，从上到下打印二叉树（层次遍历） 特别的二叉树 二叉搜索树 ：左节点小于根节点，根节点小于右节点。查找搜索时间复杂度\\(O(\\log n)\\)。 题36，二叉搜索树与双向链表；题68：树中两个节点的最低公共祖先。 堆 ：最大堆和最小堆。找最大值和最小值。 红黑树 ： 节点定义为红黑两种颜色。根节点到叶节点的最长路径不超过最短路径的两倍。 前序中序建立二叉树 前序序列：1, 2, 4, 7, 3, 5, 6, 8。 根 左 右。 中序序列：4, 7, 2, 1, 5, 3, 8, 6。 左 根 右。 使用递归，先找到根节点，找到左右子树，为左右子树分别创建各自的前序和中序序列，再进行递归创建左右子树。 关键是要构建下面的序列，注意下标值。 前序 中序 左子树 2, 4, 7 4, 7, 2 右子树 3, 5, 6, 8 5, 3, 8, 6 关键代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 递归利用先序和中序重建二叉树// Args:// vpre: 先序序列// vin: 中序序列// Returns:// root: treeTreeNode * reconstruct_binary_tree(vector&lt;int&gt;vpre, vector&lt;int&gt; vin) &#123; // 1. 为空，停止递归 if (vpre.size() == 0 || vin.size() == 0) &#123; return NULL; &#125; // 2. 构建根节点 TreeNode *root = new TreeNode(vpre[0]); // 3. 找到根节点在中序中的位置 int root_index = -1; for (int i = 0; i &lt; vin.size(); i++) &#123; // cout &lt;&lt; vin[i] &lt;&lt; \" \" &lt;&lt; vpre[0] &lt;&lt; endl; if (vin[i] == vpre[0]) &#123; root_index = i; break; &#125; &#125; // 简单判断一下 if (root_index == -1) &#123; cout &lt;&lt; \"root_index is -1\" &lt;&lt; endl; return NULL; &#125; // 4. 生成左右子树的先序序列、中序序列 int leftlen = root_index; int rightlen = vin.size() - leftlen - 1; vector&lt;int&gt; leftvpre(leftlen), leftvin(leftlen); vector&lt;int&gt; rightvpre(rightlen), rightvin(rightlen); // 重点在这里，用实际例子去对照看 for (int i = 0; i &lt; vin.size(); i++) &#123; if (i &lt; root_index) &#123; // 左子树 leftvin[i] = vin[i]; leftvpre[i] = vpre[i+1]; &#125; else if (i &gt; root_index)&#123; // 右子树，条件特别重要 int right_idx = i - root_index - 1; rightvin[right_idx] = vin[i]; rightvpre[right_idx] = vpre[leftlen + 1 + right_idx]; &#125; &#125; // 5. 递归生成左右子树 root-&gt;left = reconstruct_binary_tree(leftvpre, leftvin); root-&gt;right = reconstruct_binary_tree(rightvpre, rightvin); return root;&#125; 二叉树的下一个节点-08 二叉树：值，左孩子，右孩子，父亲节点指针。 给一个节点，找出中序序列的该节点的下一个节点。重在分析中序序列。 12345678910if \"有右子树\": # 向左走 while (\"p有左孩子\") p = \"左孩子\" t = pelse: # 向上走 while (\"p有父节点 &amp;&amp; p是父节点的右节点\"): p = \"父节点\" t = p 关键代码 123456789101112131415161718192021222324252627// 找到中序遍历的下一个节点// Args:// pnode: 当前节点// Returns:// pnext: 中序中，pnode的下一个节点TreeNode* get_next_inorder(TreeNode* pnode) &#123; if (pnode == nullptr) &#123; return nullptr; &#125; TreeNode* pnext = nullptr; if (pnode-&gt;right != nullptr) &#123; TreeNode* p = pnode-&gt;right; while (p-&gt;left != nullptr) &#123; p = p-&gt;left; &#125; pnext = p; &#125; else &#123; TreeNode* p = pnode; while (p-&gt;parent != nullptr &amp;&amp; p == p-&gt;parent-&gt;right) &#123; p = p-&gt;parent; &#125; if (p-&gt;parent != nullptr) &#123; pnext = p-&gt;parent; &#125; &#125; return pnext;&#125; 两个栈实现队列-09 栈和队列 栈 ：后进先出 题31：栈的压入、弹出序列 \\(O(n)\\) 找到最大最小元素。若\\(O(1)\\) ，则题30：包含min函数的栈 队列 ：先进先出 树的层次遍历，题32： 从上到下打印二叉树 用两个栈实现队列 分为入栈（栈A）和出栈（栈B）。 入队时：直接进入入栈 出队时：若出栈为空，则把入栈里的内容放入出栈；再从出栈里面出一个元素。 [关键代码] 1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; private: stack&lt;int&gt; stackIn; stack&lt;int&gt; stackOut; public: // 入队 void push(int node) &#123; stackIn.push(node); &#125; // 出队 int pop() &#123; if (this-&gt;empty()) &#123; cout &lt;&lt; \"empty queue\" &lt;&lt; endl; return -1; &#125; int node = -1; if (stackOut.empty() == true) &#123; while (stackIn.empty() == false) &#123; node = stackIn.top(); stackIn.pop(); stackOut.push(node); &#125; &#125; node = stackOut.top(); stackOut.pop(); return node; &#125; bool empty() &#123; return stackIn.empty() == true &amp;&amp; stackOut.empty() == true; &#125;&#125;; 两个队列实现栈 分为空队列和非空队列 入栈：进入非空队列 出栈：非空队列中中前n-1个进入空队列，出非空队列最后一个元素（最新进来的） 关键代码 123456789101112131415161718192021222324int pop() &#123; if (this-&gt;empty()) &#123; return -1; &#125; // 找到哪个队列有元素，注意使用指针 queue&lt;int&gt;* qout; queue&lt;int&gt;* qin; if (q1.empty() == true) &#123; qout = &amp;q2; qin = &amp;q1; &#125; else &#123; qout = &amp;q1; qin = &amp;q2; &#125; // qout的前n-1个元素放到qin中 while (qout-&gt;size() &gt; 1) &#123; qin-&gt;push(qout-&gt;front()); qout-&gt;pop(); &#125; int res = qout-&gt;back(); qout-&gt;pop(); return res;&#125; 算法和数据操作 总览 类型 题型 备注 递归和循环 树的遍历 递归简介，循环效率高 排序和查找 二分查找、归并排序、快速排序 正确、完整写出代码 二维数组 迷宫、棋盘 回溯法；栈模拟递归 最优解 动态规划，问题分解为多个子问题 自上而下递归分析；自下而上循环代码实现，数组保存 最优解 贪心算法 分解时是否存在某个特殊选择：贪心得到最优解 与、或、异或、左移、右移 递归效率低的原因：函数调用自身，函数调用是由时间和空间的消耗；会在内存栈中分配空间以保存参数，返回地址和临时变量，往栈里弹入和弹出都需要时间。 递归和循环：题10，斐波那契数列；题60，n个骰子的点数。 动态规划 递归思路分析，递归分解的子问题中存在着大量的重复。用自下而上的循环来实现代码。题14 剪绳子， 题47礼物的最大价值 ， 题48最长不含重复字符的子字符串 斐波那契数列-递归循环-10 斐波那契数列 数列定义 \\[ f(n) = \\begin{cases} &amp;0 &amp; n=0 \\\\ &amp;1 &amp; n=1 \\\\ &amp;f(n-1) + f(n-2) &amp; n \\ge 1 \\end{cases} \\] 递归和循环两种实现策略 关键代码 1234567891011121314151617181920long long fibonacci_recursion(unsigned int n) &#123; if (n &lt;= 0) return 0; if (n == 1) return 1; return fibonacci_recursion(n-1) + fibonacci_recursion(n-2);&#125;long long fibonacci_loop(unsigned int n) &#123; if (n &lt;= 0) return 0; if (n == 1) return 1; long long f1 = 0; long long f2 = 1; long long fn = 0; for (unsigned int i = 2; i &lt;= n; i++) &#123; fn = f1 + f2; f1 = f2; f2 = fn; &#125; return fn;&#125; 青蛙跳台阶 青蛙可以一次跳1个台阶，一次跳2个台阶。问青蛙跳n个台阶有多少种跳法。 青蛙跳到第n个台阶有两种跳法：跳1个和2个。所以\\(f(n)=f(n-1)+f(n-2)\\) 。是斐波那契数列。 扩展 青蛙一次可以跳1个台阶、2个台阶、n个台阶。问有多少种跳法？ 数学归纳法证得：\\(f(n) = 2^{n-1}\\) 矩阵覆盖问题 \\(2\\times1\\)矩阵去覆盖\\(2 \\times 8\\) 矩阵，可以横着竖着覆盖，问多少种覆盖方法？ 同理，最后一个横着放或者竖着放。\\(f(8)=f(7)+f(6)\\)","tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://plmsmile.github.io/tags/leetcode/"},{"name":"排序","slug":"排序","permalink":"http://plmsmile.github.io/tags/排序/"}]},{"title":"简单的卷积神经网络","date":"2017-07-18T11:23:56.000Z","path":"2017/07/18/cnn-mnist/","text":"卷积神经网络 概要 卷积神经网络(Convolutional Neural Network, CNN)是人工神经网络中的一种，是一种特殊的对图像识别的方式，属于非常有效的带有前向反馈的网络。也用于音频信号、文本数据、人脸识别等等。$ $ CNN不需要把特征提取和分类训练两个过程分开，在训练的时候就提取了最有效的特征，降低了对图形数据预处理的要求。 卷积神经网络的核心思想是将输入信息切分成一个个子采样层进行采样，然后将提取的特征和权重值作为参数，传导到下一层 主要思路 每一个卷积操作只处理一小块图像，提取出最有效的特征，传给紧接着的池化层 池化层用来降采样，求局部平均和二次采样 循环上面两种操作 不停地对基础特征进行组合和抽象，得到更高阶的特征 主要特点 局部连接：减少了连接数量 权值共享：大幅度减少参数数量，防止过拟合又降低了复杂度 降采样紧跟卷积层：对样本有较高的畸变容忍能力 特征分区提取、时间或空间采样等规则 理论上具有对图像缩放、平移和旋转的不变性，有着很强的泛化性 网络结构 传统网路的问题 传统网络是全连接的，假如图像比较大是[200, 200, 3]，则会有\\(200*200*3=12000\\)个神经元。这样庞大的神经元做全连接结构是非常浪费的，并且有大量的参数会导致过拟合。 卷积神经网络 每一个像素点在空间上和周围的像素点实际上是有紧密联系的，但是和太遥远的像素点就不一定有什么联系了。这也是视觉感受野的概念，每一个感受野只接受一小块区域的信号。 一个卷积神经网络通常由多个卷积层组成，每一个卷积操作只处理一小块图像（也称作卷积核滤波），提取出最有效的特征传递给后面，主要操作如下： 一个卷积核可以提取出一种特征。一个图像经历一个卷积核之后，会输出一个新的图像，称作特征图谱(Feature Map, FM)。其实卷积操作也是\\(w * x + \\vec b\\)，只不过对于单次的卷积操作\\(w\\)和\\(\\vec b\\)是不变的，而\\(x\\)是会变的，每次取图片的一小块，会得到很多的结果，拼凑起来就是一张新的FM 将前面卷积的滤波结果FM，进行非线性的激活函数处理。之前是sigmoid，现在是Relu函数，比较完美解决梯度弥散的问题。 对激活函数的结果进行降采样（池化操作），比如将[2,2]将为[1,1]的图片。常用的有最大值合并、平均值合并和随机合并。思路和卷积差不多，只不过结果不用乘加，只是选择最大的就行了。 最后一个子采样层一般会全连接一个或多个全连接层，全连接层的输出就是最后的输出。一般是softmax","tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://plmsmile.github.io/tags/tensorflow/"},{"name":"神经网络","slug":"神经网络","permalink":"http://plmsmile.github.io/tags/神经网络/"},{"name":"卷积","slug":"卷积","permalink":"http://plmsmile.github.io/tags/卷积/"}]},{"title":"利用tensorflow实现简版word2vec","date":"2017-07-14T12:17:50.000Z","path":"2017/07/14/word2vec/","text":"相关知识 传统方法 One-Hot Encoder 是一个词对应一个向量，向量中只有一个是1，其余是0，离散表达。$ $ Bag of Words 标识当前单词那一位不是1，而是变成了当前单词的出现次数。 存在的问题 需要大量的维数去表示，编码随机的，没有任何关联的信息。 向量空间模型 Vector Space Models可以把字词转化为连续值，并将意思相近的词被映射到向量空间相近的位置。 VSM在NLP中的一个重要假设是：在相同语境中出现的词，语义也相近。 有如下两种模型 计数模型 统计语料库中相邻出现的词的频率，再把这些计数结果转为小而稠密的矩阵。 预测模型 根据一个词周围相邻的词汇推测出这个词。 Word2Vec Word2Vec是一种计算非常高效的、可以从原始语料中学习字词空间向量的预测模型。 有如下两种模型 CBOW Continuous Bag of Words 从语境推测目标词汇，适合小型数据。如“中国的首都是__”推测出“北京”。把一整段上下文信息当做一个观察对象 Skip-Gram 从目标词汇推测语境，适合大型语料。 把每一对上下文-目标词汇当做一个观察对象 Word2Vec的一些优点 连续的词向量能够捕捉到更多的语义和关联信息 意思相近的词语在向量空间中的位置也会比较近。如北京-成都、狗-猫等词汇会分别聚集在一起。 能学会一些高阶语言的抽象概念。如&quot;man-woman&quot;和&quot;king-queen&quot;的向量很相似。 Word2Vec学习的抽象概念 噪声对比训练 神经概率化语言模型通常使用极大似然法进行训练，再使用Softmax函数得到在给出上下文单词\\(h\\)的情况下，目标词\\(w_t\\)出现的最大概率，设为\\(P(w_t|h)\\)。 设\\(score(w_t, h)\\)为当前词\\(w_t\\)和上下文单词\\(h\\)的相容性，通常使用向量积获得。 \\[ P(w_t|h) = Softmax(score(w_i, h))=\\frac{e^{score(w_i, h)}} {\\sum_{i=1}^v {e^{score(w_i, h)}}} \\] 通过对数似然函数max likelihood来进行训练 \\[ J_{ml}=\\ln{P(w_t|h)}=score(w_t,h)-\\ln{\\sum_{i=1}^v e^{score(w_i, h)}} \\] 这个方法看起来可行，但是消耗太大了，因为要对当前\\(h\\)与所有单词\\(w\\)的相容性\\(score(w, h)\\)。 在使用word2vec模型中，我们并不需要对所有的特征进行学习。所以在CBOW模型和Skip-Gram模型中，会构造\\(k\\)个噪声单词，而我们只需要从这k个中找出真正目标单词\\(w_t\\)即可，使用了一个二分类器（lr）。下面是CBOW模型，对于Skip-Gram模型只需要相反就行了。 设\\(Q_\\theta(D=1|w, h)\\)是二元逻辑回归的概率，即在当前条件下出现词语\\(w\\)的概率。 \\(\\theta\\) 输入的embedding vector \\(h\\) 当前上下文 \\(d\\) 输入数据集 \\(w\\) 目标词汇（就是他出现的概率） 此时，最大化目标函数如下： \\[ J_{NEG}=\\ln{Q_\\theta(D=1|w_t, h)} + \\frac {\\sum_{i=1}^{k}{\\ln Q_\\theta(D=0|w_I, h)}} {k} \\] 前半部分为词\\(w\\)出现的概率，后面为\\(k\\)个噪声概率的期望值（如果写法有错误，希望提出，再改啦），有点像蒙特卡洛。 负采样Negative Sampling 当模型预测的真实目标词汇\\(w_t\\)的概率越高，其他噪声词汇概率越低，模型就得到优化了 用编造的噪声词汇进行训练 计算loss效率非常高，只需要随机选择\\(k\\)个，而不是全部词汇 实现Skip-Gram模型 数据说明 Skip-Gram模型是通过目标词汇预测语境词汇。如数据集如下 1I hope you always find a reason to smile 从中我们可以得到很多目标单词和所对应的上下文信息（多个单词）。如假设设左右词的窗口距离为1，那么相应的信息如下 1&#123;'hope':['i', 'you'], 'you':['hope', 'alawys']...&#125; 训练时，希望给出目标词汇就能够预测出语境词汇，所以需要这样的训练数据 12345# 前面是目标单词，后面是语境词汇，实际上相当于数据的label('hope', 'i')('hope', 'you')('you', 'hope')('you', 'always') 同时在训练时，制造一些随机单词作为负样本（噪声）。我们希望预测的概率分布在正样本上尽可能大，在负样本上尽可能小。 使用随机梯度下降算法(SGD)来进行最优化求解，并且使用mini-batch的方法，这样来更新embedding中的参数\\(\\theta\\)，让损失函数(NCE loss)尽可能小。这样，每个单词的词向量就会在训练的过程中不断调整，最后会处在一个最合适的语料空间位置。 例如，假设训练第\\(t\\)步，输入目标单词hope，希望预测出you，选择一个噪声词汇reason。则目标函数如下 \\[ J_{NEG}^{(t)}=\\ln {Q_\\theta(D=1|hope, you)} + \\ln{Q_\\theta(D=0|hope, reason)} \\] 目标是更新embedding的参数\\(\\theta\\)以增大目标值，更新方式是计算损失函数对参数\\(\\theta\\)的导数，使得参数\\(\\theta\\)朝梯度方向进行调整。多次以后，模型就能够很好区别出真实语境单词和噪声词。 构建数据集 先来分析数据，对所有的词汇进行编码。对高频词汇给一个id，对于出现次数很少词汇，id就设置为0。高频是选择出现频率最高的50000个词汇。 1234567891011121314151617181920212223242526272829303132def build_dataset(self, words): ''' 构建数据集 Args: words: 单词列表 Returns: word_code: 所有word的编码，top的单词：数量；其余的：0 topword_id: topword-id id_topword: id-word topcount: 包含所有word的一个Counter对象 ''' # 获取top50000频数的单词 unk = 'UNK' topcount = [[unk, -1]] topcount.extend( collections.Counter(words).most_common( self.__vocab_size - 1)) topword_id = &#123;&#125; for word, _ in topcount: topword_id[word] = len(topword_id) # 构建单词的编码。top单词：出现次数；其余单词：0 word_code = [] unk_count = 0 for w in words: if w in topword_id: c = topword_id[w] else: c = 0 unk_count += 1 word_code.append(c) topcount[0][1] = unk_count id_topword = dict(zip(topword_id.values(), topword_id.keys())) return word_code, topword_id, id_topword, topcount 产生batch训练样本 由于是使用mini-batch的训练方法，所以每次要产生一些样本。对于每个单词，要确定要产生多少个语境单词，和最多可以左右选择多远。 12345678910111213141516171819202122232425262728293031323334353637383940414243def generate_batch(self, batch_size, single_num, skip_window, word_code): '''产生训练样本。Skip-Gram模型，从当前推测上下文 如 i love you. (love, i), (love, you) Args: batch_size: 每一个batch的大小，即多少个() single_num: 对单个单词生成多少个样本 skip_window: 单词最远可以联系的距离 word_code: 所有单词，单词以code形式表示 Returns: batch: 目标单词 labels: 语境单词 ''' # 条件判断 # 确保每个batch包含了一个词汇对应的所有样本 assert batch_size % single_num == 0 # 样本数量限制 assert single_num &lt;= 2 * skip_window # batch label batch = np.ndarray(shape=(batch_size), dtype=np.int32) labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # 目标单词和相关单词 span = 2 * skip_window + 1 word_buffer = collections.deque(maxlen=span) for _ in range(span): word_buffer.append(word_code[self.__data_index]) self.__data_index = (self.__data_index + 1) % len(word_code) # 遍历batchsize/samplenums次，每次一个目标词汇，一次samplenums个语境词汇 for i in range(batch_size // single_num): target = skip_window # 当前的单词 targets_to_void = [skip_window] # 已经选过的单词+自己本身 # 为当前单词选取样本 for j in range(single_num): while target in targets_to_void: target = random.randint(0, span - 1) targets_to_void.append(target) batch[i * single_num + j] = word_buffer[skip_window] labels[i * single_num + j, 0] = word_buffer[target] # 当前单词已经选择完毕，输入下一个单词，skip_window单词也成为下一个 self.__data_index = (self.__data_index + 1) % len(word_code) word_buffer.append(word_code[self.__data_index]) return batch, labels 一些配置信息 12345678910111213141516171819# 频率top50000个单词vocab_size = 50000# 一批样本的数量batch_size = 128# 将单词转化为稠密向量的维度embedding_size = 128# 为单词找相邻单词，向左向右最多能取得范围skip_window = 1# 每个单词的语境单词数量single_num = 2# 验证单词的数量valid_size = 16# 验证单词从频数最高的100个单词中抽取valid_window = 100# 从100个中随机选择16个valid_examples = np.random.choice(valid_window, valid_size, replace=False)# 负样本的噪声数量noise_num = 64 计算图 1234567891011121314151617181920212223242526272829303132333435363738394041424344graph = tf.Graph()with graph.as_default(): # 输入数据 train_inputs = tf.placeholder(tf.int32, shape=[batch_size]) train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) valid_dataset = tf.constant(valid_examples, dtype=tf.int32) with tf.device('/cpu:0'): # 随机生成单词的词向量，50000*128 embeddings = tf.Variable( tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0)) # 查找输入inputs对应的向量 embed = tf.nn.embedding_lookup(embeddings, train_inputs) nce_weights = tf.Variable( tf.truncated_normal([vocab_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) nce_biases = tf.Variable(tf.zeros([vocab_size])) # 为每个batch计算nceloss loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, labels = train_labels, inputs=embed, num_sampled=noise_num, num_classes=vocab_size)) # sgd optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss) # 计算embeddings的L2范式，各元素的平方和然后求平方根，防止过拟合 norm = tf.sqrt( tf.reduce_sum( tf.square(embeddings), axis=1, keep_dims=True)) # 标准化词向量 normalized_embeddings = embeddings / norm valid_embeddings = tf.nn.embedding_lookup( normalized_embeddings, valid_dataset) # valid单词和所有单词的相似度计算，向量相乘 similarity = tf.matmul( valid_embeddings, normalized_embeddings, transpose_b=True) init = tf.global_variables_initializer() 训练过程 123456789101112131415161718192021222324252627282930313233num_steps = 100001with tf.Session(graph=graph) as sess: init.run() print('Initialized') avg_loss = 0 for step in range(num_steps): batch_inputs, batch_labels = wu.generate_batch( batch_size, single_num, skip_window, word_code) feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125; _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict) avg_loss += loss_val if step % 2000 == 0: if step &gt; 0: avg_loss /= 2000 print (\"avg loss at step %s : %s\" % (step, avg_loss)) avg_loss = 0 if step % 10000 == 0: # 相似度，16*50000 sim = similarity.eval() for i in range(valid_size): valid_word = id_topword[valid_examples[i]] # 选相似的前8个 top_k = 8 # 排序，获得id nearest = (-sim[i, :]).argsort()[1:top_k+1] log_str = \"Nearest to %s: \" % valid_word for k in range(top_k): close_word = id_topword[nearest[k]] log_str = \"%s %s,\" % (log_str, close_word) print log_str final_embeddings = normalized_embeddings.eval()","tags":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://plmsmile.github.io/tags/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://plmsmile.github.io/tags/深度学习/"},{"name":"word2vec","slug":"word2vec","permalink":"http://plmsmile.github.io/tags/word2vec/"}]},{"title":"机器学习-朴素贝叶斯","date":"2017-05-06T06:36:58.000Z","path":"2017/05/06/ml-ch03-bayes/","text":"条件概率 基础知识 条件概率 在\\(B\\)发生的情况下\\(A\\)的概率$ $ ​ \\(P(A|B) = \\frac{P(AB)}{P(B)}\\) ​ \\(P(c_i|x)=\\frac{P(c_ix)}{P(x)}\\)。\\(c_i\\)是类别，\\(x\\)是一个向量。\\(x\\)属于类别\\(c_i\\)的概率。 贝叶斯准则 交换条件概率中的条件与结果，得到想要的值。 \\(P(A|B) = \\frac{P(AB)}{P(B)}\\), \\(P(B|A) = \\frac{P(AB)}{P(A)}\\) \\(\\to\\) \\(P(B|A)=\\frac{P(A|B)P(B)}{P(A)}\\) 所以可以得到\\(\\color{red}{P(c_i|x)}=\\frac{P(x|c_i)P(c_i)}{P(x)}\\) 条件概率分类 贝叶斯决策理论 计算两个概率\\(x\\)属于类别1和类别2的概率\\(p_1(x)\\)和\\(p_2(x)\\)。 如果\\(p_1(x) &gt; p_2(x)\\)，则\\(x\\)属于类别1 如果\\(p_2(x) &gt; p_1(x)\\)，则\\(x\\)属于类别2 贝叶斯准则 \\(x\\)属于类别\\(c_i\\)的概率是\\(\\color{red}{P(c_i|x)}\\)。 如果\\(P(c_1|x) &gt; P(c_2|x)\\)，则\\(x\\)属于\\(c_1\\) 如果\\(P(c_2|x) &gt; P(c_1|x)\\)，则\\(x\\)属于\\(c_2\\) 朴素贝叶斯文档分类 简介 机器学习的一个重要应用就是文档的自动分类。我们可以观察文档中出现的词，并把每个词出现与否或者出现次数作为一个特征。朴素贝叶斯就是用于文档分类的常用算法，当然它可以用于任意场景的分类。 向量\\(\\color{red}{\\vec{w}}={(w_1,w_2,...,w_n)}\\)代表一篇文章。其中\\(w_i=0,1\\)，代表词汇表中第\\(i\\)个词汇出现与否。词汇表是指一个总体的全局词汇表。文章\\(\\vec{w}\\)属于第\\(i\\)类的概率\\(\\color{red}{P(c_i|\\vec{w})}=\\frac{P(\\vec{w}|c_i)P(c_i)}{P(\\vec{w})}\\)。 朴素贝叶斯分类器的两个假设： 特征之间相互独立 每个特征同等重要 尽管这有瑕疵，但是朴素贝叶斯的实际效果却很好了。 朴素贝叶斯分类器的两种实现： 伯努利模型：只考虑出现或者不出现 多项式模型：考虑词在文档中的出现次数 文档分类中的独立：每个单词出现的可能性和其他单词没有关系。独立的好处在下面概率计算中会体现出来。 概率计算 对每一个文章的各个分类概率计算，其实只需要计算上式的分母就行了。 对于\\(P(c_i)=\\frac{c_i数量}{总数量}\\)，即\\(c_i\\)类文章的数量除以所有类别的文章的总数量。 对于\\(P(\\vec{w}|c_i)\\)，要稍微复杂一些。由于各个特征（单词出现否）独立，则有如下推导公式： \\[P(\\vec{w}|c_i)=P(w_1,w_2,...,w_n|c_i)=P(w_1|c_i)P(w_2|c_i)\\cdots P(w_n|c_i)\\] 其中\\(\\color{red}{P(w_i|c_i)}\\)代表第\\(i\\)个单词在\\(c_i\\)类别文章的总词汇里出现的概率。 实际操作的一个小技巧，由于概率都很小多个小值做乘法会导致下溢出，所以决定对概率取对数做加法，最后再比较对数的大小。 \\[\\ln(P(\\vec{w}|c_i))=\\ln(P(w_1|c_i))+\\ln(P(w_2|c_i))+\\dots+\\ln(P(w_n|c_i))\\] 如上，可以求得每个单词在各个类别文章里出现的概率。用\\(\\color{red}{\\vec{wp_0}}\\)、\\(\\color{red}{\\vec{wp_1}}\\)来分别表示所有单词在类别0、类别1中总词汇中的概率。当然，在程序中实际上这个概率是取对数了的。 当要求一篇新的文章\\(\\color{red}{\\vec{w}}={(0,1,0,0,\\dots)}\\)，此时为出现或者不出现，当然也可以统计出现次数，属于哪个类别的时候，要先求出\\(\\color{red}{P(w|c_0)}\\)和\\(\\color{red}{P(w|c_1)}\\)，然后根据贝叶斯准则选择概率大的分类为结果。 \\[P(w|c_0)=\\vec{w}\\cdot\\vec{wp_0}, P(w|c_1)=\\vec{w}\\cdot\\vec{wp_1}\\] 程序实现 朴素贝叶斯的实例应有很多，这里主要是介绍垃圾邮件分类。数据集中的邮件有两种：垃圾邮件和正常邮件。每个类型都有25个样本，一共是50个样本。我们对数据集进行划分为训练集和测试集。训练集用来训练获得\\(\\vec{wp_0}\\)、\\(\\vec{wp_1}\\)和\\(p(c_1)\\)。然后用测试集去进行朴素贝叶斯分类，计算错误率，查看效果。 加载数据 数据是存放在两个文件夹中的，以txt格式的形式存储。取出来后要进行单词切割。然后得到邮件列表email_list和它对应的分类列表class_list。 1234567891011121314151617181920212223242526272829303132333435def parse_str(big_str): ''' 解析文本为单词列表 Args: big_str: 长文本 Returns: 单词列表 ''' # 以任何非单词字符切割 word_list = re.split(r'\\W*', big_str) # 只保留长度大于3的单词，并且全部转化为小写 return [word.lower() for word in word_list if len(word) &gt; 2]def load_dataset(spam_dir, ham_dir): ''' 从文件夹中加载文件 Args: spam_dir: 垃圾邮件文件夹 ham_dir: 正常邮件文件夹 Returns: email_list: 邮件列表 class_list: 分类好的列表 ''' email_list = [] class_list = [] txt_num = 25 # 每个文件夹有25个文件 for i in range(1, txt_num + 1): for j in range(2): file_dir = spam_dir if j == 1 else ham_dir f = open(('&#123;&#125;/&#123;&#125;.txt').format(file_dir, i)) f_str = f.read() f.close() words = parse_str(f_str) email_list.append(words) # 邮件列表 class_list.append(j) # 分类标签，1垃圾邮件，0非垃圾邮件 return email_list, class_list 划分数据集 由于前面email_list包含所有的邮件，下标是从0-49，所以我们划分数据集只需要获得对应的索引集合就可以了。 123456789101112131415def get_train_test_indices(data_num): ''' 划分训练集和测试集 Args: data_num: 数据集的数量 Returns: train_indices: 训练集的索引列表 test_indices: 测试集的索引列表 ''' train_indices = range(data_num) test_ratio = 0.3 # 测试数据的比例 test_num = int(data_num * test_ratio) test_indices = random.sample(train_indices, test_num) # 随机抽样选择 for i in test_indices: train_indices.remove(i) return train_indices, test_indices 获得训练矩阵 获得训练数据之后，要把训练数据转化为训练矩阵。 获得所有的词汇 1234567891011def get_vocab_list(post_list): ''' 从数据集中获取所有的不重复的词汇列表 Args: post_list: 多个文章的列表，一篇文章：由单词组成的list Returns: vocab_list: 单词列表 ''' vocab_set = set([]) for post in post_list: vocab_set = vocab_set | set(post) return list(vocab_set) 获得一篇文章的文档向量 12345678910111213141516171819202122def get_doc_vec(doc, vocab_list, is_bag = False): ''' 获得一篇doc的文档向量 词集模型：每个词出现为1，不出现为0。每个词出现1次 词袋模型：每个词出现次数，可以多次出现。 Args: vocab_list: 总的词汇表 doc: 一篇文档，由word组成的list is_bag: 是否是词袋模型，默认为Fasle Returns: doc_vec: 文档向量，1出现，0未出现 ''' doc_vec = [0] * len(vocab_list) for word in doc: if word in vocab_list: idx = vocab_list.index(word) if is_bag == False: # 词集模型 doc_vec[idx] = 1 else: doc_vec[idx] += 1 # 词袋模型 else: print '词汇表中没有 %s ' % word return doc_vec 获得训练矩阵 1234567891011121314151617181920def go_bayes_email(): ''' 贝叶斯垃圾邮件过滤主程序 Returns: error_rate: 错误率 ''' # 源数据 email_list, class_list = load_dataset('email/spam', 'email/ham') # 总的词汇表 vocab_list = bys.get_vocab_list(email_list) # 训练数据，测试数据的索引列表 data_num = len(email_list) train_indices, test_indices = get_train_test_indices(data_num) # 训练数据的矩阵和分类列表 train_mat = [] train_class = [] for i in train_indices: vec = bys.get_doc_vec(email_list[i], vocab_list) train_mat.append(vec) train_class.append(class_list[i]) # 后续还有训练数据和测试数据，在下文给出 贝叶斯算法 贝叶斯训练算法 通过训练数据去计算上文提到的\\(\\vec{wp_0}\\)、\\(\\vec{wp_1}\\)和\\(p(c_1)\\)。 1234567891011121314151617181920212223242526272829303132333435def train_nb0(train_mat, class_list): ''' 朴素贝叶斯训练算法，二分类问题 Args: train_mat: 训练矩阵，文档向量组成的矩阵 class_list: 每一篇文档对应的分类结果 Returns: p0_vec: c0中各个word占c0总词汇的概率 p1_vec: c1中各个word占c1总词汇的概率 p1: 文章是c1的概率 ''' # 文档数目，单词数目 doc_num = len(train_mat) word_num = len(train_mat[0]) # 两个类别的总单词数量 c0_word_count = 2.0 c1_word_count = 2.0 # 向量累加 c0_vec_sum = np.ones(word_num) c1_vec_sum = np.ones(word_num) for i in range(doc_num): if class_list[i] == 0: c0_word_count += sum(train_mat[i]) c0_vec_sum += train_mat[i] else: c1_word_count += sum(train_mat[i]) c1_vec_sum += train_mat[i] c1_num = sum(class_list) p1 = c1_num / float(doc_num) p0_vec = c0_vec_sum / c0_word_count p1_vec = c1_vec_sum / c1_word_count # 由于后面做乘法会下溢出，所以取对数做加法 for i in range(word_num): p0_vec[i] = math.log(p0_vec[i]) p1_vec[i] = math.log(p1_vec[i]) return p0_vec, p1_vec, p1 贝叶斯分类 123456789101112131415def classify_nb(w_vec, p0_vec, p1_vec, p1): ''' 使用朴素贝叶斯分类 Args: w_vec: 要测试的向量 p0_vec: c0中所有词汇占c0的总词汇的概率 p1_vec: c1中所有词汇占c1的总词汇的概率 p1: 文章为类型1的概率，即P(c1) ''' # P(w|c0)*P(c0) = P(w1|c0)*...*P(wn|c0)*P(c0) # 由于下溢出，所以上文取了对数，来做加法 w_p0 = sum(w_vec * p0_vec) + math.log(1 - p1) w_p1 = sum(w_vec * p1_vec) + math.log(p1) if w_p0 &gt; w_p1: return 0 return 1 训练数据 1p0_vec, p1_vec, p1 = bys.train_nb0(train_mat, train_class) 测试数据 一次执行 12345678910111213141516def go_bayes_email(): # 此处省略上文的部分内容 # 训练数据 p0_vec, p1_vec, p1 = bys.train_nb0(train_mat, train_class) # 测试数据 error_count = 0 for i in test_indices: vec = bys.get_doc_vec(email_list[i], vocab_list) res = bys.classify_nb(vec, p0_vec, p1_vec, p1) if res != class_list[i]: error_count += 1 error_rate = error_count / float(data_num) print 'error=%d, rate=%s, test=%d, all=%d' % (error_count, error_rate, len(test_indices), data_num) return error_rate 多次执行，取平均值 1234567def test_bayes_email(): ''' 执行多次go_bayes_email，计算平均错误率 ''' times = 100 error_rate_sum = 0.0 for i in range(10): error_rate_sum += go_bayes_email() print 'average_rate = %s' % (error_rate_sum / 10) 源代码","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","permalink":"http://plmsmile.github.io/tags/朴素贝叶斯/"}]},{"title":"NumPy","date":"2017-04-15T07:32:52.000Z","path":"2017/04/15/NumPy/","text":"NumPy教程 基础 简单demo NumPy中最重要的对象是ndarray，是一个N维数组。它存储着相同类型的元素集合。通过dtype来获取类型，索引来获取值。 通过numpy.array来创建ndarray。 123456789101112# 1. numpy定义numpy.array(object, dtype = None, copy = True, order = None, subok = False, ndmin = 0)# 2. demoimport numpy as npa = np.array([[1, 2], [3, 4]])# 3. 使用dtype#int8, int16, int32, int64 可替换为等价的字符串 'i1', 'i2', 'i4', 以及其他。dt = np.dtype(np.int32)student = np.dtype([('name','S20'), ('age', 'i1'), ('marks', 'f4')])a = np.array([('tom', 23, 89), ('sara', 22, 97)], dtype=student) ndarray.shape 和reshape 获取数组维度 ，也可以调整大小 123456789a = np.array([[1,2,3], [4,5,6]]) print a.shape# (2, 3)b = a.reshape(3,2) b.shapeprint b[[1, 2] [3, 4] [5, 6]] ndarray.ndim 数组的维数 123import numpy as npa = np.arange(24).reshape(2, 12) # 2b = a.reshape(2, 3, 4) # 3 创建数组 输入数组建立 1a = np.array([[1,2,3], [4,5,6]]) zeros, ones, empty 123456# zeros创建0矩阵np.zeros((3, 4))# ones创建1矩阵np.ones((2, 3, 4), dtype=np.int16)# empty不初始化数组，值随机np.empty((2, 3)) arange, linspace 创建随机数，整数和浮点数，步长 12345678# 创建[0, n-1]的数组np.arange(3)# 创建1-10范围类，3个数np.arange(1, 10, 3)# 取均值步长np.linspace(0, 1.5, 3)# array([ 0. , 0.75, 1.5 ]) 基本操作 基本数学操作 1234567891011121314151617181920212223242526272829303132333435363738394041a = np.array([20, 30, 40, 50])b = np.arrange(4)# 减法c = b - a # 乘法b * 2 # 新建一个矩阵b *= 2 # 直接改变b，不会新建一个矩阵 a += 2 同理# 次方b ** 2# 判断a &lt; 30 # array([ True, False, False, False], dtype=bool)10 * np.sin(a)# 矩阵乘法A = np.array([[1, 1], [0, 1]])B = np.array([[2, 0], [3, 4]])A.dot(B)B.dot(A)np.dot(A, B)# sum, max, mina = np.arange(12).reshape(3, 4)array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])# 所有元素sum, min, maxa.sum()a.max()# 使用axis=0按列, axis=1按行a.sum(axis=0)array([12, 15, 18, 21])a.sum(axis=1)array([ 6, 22, 38])# 通用函数B = np.arange(3)np.exp(B) # 求e的次方np.sqrt(B) # 开方C = np.array([2, -1, 4])np.add(B, C) # 相加 访问元素，index, slice, iterator 12345678910111213141516171819202122232425262728293031# 1. 一维数组a = np.arange(4)**2 # array([0, 1, 4, 9])# 下标访问a[1] # 从0开始 # 1# 切片，同python切片a[1:3] # array([1, 4])# 迭代for i in a: print (i*2) # 2. 多维数组a = np.fromfunction(lambda i, j: i + j, (3, 3), dtype=int) # 对下标进行操作array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])a[1, 2] # 访问到 3# 访问第2列a[0:3, 1] # array([1, 2, 3])a[:, 1] # array([1, 2, 3]) # 第i+1行a[1] # 第2行a[-1] # 最后一行a[1, ...] # 第2行，多维的时候这样写# 第2、3行a[1:3, :]a[1:3]for row in a: print rowfor e in a.flat: print e 切片(start, end, step) 123456789101112131415161718# 1. (start, end, step)a = np.arange(10)s = slice(2, 7, 2) b = a[s]# 2. 1-7, step=3, 不包括7b = a[1:7:3]# 3. 从idx开始向后切，包括idxb = a[2:]# 4. start, end, 不包括endb = a[2:5]# 5. a = np.array([[1,2,3],[3,4,5],[4,5,6]]) a[..., 1] #第2列 [2 4 5]a[1, ...] #第2行 [3 4 5]a[...,2:] #第2列及其剩余元素[[2 3] [4 5] [5 6]] 索引 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 1. 一维时a = np.arange(5)**2 # array([ 0, 1, 4, 9, 16])# 索引1i = np.array([1, 3, 4]) # idx 为1,3,4的元素a[i] # 访问元素 array([ 1, 9, 16])# 索引2j = np.array([ [1, 2], [3, 4]])a[j]array([[ 1, 4], [ 9, 16]])# 2. 二维时x = np.array([[1, 2], [3, 4], [5, 6]]) [[1 2] [3 4] [5 6]]# 索引y = x[ [0, 1, 2], [0, 1, 0]] # [0, 1, 2]是行, [0, 1, 0]是对应行的列[1 4 5]# 3. 没看懂x = np.array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]]) [[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]# 索引 rows = np.array([ [0,0], [3,3] ])cols = np.array([ [0,2], [0,2] ])i = [rows, cols]y = x[i][[ 0 2] [ 9 11]]# 4. 切片+索引x = np.array([[ 0, 1, 2],[ 3, 4, 5],[ 6, 7, 8],[ 9, 10, 11]]) # 切片, 1-3行, 1-2列z = x[1:4,1:3] [[ 4 5] [ 7 8] [10 11]]# 高级索引来切片, 1-3行,1、2列y = x[1:4, [1,2]][[ 4 5] [ 7 8] [10 11]] Shape Manipulation 改变形状 1234567891011121314151617a = np.floor(10*np.random.random((3,4))) # &lt;1的小数*10，取整array([[ 9., 6., 3., 8.], [ 2., 8., 4., 2.], [ 5., 3., 3., 1.]])# 形状a.shape(3, 4)# 打平，返回arraya.ravel()array([ 9., 6., 3., 8., 2., 8., 4., 2., 5., 3., 3., 1.])# reshape 生成新的a.reshape(2, 6) a.reshape(3, -1) # 给定一个，自动计算另外的# resize 改变自己a.resize(4, 3)# 转置a.T 堆积不同的阵列 123456789101112131415161718192021a = np.floor(10*np.random.random((2, 2)))b = np.floor(10*np.random.random((2, 2)))# 垂直堆积 (4, 2)np.vstack((a, b))[[ 1., 7.], [ 9., 8.], [ 9., 0.], [ 8., 6.]]# 水平堆积 (2, 4)np.hstack((a, b))array([[ 1., 7., 9., 0.], [ 9., 8., 8., 6.]])# 特别的，针对一维的列堆积a = np.array((1,2,3))b = np.array((2,3,4))np.column_stack((a,b))[[1, 2], [2, 3], [3, 4]]","tags":[{"name":"Python","slug":"Python","permalink":"http://plmsmile.github.io/tags/Python/"},{"name":"NumPy","slug":"NumPy","permalink":"http://plmsmile.github.io/tags/NumPy/"}]},{"title":"机器学习-西瓜书-第一章习题","date":"2017-04-04T04:07:22.000Z","path":"2017/04/04/ml-watermelon-chap1/","text":"$ $ 1.1版本空间 题目 表1.1中若只包含编号为1和4的两个用例，试给出相应的版本空间。 背景知识 假设空间：假设数据有\\(n\\)中属性，第\\(i\\)个属性可能的取值有\\(t_i\\)种，加上该属性的泛化取值(*)，所以可能的假设有\\(\\prod_{i=1}^n {(t_i+1)}\\)种。再用空集表示没有正例，所以假设空间一共有\\(\\prod_{i=1}^n {(t_i+1)} + 1\\)种假设。 学习过程：在假设空间中进行搜索去找到与训练集匹配的假设。即能够将训练集中的瓜判断正确的假设。 版本空间：多个与训练集一致的假设组成的集合。 解答 西瓜数据集，本题只取1和4。 编号 色泽 根蒂 敲声 好瓜 1 青绿 蜷缩 浊响 是 2 乌黑 蜷缩 浊响 是 3 青绿 硬挺 清脆 否 4 乌黑 稍蜷 沉闷 否 获得好瓜的布尔表达式是：\\(好瓜\\leftrightarrow (色泽=?)\\wedge(根蒂=?)\\wedge(敲声=?)\\)。 三个特征的取值分别是： 色泽：青绿，乌黑，* (*是说什么色泽都行，下面同理) 根蒂：蜷缩，稍蜷，* 敲声，浊响，沉闷，* 当然也可能本身没有“好瓜”这种东西，我们用\\(\\emptyset\\)来表示。 综上，一共有 \\(3\\times3\\times3+1=27\\)种假设。所以假设空间如下(先对正样本最大泛化)： 编号 色泽 根蒂 敲声 符合正样本？ 1 青绿 蜷缩 浊响 是 2 青绿 蜷缩 沉闷 3 青绿 蜷缩 * 是 4 青绿 稍蜷 浊响 5 青绿 稍蜷 沉闷 6 青绿 稍蜷 * 7 青绿 * 浊响 是 8 青绿 * 沉闷 9 青绿 * * 是 10 乌黑 蜷缩 浊响 11 乌黑 蜷缩 沉闷 12 乌黑 蜷缩 * 13 乌黑 稍蜷 浊响 14 乌黑 稍蜷 沉闷 15 乌黑 稍蜷 * 16 乌黑 * 浊响 17 乌黑 * 沉闷 18 乌黑 * * 19 * 蜷缩 浊响 是 20 * 蜷缩 沉闷 21 * 蜷缩 * 22 * 稍蜷 浊响 23 * 稍蜷 沉闷 24 * 稍蜷 * 是 25 * * 浊响 是 26 * * 沉闷 27 * * * 28 \\(\\emptyset\\) 所以版本空间为如下，共7个。 编号 色泽 根蒂 敲声 好瓜 1 青绿 蜷缩 浊响 是 3 青绿 蜷缩 * 是 7 青绿 * 浊响 是 9 青绿 * * 是 19 * 蜷缩 浊响 是 24 * 稍蜷 * 是 25 * * 浊响 是 1.2析合范式 题目 1.2 与使用单个合取式来进行假设表示相比，使用“析合范式”将使得假设空间具有更强的表示能力。例如： \\[ 好瓜\\leftrightarrow\\left((色泽=*) \\wedge(根蒂=蜷缩)\\wedge(敲声=*)\\right)\\vee\\left((色泽=乌黑)\\wedge(根蒂=*)\\wedge(敲声=沉闷)\\right) \\] 若使用包含\\(k\\)个合取式的析合范式来表达表1.1(上文中有)西瓜分类问题的假设空间，试估算共有多少种假设的可能。 解答 表1.1中4个样例，3个属性。假设空间中共有\\(3\\times4\\times4+1=49\\)种假设。 不考虑冗余 在假设空间中选取\\(k\\)个来组成析合范式，则有\\(\\sum_{k=1}^n {C_{49}^K}=\\color{red}{2^{49}}\\)种可能。但是其中包含了很多冗余的情况。 考虑冗余(忽略空集) 48种假设中： 具体假设：\\(2\\times3\\times3=18\\)种 1个属性泛化假设：\\(1\\times3\\times3+2\\times1\\times3+2\\times3\\times1=21\\)种 2个属性泛化假设：\\(2\\times1\\times1+1\\times3\\times1+1\\times1\\times3=8\\)种 3个属性泛化假设：\\(1\\times1\\times1=1\\)种 \\(k\\)的范围是：\\(1\\leq k \\leq18\\)。取1个或者所有的具体假设。 当\\(k=1\\)时，即只选取一种假设，这样不会有冗余情况，有\\(\\color {red} {48}\\)种可能。 当\\(k=18\\)时，即所有的具体假设，只有\\(\\color {red} {1}\\)种可能。 当\\(1&lt;k&lt;18\\)时，进行编程去循环遍历，按照：3属性泛化、2属性泛化、1属性泛化、具体属性排序，去遍历枚举。具体参见。 1.3归纳偏好 题目 若数据包含噪声，则假设空间中有可能不存在与所有训练样本都一致的假设。在此情形下，试设计一种归纳偏好用于假设选择。即不存在训练错误为0的假设。 解答 通常认为两个数据的属性越相近，则更倾向于把它们分为同一类。若相同属性出现了两种不同的分类，则认为它属于与它最邻近几个数据的属性。也可以考虑同时去掉所有具有相同属性而不同分类的数据，留下的就是没有误差的数据，但是可能会丢失部分信息。","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://plmsmile.github.io/tags/机器学习/"},{"name":"周志华","slug":"周志华","permalink":"http://plmsmile.github.io/tags/周志华/"},{"name":"西瓜书","slug":"西瓜书","permalink":"http://plmsmile.github.io/tags/西瓜书/"}]},{"title":"Spark-Programming","date":"2017-03-25T10:07:35.000Z","path":"2017/03/25/Spark-Programming/","text":"总览 Spark程序 有一个驱动程序，会运行用户的主要功能，并且在集群上执行各种并行操作。 RDD RDD是跨集群节点分区的、并且可以并行计算的分布式数据集合。可以通过外部文件系统或者内部集合来创建。可以在内存中持久化一个RDD，并且在并行计算中有效地重用。RDD可以从节点故障中自动恢复。 共享变量 当一组任务在不同的节点上并行运行一个函数时，Spark会为函数中的每个变量发送一个副本到各个任务中去(低效)。有时，变量需要在任务与任务、任务与驱动程序间共享。Spark有两种共享变量。 累加器：将工作节点中的值聚合到驱动程序中 广播变量：在各个节点中cache一个只读变量 SparkContext Spark的主要入口点。使用它可以连接到集群、创建RDD和广播变量。 RDD RDD是Spark中最核心的概念。 这是一个分布式的、容忍错误的、能并行操作的数据集合。 RDD是一个分布式的不可变的对象集合，可以包含任意对象。 每个RDD都会被分为多个分区，这些分区运行在不同的节点上。 Spark会自动把RDD的数据分发到集群上，并且并行化执行相关操作。 记录如何转化、计算数据的指令列表。 Spark中对数据的所有操作都是创建RDD、转化已有RDD、调用RDD操作进行求值。 创建RDD 创建RDD有两种方式：驱动程序内部的集合，外部系统的数据集(如HDFS, HBase等)。 集合 从集合中创建RDD，会把集合中的元素复制去创建一个可以并行执行的分布式数据集。 Spark可以对这些并行集合进行分区，把这些数据切割到多个分区。Spark会为集群的每个分区运行一个Task。一般，我们需要为集群中的每个CPU分配2-4个分区。默认，Spark会根据集群尝试自动设置分区数。但我们也可以手动地设置分区数。(有的代码中也称partition为slice) 123rdd = sc.parallelize([1, 2, 3, 4])rdd.reduce(lambda x, y: x + y) # 求和rdd2 = sc.parallelize(['Spark', 'Hadoop', 'ML', 'Python', 'Data'], 2) # 设置2个分区 外部数据集 Spark可以从本地文件系统、HDFS、Cassandra、HBase、Amazon S3等创建数据。支持Text、SequenceFile和任何其他Hadoop的Input Format。 Spark读取文件textFile的一些说明： 本地文件使用本地路径读取文件时，该文件也得在其它的worker node的相同路径上访问到。可以把文件复制过去或者使用network-mounted的文件共享系统。 支持文件 、文件夹、通配符、压缩文件(.gz)。 可以设置分区数。默认，Spark为文件的每一个块创建一个分区。(HDFS的block是128MB)。可以传递一个更大的值来请求更多的分区。 RDD操作 RDD主要有2个操作。 转化操作：由一个RDD生成一个新的RDD(Dataset)。惰性求值。 行动操作：会对RDD(Dataset)计算出一个结果或者写到外部系统。会触发实际的计算。 Spark会惰性计算这些RDD，只有第一次在一个行动操作中用到时才会真正计算。 一般，Spark会在每次行动操作时重新计算转换RDD。如果想复用，则用persist把RDD持久化缓存下来。可以持久化到内存、到磁盘、在多个节点上进行复制。这样，在下次查询时，集群可以更快地访问。 Spark程序大体步骤如下。 从外部数据创建输入RDD。如textFile 使用转化操作得到新的RDD。如map，filter 对重用的中间结果RDD进行持久化。如persist 使用行动操作来触发一次并行计算。如count, first 123456789# 从外部创建一个rdd。此时并没有把数据加载到内存中。lines只是一个指向文件的指针lines = sc.textFile(\"data.txt\")# 转化。没有进行真实的计算，因为惰性求值lineLengths = lines.map(lambda s: len(s))# 持久化lineLengths.persist()# 行动。Spark把计算分解为一些任务，这些任务在单独的机器上进行运算。# 每个机器只做属于自己map的部分，并且在本地reduce。返一个结果给DriverProgramtotalLength = lineLengths.reduce(lambda a, b: a + b) 传递函数给Spark Spark的API很多都依赖于传递函数来在集群上面运行。有下面3种方式可以使用： Lambda表达式：简单功能。不支持多语句函数、不支持没有返回值的语句。 本地def函数，调用spark。 模块的顶级函数。 代码较多时 1234def my_func(s): words = s.split(\" \") return len(words)len_rdd = sc.textFile(\"word.txt\").map(my_func) 对象方法时 千万不要引用self，这样会把整个对象序列化发送过去。而我们其实只需要一个方法或者属性就可以了，我们可以copy一份局部变量传递过去。 123456789101112131415161718class SearchFunctions(object): def __init__(self, query): self.query = query def is_match(self, s): return self.query in s def get_matches_func_ref(self, rdd): \"\"\"问题: self.is_match引用了整个self \"\"\" return rdd.filter(self.is_match) def get_matches_attr_ref(self, rdd): \"\"\"问题：self.query引用了整个self \"\"\" return rdd.filter(lambda s: self.query in s) def get_matches_no_ref(self, rdd): \"\"\"正确做法：使用局部变量 \"\"\" query = self.query return rdd.filter(lambda s: query in s) 理解闭包 当在集群上面执行代码时，理解变量和方法的范围和生命周期是很重要并且困难的。先看一段代码。 12345678910counter = 0rdd = sc.parallelize(data)# Wrong: Don't do this!!请使用Accumulatordef increment_counter(x): global counter counter += xrdd.foreach(increment_counter)print(\"Counter value: \", counter) 执行job的时候，Spark会把处理RDD的操作分解为多个任务，每个任务会由一个执行器executor执行。执行前，Spark会计算任务的闭包。闭包其实就是一些变量和方法，为了计算RDD，它们对于执行器是可见的。Spark会把闭包序列化并且发送到每一个执行器。 发送给执行器的闭包里的变量其实是一个副本，这些执行器程序却看不到驱动器程序节点的内存中的变量(counter)，只能看到自己的副本。当foreach函数引用counter的时候，它使用的不是驱动器程序中的counter，而是自己的副本。 本地执行时，有时候foreach函数会在和driver同一个JVM里面执行，那么访问的就是最初的counter，也会对其进行修改。 一般，我们可以使用累加器Accumulator，它可以安全地修改一个变量。闭包不应该修改全局变量。如果要进行全局聚合，则应该使用累加器。 在本地模式，rdd.foreach(println)的时候，会打印出所有的RDD。但是在集群模式的时候，执行器会打印出它自己的那一部分，在driver中并没有。如果要在driver中打印，则需要collect().foreach()，但是只适用于数据量小的情况。因为collect会拿出所有的数据。 键值对RDD 详细的知识参见Spark的键值对RDD。 Shuffle操作 shuffle说明 Shuffle是Spark中重新分布数据的机制，因此它在分区之间分组也不同。主要是复制数据到执行器和机器上，这个很复杂而且很耗费。 以reduceByKey为例，一个key的所有value不一定在同一个partition甚至不在同一个machine，但是却需要把这些values放在一起进行计算。单个任务会在单个分区上执行。为了reduceByKey的reduce任务，需要获得所有的数据。Spark执行一个all-to-all操作，会在所有分区上，查找所有key的所有value，然后跨越分区汇总，去执行reduce任务。这就是shuffle。 shuffle后，分区的顺序和分区里的元素是确定的，但是分区里元素的顺序却不是确定的。可以去设置确定顺序。 性能影响 Shuffle涉及到磁盘IO、数据序列化、网络IO。组织data：一系列map任务；shuffle这些data；聚合data：一系列reduce任务。 一些map的结果会写到内存里，当太大时，会以分区排好序，然后写到单个文件里。在reduce端，task会读取相关的有序的block。 Shuffle操作会占用大量的堆内存，在传输data之前或者之后，都会使用内存中的数据结构去组织这些record。也就是说，在map端，会创建这些structures，在reduce端会生成这些structures。在内存中存不下时，就会写到磁盘中。 Shuffle操作会在磁盘上生成大量的中间文件，并且在RDD不再被使用并且被垃圾回收之前，这些文件都将被一直保留。因为lineage(血统,DAG图)要被重新计算的话，就不会再次shuffle了。如果保留RDD的引用或者垃圾回收不频繁，那么Spark会占用大量的磁盘空间。文件目录可由spark.local.dir配置。 我们可以在Spark的配配置指南中配置各种参数。 RDD持久化 介绍 Spark一个重要的特性是可以在操作的时候持久化缓存RDD到内存中。Persist一个RDD后，每个节点都会将这个RDD计算的所有分区存储在内存中，并且会在后续的计算中进行复用。这可以让future actions快很多(一般是10倍)。缓存是迭代算法和快速交互使用的关键工具。 持久化RDD可以使用persist或cache方法。会先进行行动操作计算，然后缓存到各个节点的内存中。Spark的缓存是fault-tolerant的，如果RDD的某些分区丢失了，它会自动使用产生这个RDD的transformation进行重新计算。 类别 出于不同的目的，持久化可以设置不同的级别。例如可以缓存到磁盘，缓存到内存(以序列化对象存储，节省空间)等，然后会复制到其他节点上。可以对persist传递StorageLevel对象进行设置缓存级别，而cache方法默认的是MEMORY_ONLY，下面是几个常用的。 MEMORY_ONLY(default): RDD作为反序列化的Java对象存储在JVM中。如果not fit in memory，那么一些分区就不会存储，并且会在每次使用的时候重新计算。CPU时间快，但耗内存。 MEMORY_ONLY_SER: RDD作为序列化的Java对象存储在JVM中，每个分区一个字节数组。很省内存，可以选择一个快速的序列化器。CPU计算时间多。只是Java和Scala。 MEMORY_AND_DISK: 反序列化的Java对象存在内存中。如果not fit in memory，那么把不适合在磁盘中存放的分区存放在内存中。 MEMORY_AND_DISK_SER: 和MEMORY_ONLY_SER差不多，只是存不下的再存储到磁盘中，而不是再重新计算。只是Java和Scala。 名字 占用空间 CPU时间 在内存 在磁盘 MEMORY_ONLY 高 低 是 否 MEMORY_ONLY_SER 低 高 是 否 MEMORY_AND_DISK 高 中等 部分 部分 MEMORY_AND_DISK_SER 低 高 部分 部分 所有的类别都通过重新计算丢失的数据来保证容错能力。完整的配置见官方RDD持久化。 在Python中，我们会始终序列化要存储的数据，使用的是Pickle，所以不用担心选择serialized level。 在shuffle中，Spark会自动持久化一些中间结果，即使用户没有使用persist。这样是因为，如果一个节点failed，可以避免重新计算整个input。如果要reuse一个RDD的话，推荐使用persist这个RDD。 选择 Spark的不同storage level是为了在CPU和内存的效率之间不同的权衡，按照如下去选择： 如果适合MEMORY_ONLY，那么就这个。CPU效率最高了。RDD的操作速度会很快！ 如果不适合MEMORY_ONLY，则尽量使用MEMORY_ONLY_SER，然后选个快速序列化库。这样更加节省空间，理论上也能够快速访问。 不要溢写到磁盘。只有这两种才溢写到磁盘：计算数据集非常耗费资源；会过滤掉大量的数据。 如果要快速故障恢复，那么使用复制的storage level。虽然有容错能力，但是复制了，却可以直接继续执行任务，而不需要等待重新计算丢失的分区。 移除数据 Spark会自动监视每个节点上的缓存使用情况，并且以LRU最近最少使用的策略把最老的分区从内存中移除。当然也可以使用rdd.unpersist手动移除。 内存策略：移除分区，再次使用的时候，就需要重新计算。 内存和磁盘策略：移除的分区会写入磁盘。 共享变量 一般，把一个函数f传给Spark的操作，f会在远程集群节点上执行。当函数f在节点上执行的时候，会对所有的变量复制一份副本到该节点，然后利用这些副本单独地工作。对这些副本变量的更新修改不会传回驱动程序，只是修改这些副本。如果要在任务之间支持一般读写共享的变量是很低效的。 Spark支持两种共享变量： 广播变量：用来高效地分发较大的只读对象 累加器：用来对信息进行聚合 广播变量 简介 广播变量可以让程序高效地向所有工作节点发送一个较大的只读值，供一个或多个Spark操作共同使用。 例如较大的只读查询表、机器学习中的一个很大的特征向量，使用广播变量就很方便。这会在每台机器上cache这个变量，而不是发送一个副本。 Spark的Action操作由一组stage组成，由分布式的&quot;shuffle&quot;操作隔离。Spark会自动广播每个stage的tasks需要的common data。这种广播的数据，是以序列化格式缓存的，并且会在每个任务运行之前反序列化。 创建广播变量只有下面两种情况有用： 多个stage的task需要相同的数据 以反序列化形式缓存数据很重要 存在的问题： Spark会自动把闭包中引用到的变量发送到工作节点。方便但是低效。 可能在并行操作中使用同一个变量，但是Spark会为每个操作都发送一次这个变量。 有的变量可能很大，为每个任务都发送一次代价很大。后面再用的话，则还要重新发送。 广播变量来解决： 其实就是一个类型为spark.broadcast.BroadCast[T]的变量。 可以在Task中进行访问。 广播变量只会发送到节点一次，只读。 一种高效地类似BitTorrent的通信机制。 使用方法 对于一个类型为T的对象，使用SparkContext.broadcast创建一个BroadCast[T]。要可以序列化 通过value属性访问值 变量作为只读值会发送到各个节点一次，在自己的节点上修改不会影响到其他变量。 累加器 简介 累加器可以把工作节点中的数据聚合到驱动程序中。类似于reduce，但是更简单。常用作对事件进行计数。累加器仅仅通过关联和交换的操作来实现累加。可以有效地支持并行操作。Spark本身支持数值类型的累加器，我们也可以添加新的类型。 用法 在驱动器程序中，调用SparkContext.accumulator(initialValue)创建一个有初始值的累加器。返回值为org.apache.spark.Accumulator[T] Spark的闭包里的执行器代码可以用累加器的+=来累加。 驱动器程序中，调用累加器的value属性来访问累加器的值 工作节点上的任务不能访问累加器的值 例子 累加空行 123456789101112131415file = sc.textFile(\"callsign_file\")# 创建累加器Accumulator[Int]并且赋初值0blank_line_count = sc.accumulator(0)def extract_callsigns(line): \"\"\"提取callsigns\"\"\" global blank_line_count # 访问全局变量 if line == \"\": blank_line_count += 1 # 累加 return line.split(\" \")callsigns = file.flatMap(extract_callsigns)callsigns.saveAsTextFile(output_dir + \"/callsigns\")# 读取累加器的值 由于惰性求值，只有callsigns的action发生后，才能读取到值print \"Blank lines count: %d\" % blank_line_count.value 进行错误计数 1234567891011121314151617181920212223# 创建用来验证呼号的累加器valid_signcount = sc.accumulator(0)invalid_signcount = sc.accumulator(0)def valid_datesign(sign): global valid_signcount, invalid_sign_count if re.match(r\"\\A\\d?[a-zA-Z]&#123;1,2&#125;\\d&#123;1,4&#125;[a-zA-Z]&#123;1, 3&#125;\\Z\", sign): valid_signcount += 1 return True else: invalid_signcount += 1 return False# 对每个呼号的联系次数进行计数valid_signs = callsigns.filter(valid_datesign)contact_count = valid_signs.map(lambda sign: (sign, 1)).reduceByKey(lambda (x, y): x+y)# 强制求值计算计数contact_count.count()if invalid_signcount.value &lt; 0.1 * valid_signcount.value: contact_count.saveAsTextFile(output_dir + \"/contactcount\")else: print \"Too many errors: %d in %d\" % (invalid_signcount.value, valid_signcount.value) 12345678sign_prefixes = sc.broadcast(load_callsign_table())def process_sign_count(sign_count, sign_prefixes): country = lookup_country(sign_count[0], sign_prefixes.value) count = sign_count[1] return (country, count)country_contack_counts =","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"RDD","slug":"RDD","permalink":"http://plmsmile.github.io/tags/RDD/"}]},{"title":"Spark-SQL","date":"2017-03-19T14:34:44.000Z","path":"2017/03/19/Spark-SQL/","text":"基础 概念 Spark SQL是用来处理结构化数据的模块。与基本RDD相比，Spark SQL提供了更多关于数据结构和计算方面的信息(在内部有优化效果)。通常通过SQL和Dataset API来和Spark SQL进行交互。 SQL: 进行SQL查询，从各种结构化数据源(Json, Hive, Parquet)读取数据。返回Dataset/DataFrame。 Dataset: 分布式的数据集合。 DataFrame 是一个组织有列名的Dataset。类似于关系型数据库中的表。 可以使用结构化数据文件、Hive表、外部数据库、RDD等创建。 在Scala和Java中，DataFrame由Rows和Dataset组成。在Scala中，DataFrame只是Dataset[Row]的类型别名。在Java中，用Dataset表示DataFrame 开始 SparkSession SparkSession是Spark所有功能的入口点，用SparkSession.builder()就可以。 1234567from pyspark.sql import SparkSessionspark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL basic example\") \\ .config(\"spark.some.config.option\", \"some-value\") \\ .getOrCreate() DataFrameReader 从外部系统加载数据，返回DataFrame。例如文件系统、键值对等等。通过spark.read来获取Reader。 123456789101112131415# 1. json 键值对df1 = spark.read.json(\"python/test_support/sql/people.json\")df1.dtypes# [('age', 'bigint'), ('name', 'string')]df2 = sc.textFile(\"python/test_support/sql/people.json\")# df1.dtypes 和 df2.dtypes是一样的# 2. text 文本文件 # 每一行就是一个Row，默认的列名是Valuedf = spark.read.text(\"python/test_support/sql/text-test.txt\")df.collect()# [Row(value=u'hello'), Row(value=u'this')]# 3. load# 从数据源中读取数据 创建DataFrames 从RDD、Hive Table、Spark data source、外部文件中都可以创建DataFrames。 通过DataFrameReader，读取外部文件 1234# spark.read返回一个DataFrameReaderdf = spark.read.json(\"examples/src/main/resources/people.json\")df.show()df.dtypes 通过spark.createDataFrame()，读取RDD、List或pandas.DataFrame 12345678910111213141516171819202122232425262728person_list = [('AA', 18), ('PLM', 23)]rdd = sc.parallelize(person_list) # 1. listdf = spark.createDataFrame(person_list) # 没有指定列名，默认为_1 _2df = spark.createDataFrame(person_list, ['name', 'age']) # 指定了列名df.collect() # df.show()#[Row(name='AA', age=18), Row(name='PLM', age=23)]# 2. RDDrdd = sc.parallelize(person_list)df = spark.createDataFrame(rdd, ['name', 'age'])# 3. Rowfrom pyspark.sql import RowPerson = Row('name', 'age') # 指定模板属性persons = rdd.map(lambda r: Person(*r)) # 把每一行变成Persondf = spark.createDataFrame(persons)df.collect()# 4. 指定类型StructTypefrom pyspark.sql.types import *field_name = StructField('name', StringType(), True) # 名，类型，非空field_age = StructField('age', IntegerType, True)person_type = StructType([field_name, field_age])# 通过链式add也可以# person_type = StructType.add(\"name\", StringType(), True).add(...)df = spark.createDataFrame(rdd, person_type) Row Row是DataFrame中的，它可以定义一些属性，这些属性在DataFrame里面可以被访问。比如：row.key(像属性)和row['key'](像dict) 1234567891011121314151617181920from pyspark import Row# 1. 创建一个模板Person = Row('name', 'age') # &lt;Row(name, age)&gt;'name' in Person # True，有这个属性'sex' in Person # False# 2. 以Person为模板，创建alice, plmalice = Person('Alice', 22) # Row(name='Alice', age=22)plm = Person('PLM', 23)# 访问属性name, age = alice['name'], alice['age']# 返回dictplm.asDict()# &#123;'age': 23, 'name': 'PLM'&#125;# 3. 多个person创建一个DataFramep_list = [alice, plm]p_df = spark.createDataFrame(p_list)p_df.collect()# [Row(name=u'Alice', age=22), Row(name=u'PLM', age=23)] DataFrame的操作 在2.0中，DataFrames只是Scala和Java API中的Rows数据集。它的操作称为非类型转换，与带有强类型Scala和Java数据集的类型转换相反。 Python tips: df.age和df['age']都可以使用，前者在命令行里面方便，但是建议使用后者。 12345df.printSchema()df.select(\"name\").show()df.select(df['name'], df['age'] + 1).show()df.filter(df['age'] &gt; 21).show()df.groupBy(\"age\").count().show() DataFrame的Python Api，DataFrame的函数API 编程方式运行SQL 通过spark.sql()执行，返回一个DataFrame 12345678910111213# 通过df创建一个本地临时视图，与创建这个df的SparkSession同生命周期df.createOrReplaceTempview(\"people\")sqlDF = spark.sql(\"SELECT * FROM people\")sqlDF.show()# 展示为TablesqlDf.collect()# [Row(age=None, name=u'Michael'), Row(age=30, name=u'Andy'), Row(age=19, name=u'Justin')]# 全局临时视图# 在所有session中共享，直到spark application停止df.createGlobalTempView(\"people\")spark.sql(\"SELECT * FROM global_temp.people\").show()spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"Spark SQL","slug":"Spark-SQL","permalink":"http://plmsmile.github.io/tags/Spark-SQL/"}]},{"title":"博客搭建过程及问题","date":"2017-03-13T14:52:41.000Z","path":"2017/03/13/博客搭建过程及问题/","text":"一直都想搭建一个博客，今天终于把博客给初步搭建好了。搭建的过程其实不那么顺利，所以简答记录一下。 搭建过程 根据手把手搭建博客教程这篇文章来进行搭建，其中目前只看了它的第一页 后期在hexo主题里面选择了even主题 依照even-wiki来添加标签、分类和about页面 修改主题目录下的_config.yml的menu，把home、tags等手动改成中文了 遇到的坑 没有看wiki，自己去谷歌建立tags、categories等页面 建立好tags，却在tags页面没有看到相应的标签。是因为没有为tags/index.md设置layout为tags 中文语言，在站点目录下的_config.yml中设置language: zh-cn。 博客重新搭建 配置及源文件 因为经常重装系统，所以博客也需要重新恢复。先配置git相关信息 1234567git config --global user.name \"plmsmile\"git config --global user.email \"pulimingspark@163.com\"ssh-keygen -t rsa -C \"pulimingspark@163.com\"# 去GitHub上添加sshkeycat ~/.ssh/id_rsa.pub# 完成后，进行测试ssh -T git@github.com 然后把之前的PLMBlogs拷贝到D盘，一般目录是d/PLMBlogs 1234567cd PLMBlogs# 安装hexonpm install hexo-cli -g# 安装插件npm install hexo-deployer-git --save# 安装依赖npm install 数学公式渲染 执行完上面的操作后，执行如下 1234hexo cleanhexo generate # 这一步会报错 hexo serverhexo deploy 错误信息如下 123456789101112131415161718192021222324252627282930313233FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: write EPIPE at exports._errnoException (util.js:1020:11) at Socket._writeGeneric (net.js:711:26) at Socket._write (net.js:730:8) at doWrite (_stream_writable.js:331:12) at writeOrBuffer (_stream_writable.js:317:5) at Socket.Writable.write (_stream_writable.js:243:11) at Socket.write (net.js:657:40) at Hexo.pandocRenderer (D:\\PLMBlogs\\node_modules\\hexo-renderer-pandoc\\index.js:64:15) at Hexo.tryCatcher (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\util.js:16:23) at Hexo.ret (eval at makeNodePromisifiedEval (C:\\Users\\PLM\\AppData\\Roaming\\npm\\node_modules\\hexo-cli\\node_modules\\bluebird\\js\\release\\promisify.js:184:12), &lt;anonymous&gt;:13:39) at D:\\PLMBlogs\\node_modules\\hexo\\lib\\hexo\\render.js:61:21 at tryCatcher (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\util.js:16:23) at Promise._settlePromiseFromHandler (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\promise.js:512:31) at Promise._settlePromise (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\promise.js:569:18) at Promise._settlePromiseCtx (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\promise.js:606:10) at Async._drainQueue (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\async.js:138:12) at Async._drainQueues (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\async.js:143:10) at Immediate.Async.drainQueues (D:\\PLMBlogs\\node_modules\\bluebird\\js\\release\\async.js:17:14) at runCallback (timers.js:672:20) at tryOnImmediate (timers.js:645:5) at processImmediate [as _immediateCallback] (timers.js:617:5)events.js:160 throw er; // Unhandled 'error' event ^Error: spawn pandoc ENOENT at exports._errnoException (util.js:1020:11) at Process.ChildProcess._handle.onexit (internal/child_process.js:193:32) at onErrorNT (internal/child_process.js:367:16) at _combinedTickCallback (internal/process/next_tick.js:80:11) at process._tickCallback (internal/process/next_tick.js:104:9) 原因是有大量的数学公式，所以需要对网页进行渲染。 一般是使用pandoc进行渲染，先去官网下载，然后下一步安装即可。最后，执行下面的命令，安装就好了。 123npm install hexo-renderer-pandoc --save# 再次应该就不会报错了hexo generate 配置git 1234567git config --global user.name plmsmilegit config --global user.email plmspark@163.com# 生成Key，一路回车ssh-keygen -t rsa -C \"plmspark@163.com\" cat ~/.ssh/id_rsa.pub# 测试ssh -T git@github.com 潜在问题 本站没有搜索功能，安装插件失败了 tags页面，标签数量错误 期望 认真学习 好好做笔记 要更新博客 自己常来看看之前的知识点","tags":[{"name":"心得","slug":"心得","permalink":"http://plmsmile.github.io/tags/心得/"}]},{"title":"Spark的键值对RDD","date":"2017-03-13T11:37:06.000Z","path":"2017/03/13/Spark-PairRDD/","text":"PairRDD及其创建 键值对RDD称为PairRDD，通常用来做聚合计算。Spark为Pair RDD提供了许多专有的操作。 1234# 创建pair rdd: map 或者 读取键值对格式自动转成pairrdd# 每行的第一个单词作为key，line作为valuepairs = lines.map(lambda line: (line.split(' ')[0], line)) 转化操作 Pair RDD 的转化操作分为单个和多个RDD的转化操作。 单个Pair RDD转化 reduceByKey(func) 合并含有相同键的值，也称作聚合 123456from operator import addrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])sorted(rdd.reduceByKey(add).collect())# [('a', 2), ('b', 1)]# 这种写法也可以rdd.reduceByKey(lambda x, y: x+y).collect() groupByKey 对具有相同键的值进行分组。会生成hash分区的RDD 12345rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])sorted(rdd.groupByKey().mapValues(len).collect())# [('a', 2), ('b', 1)]sorted(rdd.groupByKey().mapValues(list).collect())[('a', [1, 1]), ('b', [1])] 说明：如果对键进行分组以便对每个键进行聚合（如sum和average），则用reduceByKey和aggregateByKey性能更好 combineByKey 合并具有相同键的值，但是返回不同类型 (K, V) - (K, C)。最常用的聚合操作。 1234x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])def add(a, b): return a + str(b)sorted(x.combineByKey(str, add, add).collect())[('a', '11'), ('b', '1')] 下面是combineByKey的源码和参数说明 123456789101112131415161718def combineByKey[C]( createCombiner: V =&gt; C, // V =&gt; C的转变 / 初始值 / 创建one-element的list mergeValue: (C, V) =&gt; C, // 将原V累加到新的C mergeCombiners: (C, C) =&gt; C, // 两个C合并成一个 partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null): RDD[(K, C)] = &#123; //实现略 &#125;// 求平均值val scores = sc.parallelize( List((\"chinese\", 88.0) , (\"chinese\", 90.5) , (\"math\", 60.0), (\"math\", 87.0)))val avg = scores.combineByKey( (v) =&gt; (v, 1), (acc: (Float, Int), V) =&gt; (acc._1 + v, acc._2 + 1), (acc1: (Float, Int), acc2: (Float, Int) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2))).map&#123;case (key, value) =&gt; (key, value._1 / value._2.toFloat)&#125; 1234567891011# 求平均值nums = sc.parallelize([('c', 90), ('m', 95), ('c', 80)])sum_count = nums.combineByKey( lambda x: (x, 1), lambda acc, x: (acc[0] + x, acc[1] + 1), lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))# [('c', (170, 2)), ('m', (95, 1))]avg_map = sum_count.mapValues(lambda (sum, count): sum/count).collectAsMap()# &#123;'c': 85, 'm': 95&#125;avg_map = sum_count.map(lambda key, s_c: (key, s_c[0]/s_c[1])).collectAsMap() mapValues(f) 对每个pair RDD中的每个Value应用一个func，不改变Key。其实也是对value做map操作。一般我们只想访问pair的值的时候，可以用mapValues。类似于map{case (x, y): (x, func(y))} 1234x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])def f(x): return len(x)x.mapValues(f).collect()[('a', 3), ('b', 1)] mapPartitions(f) 是map的一个变种，都需要传入一个函数f，去处理数据。不同点如下： map: f应用于每一个元素。 mapPartitions: f应用于每一个分区。分区的内容以Iterator[T]传入f，f的输出结果是Iterator[U]。最终RDD的由所有分区经过输入函数处理后的结果得到的。 优点：我们可以为每一个分区做一些初始化操作，而不用为每一个元素做初始化。例如，初始化数据库，次数n。map时：n=元素数量，mapPartitions时：n=分区数量。 1234567891011121314151617181920212223# 1. 每个元素加1def f(iterator): print (\"called f\") return map(lambda x: x + 1, iterator)rdd = sc.parallelize([1, 2, 3, 4, 5], 2)rdd.mapPartitions(f).collect() # 只调用2次f\"\"\"called fcalled f[2, 3, 4, 5, 6]\"\"\"# 2. 分区求和rdd = sc.parallelize([1, 2, 3, 4, 5, 6], 2)def f(iterator): print \"call f\" yield sum(iterator)rdd.mapPartitions(f).collect() # 调用2次f，分区求和\"\"\"call fcall f[6, 15]\"\"\" mapPartitionsWithIndex(f) 和mapPartitions一样，只是多了个partition的index。 12345678910111213rdd = sc.parallelize([\"yellow\",\"red\",\"blue\",\"cyan\",\"black\"], 3)def g(index, item): return \"id-&#123;&#125;, &#123;&#125;\".format(index, item)def f(index, iterator): print 'called f' return map(lambda x: g(index, x), iterator)rdd.mapPartitionsWithIndex(f).collect()\"\"\"called fcalled fcalled f['id-0, yellow', 'id-1, red', 'id-1, blue', 'id-2, cyan', 'id-2, black']\"\"\" repartition(n) 生成新的RDD，分区数目为n。会增加或者减少 RDD的并行度。会对分布式数据集进行shuffle操作，效率低。如果只是想减少分区数，则使用coalesce，不会进行shuffle操作。 1234567&gt;&gt;&gt; rdd = sc.parallelize([1,2,3,4,5,6,7], 4)&gt;&gt;&gt; sorted(rdd.glom().collect())[[1], [2, 3], [4, 5], [6, 7]]&gt;&gt;&gt; len(rdd.repartition(2).glom().collect())2&gt;&gt;&gt; len(rdd.repartition(10).glom().collect())10 coalesce(n) 合并，减少分区数，默认不执行shuffle操作。 1234sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()# [[1], [2, 3], [4, 5]]sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()# [[1, 2, 3, 4, 5]] flatMapValues(f) 打平values，[(&quot;k&quot;, [&quot;v1&quot;, &quot;v2&quot;])] -- [(&quot;k&quot;,&quot;v1&quot;), (&quot;k&quot;, &quot;v2&quot;)] 1234x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])def f(x): return xx.flatMapValues(f).collect()# [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')] keys values sortByKey 返回一个对键进行排序的RDD。会生成范围分区的RDD 123456789101112tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]sc.parallelize(tmp).sortByKey().first()# ('1', 3)sc.parallelize(tmp).sortByKey(True, 1).collect()# [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]sc.parallelize(tmp).sortByKey(True, 2).collect()# [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()# [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)] 两个Pair RDD转化 substract 留下在x中却不在y中的元素 1234x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])y = sc.parallelize([(\"a\", 3), (\"c\", None)])sorted(x.subtract(y).collect())#[('b', 4), ('b', 5)] substractByKey 删掉X中在Y中也存在的Key所包含的所有元素 join 内连接，从x中去和y中一个一个的匹配，(k, v1), (k, v2) -- (k, (v1, v2)) 1234x = sc.parallelize([(\"a\", 1), (\"b\", 4)])y = sc.parallelize([(\"a\", 2), (\"a\", 3)])sorted(x.join(y).collect())# [('a', (1, 2)), ('a', (1, 3))] leftOuterJoin 左边RDD的键都有，右边没有的配None 1234x = sc.parallelize([('a', 1), ('b', 4)])y = sc.parallelize([('a', 2)])sorted(x.leftOuterJoin(y).collect())# [('a', (1, 2)), ('b', (4, None))] rightOuterJoin 右边RDD的键都有，左边没有的配None 123456x = sc.parallelize([('a', 1), ('b', 4)])y = sc.parallelize([('a', 2)])sorted(x.rightOuterJoin(y).collect())# [('a', (1, 2))]sorted(y.rightOuterJoin(x).collect())# [('a', (2, 1)), ('b', (None, 4))] cogroup 将两个RDD中拥有相同键的value分组到一起，即使两个RDD的V不一样 123456x = sc.parallelize([('a', 1), ('b', 4)])y = sc.parallelize([('a', 2)])x.cogroup(y).collect()# 上面显示的是16进制[(x, tuple(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]# [('a', ([1], [2])), ('b', ([4], []))] 行动操作 countByKey 对每个键对应的元素分别计数 123rdd = sc.parallelize([('a', 1), ('b', 1), ('a', 1)])rdd.countByKey().items() # 转换成一个dict，再取所有元素# [('a', 2), ('b', 1)] collectAsMap 返回一个map 123m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()m[1] - 2 # 当做map操作即可m[3] - 4 lookup(key) 返回键的RDD中的值列表。如果RDD具有已知的分区器，则通过仅搜索key映射到的分区来高效地完成该操作。 12345678910l = range(1000) # 1,2,3,...,1000rdd = sc.parallelize(zip(l, l), 10) # 键和值一样，10个数据分片，10个并行度，10个taskrdd.lookup(42) # slow# [42]sorted_rdd = rdd.sortByKey()sorted_rdd.lookup(42) # fast# [42]rdd = sc.parallelize([('a', 'a1'), ('a', 'a2'), ('b', 'b1')])rdd.lookup('a')[0]# 'a1' 聚合操作 当数据是键值对组织的时候，聚合具有相同键的元素是很常见的操作。基础RDD有fold(), combine(), reduce()，Pair RDD有combineByKey()最常用,reduceByKey(), foldByKey()等。 计算均值 1234567891011121314151617181920## 方法一：mapValues和reduceByKeyrdd = sc.parallelize([('a', 1), ('a', 3), ('b', 4)])maprdd = rdd.mapValues(lambda x : (x, 1))# [('a', (1, 1)), ('a', (3, 1)), ('b', (4, 1))]reducerdd = maprdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))# [('a', (4, 2)), ('b', (4, 1))]reducerdd.mapValues(lambda x : x[0]/x[1]).collect()# [('a', 2), ('b', 4)] ## 方法二 combineByKey 最常用的nums = sc.parallelize([('c', 90), ('m', 95), ('c', 80)])sum_count = nums.combineByKey( lambda x: (x, 1), lambda acc, x: (acc[0] + x, acc[1] + 1), lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))# [('c', (170, 2)), ('m', (95, 1))]avg_map = sum_count.mapValues(lambda (sum, count): sum/count).collectAsMap()# &#123;'c': 85, 'm': 95&#125;avg_map = sum_count.map(lambda key, s_c: (key, s_c[0]/s_c[1])).collectAsMap() 统计单词计数 1234567rdd = sc.textFile('README.md')words = rdd.flatMap(lambda x: x.split(' '))# 568个result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)# 289result.top(2)# [('your', 1), ('you', 4)] 数据分区 分区说明 在分布式程序中，通信的代价是很大的。因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。Spark程序通过控制RDD分区方式来减少通信开销。使用partitionBy进行分区 不需分区：给定RDD只需要被扫描一次 需要分区：数据集在多次基于键的操作中使用，比如连接操作。partitionBy是转化操作，生成新的RDD，为了多次计算，一般要进行持久化persist Spark中所有的键值对RDD都可以进行分区。系统会根据一个针对键的函数对元素进行分组。Spark不能显示控制具体每个键落在哪一个工作节点上，但是Spark可以确保同一组的键出现在同一个节点上。 Hash分区：将一个RDD分成了100个分区，hashcode(key)%100 相同的，会在同一个节点上面 范围分区：将key在同一个范围区间内的记录放在同一个节点上 一个简单的例子，内存中有一张很大的用户信息表 -- 即(UserId, UserInfo)组成的RDD，UserInfo包含用户订阅了的所有Topics。还有一张(UserId, LinkInfo)存放着过去5分钟用户浏览的Topic。现在要找出用户浏览了但是没有订阅的Topic数量。 123456789101112val sc = new SparkContext(...)val userData = sc.sequenceFile[UserId, UserInfo](\"hdfs://...\").persist()def processNewLog(logFileName: String) &#123; val events = sc.sequenceFile[UserId, LinkInfo](logFileName) val joined = userData.join(events) // (UserId, (UserInfo, LinkInfo)) val offTopicVisits = joined.filter&#123; case (UserId, (UserInfo, LinkInfo)) =&gt; !UserInfo.topics.contains(LinkInfo.topic) &#125;.count() print (\"浏览了且未订阅的数量：\" + offTopicVisits)&#125; 这段代码不够高效。因为每次调用processNewLog都会用join操作，但我们却不知道数据集是如何分区的。 连接操作，会将两个数据集的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在机器上对所有键相同的记录进行操作。 因为userData比events要大的多并且基本不会变化，所以有很多浪费效率的事情：每次调用时都对userData表进行计算hash值计算和跨节点数据混洗。 解决方案：在程序开始的时候，对userData表使用partitionBy()转换操作，将这张表转换为哈希分区 123val userData = sc.sequenceFile[UserId, UserInfo](\"hdfs://...\") .partitionBy(new HashPartioner(100)) // 构造100个分区 .persist() // 持久化当前结果 events是本地变量，并且只使用一次，所以为它指定分区方式没有什么用处。 userData使用了partitionBy()，Spark就知道该RDD是根据键的hash值来分区的。在userData.join(events)时，Spark只会对events进行数据混洗操作。将events中特定UserId的记录发送到userData的对应分区所在的那台机器上。需要网络传输的数据就大大减少了，速度也就显著提升了。 分区相关的操作 Spark的许多操作都有将数据根据跨节点进行混洗的过程。所有这些操作都会从数据分区中获益。类似join这样的二元操作，预先进行数据分区会导致其中至少一个RDD不发生数据混洗。 获取好处的操作：cogroup, groupWith, join, leftOuterJoin , rightOuterJoin, groupByKey, reduceByKey , combineByKey, lookup 为结果设好分区的操作：cogroup, groupWith, join, leftOuterJoin , rightOuterJoin, groupByKey, reduceByKey , combineByKey, partitionBy, sort, （mapValues, flatMapValues, filter 如果父RDD有分区方式的话） 其他所有的操作的结果都不会存在特定的分区方式。对于二元操作，输出数据的分区方式取决于父RDD的分区方式。默认情况结果会采取hash分区。 PageRank PageRank的python版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#!/usr/bin/env python# -*- coding: utf-8 -*-\"\"\" PageRank算法author = PuLiming运行: bin/spark-submit files/pagerank.py data/mllib/pagerank_data.txt 10\"\"\"from __future__ import print_functionimport reimport sysfrom operator import addfrom pyspark import SparkConf, SparkContextdef compute_contribs(urls, rank): \"\"\" 给urls计算 Args: urls: 目标url相邻的urls集合 rank: 目标url的当前rank Returns: url: 相邻urls中的一个url rank: 当前url的新的rank \"\"\" num_urls = len(urls) for url in urls: yield (url, rank / num_urls)def split_url(url_line): \"\"\" 把一行url切分开来 Args: url_line: 一行url，如 1 2 Returns: url, neighbor_url \"\"\" parts = re.split(r'\\s+', url_line) # 正则 return parts[0], parts[1]def compute_pagerank(sc, url_data_file, iterations): \"\"\" 计算各个page的排名 Args: sc: SparkContext url_data_file: 测试数据文件 iterations: 迭代次数 Returns: status: 成功就返回0 \"\"\" # 读取url文件 ['1 2', '1 3', '2 1', '3 1'] lines = sc.textFile(url_data_file).map(lambda line: line.encode('utf8')) # 建立Pair RDD (url, neighbor_urls) [(1,[2,3]), (2,[1]), (3, [1])] links = lines.map(lambda line : split_url(line)).distinct().groupByKey().mapValues(lambda x: list(x)).cache() # 初始化所有url的rank为1 [(1, 1), (2, 1), (3, 1)] ranks = lines.map(lambda line : (line[0], 1)) for i in range(iterations): # (url, [(neighbor_urls), rank]) join neighbor_urls and rank # 把当前url的rank分别contribute到其他相邻的url (url, rank) contribs = links.join(ranks).flatMap( lambda url_urls_rank: compute_contribs(url_urls_rank[1][0], url_urls_rank[1][1]) ) # 把url的所有rank加起来，再赋值新的 ranks = contribs.reduceByKey(add).mapValues(lambda rank : rank * 0.85 + 0.15) for (link, rank) in ranks.collect(): print(\"%s has rank %s.\" % (link, rank)) return 0if __name__ == '__main__': if len(sys.argv) != 3: print(\"Usage: python pagerank.py &lt;data.txt&gt; &lt;iterations&gt;\", file = sys.stderr) sys.exit(-1) # 数据文件和迭代次数 url_data_file = sys.argv[1] iterations = int(sys.argv[2]) # 配置 SparkContext conf = SparkConf().setAppName('PythonPageRank') conf.setMaster('local') sc = SparkContext(conf=conf) ret = compute_pagerank(sc, url_data_file, iterations) sys.exit(ret) PageRank的scala版本 12345678910111213141516val sc = new SparkContext(...)val links = sc.objectFile[(String, Seq[String])](\"links\") .partitionBy(new HashPartitioner(100)) .persist()var ranks = links.mapValues(_ =&gt; 1.0)// 迭代10次for (i &lt;- 0 until 10) &#123; val contributions = links.join(ranks).flatMap &#123; case (pageId, (links, rank)) =&gt; links.map(dest =&gt; (dest, rank / links.size)) &#125; ranks = contributions.reduceByKey(_ + _).mapValues(0.15 + 0.85* _)&#125;ranks.saveAsTextFile(\"ranks\") 当前scala版本的PageRank算法的优点： links每次都会和ranks发生连接操作，所以一开始就对它进行分区partitionBy，就不会通过网络进行数据混洗了，节约了相当可观的网络通信开销 对links进行persist，留在内存中，每次迭代使用 第一次创建ranks，使用mapValues保留了父RDD的分区方式，第一次连接开销就会很小 reduceByKey后已经是分区了，再使用mapValues分区方式，再次和links进行join就会更加高效 所以对分区后的RDD尽量使用mapValues保留父分区方式，而不要用map丢失分区方式。","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"PairRDD","slug":"PairRDD","permalink":"http://plmsmile.github.io/tags/PairRDD/"}]},{"title":"Spark的基础RDD","date":"2017-03-13T11:26:18.000Z","path":"2017/03/13/Spark-BaseRDD/","text":"RDD基础 RDD是Spark中的核心抽象——弹性分布式数据集(Resilient Distributed Dataset)。其实RDD是分布式的元素集合，是一个不可变的分布式对象集合。每个RDD都会被分为多个分区，这些分区运行在不同的节点上。RDD可以包含任意对象。Spark会自动将这些RDD的数据分发到集群上，并将操作并行化执行。 RDD当做我们通过转化操作构建出来的、记录如何计算数据的指令列表。 对数据的所有操作都是创建RDD、转化已有RDD、调用RDD操作进行求值。 RDD主要有2个操作。 转化操作：由一个RDD生成一个新的RDD。惰性求值。 行动操作：会对RDD计算出一个结果或者写到外部系统。会触发实际的计算。 Spark会惰性计算这些RDD，只有第一次在一个行动操作中用到时才会真正计算。 Spark的RDD会在每次行动操作时重新计算。如果想复用，则用persist把RDD持久化缓存下来。 下面是总的大体步骤 从外部数据创建输入RDD。如textFile 使用转化操作得到新的RDD。如map，filter 对重用的中间结果RDD进行持久化。如persist 使用行动操作来触发一次并行计算。如count, first 创建RDD 创建RDD主要有两个方法：读取集合，读取外部数据。 1234# 读取集合words = sc.parallelize([\"hello\", \"spark\", \"good\", \"study\"])# 读取外部数据lines = sc.textFile(\"README.md\") 转化操作 RDD的转化操作会返回新的RDD，是惰性求值的。即只有真正调用这些RDD的行动操作这些RDD才会被计算。许多转化操作是针对各个元素的，即每次只会操作RDD中的一个元素。 通过转化操作，会从已有RDD派生出新的RDD。Spark会使用谱系图(lineage graph)来记录这些不同RDD之间的依赖关系。Spark会利用这些关系按需计算每个RDD，或者恢复所丢失的数据。 最常用的转化操作是map()和filter()。下面说明一下常用的转化操作。 map(f) 对每个元素使用func函数，将返回值构成新的RDD。不会保留父RDD的分区。 1234567rdd = sc.parallelize([\"b\", \"a\", \"c\"])rddnew = rdd.map(lambda x: (x, 1))# [('b', 1), ('a', 1), ('c', 1)]# 可使用sorted()进行排序sorted(rddnew.collect())# [('a', 1), ('b', 1), ('c', 1)] flatMap(f) 对每个元素使用func函数，然后展平结果。通常用来切分单词 1234567lines = sc.textFile(\"README.md\")# 104个words = lines.flatMap(lambda line : line.split(\" \"))# 568个rdd = sc.parallelize([2, 3, 4])rdd2 = rdd.flatMap(lambda x: range(1, x)) # range(1, x) 生成1-x的数，不包括x# [1, 1, 2, 1, 2, 3] filter(f) 元素满足f函数，则放到新的RDD里 123rdd = sc.parallelize([1, 2, 3, 4, 5])rdd.filter(lambda x: x % 2 == 0).collect()# [2, 4] distinct 去重。开销很大，会进行数据混洗。 12sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())# [1, 2, 3] union 合并两个RDD。会包含重复的元素 intersection 求两个RDD的共同的元素。会去掉重复的元素 subtract 留下在自己却不在other里面的元素 cartesian 两个RDD的笛卡尔积 行动操作 行动操作会把最终求得的结果返回到驱动程序，或者写入外部存储系统中。 collect 返回RDD中的所有元素。只适用于数据小的情况，因为会把所有数据加载到驱动程序的内存中。通常只在单元测试中使用 count RDD中元素的个数 countByValue 各元素在RDD中出现的次数，返回一个dictionary。在pair RDD中有countByKey方法 12sc.parallelize([1, 2, 1, 2, 2]).countByValue().items()# [(1, 2), (2, 3)] take(num) 返回RDD中的n个元素。它会首先扫描一个分区，在这个分区里面尽量满足n个元素，不够再去查别的分区。只能用于数据量小的情况下。得到的顺序可能和你预期的不一样 takeOrdered(num, key=None) 获取n个元素，按照升序或者按照传入的key function。只适用于数据小的RDD 1234sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)# [1, 2, 3, 4, 5, 6]sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)# [10, 9, 7, 6, 5, 4] top(num, key=None) 从RDD只获取前N个元素。降序排列。只适用于数据量小的RDD 1234sc.parallelize([2, 3, 4, 5, 6]).top(2)# [6, 5]sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)# [4, 3, 2] reduce(f) 并行整合RDD中的所有数据，得到一个结果。接收一个f函数。目前在本地reduce partitions。返回结果类型不变。 1234from operator import addsc.parallelize([1, 2, 3, 4, 5]).reduce(add)sc.parallelize([1, 2, 3, 4, 5]).reduce(lambda x, y: x+y)# 15 fold(zeroValue, op) 和reduce一样，但是需要提供初始值。op(t1, t2)，t1可以变，t2不能变 123from operator import addsc.parallelize([1, 2, 3, 4, 5]).fold(0, add)# 15 aggregate(zeroValue, seqOp, combOp) 聚合所有分区的元素，然后得到一个结果。和reduce相似，但是通常返回不同的类型。 1234seqOp = (lambda x, y: (x[0] + y, x[1] + 1)) # 累加combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1])) # combine多个sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)# (10, 4) foreach(f) 对rdd中的每个元素使用f函数 123def f(x): print (x)sc.parallelize([1, 2, 3, 4]).foreach(f) glom 将分区中的元素合并到一个list 123rdd = sc.parallelize([1, 2, 3, 4, 5], 2)rdd.glom().collect()# [[1, 2], [3, 4, 5]]","tags":[{"name":"Spark","slug":"Spark","permalink":"http://plmsmile.github.io/tags/Spark/"},{"name":"RDD","slug":"RDD","permalink":"http://plmsmile.github.io/tags/RDD/"}]},{"title":"HDFS初步学习","date":"2016-12-05T11:59:45.000Z","path":"2016/12/05/HDFS初步学习/","text":"HDFS是Hadoop的一个分布式文件系统 HDFS设计原理 HDFS设计原理 提供了一个抽象访问界面，通过路径访问文件 抽象界面上所展示的文件，存储在很多个服务器上面 抽象路径与实际存储的映射关系，由元数据管理系统来进行管理 为了数据的安全性，数据被存成多个副本 为了负载均衡和吞吐量，这些文件被分隔成为若干个块 元数据存储细节 职责及存储格式 响应客户端请求，维护hdfs目录树 管理元数据，维护映射（HDFS上的文件 --- Blocks --- DataNode fileName, replicas, blockIds, idToHosts /test/a.txt, 3, {blk_1, blk_2}, [{blk1 : [h0, h1, h2]}, {blk2 : [{h1, h2}]}] 元数据存储信息 Meta.data 内存中的元数据 Meta.edits 元数据最新的修改信息，存在磁盘上 Meta.data.image","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://plmsmile.github.io/tags/Hadoop/"},{"name":"HDFS","slug":"HDFS","permalink":"http://plmsmile.github.io/tags/HDFS/"}]}]