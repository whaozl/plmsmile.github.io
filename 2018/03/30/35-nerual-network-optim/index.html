<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>网络优化 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="神经网络,数据预处理,归一化,优化方法">
    <meta name="description" content="任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法   神经网络的问题 神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过优化和正则化来提升网络。 优化问题 优化问题的难点  网络是一个非凸函数，深层网络的梯度消失问题，很难优化 网络结构多样性，很难找到通用优化方法 参数多、数据大，训练">
<meta name="keywords" content="神经网络,数据预处理,归一化,优化方法">
<meta property="og:type" content="article">
<meta property="og:title" content="网络优化">
<meta property="og:url" content="http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法   神经网络的问题 神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过优化和正则化来提升网络。 优化问题 优化问题的难点  网络是一个非凸函数，深层网络的梯度消失问题，很难优化 网络结构多样性，很难找到通用优化方法 参数多、数据大，训练效率低 参数多，存在高维变量的非凸优化  低维空间非凸优化：存在局部最">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/data_standard.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-process.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-pca-process.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/layer-batch-norm.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd_batch.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd-whole.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/momentumjpg.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/optimizers.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/optimizer-1.gif">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/optimizer-2.gif">
<meta property="og:updated_time" content="2018-04-03T14:43:40.128Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="网络优化">
<meta name="twitter:description" content="任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法   神经网络的问题 神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过优化和正则化来提升网络。 优化问题 优化问题的难点  网络是一个非凸函数，深层网络的梯度消失问题，很难优化 网络结构多样性，很难找到通用优化方法 参数多、数据大，训练效率低 参数多，存在高维变量的非凸优化  低维空间非凸优化：存在局部最">
<meta name="twitter:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/data_standard.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.0">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="https://plmsmile.github.io/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">网络优化</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">网络优化</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-03-30T05:54:34.000Z" itemprop="datePublished" class="page-time">
  2018-03-30
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/深度学习/">深度学习</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#神经网络的问题"><span class="post-toc-number">1.</span> <span class="post-toc-text">神经网络的问题</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#优化问题"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">优化问题</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#泛化问题"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">泛化问题</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#参数初始化"><span class="post-toc-number">2.</span> <span class="post-toc-text">参数初始化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#对称权重问题"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">对称权重问题</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#高斯分布初始化"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">高斯分布初始化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#均匀分布初始化"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">均匀分布初始化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#xavier均匀分布初始化"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">Xavier均匀分布初始化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#数据预处理"><span class="post-toc-number">3.</span> <span class="post-toc-text">数据预处理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#为什么要归一化"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">为什么要归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#标准归一化"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">标准归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#缩放归一化"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">缩放归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#白化"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">白化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#逐层归一化"><span class="post-toc-number">4.</span> <span class="post-toc-text">逐层归一化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#原因"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">原因</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#批量归一化"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">批量归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#层归一化"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">层归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#批归和层归对比"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">批归和层归对比</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#其它归一化"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">其它归一化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#梯度下降法的改进"><span class="post-toc-number">5.</span> <span class="post-toc-text">梯度下降法的改进</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#梯度下降法"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">梯度下降法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#学习率递减"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">学习率递减</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#动量法"><span class="post-toc-number">5.3.</span> <span class="post-toc-text">动量法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#adam"><span class="post-toc-number">5.4.</span> <span class="post-toc-text">Adam</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#梯度截断"><span class="post-toc-number">5.5.</span> <span class="post-toc-text">梯度截断</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#超参数优化"><span class="post-toc-number">6.</span> <span class="post-toc-text">超参数优化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#优化内容和难点"><span class="post-toc-number">6.1.</span> <span class="post-toc-text">优化内容和难点</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#超参数设置-搜索"><span class="post-toc-number">6.2.</span> <span class="post-toc-text">超参数设置-搜索</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#贝叶斯优化"><span class="post-toc-number">6.3.</span> <span class="post-toc-text">贝叶斯优化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#动态资源分配"><span class="post-toc-number">6.4.</span> <span class="post-toc-text">动态资源分配</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经架构搜索"><span class="post-toc-number">6.5.</span> <span class="post-toc-text">神经架构搜索</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-35-nerual-network-optim"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">网络优化</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-03-30 13:54:34" datetime="2018-03-30T05:54:34.000Z"  itemprop="datePublished">2018-03-30</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/深度学习/">深度学习</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <blockquote>
<p>任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法</p>
</blockquote>
<p><img src="" style="display:block; margin:auto" width="70%"></p>
<h1 id="神经网络的问题">神经网络的问题</h1>
<p>神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过<code>优化</code>和<code>正则化</code>来提升网络。</p>
<h2 id="优化问题">优化问题</h2>
<p><strong>优化问题的难点</strong></p>
<ul>
<li>网络是一个<strong>非凸函数</strong>，深层网络的<strong>梯度消失</strong>问题，很难优化</li>
<li>网络<strong>结构多样性</strong>，很难找到通用优化方法</li>
<li>参数多、数据大，<strong>训练效率低</strong></li>
<li>参数多，存在<strong>高维变量的非凸优化</strong></li>
</ul>
<p>低维空间非凸优化：存在局部最优点，难在初始化参数和逃离局部最优点</p>
<p>高维空间非凸优化：难在如何逃离<code>鞍点</code>。 鞍点是梯度为0，但一些维度是最高点，另一些维度是最低点。</p>
<p>梯度下降法<strong>很难逃离鞍点</strong>。</p>
<p><strong>梯度下降法面临的问题</strong></p>
<ul>
<li>如何初始化参数</li>
<li>预处理数据</li>
<li>如何选择合适的学习率，避免陷入局部最优</li>
</ul>
<h2 id="泛化问题">泛化问题</h2>
<p>神经网络拟合能力很强，容易过拟合。<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/">解决过拟合的5个方法</a></p>
<h1 id="参数初始化">参数初始化</h1>
<p><a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96">我之前的参数初始化笔记</a></p>
<h2 id="对称权重问题">对称权重问题</h2>
<p><strong>全0产生的对称权重问题</strong></p>
<p><strong>参数千万不能全0初始化</strong>。如果全0初始化，会导致隐层神经元激活值都相同，导致深层神经元没有区分性。这就是<code>对称权重</code>现象。</p>
<p>通俗点：</p>
<ul>
<li>每个神经元输出相同 -- BP时梯度也相同 -- 参数更新也相同</li>
<li>神经元之间就<strong>失去了不对称性的源头</strong></li>
</ul>
<p>应该对每个参数<code>随机初始化</code>，打破这个对称权重现象，<strong>使得不同神经元之间区分性更好</strong>。</p>
<p><strong>参数区间的选择</strong></p>
<p><code>参数太小</code>时</p>
<p>使得<code>Sigmoid</code>激活函数<strong>丢失非线性的能力</strong>。在0附近近似线性，多层神经网络的优势也不存在。</p>
<p><code>参数太大</code>时</p>
<p><code>Sigmoid</code>的输入会变得很大，<strong>输出接近1</strong>。<strong>梯度直接等于0</strong>。</p>
<p>选择一个<strong>合适的初始化区间非常重要</strong>。如果，一个神经元输入连接很多，那么每个输入连接上的权值就应该小一些。</p>
<h2 id="高斯分布初始化">高斯分布初始化</h2>
<p>高斯分布也就是正态分布。</p>
<p>初始化一个深度网络，比较好的方案是<strong>保持每个神经元输入的方差</strong>为一个<code>常量</code>。</p>
<p>如果神经元输入是<span class="math inline">\(n_{in}\)</span>， 输出是<span class="math inline">\(n_{out}\)</span>， 则按照<span class="math inline">\(N(0, \sqrt{\frac {2}{n_{in} + n_{out}}})\)</span> 来初始化参数。</p>
<h2 id="均匀分布初始化">均匀分布初始化</h2>
<p>在<span class="math inline">\([-r, r]\)</span>区间内，采用均匀分布来初始化参数</p>
<h2 id="xavier均匀分布初始化">Xavier均匀分布初始化</h2>
<p>会自动计算超参数<span class="math inline">\(r\)</span>， 来对参数进行<span class="math inline">\([-r, r]\)</span>均匀分布初始化。</p>
<p>设<span class="math inline">\(n^{l}\)</span>为第<span class="math inline">\(l\)</span> 层神经元个数， <span class="math inline">\(n^{l-1}\)</span> 是第<span class="math inline">\(l-1\)</span>层神经元个数。</p>
<ul>
<li><code>logsitic</code>激活函数 ：<span class="math inline">\(r = \sqrt{\frac{6}{n^{l-1} + n^l}}\)</span></li>
<li><code>tanh</code>激活函数： <span class="math inline">\(r = 4 \sqrt{\frac{6}{n^{l-1} + n^l}}\)</span></li>
</ul>
<p><span class="math inline">\(l\)</span>层的一个神经元<span class="math inline">\(z^l\)</span>，收到<span class="math inline">\(l-1\)</span>层的<span class="math inline">\(n^{l-1}\)</span>个神经元的输出<span class="math inline">\(a_i^{l-1}\)</span>, <span class="math inline">\(i \in [1, n^{(l-1)}]\)</span>。 <span class="math display">\[
z^l = \sum_{i=1}^n w_i^l a_i^{l-1}
\]</span> 为了避免初始化参数使得激活值变得饱和，尽量使<span class="math inline">\(z^l\)</span>处于线性区间，即<strong>神经元的输出</strong><span class="math inline">\(a^l = f(z^l) \approx z^l\)</span>。</p>
<p>假设<span class="math inline">\(w_i^l\)</span>和<span class="math inline">\(a_i^{l-1}\)</span>相互独立，均值均为0，则a的均值为 <span class="math display">\[
E[a^l] = E[\sum_{i=1}^n w_i^l a_i^{l-1}] = \sum_{i=1}^d E[\mathbf w_i] E[a_i^{l-1}] = 0
\]</span> <span class="math inline">\(a^l\)</span>的方差 <span class="math display">\[
\mathrm{Var} [a^l] = n^{l-1} \cdot \mathrm{Var} [w_i^l] \cdot \mathrm{Var} [a^{l-1}_i]
\]</span> 输入信号经过该神经元后，被放大或缩小了<span class="math inline">\(n^{l-1} \cdot \mathrm{Var} [w_i^l]\)</span>倍。</p>
<p>为了使输入信号经过多层网络后，不被过分放大或过分缩小，应该使<span class="math inline">\(n^{l-1} \cdot \mathrm{Var} [w_i^l]=1\)</span>。</p>
<p>综合前向和后向，使<strong>信号在前向和反向传播中都不被放大或缩小</strong>，综合设置方差： <span class="math display">\[
\mathrm{Var} [w_i^l] = \frac{2} {n^{l-1} + n^l}
\]</span></p>
<h1 id="数据预处理">数据预处理</h1>
<h2 id="为什么要归一化">为什么要归一化</h2>
<p>每一维的特征的来源和度量单位不同，导致特征分布不同。</p>
<p><strong>未归一化数据的3个坏处</strong></p>
<ol style="list-style-type: decimal">
<li>样本之间的欧式距离度量不准。取值范围大的特征会占主导作用。类似于<a href="https://plmsmile.github.io/2018/03/05/29-desicion-tree/#%E7%86%B5%E5%92%8C%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A">信息增益和信息增益比</a></li>
<li>降低神经网络的训练效率</li>
<li>降低梯度下降法的搜索效率</li>
</ol>
<p><strong>未归一化对梯度下降的影响</strong></p>
<ul>
<li>取值范围不同：大多数位置的梯度方向不是最优的，要多次迭代才能收敛</li>
<li>取值范围相同：大部分位置的梯度方向近似于最优搜索方向，每一步都指向最小值，训练效率大大提高</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/data_standard.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>归一化要做的事情</strong></p>
<ol style="list-style-type: decimal">
<li>各个维度特征归一化到同一个取值区间</li>
<li>消除不同特征的相关性</li>
</ol>
<h2 id="标准归一化">标准归一化</h2>
<p>实际上是由<code>中心化</code>和<code>标准化</code>结合的。 把<strong>数据归一化到标准正态分布</strong>。<span class="math inline">\(X \sim N(0, 1^2)\)</span></p>
<p>计算均值和方差 <span class="math display">\[
\mu = \frac{1}{N} \sum_{i=1}^n x^{(i)} \\
\sigma^2 =  \frac{1}{N} \sum_{i=1}^n(x^{(i)} - \mu)^2
\]</span> 归一化数据，减均值除以标准差。如果<span class="math inline">\(\sigma = 0\)</span>， 说明特征没有区分性，应该直接删掉。 <span class="math display">\[
\hat x^{(i)} = \frac {x^{(i)} - \mu}{ \sigma }
\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-process.jpg" style="display:block; margin:auto" width="70%"></p>
<h2 id="缩放归一化">缩放归一化</h2>
<p>把数据归一化到<span class="math inline">\([0, 1]\)</span> 或者<span class="math inline">\([-1, 1]\)</span> 直接。 <span class="math display">\[
x^{(i)} = \frac {x^{(i)} - \min(x)}{\max(x) - \min (x)}
\]</span></p>
<h2 id="白化">白化</h2>
<p><code>白化</code>用来降低输入数据特征之间的冗余性。白化主要使用PCA来去掉特征之间的相关性。<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#%E7%99%BD%E5%8C%96">我的白化笔记</a></p>
<p>处理后的数据</p>
<ul>
<li>特征之间相关性较低</li>
<li>所有特征具有相同的方差</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-pca-process.jpg" style="display:block; margin:auto" width="70%"></p>
<p><strong>白化的缺点</strong></p>
<p>可能会夸大数据中的噪声。所有维度都拉到了相同的数值范围。可能有一些差异性小、但大多数是噪声的维度。可以使用平滑来解决。</p>
<h1 id="逐层归一化">逐层归一化</h1>
<h2 id="原因">原因</h2>
<p>深层神经网络，中间层的输入是上一层的输出。每次SGD参数更新，都会导致<strong>每一层的输入分布发生改变</strong>。</p>
<p>像高楼，低楼层发生较小偏移，就会导致高楼层发生较大偏移。</p>
<p>如果<strong>某个层的输入发生改变</strong>，其<strong>参数就需要重新学习</strong>，这也是<code>内部协变量偏移</code>问题。</p>
<p>在训练过程中，要使得每一层的输入分布保持一致。简单点，对每一个神经层进行归一化。</p>
<ul>
<li>批量归一化</li>
<li>层归一化</li>
<li>其它方法</li>
</ul>
<h2 id="批量归一化">批量归一化</h2>
<p>针对<strong>每一个维度</strong>，对<strong>每个batch的数据</strong>进行<strong>归一化+缩放平移</strong>。</p>
<p>批量归一化<code>Batch Normalization</code> ，<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#batch-normalization">我的BN详细笔记</a>。 对每一层（<strong>单个神经元</strong>）的输入进行归一化 <span class="math display">\[
\begin{align}
&amp; \mu = \frac{1}{m} \sum_{i=1}^m x_i &amp;  \text{求均值} \\
&amp; \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 &amp; \text{求方差} \\
&amp; \hat x = \frac{x - E(x)} {\sqrt{\sigma^2 + \epsilon}} &amp;  \text{标准归一化} \\
&amp; y =  \gamma \hat x+ \beta &amp; \text{缩放和平移}
 \end{align}
\]</span> <code>缩放参数</code><span class="math inline">\(\gamma\)</span> ，和<code>平移参数</code> <span class="math inline">\(\beta\)</span> 的作用</p>
<ul>
<li>强行归一化会破坏刚学习到的特征。用这两个变量去还原应该学习到的数据分布</li>
<li>归一化会聚集在0处，会减弱神经网络的非线性性质。缩放和平移解决这个问题</li>
</ul>
<p>注意：</p>
<ul>
<li>BN是对中间层的<strong>单个神经元</strong>进行归一化</li>
<li>要求<strong>批量样本数量不能太小</strong>，否则难以计算单个神经元的统计信息</li>
<li>如果层的净输入的分布是<strong>动态变化</strong>的，则<strong>无法使用批量归一化</strong>。如循环神经网络</li>
</ul>
<h2 id="层归一化">层归一化</h2>
<p>对每个样本，对所有维度做一个归一化，即对<strong>同层的所有神经元</strong>的输入做归一化。</p>
<ul>
<li><code>层归一化</code>是<strong>对一个中间层的所有神经元进行归一化</strong></li>
<li>批量归一化是对一个中间层的单个神经元进行归一化</li>
</ul>
<p>设第<span class="math inline">\(l\)</span>层的净输入为<span class="math inline">\(\mathbf z^{(l)}\)</span>， 求<strong>第<span class="math inline">\(l\)</span>层所有输入</strong>的<code>均值</code>和<code>方差</code> <span class="math display">\[
\begin{align}
&amp; \mu^{(l)} = \frac{1}{n^l} \sum_{i=1}^{n^l} z_i^{(l)} &amp;  \text{第l层输入的均值} \\
&amp; \sigma^{(l)^2} = \frac{1}{n^l} \sum_{i=1}^{n^l} (z_i^{(l)} - \mu^{(l)})^2 &amp; \text{第l层输入的方差} \\
 \end{align}
\]</span> <code>层归一化</code> 如下，其中<span class="math inline">\(\gamma, \beta\)</span>是缩放和平移的参数向量，与<span class="math inline">\(\mathbf z^{(l)}\)</span>维数相同 <span class="math display">\[
\hat {\mathbf z}^{(l)} = \rm{LayerNorm}_{\gamma, \beta} (\mathbf z^{(l)}) 
= \frac {\mathbf z^{(l) - \mu^{(l)}}}{\sqrt{\sigma ^{(l)^2} + \epsilon}} \cdot \gamma + \beta
\]</span> <strong>层归一化的RNN</strong> <span class="math display">\[
\mathbf z_t = U\mathbf h_{t-1} + W \mathbf x_t \\
\mathbf h_t = f (\rm{LN}_{\gamma, \beta}(\mathbf z_t)))
\]</span> RNN的净输入一般会随着时间慢慢变大或变小，导致梯度爆炸或消失。</p>
<p>层归一化的RNN可以有效缓解梯度消失和梯度爆炸。</p>
<h2 id="批归和层归对比">批归和层归对比</h2>
<p>思想类似，都是<code>标准归一化</code> + <code>缩放和平移</code>。</p>
<ul>
<li>批量归一化：针对每一个维度，对batch的所有数据做归一化</li>
<li>层归一化：针对每一个样本，对所有维度做归一化。可以用在RNN上，减小梯度消失和梯度爆炸。</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/layer-batch-norm.jpg" style="display:block; margin:auto" width="70%"></p>
<h2 id="其它归一化">其它归一化</h2>
<p><strong>权重归一化</strong></p>
<p>对神经网络的连接权重进行归一化。</p>
<p><strong>局部相应归一化</strong></p>
<p>对同层的神经元进行归一化。但是局部响应归一化，用在激活函数之后，对邻近的神经元进行局部归一化。</p>
<h1 id="梯度下降法的改进">梯度下降法的改进</h1>
<h2 id="梯度下降法">梯度下降法</h2>
<p><code>Mini-Batch</code>梯度下降法。设<span class="math inline">\(f(\mathbf x ^{(i)}, \theta)\)</span> 是神经网络。</p>
<p>在第<span class="math inline">\(t\)</span>次迭代(epoch)时，选取<span class="math inline">\(m\)</span>个训练样本<span class="math inline">\(\{\mathbf x^{(i)}, y^{(i)} \}_{i=1}^m\)</span>。 计算梯度<span class="math inline">\(\mathbf g_t\)</span> <span class="math display">\[
\mathbf g_t = \frac{1}{m} \sum_{i \in I_t} \frac {\partial J(y^{(i)}, f(\mathbf x ^{(i)}, \theta))}{\partial \theta} 
+ \lambda \|\theta\| ^2   
\]</span> 更新参数，其中学习率<span class="math inline">\(\alpha \ge 0\)</span> ： <span class="math display">\[
\theta_t = \theta_{t-1} - \alpha \mathbf g_t 
\]</span></p>
<p><span class="math display">\[
\theta_t = \theta_{t-1}+ \Delta \theta_t
\]</span></p>
<p><strong>1. BGD</strong></p>
<p>Batch Gradient Descent</p>
<p><code>意义</code>：每一轮选择所有整个数据集去计算梯度更新参数</p>
<p><code>优点</code></p>
<ul>
<li>凸函数，可以保证收敛到全局最优点；非凸函数，保证收敛到局部最优点</li>
</ul>
<p><code>缺点</code></p>
<ul>
<li>批量梯度下降非常慢。因为在整个数据集上计算</li>
<li>训练次数多时，耗费内存</li>
<li>不允许在线更新模型，例如更新实例</li>
</ul>
<p><strong>2. SGD</strong></p>
<p>Stochastic Gradient Descent</p>
<p><code>意义</code>：每轮值选择一条数据去计算梯度更新参数</p>
<p><code>优点</code></p>
<ul>
<li>算法收敛快（BGD每轮会计算很多相似样本的梯度，冗余的）</li>
<li>可以在线更新</li>
<li>有一定几率跳出比较差的局部最优而到达更好的局部最优或者全局最优</li>
</ul>
<p><code>缺点</code></p>
<ul>
<li>容易收敛到局部最优，并且容易困在鞍点</li>
</ul>
<p><strong>3. Mini-BGD</strong></p>
<p>Mini-Batch Gradient Descent</p>
<p><code>意义</code>： 每次迭代只计算一个mini-batch的梯度去更新参数</p>
<p>优点</p>
<ul>
<li>计算效率高，收敛较为稳定</li>
</ul>
<p><code>缺点</code></p>
<ul>
<li>更新方向依赖于当前batch算出的梯度，不稳定</li>
</ul>
<p><strong>4. 梯度下降法的难点</strong></p>
<ol style="list-style-type: decimal">
<li>学习率<span class="math inline">\(\alpha\)</span>难以选择。太小，导致收敛缓慢；太大，造成较大波动妨碍收敛</li>
<li>学习率一直相同是不合理的。出现频率低的特征，大学习率；出现频率小的特征，小学习率</li>
<li>按迭代次数和loss阈值在训练时去调整学习率。然而次数和阈值难以设定，无法适应所有数据</li>
<li>很难逃离鞍点。梯度为0，一些特征是最高点（上升），一些特征是最低点（下降）</li>
<li>更新方向依赖于当前batch算出的梯度，不稳定</li>
</ol>
<p>主要通过<strong>学习率递减</strong>和<strong>动量法</strong>来优化梯度下降法。</p>
<p>可以看出</p>
<ul>
<li>SGD，整体下降，但局部会来回震荡</li>
<li>MBGD，一个batch来说，batch越大，下降越快，越平滑</li>
<li>MBGD，整体来说，batch越小，下降越明显</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd_batch.png" style="display:block; margin:auto" width="70%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd-whole.png" style="display:block; margin:auto" width="71%"></p>
<h2 id="学习率递减">学习率递减</h2>
<p><strong>0 指数加权平均</strong></p>
<p>求10天的平均温度，可以直接利用平均数求，每天的权值是一样的，且要保存所有的数值才能计算。 <span class="math display">\[
v_{avg} = \frac {v_1 + \cdots + v_{100}}{100}
\]</span> 设<span class="math inline">\(v_t\)</span>是到第t天的平均温度，<span class="math inline">\(\theta_t\)</span>是第t天的真实温度，<span class="math inline">\(\beta=0.9\)</span>是衰减系数。</p>
<p>则有<code>指数加权平均</code>：<br>
<span class="math display">\[
v_t = \beta * v_{t-1} + (1-\beta) \theta_t
\]</span></p>
<p><span class="math display">\[
v_{100} = 0.1 \cdot \theta_{100} + 0.1(0.9)^1 \cdot \theta_{99} + 0.1 (0.9)^2 \cdot \theta_{98} + 0.1(0.9)^3 \cdot \theta_{97} + \ldots
\]</span></p>
<p>离当前越近，权值越大。越远，权值越小（指数递减），也有一定权值。</p>
<p><strong>1. 按迭代次数递减</strong></p>
<p>设置<span class="math inline">\(\beta = 0.96\)</span>为衰减率</p>
<p><code>反时衰减</code> <span class="math display">\[
\alpha_t = \alpha_0 \cdot \frac {1} {1 + \beta \times t}
\]</span> <code>指数衰减</code> : <span class="math display">\[
\alpha_t = \alpha_0 \cdot \beta^t
\]</span> <code>自然指数衰减</code> <span class="math display">\[
\alpha_t = \alpha_0 \cdot e^{-\beta \cdot t}
\]</span> <strong>2. AdaGrad</strong></p>
<p><code>Adaptive Gradient</code></p>
<p><code>意义</code>：每次迭代时，根据历史梯度累积量来减小学习率，减小梯度。<strong>梯度平方的累计值</strong>来减小梯度</p>
<p>初始学习率<span class="math inline">\(\alpha_0\)</span>不变，实际学习率减小。<span class="math inline">\(\alpha = \frac {\alpha_0} {\sqrt {G_t + \epsilon}}\)</span> <span class="math display">\[
G_t = \sum_{i=1}^t g_i^2
\]</span></p>
<p><span class="math display">\[
\Delta \theta_t = - \frac {\alpha_0}{\sqrt {G_t + \epsilon}} \cdot g_t
\]</span></p>
<p><code>优点</code></p>
<ul>
<li>累积梯度<span class="math inline">\(G_t\)</span>的<span class="math inline">\(\frac{1}{\sqrt{G_t + \epsilon}}\)</span>实际上构成了一个约束项<br>
</li>
<li>前期<span class="math inline">\(G_t\)</span>较小， 约束值大，能够放大梯度</li>
<li>后期<span class="math inline">\(G_t\)</span>较大， 约束值小，能够约束梯度</li>
<li>适合处理稀疏梯度</li>
</ul>
<p><code>缺点</code></p>
<ul>
<li>经过一些迭代，学习率会变非常小，参数难以更新。过早停止训练</li>
<li>依赖于人工设置的全局学习率<span class="math inline">\(\alpha_0\)</span></li>
<li><span class="math inline">\(\alpha_0\)</span>设置过大，约束项大，则对梯度的调节太大</li>
</ul>
<p><strong>3. RMSprop</strong></p>
<p>意义：计算<strong>梯度<span class="math inline">\(\mathbf g_t\)</span>平方</strong>的<code>指数递减移动平均</code>， 即<strong>梯度平方的平均值</strong>来减小梯度 <span class="math display">\[
G_t = \beta G_{t-1} + (1-\beta) \cdot \mathbf g_t^2
\]</span></p>
<p><span class="math display">\[
\Delta \theta_t = - \frac {\alpha_0}{\sqrt {G_t + \epsilon}} \cdot \mathbf g_t
\]</span></p>
<p><code>优点</code></p>
<ul>
<li>解决了AdaGrad学习率一直递减过早停止训练的问题，学习率可大可小</li>
<li>训练初中期，加速效果不错，很快；训练后期，反复在局部最小值抖动</li>
<li><strong>适合处理非平稳目标</strong>，对于RNN效果很好</li>
</ul>
<p><code>缺点</code></p>
<ul>
<li>依然依赖于全局学习率<span class="math inline">\(\alpha_0\)</span></li>
</ul>
<p><strong>4. AdaDelta</strong></p>
<p><code>意义</code> 不初始化学习率。计算<strong>梯度更新差平方</strong>的<code>指数衰减移动平均</code>来作为分子学习率， <span class="math display">\[
G_t = \beta G_{t-1} + (1-\beta) \cdot \mathbf g_t^2
\]</span></p>
<p><span class="math display">\[
\Delta X_{t-1}^2 = \beta \Delta X_{t-2}^2 + (1-\beta) \Delta \theta_{t-1}^2
\]</span></p>
<p><span class="math display">\[
\Delta \theta_t = - \frac { \sqrt {\Delta X_{t-1}^2 + \epsilon}}{\sqrt {G_t + \epsilon}} \cdot \mathbf g_t
\]</span></p>
<p><code>优点</code></p>
<ul>
<li>初始学习率<span class="math inline">\(\alpha_0\)</span>改成了动态计算的<span class="math inline">\(\sqrt {\Delta X_{t-1}^2 + \epsilon}\)</span> ，一定程度上平抑了学习率的波动。</li>
</ul>
<h2 id="动量法">动量法</h2>
<p>结合<strong>前面更新的方向</strong>和<strong>当前batch的方向</strong>，来更新参数。</p>
<p>解决了MBGD的不稳定性，增加了<code>稳定性</code>。可以<code>加速</code>或者<code>减速</code>。</p>
<p><strong>1. 普通动量法</strong></p>
<p>设<span class="math inline">\(\rho = 0.9\)</span>为动量因子，计算<strong>负梯度</strong>的<code>移动加权平均</code> <span class="math display">\[
\Delta \theta_t = \rho \cdot \Delta \theta_{t-1} - \alpha \cdot \mathbf g_t
\]</span></p>
<p>当前梯度与最近时刻的梯度方向：</p>
<ul>
<li>前后<strong>梯度方向一致</strong>：参数更新幅度变大，<strong>会加速</strong></li>
<li>前后<strong>梯度方向不一致</strong>：参数更新幅度变小，<strong>会减速</strong></li>
</ul>
<p>优点：</p>
<ul>
<li>迭代初期，梯度方向一致，动量法加速，更快到达最优点</li>
<li>迭代后期，梯度方向不一致，在收敛值附近震荡，动量法会减速，增加稳定性</li>
</ul>
<p>当前梯度叠加上上次的梯度，可以近似地看成二阶梯度。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/momentumjpg.jpg" style="display:block; margin:auto" width="70%"></p>
<h2 id="adam">Adam</h2>
<p><code>Adaptive Momentum Estimation</code> = <code>RMSProp</code> + <code>Momentum</code>， 即<strong>自适应学习率+稳定性</strong>（动量法）。</p>
<p>意义：计算梯度<span class="math inline">\(\mathbf g_t\)</span>的指数权值递减移动平均(<code>动量</code>)，计算梯度平方<span class="math inline">\(\mathbf g_t^2\)</span>的指数权值递减移动平均(<code>自适应alpha</code>)</p>
<p>设<span class="math inline">\(\beta_1 = 0.9\)</span>， <span class="math inline">\(\beta_2 = 0.99\)</span> 为衰减率 <span class="math display">\[
M_t = \beta_1M_{t-1} + (1-\beta_1) \mathbf g_t \quad \quad \sim E(\mathbf g_t)
\]</span></p>
<p><span class="math display">\[
G_t = \beta_2 G_{t-1} + (1-\beta_2) \mathbf g_t^2 \quad \quad \sim E(\mathbf g_t^2)
\]</span></p>
<p><span class="math display">\[
\hat M_t = \frac {M_t}{1 - \beta_1^t}, \quad \hat G_t = \frac{G_t}{1 - \beta_2^t} \quad \quad \text{初始化偏差修正}
\]</span></p>
<p><span class="math display">\[
\Delta \theta_t = - \frac {\alpha_0}{\sqrt{\hat G_t + \epsilon}} \hat M_t 
\]</span></p>
<p><code>优点</code></p>
<ul>
<li>有RMSprop的处理<strong>非稳态</strong>目标的优点，有Adagrad处理<strong>稀疏梯度</strong>的优点</li>
<li>对内存需求比较小，高效地计算</li>
<li>为不同的参数计算不同的自适应学习率</li>
<li>适用于大多数的非凸优化</li>
<li>超参数好解释，只需极少量的调参</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/optimizers.png" style="display:block; margin:auto" width="80%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/optimizer-1.gif" style="display:block; margin:auto" width="70%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/optimizer-2.gif" style="display:block; margin:auto" width="70%"></p>
<h2 id="梯度截断">梯度截断</h2>
<p>一般按模截断，如果<span class="math inline">\(\|\mathbf g_t\|^2 &gt; b\)</span>， 则 <span class="math display">\[
\mathbf g_t = \frac{b}{\|\mathbf g_t\|} \mathbf g_t
\]</span></p>
<h1 id="超参数优化">超参数优化</h1>
<h2 id="优化内容和难点">优化内容和难点</h2>
<p><strong>优化内容</strong></p>
<ul>
<li>网络结构：神经元之间连接关系、层数、每层的神经元数量、激活函数类型等</li>
<li>优化参数：优化方法、学习率、小批量样本数量</li>
<li>正则化系数</li>
</ul>
<p><strong>优化难点</strong></p>
<ul>
<li>参数优化是组合优化问题，没有梯度下降法来优化，没有通用的有效的方法</li>
<li>评估一组超参数配置的实际代价非常高</li>
</ul>
<p><strong>配置说明</strong></p>
<ul>
<li>有<span class="math inline">\(K\)</span>个超参数， 每个超参数配置表示为1个向量<span class="math inline">\(\mathbf x \in X\)</span></li>
<li><span class="math inline">\(f(\mathbf x)\)</span> 是衡量超参数配置<span class="math inline">\(\mathbf x\)</span>效果的函数</li>
<li><span class="math inline">\(f(\mathbf x)\)</span>不是<span class="math inline">\(\mathbf x\)</span>的连续函数，<span class="math inline">\(\mathbf x\)</span>也不同。 无法使用梯度下降等优化方法</li>
</ul>
<h2 id="超参数设置-搜索">超参数设置-搜索</h2>
<p>超参数设置：人工搜索、网格搜索、随机搜索。</p>
<p>缺点：没有利用到不同超参数组合之间的相关性，搜索方式都比较低效。</p>
<p><strong>1. 网格搜索</strong></p>
<p>对于<span class="math inline">\(K\)</span>个超参数，第<span class="math inline">\(k\)</span>个参数有<span class="math inline">\(m_k\)</span>种取值。总共的配置数量： <span class="math display">\[
N = m_1 \times m_2 \times \cdots \times m_K
\]</span> 如果超参数是连续的，可以根据经验选择一些经验值，比如学习率 <span class="math display">\[
\alpha \in \{0.01, 0.1, 0.5, 1.0\}
\]</span> 对这些超参数的不同组合，分别训练一个模型，测试在开发集上的性能。选取一组性能最好的配置。</p>
<p><strong>2. 随机搜索</strong></p>
<p>有的超参数对模型影响力有限（正则化），有的超参数对模型性能影响比较大。网格搜索会遍历所有的可能性。</p>
<p>随机搜索：对超参数进行随机组合，选择一个性能最好的配置。</p>
<p>优点：比网格搜索好，更容易实现，更有效。</p>
<h2 id="贝叶斯优化">贝叶斯优化</h2>
<p>根据当前已经试验的超参数组合，来预测下一个可能带来的最大收益的组合。</p>
<p>贝叶斯优化过程：根据已有的N组试验结果来建立高斯过程，计算<span class="math inline">\(f(\mathbf x)\)</span>的后验分布。</p>
<h2 id="动态资源分配">动态资源分配</h2>
<p>在早期阶段，估计出一组配置的效果会比较差，则中止这组配置的评估。把更多的资源留给其他配置。</p>
<p>这是多臂赌博机的泛化问题：最优赌博机。在给定有限次数的情况下，玩赌博机，找到收益最大的臂。</p>
<h2 id="神经架构搜索">神经架构搜索</h2>
<p>通过神经网络来自动实现网络架构的设计。</p>
<ul>
<li>变长字符串 -- 描述神经网络的架构</li>
<li>控制器 -- 生成另一个子网络的架构描述</li>
<li>控制器 -- RNN来实现</li>
<li>控制器训练 -- 强化学习来完成</li>
<li>奖励信号 -- 生成的子网络在开发集上的准确率</li>
</ul>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-04-03T14:43:40.128Z" itemprop="dateUpdated">2018-04-03 22:43:40</time>
</span><br>


        
        <br>原始链接：<a href="/2018/03/30/35-nerual-network-optim/" target="_blank" rel="external">http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/</a>
        
    </div>
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/优化方法/">优化方法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/归一化/">归一化</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据预处理/">数据预处理</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&title=《网络优化》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&title=《网络优化》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《网络优化》 — PLM's Notes&url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2018/03/31/36-alime-chat/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">阿里小蜜论文</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/03/25/33-attention-summary/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">各种注意力总结</h4>
      </a>
    </div>
  
</nav>



    














</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <!-- <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div> -->
    <div class="bottom">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
          <span>

          <!-- PLM  -->
          PLM's Notes &nbsp;
          &copy;
          &nbsp;
          2016 - 2018

          </span>
            <!-- <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span> -->
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&title=《网络优化》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&title=《网络优化》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《网络优化》 — PLM's Notes&url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACPElEQVR42u3azY6DMAxF4b7/S1NptoVwrk2rwZysKmaa5mNhxT+vF17b3+J/3T4W+S75/9aSIUPGbRnbcqUHXT9Z/8oadvJqZMiQ8QBGJxTyo/Rf3+FzGTJkyADXu/UhakeUIUOGjE7AJanmmkR2liFDhoz0isbLZ/00+Iu5uAwZMm7ISBsDv/z8xf6GDBkybsLYwkUCYlqAS8twO9+VIUPGaAb5yTSt5cnq+tBx8ixDhozRjLQcRqhpuksSXTSmJkOGjHGM2qBDyqjtz/EyZMiYzeAHSi9/aRjtXzFlyJAxm5GWwHgQTK+PpFUZ92NlyJAxgtG5AtaQ6WBHkMrKkCFjHKNTxE+HvXi6S8YvdjqxMmTIGM24KjUNSmPht06eyJAhYzSDD1v0AysfFys2VmXIkDGakZbAauG1E3yDfWTIkDGO0T80T3prjQcUcGXIkDGa0Rm2IKUxfqGslecumBmRIUPGv2ekG111oFpx7TDoy5AhYzSDxGOSmtaSVf4KEEyGDBlDGbw9SdLRuOlYKrTFsV+GDBkjGGnJjARBUrZLm5eHL1GGDBmjGfz+SIr7teLdSTDF+8uQIWMqYwsXby6mjQQerHf2kSFDxmhGbTwrvcZ1ktVaw0CGDBnzGGkprVbE75T20LuXIUPGAxhpvZ0X7oMSf6dVIEOGDBngKOmTtFF6cumUIUOGjLAN0AnZ6XVThgwZz2GQVJMPQKSjG6TluX4uQ4aM2YzOpEZaROOfyf6t/oYMGTLuwXgDvVBBz/UzP3AAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.0"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.0" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
