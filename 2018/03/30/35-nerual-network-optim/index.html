<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>网络优化 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="神经网络,深度学习">
    <meta name="description" content="任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法   神经网络的问题 神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过优化和正则化来提升网络。 优化问题 优化问题的难点  网络是一个非凸函数，深层网络的梯度消失问题，很难优化 网络结构多样性，很难找到通用优化方法 参数多、数据大，训练">
<meta name="keywords" content="神经网络,深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="网络优化">
<meta property="og:url" content="http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法   神经网络的问题 神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过优化和正则化来提升网络。 优化问题 优化问题的难点  网络是一个非凸函数，深层网络的梯度消失问题，很难优化 网络结构多样性，很难找到通用优化方法 参数多、数据大，训练效率低 参数多，存在高维变量的非凸优化  低维空间非凸优化：存在局部最">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/data_standard.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-process.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-pca-process.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/layer-batch-norm.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd_batch.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd-whole.png">
<meta property="og:updated_time" content="2018-03-30T14:26:58.061Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="网络优化">
<meta name="twitter:description" content="任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法   神经网络的问题 神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过优化和正则化来提升网络。 优化问题 优化问题的难点  网络是一个非凸函数，深层网络的梯度消失问题，很难优化 网络结构多样性，很难找到通用优化方法 参数多、数据大，训练效率低 参数多，存在高维变量的非凸优化  低维空间非凸优化：存在局部最">
<meta name="twitter:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/data_standard.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.0">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="https://plmsmile.github.io/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">网络优化</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">网络优化</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-03-30T05:54:34.000Z" itemprop="datePublished" class="page-time">
  2018-03-30
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/深度学习/">深度学习</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#神经网络的问题"><span class="post-toc-number">1.</span> <span class="post-toc-text">神经网络的问题</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#优化问题"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">优化问题</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#泛化问题"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">泛化问题</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#参数初始化"><span class="post-toc-number">2.</span> <span class="post-toc-text">参数初始化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#对称权重问题"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">对称权重问题</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#高斯分布初始化"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">高斯分布初始化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#均匀分布初始化"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">均匀分布初始化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#xavier均匀分布初始化"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">Xavier均匀分布初始化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#数据预处理"><span class="post-toc-number">3.</span> <span class="post-toc-text">数据预处理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#为什么要归一化"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">为什么要归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#标准归一化"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">标准归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#缩放归一化"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">缩放归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#白化"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">白化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#逐层归一化"><span class="post-toc-number">4.</span> <span class="post-toc-text">逐层归一化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#原因"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">原因</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#批量归一化"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">批量归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#层归一化"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">层归一化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#批归和层归对比"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">批归和层归对比</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#其它归一化"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">其它归一化</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#梯度下降法的改进"><span class="post-toc-number">5.</span> <span class="post-toc-text">梯度下降法的改进</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#梯度下降法"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">梯度下降法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#学习率递减方法"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">学习率递减方法</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-35-nerual-network-optim"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">网络优化</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-03-30 13:54:34" datetime="2018-03-30T05:54:34.000Z"  itemprop="datePublished">2018-03-30</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/深度学习/">深度学习</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <blockquote>
<p>任何数学技巧都不能弥补信息的缺失。本文介绍网络优化方法</p>
</blockquote>
<p><img src="" style="display:block; margin:auto" width="70%"></p>
<h1 id="神经网络的问题">神经网络的问题</h1>
<p>神经网络有很强的表达能力。但有优化问题和泛化问题。主要通过<code>优化</code>和<code>正则化</code>来提升网络。</p>
<h2 id="优化问题">优化问题</h2>
<p><strong>优化问题的难点</strong></p>
<ul>
<li>网络是一个<strong>非凸函数</strong>，深层网络的<strong>梯度消失</strong>问题，很难优化</li>
<li>网络<strong>结构多样性</strong>，很难找到通用优化方法</li>
<li>参数多、数据大，<strong>训练效率低</strong></li>
<li>参数多，存在<strong>高维变量的非凸优化</strong></li>
</ul>
<p>低维空间非凸优化：存在局部最优点，难在初始化参数和逃离局部最优点</p>
<p>高维空间非凸优化：难在如何逃离<code>鞍点</code>。 鞍点是梯度为0，但一些维度是最高点，另一些维度是最低点。</p>
<p>梯度下降法很难逃离鞍点。</p>
<p><strong>梯度下降法面临的问题</strong></p>
<ul>
<li>如何初始化参数</li>
<li>预处理数据</li>
<li>如何选择合适的学习率，避免陷入局部最优</li>
</ul>
<h2 id="泛化问题">泛化问题</h2>
<p>神经网络拟合能力很强，容易过拟合。<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/">解决过拟合的5个方法</a></p>
<h1 id="参数初始化">参数初始化</h1>
<p><a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96">我之前的参数初始化笔记</a></p>
<h2 id="对称权重问题">对称权重问题</h2>
<p><strong>全0产生的对称权重问题</strong></p>
<p><strong>参数千万不能全0初始化</strong>。如果全0初始化，会导致隐层神经元激活值都相同，导致深层神经元没有区分性。这就是<code>对称权重</code>现象。</p>
<p>通俗点：</p>
<ul>
<li>每个神经元输出相同 -- BP时梯度也相同 -- 参数更新也相同</li>
<li>神经元之间就<strong>失去了不对称性的源头</strong></li>
</ul>
<p>应该对每个参数<code>随机初始化</code>，打破这个对称权重现象，<strong>使得不同神经元之间区分性更好</strong>。</p>
<p><strong>参数区间的选择</strong></p>
<p><code>参数太小</code>时</p>
<p>使得<code>Sigmoid</code>激活函数<strong>丢失非线性的能力</strong>。在0附近近似线性，多层神经网络的优势也不存在。</p>
<p><code>参数太大</code>时</p>
<p><code>Sigmoid</code>的输入会变得很大，<strong>输出接近1</strong>。<strong>梯度直接等于0</strong>。</p>
<p>选择一个<strong>合适的初始化区间非常重要</strong>。如果，一个神经元输入连接很多，那么每个输入连接上的权值就应该小一些。</p>
<h2 id="高斯分布初始化">高斯分布初始化</h2>
<p>高斯分布也就是正态分布。</p>
<p>初始化一个深度网络，比较好的方案是<strong>保持每个神经元输入的方差</strong>为一个<code>常量</code>。</p>
<p>如果神经元输入是<span class="math inline">\(n_{in}\)</span>， 输出是<span class="math inline">\(n_{out}\)</span>， 则按照<span class="math inline">\(N(0, \sqrt{\frac {2}{n_{in} + n_{out}}})\)</span> 来初始化参数。</p>
<h2 id="均匀分布初始化">均匀分布初始化</h2>
<p>在<span class="math inline">\([-r, r]\)</span>区间内，采用均匀分布来初始化参数</p>
<h2 id="xavier均匀分布初始化">Xavier均匀分布初始化</h2>
<p>会自动计算超参数<span class="math inline">\(r\)</span>， 来对参数进行<span class="math inline">\([-r, r]\)</span>均匀分布初始化。</p>
<p>设<span class="math inline">\(n^{l}\)</span>为第<span class="math inline">\(l\)</span> 层神经元个数， <span class="math inline">\(n^{l-1}\)</span> 是第<span class="math inline">\(l-1\)</span>层神经元个数。</p>
<ul>
<li><code>logsitic</code>激活函数 ：<span class="math inline">\(r = \sqrt{\frac{6}{n^{l-1} + n^l}}\)</span></li>
<li><code>tanh</code>激活函数： <span class="math inline">\(r = 4 \sqrt{\frac{6}{n^{l-1} + n^l}}\)</span></li>
</ul>
<p><span class="math inline">\(l\)</span>层的一个神经元<span class="math inline">\(z^l\)</span>，收到<span class="math inline">\(l-1\)</span>层的<span class="math inline">\(n^{l-1}\)</span>个神经元的输出<span class="math inline">\(a_i^{l-1}\)</span>, <span class="math inline">\(i \in [1, n^{(l-1)}]\)</span>。 <span class="math display">\[
z^l = \sum_{i=1}^n w_i^l a_i^{l-1}
\]</span> 为了避免初始化参数使得激活值变得饱和，尽量使<span class="math inline">\(z^l\)</span>处于线性区间，即<strong>神经元的输出</strong><span class="math inline">\(a^l = f(z^l) \approx z^l\)</span>。</p>
<p>假设<span class="math inline">\(w_i^l\)</span>和<span class="math inline">\(a_i^{l-1}\)</span>相互独立，均值均为0，则a的均值为 <span class="math display">\[
E[a^l] = E[\sum_{i=1}^n w_i^l a_i^{l-1}] = \sum_{i=1}^d E[\mathbf w_i] E[a_i^{l-1}] = 0
\]</span> <span class="math inline">\(a^l\)</span>的方差 <span class="math display">\[
\mathrm{Var} [a^l] = n^{l-1} \cdot \mathrm{Var} [w_i^l] \cdot \mathrm{Var} [a^{l-1}_i]
\]</span> 输入信号经过该神经元后，被放大或缩小了<span class="math inline">\(n^{l-1} \cdot \mathrm{Var} [w_i^l]\)</span>倍。</p>
<p>为了使输入信号经过多层网络后，不被过分放大或过分缩小，应该使<span class="math inline">\(n^{l-1} \cdot \mathrm{Var} [w_i^l]=1\)</span>。</p>
<p>综合前向和后向，使<strong>信号在前向和反向传播中都不被放大或缩小</strong>，综合设置方差： <span class="math display">\[
\mathrm{Var} [w_i^l] = \frac{2} {n^{l-1} + n^l}
\]</span></p>
<h1 id="数据预处理">数据预处理</h1>
<h2 id="为什么要归一化">为什么要归一化</h2>
<p>每一维的特征的来源和度量单位不同，导致特征分布不同。</p>
<p><strong>未归一化数据的3个坏处</strong></p>
<ol style="list-style-type: decimal">
<li>样本之间的欧式距离度量不准。取值范围大的特征会占主导作用。类似于<a href="https://plmsmile.github.io/2018/03/05/29-desicion-tree/#%E7%86%B5%E5%92%8C%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A">信息增益和信息增益比</a></li>
<li>降低神经网络的训练效率</li>
<li>降低梯度下降法的搜索效率</li>
</ol>
<p><strong>未归一化对梯度下降的影响</strong></p>
<ul>
<li>取值范围不同：大多数位置的梯度方向不是最优的，要多次迭代才能收敛</li>
<li>取值范围相同：大部分位置的梯度方向近似于最优搜索方向，每一步都指向最小值，训练效率大大提高</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/data_standard.png" style="display:block; margin:auto" width="70%"></p>
<p><strong>归一化要做的事情</strong></p>
<ol style="list-style-type: decimal">
<li>各个维度特征归一化到同一个取值区间</li>
<li>消除不同特征的相关性</li>
</ol>
<h2 id="标准归一化">标准归一化</h2>
<p>实际上是由<code>中心化</code>和<code>标准化</code>结合的。 把<strong>数据归一化到标准正态分布</strong>。<span class="math inline">\(X \sim N(0, 1^2)\)</span></p>
<p>计算均值和方差 <span class="math display">\[
\mu = \frac{1}{N} \sum_{i=1}^n x^{(i)} \\
\sigma^2 =  \frac{1}{N} \sum_{i=1}^n(x^{(i)} - \mu)^2
\]</span> 归一化数据，减均值除以标准差。如果<span class="math inline">\(\sigma = 0\)</span>， 说明特征没有区分性，应该直接删掉。 <span class="math display">\[
\hat x^{(i)} = \frac {x^{(i)} - \mu}{ \sigma }
\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-process.jpg" style="display:block; margin:auto" width="70%"></p>
<h2 id="缩放归一化">缩放归一化</h2>
<p>把数据归一化到<span class="math inline">\([0, 1]\)</span> 或者<span class="math inline">\([-1, 1]\)</span> 直接。 <span class="math display">\[
x^{(i)} = \frac {x^{(i)} - \min(x)}{\max(x) - \min (x)}
\]</span></p>
<h2 id="白化">白化</h2>
<p><code>白化</code>用来降低输入数据特征之间的冗余性。白化主要使用PCA来去掉特征之间的相关性。<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#%E7%99%BD%E5%8C%96">我的白化笔记</a></p>
<p>处理后的数据</p>
<ul>
<li>特征之间相关性较低</li>
<li>所有特征具有相同的方差</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/image/nlp/cs224n/notes3/data-pca-process.jpg" style="display:block; margin:auto" width="70%"></p>
<p><strong>白化的缺点</strong></p>
<p>可能会夸大数据中的噪声。所有维度都拉到了相同的数值范围。可能有一些差异性小、但大多数是噪声的维度。可以使用平滑来解决。</p>
<h1 id="逐层归一化">逐层归一化</h1>
<h2 id="原因">原因</h2>
<p>深层神经网络，中间层的输入是上一层的输出。每次SGD参数更新，都会导致<strong>每一层的输入分布发生改变</strong>。</p>
<p>像高楼，低楼层发生较小偏移，就会导致高楼层发生较大偏移。</p>
<p>如果<strong>某个层的输入发生改变</strong>，其<strong>参数就需要重新学习</strong>，这也是<code>内部协变量偏移</code>问题。</p>
<p>在训练过程中，要使得每一层的输入分布保持一致。简单点，对每一个神经层进行归一化。</p>
<ul>
<li>批量归一化</li>
<li>层归一化</li>
<li>其它方法</li>
</ul>
<h2 id="批量归一化">批量归一化</h2>
<p>针对<strong>每一个维度</strong>，对<strong>每个batch的数据</strong>进行<strong>归一化+缩放平移</strong>。</p>
<p>批量归一化<code>Batch Normalization</code> ，<a href="https://plmsmile.github.io/2017/11/26/cs224n-notes3-neural-networks-2/#batch-normalization">我的BN详细笔记</a>。 对每一层（<strong>单个神经元</strong>）的输入进行归一化 <span class="math display">\[
\begin{align}
&amp; \mu = \frac{1}{m} \sum_{i=1}^m x_i &amp;  \text{求均值} \\
&amp; \sigma^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 &amp; \text{求方差} \\
&amp; \hat x = \frac{x - E(x)} {\sqrt{\sigma^2 + \epsilon}} &amp;  \text{标准归一化} \\
&amp; y =  \gamma \hat x+ \beta &amp; \text{缩放和平移}
 \end{align}
\]</span> <code>缩放参数</code><span class="math inline">\(\gamma\)</span> ，和<code>平移参数</code> <span class="math inline">\(\beta\)</span> 的作用</p>
<ul>
<li>强行归一化会破坏刚学习到的特征。用这两个变量去还原应该学习到的数据分布</li>
<li>归一化会聚集在0处，会减弱神经网络的非线性性质。缩放和平移解决这个问题</li>
</ul>
<p>注意：</p>
<ul>
<li>BN是对中间层的<strong>单个神经元</strong>进行归一化</li>
<li>要求<strong>批量样本数量不能太小</strong>，否则难以计算单个神经元的统计信息</li>
<li>如果层的净输入的分布是<strong>动态变化</strong>的，则<strong>无法使用批量归一化</strong>。如循环神经网络</li>
</ul>
<h2 id="层归一化">层归一化</h2>
<p>对每个样本，对所有维度做一个归一化，即对<strong>同层的所有神经元</strong>的输入做归一化。</p>
<ul>
<li><code>层归一化</code>是<strong>对一个中间层的所有神经元进行归一化</strong></li>
<li>批量归一化是对一个中间层的单个神经元进行归一化</li>
</ul>
<p>设第<span class="math inline">\(l\)</span>层的净输入为<span class="math inline">\(\mathbf z^{(l)}\)</span>， 求<strong>第<span class="math inline">\(l\)</span>层所有输入</strong>的<code>均值</code>和<code>方差</code> <span class="math display">\[
\begin{align}
&amp; \mu^{(l)} = \frac{1}{n^l} \sum_{i=1}^{n^l} z_i^{(l)} &amp;  \text{第l层输入的均值} \\
&amp; \sigma^{(l)^2} = \frac{1}{n^l} \sum_{i=1}^{n^l} (z_i^{(l)} - \mu^{(l)})^2 &amp; \text{第l层输入的方差} \\
 \end{align}
\]</span> <code>层归一化</code> 如下，其中<span class="math inline">\(\gamma, \beta\)</span>是缩放和平移的参数向量，与<span class="math inline">\(\mathbf z^{(l)}\)</span>维数相同 <span class="math display">\[
\hat {\mathbf z}^{(l)} = \rm{LayerNorm}_{\gamma, \beta} (\mathbf z^{(l)}) 
= \frac {\mathbf z^{(l) - \mu^{(l)}}}{\sqrt{\sigma ^{(l)^2} + \epsilon}} \cdot \gamma + \beta
\]</span> <strong>层归一化的RNN</strong> <span class="math display">\[
\mathbf z_t = U\mathbf h_{t-1} + W \mathbf x_t \\
\mathbf h_t = f (\rm{LN}_{\gamma, \beta}(\mathbf z_t)))
\]</span> RNN的净输入一般会随着时间慢慢变大或变小，导致梯度爆炸或消失。</p>
<p>层归一化的RNN可以有效缓解梯度消失和梯度爆炸。</p>
<h2 id="批归和层归对比">批归和层归对比</h2>
<p>思想类似，都是<code>标准归一化</code> + <code>缩放和平移</code>。</p>
<ul>
<li>批量归一化：针对每一个维度，对batch的所有数据做归一化</li>
<li>层归一化：针对每一个样本，对所有维度做归一化。可以用在RNN上，减小梯度消失和梯度爆炸。</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/layer-batch-norm.jpg" style="display:block; margin:auto" width="70%"></p>
<h2 id="其它归一化">其它归一化</h2>
<p><strong>权重归一化</strong></p>
<p>对神经网络的连接权重进行归一化。</p>
<p><strong>局部相应归一化</strong></p>
<p>对同层的神经元进行归一化。但是局部响应归一化，用在激活函数之后，对邻近的神经元进行局部归一化。</p>
<h1 id="梯度下降法的改进">梯度下降法的改进</h1>
<h2 id="梯度下降法">梯度下降法</h2>
<p><code>Mini-Batch</code>梯度下降法。设<span class="math inline">\(f(\mathbf x ^{(i)}, \theta)\)</span> 是神经网络。</p>
<p>在第<span class="math inline">\(t\)</span>次迭代(epoch)时，选取<span class="math inline">\(m\)</span>个训练样本<span class="math inline">\(\{\mathbf x^{(i)}, y^{(i)} \}_{i=1}^m\)</span>。 计算梯度<span class="math inline">\(\mathbf g_t\)</span> <span class="math display">\[
\mathbf g_t = \frac{1}{m} \sum_{i \in I_t} \frac {\partial J(y^{(i)}, f(\mathbf x ^{(i)}, \theta))}{\partial \theta} 
+ \lambda \|\theta\| ^2   
\]</span> 更新参数： <span class="math display">\[
\theta_t = \theta_{t-1} - \alpha \mathbf g_t \;, \quad \quad \alpha \ge 0 \\
\]</span></p>
<p><span class="math display">\[
\theta_t = \theta_{t-1}+ \Delta \theta_t
\]</span></p>
<p>可以看出</p>
<ul>
<li>SGD，整体下降，但局部会来回震荡</li>
<li>MBGD，一个batch来说，batch越大，下降越快，越平滑</li>
<li>MBGD，整体来说，batch越小，下降越明显</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd_batch.png" style="display:block; margin:auto" width="70%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/sgd-whole.png" style="display:block; margin:auto" width="71%"></p>
<p>主要通过<code>学习率衰减</code>和<code>动量法</code>来改进。</p>
<h2 id="学习率递减方法">学习率递减方法</h2>
<p><strong>1. 按迭代次数递减</strong></p>
<p>设置<span class="math inline">\(\beta = 0.96\)</span>为衰减率</p>
<p><code>反时衰减</code> <span class="math display">\[
\alpha_t = \alpha_0 \cdot \frac {1} {1 + \beta \times t}
\]</span> <code>指数衰减</code> : <span class="math display">\[
\alpha_t = \alpha_0 \cdot \beta^t
\]</span> <code>自然指数衰减</code> <span class="math display">\[
\alpha_t = \alpha_0 \cdot e^{-\beta \cdot t}
\]</span> <strong>2. AdaGrad</strong></p>
<p><code>Adaptive Gradient</code> 自适应调整每个参数的学习率</p>
<p><strong>3. RMSprop</strong></p>
<p><strong>4. AdaDelta</strong></p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-03-30T14:26:58.061Z" itemprop="dateUpdated">2018-03-30 22:26:58</time>
</span><br>


        
        <br>原始链接：<a href="/2018/03/30/35-nerual-network-optim/" target="_blank" rel="external">http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/</a>
        
    </div>
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/神经网络/">神经网络</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&title=《网络优化》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&title=《网络优化》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《网络优化》 — PLM's Notes&url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/03/25/33-attention-summary/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">各种注意力总结</h4>
      </a>
    </div>
  
</nav>



    














</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>PLM &copy; 2016 - 2018</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&title=《网络优化》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&title=《网络优化》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《网络优化》 — PLM's Notes&url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/03/30/35-nerual-network-optim/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACPElEQVR42u3azY6DMAxF4b7/S1NptoVwrk2rwZysKmaa5mNhxT+vF17b3+J/3T4W+S75/9aSIUPGbRnbcqUHXT9Z/8oadvJqZMiQ8QBGJxTyo/Rf3+FzGTJkyADXu/UhakeUIUOGjE7AJanmmkR2liFDhoz0isbLZ/00+Iu5uAwZMm7ISBsDv/z8xf6GDBkybsLYwkUCYlqAS8twO9+VIUPGaAb5yTSt5cnq+tBx8ixDhozRjLQcRqhpuksSXTSmJkOGjHGM2qBDyqjtz/EyZMiYzeAHSi9/aRjtXzFlyJAxm5GWwHgQTK+PpFUZ92NlyJAxgtG5AtaQ6WBHkMrKkCFjHKNTxE+HvXi6S8YvdjqxMmTIGM24KjUNSmPht06eyJAhYzSDD1v0AysfFys2VmXIkDGakZbAauG1E3yDfWTIkDGO0T80T3prjQcUcGXIkDGa0Rm2IKUxfqGslecumBmRIUPGv2ekG111oFpx7TDoy5AhYzSDxGOSmtaSVf4KEEyGDBlDGbw9SdLRuOlYKrTFsV+GDBkjGGnJjARBUrZLm5eHL1GGDBmjGfz+SIr7teLdSTDF+8uQIWMqYwsXby6mjQQerHf2kSFDxmhGbTwrvcZ1ktVaw0CGDBnzGGkprVbE75T20LuXIUPGAxhpvZ0X7oMSf6dVIEOGDBngKOmTtFF6cumUIUOGjLAN0AnZ6XVThgwZz2GQVJMPQKSjG6TluX4uQ4aM2YzOpEZaROOfyf6t/oYMGTLuwXgDvVBBz/UzP3AAAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.0"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.0" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
