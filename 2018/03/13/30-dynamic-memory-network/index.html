<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>使用Dynamic Memory Network实现一个简单QA | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="DMN,QA">
    <meta name="description" content="论文：Ask Me Anything: Dynamic Memory Networks for Natural Language Processing  本文概要：介绍DMN的基本原理，使用PyTorch进行实现一个简单QA   模型简介 概要说明 许多NLP问题都可以看做一个Question-Answe">
<meta name="keywords" content="DMN,QA">
<meta property="og:type" content="article">
<meta property="og:title" content="使用Dynamic Memory Network实现一个简单QA">
<meta property="og:url" content="http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="论文：Ask Me Anything: Dynamic Memory Networks for Natural Language Processing  本文概要：介绍DMN的基本原理，使用PyTorch进行实现一个简单QA   模型简介 概要说明 许多NLP问题都可以看做一个Question-Answer问题。Dynamic Memory Network 由4部分组成。 输入模块">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dmn-simple.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dmn-detail.png">
<meta property="og:updated_time" content="2018-03-13T13:15:10.221Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="使用Dynamic Memory Network实现一个简单QA">
<meta name="twitter:description" content="论文：Ask Me Anything: Dynamic Memory Networks for Natural Language Processing  本文概要：介绍DMN的基本原理，使用PyTorch进行实现一个简单QA   模型简介 概要说明 许多NLP问题都可以看做一个Question-Answer问题。Dynamic Memory Network 由4部分组成。 输入模块">
<meta name="twitter:image" content="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dmn-simple.png">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.0">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="https://plmsmile.github.io/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">使用Dynamic Memory Network实现一个简单QA</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">使用Dynamic Memory Network实现一个简单QA</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-03-13T08:07:29.000Z" itemprop="datePublished" class="page-time">
  2018-03-13
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#模型简介"><span class="post-toc-number">1.</span> <span class="post-toc-text">模型简介</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#概要说明"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">概要说明</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#输入模块"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">输入模块</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#问题模块"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">问题模块</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#情景记忆模块"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">情景记忆模块</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#回答模块"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">回答模块</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#实现细节"><span class="post-toc-number">2.</span> <span class="post-toc-text">实现细节</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#数据处理"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">数据处理</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#模型"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">模型</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#配置信息"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">配置信息</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#训练"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">训练</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#预测和效果"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">预测和效果</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-30-dynamic-memory-network"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">使用Dynamic Memory Network实现一个简单QA</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-03-13 16:07:29" datetime="2018-03-13T08:07:29.000Z"  itemprop="datePublished">2018-03-13</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/自然语言处理/">自然语言处理</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>论文：<a href="https://arxiv.org/abs/1506.07285" target="_blank" rel="noopener">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a></p>
<blockquote>
<p>本文概要：介绍DMN的基本原理，使用PyTorch进行实现一个简单QA</p>
</blockquote>
<p><img src="" style="display:block; margin:auto" width="50%"></p>
<h1 id="模型简介">模型简介</h1>
<h2 id="概要说明">概要说明</h2>
<p>许多NLP问题都可以看做一个Question-Answer问题。<code>Dynamic Memory Network</code> 由4部分组成。</p>
<p><strong>输入模块</strong></p>
<p>对输入的句子<code>facts</code>(先<code>embedding</code>)使用<a href="https://plmsmile.github.io/2017/10/18/rnn/#gru">GRU</a>进行编码，得到<code>encoded_facts</code>，给到后面的<code>情景记忆模块</code>。</p>
<p><strong>问题模块</strong></p>
<p>对输入的问题<code>question</code>使用<code>GRU</code>进行编码，得到<code>encoded_question</code>， 给到后面的<code>情景记忆模块</code> 和<code>回答模块</code> 。</p>
<p><strong>情景记忆模块</strong></p>
<p><code>Episodic Memory Module</code>由<code>memory</code>和<code>attention</code>组成。</p>
<ul>
<li>attention：会选择更重要的<code>facts</code></li>
<li>memory：根据<code>question</code>、<code>facts</code>和 <code>旧memory</code>来生成<code>新momery</code> 。初始：<code>memory=encoded_question</code></li>
</ul>
<p>会在<code>facts</code>上迭代多次去计算<code>memory</code>。 每一次迭代会提取出新的信息。</p>
<p>输出最终的<code>momery</code>， 给到<code>回答模块</code>。</p>
<p><strong>回答模块</strong></p>
<p><code>memory</code> + <code>question</code>， 在<code>GRUCell</code>上迭代<code>原本的回答长度</code>次， 得到最终的预测结果。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dmn-simple.png" style="display:block; margin:auto" width="50%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/nlp/papers/dmn-detail.png" style="display:block; margin:auto" width="90%"></p>
<h2 id="输入模块">输入模块</h2>
<p><strong>输入</strong></p>
<ul>
<li>一个句子，有<span class="math inline">\(T_I\)</span>个单词</li>
<li><span class="math inline">\(T_I\)</span>个句子，则把这些句子合并成一个大句子。在每个句子的末尾添加一个<code>句子结束标记&lt;/s&gt;</code>。如上图蓝色的部分</li>
</ul>
<p><strong>GRU计算隐状态</strong></p>
<p>句子过RNN时，对于每一时刻<span class="math inline">\(t\)</span>的单词<span class="math inline">\(w_t\)</span> ，有<span class="math inline">\(h_t\)</span> : <span class="math display">\[
h_t = \rm{RNN}(w_t, h_{t-1})
\]</span> <strong>输出</strong></p>
<p>使用RNN的<code>h = hidden states</code> 作为<code>输入句子的向量表达</code>，也就是<code>encoded_facts</code></p>
<ul>
<li>一个句子，输出所有时刻的<span class="math inline">\(h_t\)</span></li>
<li>多个句子，输出每个句子<code>结束标记&lt;/s&gt;</code>时刻的<span class="math inline">\(h_t\)</span>。</li>
</ul>
<h2 id="问题模块">问题模块</h2>
<p><strong>输入</strong></p>
<p>输入一个句子<code>question</code>，有<span class="math inline">\(T_Q\)</span>个单词。</p>
<p><strong>GRU计算隐状态</strong> <span class="math display">\[
q_t = \rm{RNN}(w_t^Q, q_{t-1})
\]</span> <strong>输出Q编码</strong></p>
<p><code>最后时刻的隐状态</code><span class="math inline">\(q_{T_Q}\)</span>作为句子的编码。</p>
<hr>
<h2 id="情景记忆模块">情景记忆模块</h2>
<p><strong>总体思路</strong></p>
<p>记忆模块收到两个编码表达：<code>encoded_facts</code>和<code>encoded_question</code> ， 也就是<span class="math inline">\(h\)</span>和<span class="math inline">\(q\)</span>。</p>
<p>模块会生成一个记忆<code>memory</code>，初始时<code>memory = encoded_question</code></p>
<p>记忆模块在<code>encoded_facts</code>上反复迭代多轮，每一轮去提取新的信息<code>episode</code>， 更新<code>memory</code></p>
<ul>
<li>遍历所有<code>facts</code>， 对于每一个的<code>fact</code>， 不停地更新当前轮的信息<code>e</code></li>
<li>计算新的信息：<span class="math inline">\(e_{new}=\rm{RNN}(fact, e)\)</span> ，使用当前fact和当前信息</li>
<li>计算新信息的保留比例注意门<span class="math inline">\(g\)</span></li>
<li><code>更新信息</code>：<span class="math inline">\(e = g * e_{new} + (1-g) * e\)</span></li>
<li><p>计算保留比例g：结合当前<code>fact</code> 、<code>memory</code>、 <code>question</code> 去生成多个特征，再过一个<code>两层前向网络G</code>得到一个比例数值</p></li>
<li><p><code>更新memory</code> ，<span class="math inline">\(m^i = \rm{GRU}(e, m^{i-1})\)</span></p></li>
</ul>
<p><strong>特征函数与前向网络</strong></p>
<p>保留比例门<code>g</code>充当着<code>attention</code>的作用 。</p>
<p>特征函数<span class="math inline">\(z(c, m, q)\)</span>， 其中c就是当前的<code>fact</code> ，（论文里面是9个特征）： <span class="math display">\[
z(c, m, q) = [c \circ q,  c \circ m, \vert c-q\vert, \vert c-m\vert]
\]</span> 前向网络<span class="math inline">\(g=G(c, m ,q)\)</span> ： <span class="math display">\[
t = \rm{tanh}(W^1z(c, m, q) + b^1)  \\
g = G(c, m, q) = \sigma(W^2 t + b^2)
\]</span> <strong>e更新</strong></p>
<p>在每个fact遍历中，e会结合fact和旧e去生成新的信息<span class="math inline">\(e_{new}\)</span>，再结合旧<span class="math inline">\(e\)</span>和新<span class="math inline">\(e_{new}\)</span> 去生成最终的<span class="math inline">\(e^i\)</span> ： <span class="math display">\[
e_{new}=\rm{RNN}(fact, e)
\]</span></p>
<p><span class="math display">\[
e = g * e_{new} + (1-g) * e
\]</span></p>
<p><strong>记忆更新</strong></p>
<p>每一轮迭代后，结合旧记忆和当前轮的信息e去更新记忆： <span class="math display">\[
m^i = \rm{GRU}(e, m^{i-1})
\]</span> <strong>迭代停止条件</strong></p>
<ul>
<li>设置最大迭代次数<span class="math inline">\(T_M\)</span></li>
<li>在输入里面追加停止迭代信号，如果注意门选择它，则停止。</li>
</ul>
<h2 id="回答模块">回答模块</h2>
<p>回答模块结合memory和question，来生成对问题的答案。也是通过GRU来生成答案的。</p>
<p>设<code>a</code> 是<code>answer_gru</code>的hidden state，初始<span class="math inline">\(a_0= m^{T_M}\)</span> <span class="math display">\[
y_t = \rm{softmax}(W^a a_t) \\
a_t = \rm{GRU} ([y_{t-1}, q], a_{t-1})
\]</span> 使用<code>交叉熵</code>去计算loss，进行优化。</p>
<h1 id="实现细节">实现细节</h1>
<p><a href="https://github.com/plmsmile/NLP-Demos/tree/master/question-answer-DMN" target="_blank" rel="noopener">我的github源代码</a> ，实现参考自<a href="https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/10.Dynamic-Memory-Network-for-Question-Answering.ipynb" target="_blank" rel="noopener">DSKSD的代码</a> 。</p>
<h2 id="数据处理">数据处理</h2>
<p><strong>原始数据</strong></p>
<p>使用过的数据是facebook的<a href="http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz" target="_blank" rel="noopener">bAbi Tasks Data 1-20</a>里面的 <code>en-10k</code>下的<code>qa5_three-arg-relations_train.txt</code> 和test数据。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> Bill travelled to the office.</span><br><span class="line"><span class="number">2</span> Bill picked up the football there.</span><br><span class="line"><span class="number">3</span> Bill went to the bedroom.</span><br><span class="line"><span class="number">4</span> Bill gave the football to Fred.</span><br><span class="line">5 What did Bill give to Fred?   football        4</span><br><span class="line"><span class="number">6</span> Fred handed the football to Bill.</span><br><span class="line"><span class="number">7</span> Jeff went back to the office.</span><br><span class="line">8 Who received the football?    Bill    6</span><br><span class="line"><span class="number">9</span> Bill travelled to the office.</span><br><span class="line"><span class="number">10</span> Bill got the milk there.</span><br><span class="line">11 Who received the football?   Bill    6</span><br><span class="line"><span class="number">12</span> Fred travelled to the garden.</span><br><span class="line"><span class="number">13</span> Fred went to the hallway.</span><br><span class="line"><span class="number">14</span> Bill journeyed to the bedroom.</span><br><span class="line"><span class="number">15</span> Jeff moved to the hallway.</span><br><span class="line"><span class="number">16</span> Jeff journeyed to the bathroom.</span><br><span class="line"><span class="number">17</span> Bill journeyed to the office.</span><br><span class="line"><span class="number">18</span> Fred travelled to the bathroom.</span><br><span class="line"><span class="number">19</span> Mary journeyed to the kitchen.</span><br><span class="line"><span class="number">20</span> Jeff took the apple there.</span><br><span class="line"><span class="number">21</span> Jeff gave the apple to Fred.</span><br><span class="line">22 Who did Jeff give the apple to?      Fred    21</span><br><span class="line"><span class="number">23</span> Bill went back to the bathroom.</span><br><span class="line"><span class="number">24</span> Bill left the milk.</span><br><span class="line">25 Who received the apple?      Fred    21</span><br><span class="line"><span class="number">1</span> Mary travelled to the garden.</span><br><span class="line"><span class="number">2</span> Mary journeyed to the kitchen.</span><br><span class="line"><span class="number">3</span> Bill went back to the office.</span><br><span class="line"><span class="number">4</span> Bill journeyed to the hallway.</span><br><span class="line"><span class="number">5</span> Jeff went back to the bedroom.</span><br><span class="line"><span class="number">6</span> Fred moved to the hallway.</span><br><span class="line"><span class="number">7</span> Bill moved to the bathroom.</span><br><span class="line"><span class="number">8</span> Jeff went back to the garden.</span><br><span class="line"><span class="number">9</span> Jeff went back to the kitchen.</span><br><span class="line"><span class="number">10</span> Fred went back to the garden.</span><br><span class="line"><span class="number">11</span> Mary got the football there.</span><br><span class="line"><span class="number">12</span> Mary handed the football to Jeff.</span><br><span class="line">13 What did Mary give to Jeff?  football        12</span><br></pre></td></tr></table></figure>
<p>比如1-25是一个大的情景</p>
<ul>
<li>没有问号的都是陈述句，是情景数据<code>fact</code>。只有<code>.</code>号， 都是简单句</li>
<li>带问号的：是问句，带有答案和答案所在句子。使用<code>tab</code>分割</li>
</ul>
<p><strong>加载原始数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_raw_data</span><span class="params">(file_path, seq_end=<span class="string">'&lt;/s&gt;'</span>)</span>:</span></span><br><span class="line">    <span class="string">''' 从文件中读取文本数据，并整合成[facts, question, answer]一条一条的可用数据，原始word形式</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        file_path -- 数据文件</span></span><br><span class="line"><span class="string">        seq_end -- 句子结束标记</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        data -- list，元素是[facts, question, answer]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    source_data = open(file_path).readlines()</span><br><span class="line">    <span class="keyword">print</span> (file_path, <span class="string">":"</span>, len(source_data), <span class="string">"lines"</span>)</span><br><span class="line">    <span class="comment"># 去掉换行符号</span></span><br><span class="line">    source_data = [line[:<span class="number">-1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> source_data]</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> source_data:</span><br><span class="line">        index = line.split(<span class="string">' '</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> index == <span class="string">'1'</span>:</span><br><span class="line">            <span class="comment"># 一个新的QA开始</span></span><br><span class="line">            facts = []</span><br><span class="line">            <span class="comment">#qa = []</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'?'</span> <span class="keyword">in</span> line:</span><br><span class="line">            <span class="comment"># 当前QA的一个问句</span></span><br><span class="line">            <span class="comment"># 问题 答案 答案所在句子的编号 \t分隔</span></span><br><span class="line">            tmp = line.split(<span class="string">'\t'</span>)</span><br><span class="line">            question = tmp[<span class="number">0</span>].strip().replace(<span class="string">'?'</span>, <span class="string">''</span>).split(<span class="string">' '</span>)[<span class="number">1</span>:] + [<span class="string">'?'</span>]</span><br><span class="line">            answer = tmp[<span class="number">1</span>].split() + [seq_end]</span><br><span class="line">            facts_for_q = deepcopy(facts)</span><br><span class="line">            data.append([facts_for_q, question, answer])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 普通的事件描述，简单句，只有.和空格</span></span><br><span class="line">            sentence = line.replace(<span class="string">'.'</span>, <span class="string">''</span>).split(<span class="string">' '</span>)[<span class="number">1</span>:] + [seq_end]</span><br><span class="line">            facts.append(sentence)</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
<p><strong>把数据转成id格式</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triple_word2id</span><span class="params">(triple_word_data, th)</span>:</span></span><br><span class="line">    <span class="string">'''把文字转成id</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        triple_word_data -- [(facts, q, a)] word形式</span></span><br><span class="line"><span class="string">        th -- textheler</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        triple_id_data -- [(facts, q, a)]index形式</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 把各个word转成数字id</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> triple_word_data:</span><br><span class="line">        <span class="comment"># 处理facts句子</span></span><br><span class="line">        <span class="keyword">for</span> i, fact <span class="keyword">in</span> enumerate(t[<span class="number">0</span>]):</span><br><span class="line">            t[<span class="number">0</span>][i] = th.sentence2indices(fact)</span><br><span class="line">        <span class="comment"># 问题与答案</span></span><br><span class="line">        t[<span class="number">1</span>] = th.sentence2indices(t[<span class="number">1</span>])</span><br><span class="line">        t[<span class="number">2</span>] = th.sentence2indices(t[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> triple_word_data</span><br></pre></td></tr></table></figure>
<p><strong>根据batch_size取数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_loader</span><span class="params">(data, batch_size=<span class="number">1</span>, shuffle=False)</span>:</span></span><br><span class="line">    <span class="string">''' 以batch的格式返回数据</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data -- list格式的data</span></span><br><span class="line"><span class="string">        batch_size -- </span></span><br><span class="line"><span class="string">        shuffle -- 每一个epoch开始的时候，对数据进行shuffle</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        数据遍历的iterator</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        random.shuffle(data)</span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    end = batch_size</span><br><span class="line">    <span class="keyword">while</span> (start &lt; len(data)):</span><br><span class="line">        batch = data[start:end]</span><br><span class="line">        start, end = end, end + batch_size</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br><span class="line">    <span class="keyword">if</span> end &gt;= len(data) <span class="keyword">and</span> start &lt; len(data):</span><br><span class="line">        batch = data[start:]</span><br><span class="line">        <span class="keyword">yield</span> batch</span><br></pre></td></tr></table></figure>
<p><strong>对每一个batch进行padding</strong></p>
<p>这部分有点复杂。要求问题、答案、fact的长度一致，每个问题的fact的数量也要一样。</p>
<blockquote>
<p>其实和模型也有关，模型写的有点坑，就是每条数据的所有fact应该连接在一起成为一个大的fact送进GRU里，在每个fact后面加上结束标记。但是我这却分开了，分成了多个标记好的fact，也怪当时没有仔细看好论文，这个也是参考别人的实现。循环也导致训练贼慢，但是现在忙着找实习，就先不改了。后面好好写DMNPLUS吧。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_batch_data</span><span class="params">(raw_batch_data, th)</span>:</span></span><br><span class="line">    <span class="string">''' 对数据进行padding，问题、答案、fact长度分别一致，同时每条数据的fact的数量一致。输入到网络的时候要用</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        raw_batch_data -- [[facts, q, a]]，都是以list wordid表示</span></span><br><span class="line"><span class="string">        th -- TextHelper</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        all_facts -- [b, nfact, flen]，pad后的facts，Variable</span></span><br><span class="line"><span class="string">        all_facts_mask -- [b, nfact, flen]，facts的mask，Variable</span></span><br><span class="line"><span class="string">        questions -- [b, qlen]，pad后的questions，Variable</span></span><br><span class="line"><span class="string">        questions_mask -- [b, qlen]，questions的mask，Variable</span></span><br><span class="line"><span class="string">        answers -- [b, alen]，pad后的answers，Variable</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    all_facts, questions, answers = [list(i) <span class="keyword">for</span> i <span class="keyword">in</span> zip(*raw_batch_data)]</span><br><span class="line">    batch_size = len(raw_batch_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 计算各种长度。一个QA的facts数量，fact、Q、A句子的最大长度</span></span><br><span class="line">    n_fact = max([len(facts) <span class="keyword">for</span> facts <span class="keyword">in</span> all_facts])</span><br><span class="line">    flen = max([len(f) <span class="keyword">for</span> f <span class="keyword">in</span> flatten(all_facts)])</span><br><span class="line">    qlen = max([len(q) <span class="keyword">for</span> q <span class="keyword">in</span> questions])</span><br><span class="line">    alen = max([len(a) <span class="keyword">for</span> a <span class="keyword">in</span> answers])</span><br><span class="line">    padid = th.word2index(th.pad)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 对数据进行padding</span></span><br><span class="line">    all_facts_mask = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        <span class="comment"># 2.1 pad fact</span></span><br><span class="line">        facts = all_facts[i]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(facts)):</span><br><span class="line">            t = flen - len(facts[j])</span><br><span class="line">            <span class="keyword">if</span> t &gt; <span class="number">0</span>:</span><br><span class="line">                all_facts[i][j] = facts[j] + [padid] * t</span><br><span class="line">        <span class="comment"># fact数量pad</span></span><br><span class="line">        <span class="keyword">while</span> (len(facts) &lt; n_fact):</span><br><span class="line">            all_facts[i].append([padid] * flen)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算facts内容是否是填充给的，填充为1，不填充为0</span></span><br><span class="line">        mask = [tuple(map(<span class="keyword">lambda</span> v: v == padid, fact)) <span class="keyword">for</span> fact <span class="keyword">in</span> all_facts[i]]</span><br><span class="line">        all_facts_mask.append(mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2.2 pad question</span></span><br><span class="line">        q = questions[i]</span><br><span class="line">        <span class="keyword">if</span> len(q) &lt; qlen:</span><br><span class="line">            questions[i] = q + [padid] * (qlen - len(q))</span><br><span class="line">        <span class="comment"># 2.3 pad answer</span></span><br><span class="line">        a = answers[i]</span><br><span class="line">        <span class="keyword">if</span> len(a) &lt; alen:</span><br><span class="line">            answers[i] = a + [padid] * (alen - len(a))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 把list数据转成Variable</span></span><br><span class="line">    all_facts = get_variable(torch.LongTensor(all_facts))</span><br><span class="line">    all_facts_mask = get_variable(torch.ByteTensor(all_facts_mask))</span><br><span class="line">    answers = get_variable(torch.LongTensor(answers))</span><br><span class="line">    questions = torch.LongTensor(questions)</span><br><span class="line">    questions_mask = [(tuple(map(<span class="keyword">lambda</span> v: v == padid, q))) <span class="keyword">for</span> q <span class="keyword">in</span> questions]</span><br><span class="line">    questions_mask = torch.ByteTensor(questions_mask)</span><br><span class="line">    questions, questions_mask = get_variable(questions), get_variable(questions_mask)</span><br><span class="line">    <span class="keyword">return</span> all_facts, all_facts_mask, questions, questions_mask, answers</span><br></pre></td></tr></table></figure>
<h2 id="模型">模型</h2>
<p><strong>模型定义</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DMN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, hidden_size, padding_idx, seqbegin_id, dropout_p=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            vocab_size -- 词汇表大小</span></span><br><span class="line"><span class="string">            embed_size -- 词嵌入维数</span></span><br><span class="line"><span class="string">            hidden_size -- GRU的输出维数</span></span><br><span class="line"><span class="string">            padding_idx -- pad标记的wordid</span></span><br><span class="line"><span class="string">            seqbegin_id -- 句子起始的wordid</span></span><br><span class="line"><span class="string">            dropout_p -- dropout比率</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(DMN, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.seqbegin_id = seqbegin_id</span><br><span class="line">        </span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=padding_idx)</span><br><span class="line">        self.input_gru = nn.GRU(embed_size, hidden_size, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        self.question_gru = nn.GRU(embed_size, hidden_size, batch_first=<span class="keyword">True</span>)    </span><br><span class="line">        self.gate = nn.Sequential(</span><br><span class="line">                        nn.Linear(hidden_size * <span class="number">4</span>, hidden_size),</span><br><span class="line">                        nn.Tanh(),</span><br><span class="line">                        nn.Linear(hidden_size, <span class="number">1</span>),</span><br><span class="line">                        nn.Sigmoid()</span><br><span class="line">                    )</span><br><span class="line">        self.attention_grucell = nn.GRUCell(hidden_size, hidden_size)</span><br><span class="line">        self.memory_grucell = nn.GRUCell(hidden_size, hidden_size)</span><br><span class="line">        self.answer_grucell = nn.GRUCell(hidden_size * <span class="number">2</span>, hidden_size)</span><br><span class="line">        self.answer_fc = nn.Linear(hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout_p)</span><br><span class="line">        </span><br><span class="line">        self.init_weight()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="string">'''GRU的初始hidden。单层单向'''</span></span><br><span class="line">        hidden = torch.zeros(<span class="number">1</span>, batch_size, self.hidden_size)</span><br><span class="line">        hidden = get_variable(hidden)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weight</span><span class="params">(self)</span>:</span></span><br><span class="line">        nn.init.xavier_uniform(self.embed.state_dict()[<span class="string">'weight'</span>])</span><br><span class="line">        components = [self.input_gru, self.question_gru, self.gate, self.attention_grucell,</span><br><span class="line">                     self.memory_grucell, self.answer_grucell]</span><br><span class="line">        <span class="keyword">for</span> component <span class="keyword">in</span> components:</span><br><span class="line">            <span class="keyword">for</span> name, param <span class="keyword">in</span> component.state_dict().items():</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">                    nn.init.xavier_normal(param)</span><br><span class="line">        nn.init.xavier_uniform(self.answer_fc.state_dict()[<span class="string">'weight'</span>])</span><br><span class="line">        self.answer_fc.bias.data.fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p><strong>前向计算参数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, allfacts, allfacts_mask, questions, questions_mask, alen, n_episode=<span class="number">3</span>)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            allfacts -- [b, n_fact, flen]，输入的多个句子</span></span><br><span class="line"><span class="string">            allfacts_mask -- [b, n_fact, flen]，mask=1表示是pad的，否则不是</span></span><br><span class="line"><span class="string">            questions -- [b, qlen]，问题</span></span><br><span class="line"><span class="string">            questions_mask -- [b, qlen]，mask=1：pad</span></span><br><span class="line"><span class="string">            alen -- Answer len</span></span><br><span class="line"><span class="string">            seqbegin_id -- 句子开始标记的wordid</span></span><br><span class="line"><span class="string">            n_episodes -- </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            preds -- [b * alen,  vocab_size]，预测的句子。b*alen合在一起方便后面算交叉熵</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 0. 计算常用的信息，batch_size，一条数据nfact条句子，每个fact长度为flen，每个问题长度为qlen</span></span><br><span class="line">        bsize = allfacts.size(<span class="number">0</span>)</span><br><span class="line">        nfact = allfacts.size(<span class="number">1</span>)</span><br><span class="line">        flen = allfacts.size(<span class="number">2</span>)</span><br><span class="line">        qlen = questions.size(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><strong>输入模块</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 输入模块，用RNN编码输入的句子</span></span><br><span class="line"><span class="comment"># TODO 两层循环，待优化</span></span><br><span class="line">encoded_facts = []</span><br><span class="line"><span class="comment"># 对每一条数据，计算facts编码</span></span><br><span class="line"><span class="keyword">for</span> facts, facts_mask <span class="keyword">in</span> zip(allfacts, allfacts_mask):</span><br><span class="line">    facts_embeds = self.embed(facts)</span><br><span class="line">    facts.embeds = self.dropout(facts_embeds)</span><br><span class="line">    hidden = self.init_hidden(nfact)</span><br><span class="line">    <span class="comment"># 1.1 把输入(多条句子)给到GRU</span></span><br><span class="line">    <span class="comment"># b=nf, [nf, flen, h], [1, nf, h]</span></span><br><span class="line">    outputs, hidden = self.input_gru(facts_embeds, hidden)</span><br><span class="line">    <span class="comment"># 1.2 每条句子真正结束时(real_len)对应的输出，作为该句子的hidden。GRU：ouput=hidden</span></span><br><span class="line">    real_hiddens = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, o <span class="keyword">in</span> enumerate(outputs):</span><br><span class="line">        real_len = facts_mask[i].data.tolist().count(<span class="number">0</span>)</span><br><span class="line">        real_hiddens.append(o[real_len - <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 1.3 把所有单个fact连接起来，unsqueeze(0)是为了后面的所有batch的cat</span></span><br><span class="line">        hiddens = torch.cat(real_hiddens).view(nfact, <span class="number">-1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        encoded_facts.append(hiddens)</span><br><span class="line">        <span class="comment"># [b, nfact, h]</span></span><br><span class="line">        encoded_facts = torch.cat(encoded_facts)</span><br></pre></td></tr></table></figure>
<p><strong>问句模块</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. 问题模块，对问题使用RNN编码</span></span><br><span class="line">questions_embeds = self.embed(questions)</span><br><span class="line">questions_embeds = self.dropout(questions_embeds)</span><br><span class="line">hidden = self.init_hidden(bsize)</span><br><span class="line"><span class="comment"># [b, qlen, h], [1, b, h]</span></span><br><span class="line">outputs, hidden = self.question_gru(questions_embeds, hidden)</span><br><span class="line">real_questions = []</span><br><span class="line"><span class="keyword">for</span> i, o <span class="keyword">in</span> enumerate(outputs):</span><br><span class="line">    real_len = questions_mask[i].data.tolist().count(<span class="number">0</span>)</span><br><span class="line">    real_questions.append(o[real_len - <span class="number">1</span>])</span><br><span class="line">    encoded_questions = torch.cat(real_questions).view(bsize, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. Memory模块</span></span><br><span class="line">memory = encoded_questions</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_episode):</span><br><span class="line">    <span class="comment"># e</span></span><br><span class="line">    e = self.init_hidden(bsize).squeeze(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># [nfact, b, h]</span></span><br><span class="line">    encoded_facts_t = encoded_facts.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 根据memory, episode，计算每一时刻的e。最终的e和memory来计算新的memory</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(nfact):</span><br><span class="line">        <span class="comment"># [b, h]</span></span><br><span class="line">        bfact = encoded_facts_t[t]</span><br><span class="line">        <span class="comment"># TODO 计算4个特征，论文是9个</span></span><br><span class="line">        f1 = bfact * encoded_questions</span><br><span class="line">        f2 = bfact * memory</span><br><span class="line">        f3 = torch.abs(bfact - encoded_questions)</span><br><span class="line">        f4 = torch.abs(bfact - memory)</span><br><span class="line">        z = torch.cat([f1, f2, f3, f4], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [b, 1] 对每个fact的注意力</span></span><br><span class="line">        gt = self.gate(z)</span><br><span class="line">        e = gt * self.attention_grucell(bfact, e) + (<span class="number">1</span> - gt) * e</span><br><span class="line">        <span class="comment"># 每一轮的e和旧memory计算新的memory</span></span><br><span class="line">        memory = self.memory_grucell(e, memory)</span><br></pre></td></tr></table></figure>
<p><strong>回答模块</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4. Answer模块</span></span><br><span class="line"><span class="comment"># [b, h]</span></span><br><span class="line">answer_hidden = memory</span><br><span class="line">begin_tokens = get_variable(torch.LongTensor([self.seqbegin_id]*bsize))</span><br><span class="line"><span class="comment"># [b, h]</span></span><br><span class="line">last_word = self.embed(begin_tokens)</span><br><span class="line">preds = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(alen):</span><br><span class="line">    inputs = torch.cat([last_word, encoded_questions], dim=<span class="number">1</span>)</span><br><span class="line">    answer_hidden = self.answer_grucell(inputs, answer_hidden)</span><br><span class="line">    <span class="comment"># to vocab_size</span></span><br><span class="line">    probs = self.answer_fc(answer_hidden)</span><br><span class="line">    <span class="comment"># [b, v]</span></span><br><span class="line">    probs = F.log_softmax(probs.float())</span><br><span class="line">    _, indics = torch.max(probs, <span class="number">1</span>)</span><br><span class="line">    last_word = self.embed(indics)</span><br><span class="line">    <span class="comment"># for cross entropy</span></span><br><span class="line">    preds.append(probs.view(bsize, <span class="number">1</span>, <span class="number">-1</span>))</span><br><span class="line">    <span class="comment">#print (preds[0].data.shape)</span></span><br><span class="line">    preds = torch.cat(preds, dim=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">return</span> preds.view(bsize * alen, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="配置信息">配置信息</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultConfig</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">'''配置文件'''</span></span><br><span class="line">    <span class="comment"># 数据信息</span></span><br><span class="line">    train_file = <span class="string">"./datasets/tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_train.txt"</span></span><br><span class="line">    test_file = <span class="string">"./datasets/tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_test.txt"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 一些特殊符号</span></span><br><span class="line">    seq_end = <span class="string">'&lt;/s&gt;'</span></span><br><span class="line">    seq_begin = <span class="string">'&lt;s&gt;'</span></span><br><span class="line">    pad = <span class="string">'&lt;pad&gt;'</span></span><br><span class="line">    unk = <span class="string">'&lt;unk&gt;'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># DataLoader信息</span></span><br><span class="line">    batch_size = <span class="number">128</span></span><br><span class="line">    shuffle = <span class="keyword">False</span></span><br><span class="line">    <span class="comment"># TODO</span></span><br><span class="line">    num_workers = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># model</span></span><br><span class="line">    embed_size = <span class="number">64</span></span><br><span class="line">    hidden_size = <span class="number">64</span></span><br><span class="line">    <span class="comment"># 对inputs推理的轮数</span></span><br><span class="line">    n_episode = <span class="number">3</span></span><br><span class="line">    dropout_p = <span class="number">0.1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># train</span></span><br><span class="line">    max_epoch = <span class="number">500</span></span><br><span class="line">    learning_rate = <span class="number">0.001</span></span><br><span class="line">    min_loss = <span class="number">0.01</span></span><br><span class="line">    print_every_epoch = <span class="number">5</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># cuda信息</span></span><br><span class="line">    use_cuda = <span class="keyword">True</span></span><br><span class="line">    device_id = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># model_path</span></span><br><span class="line">    model_path = <span class="string">"./models/DMN.pkl"</span></span><br></pre></td></tr></table></figure>
<h2 id="训练">训练</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(opt, th, train_data)</span>:</span></span><br><span class="line">    <span class="string">''' 训练</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        opt -- 配置信息</span></span><br><span class="line"><span class="string">        th -- TextHelper实例</span></span><br><span class="line"><span class="string">        train_data -- 训练数据，[[facts, question, answer]]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 加载原始数据</span></span><br><span class="line">    seqbegin_id = th.word2index(th.seq_begin)</span><br><span class="line">    </span><br><span class="line">    model = DMN(th.vocab_size, opt.embed_size, opt.hidden_size, seqbegin_id, th.word2index(th.pad))</span><br><span class="line">    <span class="keyword">if</span> opt.use_cuda:</span><br><span class="line">        model = model.cuda(opt.device_id)</span><br><span class="line">    </span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr = opt.learning_rate)</span><br><span class="line">    loss_func = nn.CrossEntropyLoss(ignore_index=th.word2index(th.pad))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> range(opt.max_epoch):</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> batch_data <span class="keyword">in</span> get_data_loader(train_data, opt.batch_size, opt.shuffle):</span><br><span class="line">            <span class="comment"># batch内的数据进行pad，转成Variable</span></span><br><span class="line">            allfacts, allfacts_mask, questions, questions_mask, answers = \</span><br><span class="line">                    pad_batch_data(batch_data, th)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 前向</span></span><br><span class="line">            preds = model(allfacts, allfacts_mask, questions, questions_mask, </span><br><span class="line">                          answers.size(<span class="number">1</span>), opt.n_episode)</span><br><span class="line">            <span class="comment"># loss</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss = loss_func(preds, answers.view(<span class="number">-1</span>))</span><br><span class="line">            losses.append(loss.data.tolist()[<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># 反向</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        avg_loss = np.mean(losses)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> avg_loss &lt;= opt.min_loss <span class="keyword">or</span> e % opt.print_every_epoch == <span class="number">0</span> <span class="keyword">or</span> e == opt.max_epoch - <span class="number">1</span>:    </span><br><span class="line">            info = <span class="string">"e=&#123;&#125;, loss=&#123;&#125;"</span>.format(e, avg_loss)</span><br><span class="line">            losses = []</span><br><span class="line">            <span class="keyword">print</span> (info)</span><br><span class="line">            <span class="keyword">if</span> e == opt.max_epoch - <span class="number">1</span> <span class="keyword">and</span> avg_loss &gt; opt.min_loss:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"epoch finish, loss &gt; min_loss"</span>)</span><br><span class="line">                torch.save(model, opt.model_path)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">elif</span> avg_loss &lt;= opt.min_loss:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Early stop"</span>)</span><br><span class="line">                torch.save(model, opt.model_path)</span><br><span class="line">                <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h2 id="预测和效果">预测和效果</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_test_accuracy</span><span class="params">(model, test_data, th, n_episode=DefaultConfig.n_episode)</span>:</span></span><br><span class="line">    <span class="string">'''测试，测试数据'''</span></span><br><span class="line">    batch_size = <span class="number">1</span></span><br><span class="line">    model.eval()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> get_data_loader(test_data, batch_size, <span class="keyword">False</span>):</span><br><span class="line">        facts, facts_mask, question, question_mask, answer = pad_batch_data(item, th)</span><br><span class="line">        preds = model(facts, facts_mask, question, question_mask, answer.size(<span class="number">1</span>), n_episode)</span><br><span class="line">        <span class="comment">#print (answer.data.shape, preds.data.shape)</span></span><br><span class="line">        preds = preds.max(<span class="number">1</span>)[<span class="number">1</span>].data.tolist()</span><br><span class="line">        answer = answer.view(<span class="number">-1</span>).data.tolist()</span><br><span class="line">        <span class="keyword">if</span> preds == answer:</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"acccuracy = "</span>, correct / len(test_data)) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_one_data</span><span class="params">(model, item, th, n_episode=DefaultConfig.n_episode)</span>:</span></span><br><span class="line">    <span class="string">''' 测试一条数据</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model -- DMN模型</span></span><br><span class="line"><span class="string">        item -- [facts, question, answer]</span></span><br><span class="line"><span class="string">        th -- TextHelper</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        None</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># batch_size = 1</span></span><br><span class="line">    model.eval()</span><br><span class="line">    item = [item]</span><br><span class="line">    facts, facts_mask, question, question_mask, answer = pad_batch_data(item, th)</span><br><span class="line">    preds = model(facts, facts_mask, question, question_mask, answer.size(<span class="number">1</span>), n_episode)</span><br><span class="line">    </span><br><span class="line">    item = item[<span class="number">0</span>]</span><br><span class="line">    preds = preds.max(<span class="number">1</span>)[<span class="number">1</span>].data.tolist()</span><br><span class="line">    fact = item[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    facts = [th.indices2sentence(fact) <span class="keyword">for</span> fact <span class="keyword">in</span> item[<span class="number">0</span>]]</span><br><span class="line">    facts = [<span class="string">" "</span>.join(fact) <span class="keyword">for</span> fact <span class="keyword">in</span> facts]</span><br><span class="line">    q = <span class="string">" "</span>.join(th.indices2sentence(item[<span class="number">1</span>]))</span><br><span class="line">    a = <span class="string">" "</span>.join(th.indices2sentence(item[<span class="number">2</span>]))</span><br><span class="line">    preds = <span class="string">" "</span>.join(th.indices2sentence(preds))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Facts:"</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"\n"</span>.join(facts))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Question:"</span>, q)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Answer:"</span>, a)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Predict:"</span>, preds)</span><br><span class="line">    <span class="keyword">print</span> ()</span><br></pre></td></tr></table></figure>
<p>在本数据集上效果较好，但是数据量小、句子简单，还没有在别的数据集上面进行测试。等忙完了测试一下。</p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-03-13T13:15:10.221Z" itemprop="dateUpdated">2018-03-13 21:15:10</time>
</span><br>


        
        <br>原始链接：<a href="/2018/03/13/30-dynamic-memory-network/" target="_blank" rel="external">http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/</a>
        
    </div>
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DMN/">DMN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/QA/">QA</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/&title=《使用Dynamic Memory Network实现一个简单QA》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/&title=《使用Dynamic Memory Network实现一个简单QA》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《使用Dynamic Memory Network实现一个简单QA》 — PLM's Notes&url=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2018/03/14/31-co-attention-vqa/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">协同注意力简介</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/03/05/29-desicion-tree/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">决策树笔记</h4>
      </a>
    </div>
  
</nav>



    














</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div>
    <div class="bottom">
        <p><span>PLM &copy; 2016 - 2018</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/&title=《使用Dynamic Memory Network实现一个简单QA》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/&title=《使用Dynamic Memory Network实现一个简单QA》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《使用Dynamic Memory Network实现一个简单QA》 — PLM's Notes&url=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/03/13/30-dynamic-memory-network/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACrklEQVR42u3ay26DMBAF0P7/T6dSV90Adx5OU+mwioCAjyPZzh1/fcXH6+f4/fn3mft7ru7Mr17duXbg4eHhtZp+/7h7THLmvomv+Ch3AR4eHt4xXjIZXDXo/nw+DeRdk7QZDw8P7xN41Rfki+C8m/Dw8PD+Fy+5mr+4t3TGw8PD+xxe78//JA6uLtyPZy14eHh4B6pIn/P5eH0PDw8Pr1VVzwffre9W4+CHt+Dh4eEd4CVF/eoIXB3Q51sQHlqOh4eHt8qbD8eTiDYJHZKrlx2Kh4eHd5iXL3/zwKIay1ZH9Qc8Hh4e3jFe0uj5hNGLEhbKWnh4eHjHeHkBvgeohhfzkAIPDw/vBK9XxOqlxdUIYzmwwMPDw1viJUNtEu/2oor8au9OPDw8vBO8aiDbix6qmwAOxhN4eHh4q7zJJJEUyXolsfwqHh4e3jt51RL+HFkd4idFODw8PLzTvHxi6BWlel2Zb9i6zKfx8PDwlnjJArca3b4GR5X6MFHh4eHhHeBN4on8W5PJJl+yL6cveHh4eBe8/EG7ZbDqEN9cxOPh4eEd4OV/8rdmm/Ls1NriUM4w8PDw8Fr1pl5wMIktkobmXfywrMfDw8M7wOstaqtxai8I7kXGeHh4eO/hJX/17xnVAX2yKC8/Bw8PD2+VNxmIe7xq6ND8xfDw8PDeMhnkj66GttXleK/A9rCzDA8PD2+Jlw/6ediaN64aauQ/Bh4eHt4ub6vo1Ytu87c0i214eHh4B3i97U3VR8wj2nnhDQ8PD2+Xlw/TOSwhVQf3fGLAw8PDew8veX3SNb1NUb2YOGoJHh4e3gfwemWqXpAxX3zj4eHh/S0vH9aru556C+vlJTUeHh7egTBiHrPm7823apXX2nh4eHhjXrUA1iv259/tBRZr9T08PDy8uzPfo9DaTzM7L60AAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.0"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.0" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
