<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    <title>卷积相关 | PLM&#39;s Notes | 好好学习，天天笔记</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="卷积,可分离卷积">
    <meta name="description" content="卷积神经网络 全连接网络的两个问题：  参数太多：训练效率低、容易过拟合 局部不变形特征：全连接很难提取出图片的不变性特征  三个特性 1. 局部性 图片特征只在局部。图片特征决定图片类别，这些图片特征在一些局部的区域中。 局部连接。 2. 相同性 用同样的检测模式去检测不同图片的相同特征。只是这">
<meta name="keywords" content="卷积,可分离卷积">
<meta property="og:type" content="article">
<meta property="og:title" content="卷积相关">
<meta property="og:url" content="http://plmsmile.github.io/2018/04/11/38-convolution/index.html">
<meta property="og:site_name" content="PLM&#39;s Notes">
<meta property="og:description" content="卷积神经网络 全连接网络的两个问题：  参数太多：训练效率低、容易过拟合 局部不变形特征：全连接很难提取出图片的不变性特征  三个特性 1. 局部性 图片特征只在局部。图片特征决定图片类别，这些图片特征在一些局部的区域中。 局部连接。 2. 相同性 用同样的检测模式去检测不同图片的相同特征。只是这些特征出现在图片的不同位置。 参数共享。 3. 不变性 对于一张大图片">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/ondim-conv-2.jpeg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/onedim-conv-0.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/onedim-conv-1.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/twodim-conv-0.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/twodim-conv-2.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv_layer.gif">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/full-conv.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/threedim-conv.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/max-pool.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/pool.jpeg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv-nn.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/convnet.jpeg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/LeNet.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/AlexNet.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/inception.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/depth-wise-conv.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/depth-wise-conv2.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/resnet.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv-stantard.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/xception-2.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/depthwise-conv-compare.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/xception.jpg">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/Reading_Note_20170720_ShuffleNet_0.png">
<meta property="og:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv-compare.jpg">
<meta property="og:updated_time" content="2018-04-12T03:35:38.783Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="卷积相关">
<meta name="twitter:description" content="卷积神经网络 全连接网络的两个问题：  参数太多：训练效率低、容易过拟合 局部不变形特征：全连接很难提取出图片的不变性特征  三个特性 1. 局部性 图片特征只在局部。图片特征决定图片类别，这些图片特征在一些局部的区域中。 局部连接。 2. 相同性 用同样的检测模式去检测不同图片的相同特征。只是这些特征出现在图片的不同位置。 参数共享。 3. 不变性 对于一张大图片">
<meta name="twitter:image" content="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/ondim-conv-2.jpeg">
    
        <link rel="alternate" type="application/atom+xml" title="PLM&#39;s Notes" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.7.0">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="https://plmsmile.github.io/about" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">PLM</h5>
          <a href="mailto:plmsmile@126.com" title="plmsmile@126.com" class="mail">plmsmile@126.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                类别
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user"></i>
                关于我
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/plmsmile" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">卷积相关</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">卷积相关</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-04-11T07:42:36.000Z" itemprop="datePublished" class="page-time">
  2018-04-11
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/深度学习/">深度学习</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#卷积神经网络"><span class="post-toc-number">1.</span> <span class="post-toc-text">卷积神经网络</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#三个特性"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">三个特性</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#卷积"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">卷积</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#卷积层"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">卷积层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#卷积代替全连接"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">卷积代替全连接</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#汇聚层"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">汇聚层</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#典型的卷积网络结构"><span class="post-toc-number">1.6.</span> <span class="post-toc-text">典型的卷积网络结构</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#常见卷积网络"><span class="post-toc-number">2.</span> <span class="post-toc-text">常见卷积网络</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#lenet"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">LeNet</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#alex-net"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">Alex Net</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#inception-net"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">Inception Net</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#res-net"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">Res Net</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#xception"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">Xception</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#shuffle-net"><span class="post-toc-number">2.6.</span> <span class="post-toc-text">Shuffle Net</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#senet"><span class="post-toc-number">2.7.</span> <span class="post-toc-text">SENet</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#总结"><span class="post-toc-number">2.8.</span> <span class="post-toc-text">总结</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-38-convolution"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">卷积相关</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-04-11 15:42:36" datetime="2018-04-11T07:42:36.000Z"  itemprop="datePublished">2018-04-11</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/深度学习/">深度学习</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p><img src="" style="display:block; margin:auto" width="70%"></p>
<h1 id="卷积神经网络">卷积神经网络</h1>
<p>全连接网络的两个问题：</p>
<ul>
<li>参数太多：训练效率低、容易过拟合</li>
<li>局部不变形特征：全连接很难提取出图片的不变性特征</li>
</ul>
<h2 id="三个特性">三个特性</h2>
<p><strong>1. 局部性</strong></p>
<p>图片特征只在局部。图片特征决定图片类别，这些图片特征在一些局部的区域中。</p>
<p>局部连接。</p>
<p><strong>2. 相同性</strong></p>
<p>用同样的检测模式去检测不同图片的相同特征。只是这些特征出现在图片的不同位置。</p>
<p>参数共享。</p>
<p><strong>3. 不变性</strong></p>
<p>对于一张大图片，进行下采样，图片的性质基本保持不变。</p>
<p>下采样保持不变性。</p>
<h2 id="卷积">卷积</h2>
<blockquote>
<ol style="list-style-type: decimal">
<li>一维卷积：卷积核、步长、首位0填充</li>
<li>三种卷积：窄卷积、宽卷积、等长卷积</li>
<li>二维卷积</li>
</ol>
</blockquote>
<p><strong>1. 一维卷积</strong></p>
<ul>
<li>卷积核：参数<span class="math inline">\([1, 0, -1]\)</span>就是一个<code>卷积核</code> 或 <code>滤波器</code></li>
<li>步长：卷积核滑动的间隔</li>
<li>零填充：在输入向量两端进行补零</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/ondim-conv-2.jpeg" style="display:block; margin:auto" width="70%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/onedim-conv-0.png" style="display:block; margin:auto" width="50%"> <strong>2. 三种卷积</strong></p>
<p>输入n，卷积大小m，步长s，输入神经元各两端填补p个0</p>
<ul>
<li>窄卷积：<code>s=1</code>，不补0，输出长度为<code>n-m+1</code></li>
<li>宽卷积：<code>s=1</code>，两端补0，<span class="math inline">\(p=m-1\)</span>， 输出长度为<code>n+m-1</code></li>
<li>等长卷积：<code>s=1</code>，两端补0，<span class="math inline">\(p=\frac{m-1}{2}\)</span>， 输出长度为<code>n</code></li>
</ul>
<p>一般卷积默认为窄卷积。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/onedim-conv-1.png" style="display:block; margin:auto" width="50%"></p>
<p><strong>3. 二维卷积</strong></p>
<p>输入一张图片（假设深度为1），<span class="math inline">\(X \in \mathbb R^{M \times N}\)</span>， 卷积核<span class="math inline">\(W \in \mathbb R ^{m \times n}\)</span> ，则卷积（互相关代替）结果为： <span class="math display">\[
y_{ij} = \sum_{u=1}^m \sum_{v=1}^n w_{uv} x_{i+u-1, j+v-1}
\]</span> <img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/twodim-conv-0.png" style="display:block; margin:auto" width="70%"></p>
<p>一个卷积核提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器。</p>
<p>卷积后的结果称为<code>特征映射（feature map）</code>。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/twodim-conv-2.png" style="display:block; margin:auto" width="50%"></p>
<h2 id="卷积层">卷积层</h2>
<blockquote>
<ol style="list-style-type: decimal">
<li>一个卷积核</li>
</ol>
<p><span class="math inline">\(W_p \in \mathbb R ^{m \times n \times D}\)</span>， 对D个通道做卷积，结果相加求和，过激活函数，得到一个特征图<span class="math inline">\(Y^p \in\mathbb R^{M^\prime \times N^\prime}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>多个卷积核：得到P个特征图</li>
</ol>
</blockquote>
<p>输入图片(feature map)是<span class="math inline">\(X \in \mathbb R^{M \times N \times D}\)</span>，深度是D</p>
<p><strong>1. 一个卷积核</strong></p>
<ul>
<li>用1个卷积核<span class="math inline">\(W_p \in \mathbb R ^{m \times n \times D}\)</span>（实际上是D个<span class="math inline">\(\mathbb R^{m\times n}\)</span>）去卷积这张图片（所有深度）</li>
<li>对各个深度的卷积结果进行<strong>相加求和</strong>，再<strong>加上偏置</strong></li>
<li><strong>过激活函数</strong>，输出最终的FM，是<span class="math inline">\(Y^p\)</span></li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv.png" style="display:block; margin:auto" width="100%"></p>
<p><strong>2. 多个卷积核</strong></p>
<p>多个卷积核可以提取出多种不同的特征。输入图片是<span class="math inline">\(X \in \mathbb R^{M \times N \times D}\)</span>，</p>
<ul>
<li>有P个不同的卷积核<span class="math inline">\(W_p \in \mathbb R ^{m \times n \times D}\)</span>， 实际上是四维的[m, n, D, P]，后两维是<code>in_channel</code>、<code>out_channel</code></li>
<li>输出P个特征图<span class="math inline">\(\mathbb R^{M^\prime \times N^\prime \times P}\)</span></li>
<li>对每一个卷积核<span class="math inline">\(W \in \mathbb R ^{m \times n \times D}\)</span>，对D个深度<span class="math inline">\(\mathbb R ^{m \times n}\)</span>分别做卷积，对D个卷积结果进行求和相加，经过激活函数，得到一个特征图 <span class="math inline">\(Y^p \in\mathbb R^{M^\prime \times N^\prime}\)</span></li>
<li>一共需要<span class="math inline">\(P \times D \times (m \times n) + P\)</span>个参数</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv_layer.gif" style="display:block; margin:auto" width="80%"></p>
<h2 id="卷积代替全连接">卷积代替全连接</h2>
<blockquote>
<ol style="list-style-type: decimal">
<li>局部连接：卷积核只与输入的一个局部做连接，计算出FM中的一个值，局部性</li>
<li>权值共享：同一个卷积核与图片的各个位置进行连接，权值是一样的，提取出同样的特征</li>
</ol>
</blockquote>
<p><strong>1. 局部连接</strong></p>
<ul>
<li>卷积层的神经元只与输入数据的一个局部区域做连接</li>
<li>因为图片的局部性，图片的特征在局部</li>
<li>FM中的每一个值，只与输入的局部相关。而不是与所有的相关</li>
</ul>
<p><strong>2. 权值共享</strong></p>
<ul>
<li>一个卷积核会分多次对输入数据的各个部分做卷积操作</li>
<li>对每个部分的连接参数实际上是相同的，因为是同一个卷积核</li>
<li>因为图片的相同性，同样的卷积核可以检测出相同的特征，只是特征在不同的位置</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/full-conv.png" style="display:block; margin:auto" width="60%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/threedim-conv.png" style="display:block; margin:auto" width="60%"></p>
<h2 id="汇聚层">汇聚层</h2>
<blockquote>
<ol style="list-style-type: decimal">
<li>卷积层的不足：FM的维数很高</li>
<li>汇聚层的作用：选择特征、降低特征数量、减少参数数量、避免过拟合</li>
<li>两种汇聚方式：最大和平均。</li>
</ol>
</blockquote>
<p><strong>1. 卷积层的不足</strong></p>
<ul>
<li>减少网络连接数量</li>
<li>但是<strong>FM中的神经元个数依然很多</strong></li>
<li>如果直接接分类器全连接，则维数会很高，<strong>容易过拟合</strong></li>
</ul>
<p><strong>2. 汇聚层的作用</strong></p>
<p><code>汇聚层</code>(pooling layer)，也作子采样层(subsampling layer)。作用是：</p>
<ul>
<li>进行特征选择</li>
<li>降低特征数量</li>
<li>进而减少参数数量、避免过拟合</li>
<li>拥有更大感受野，大图片缩小，保持<code>不变性</code></li>
</ul>
<p><strong>3. 两种汇聚方式</strong></p>
<ul>
<li>最大汇聚：一个区域内所有神经元的最大值</li>
<li>平均汇聚：一个区域内所有神经元的平均值</li>
</ul>
<p>过大采样区会急剧减少神经元的数量，造成过多的信息损失！</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/max-pool.png" style="display:block; margin:auto" width="90%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/pool.jpeg" style="display:block; margin:auto" width="40%"></p>
<h2 id="典型的卷积网络结构">典型的卷积网络结构</h2>
<p>由多个捐几块组成，一个卷积块：</p>
<ul>
<li>连续2~5个卷积层，ReLU激活函数</li>
<li>0~1个汇聚层</li>
</ul>
<p>目前，趋向于使用更小的卷积核，比如<span class="math inline">\(1\times 1, 3 \times 3\)</span>。汇聚层的比例也逐渐降低，趋向于全卷积网络。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv-nn.png" style="display:block; margin:auto" width="100%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/convnet.jpeg" style="display:block; margin:auto" width="70%"></p>
<h1 id="常见卷积网络">常见卷积网络</h1>
<h2 id="lenet">LeNet</h2>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/LeNet.png" style="display:block; margin:auto" width="70%"></p>
<h2 id="alex-net">Alex Net</h2>
<p>使用ReLU作为非线性激活函数、Dropout防止过拟合、数据增强提高模型准确率。</p>
<p><strong>AlexNet分组卷积</strong></p>
<ul>
<li>对所有通道进行分组，进行分组卷积，执行<strong>标准卷积操作</strong></li>
<li>在最后时刻才使用两个全连接融合通道的信息</li>
<li>降低了模型的泛化能力</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/AlexNet.png" style="display:block; margin:auto" width="80%"></p>
<h2 id="inception-net">Inception Net</h2>
<p>如何选择卷积核大小非常关键：</p>
<ul>
<li><strong>一个卷积层同时使用多种尺寸的卷积核</strong></li>
<li>先过<span class="math inline">\(1\times 1\)</span>卷积减少卷积层参数量</li>
</ul>
<p>Inception Net由多个Inception模块堆叠而成。一个Inception同时使用<span class="math inline">\(1\times 1\)</span>、<span class="math inline">\(3\times 3\)</span>、<span class="math inline">\(5\times 5\)</span> 的卷积，如下：</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/inception.png" style="display:block; margin:auto" width="70%"></p>
<p><span class="math inline">\(3\times 3\)</span> 、<span class="math inline">\(5\times 5\)</span> 卷积前，先进行<span class="math inline">\(1\times 1\)</span>卷积的作用：</p>
<ul>
<li>减少输入数据的深度</li>
<li>减少各个深度的冗余信息，先进行一次特征抽取</li>
</ul>
<p>后续还有各种各样的Inception Net，最终演变成Xception Net。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/depth-wise-conv.jpg" style="display:block; margin:auto" width="60%"></p>
<p>Inception Net的极限就是，对每个channel做一个单独的卷积。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/depth-wise-conv2.jpg" style="display:block; margin:auto" width="60%"></p>
<h2 id="res-net">Res Net</h2>
<p>越深的网络可以用ResNet来训练。<a href="https://zhuanlan.zhihu.com/p/28124810?group_id=883267168542789632" target="_blank" rel="noopener">ResNet可以很深的原因</a></p>
<p><code>残差连接</code>通过<strong>给非线性的卷积层增加直连边</strong>的方式</p>
<ul>
<li>来<strong>提高信息的传播效率</strong></li>
<li>可以减小梯度消失问题</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/resnet.png" style="display:block; margin:auto" width="70%"></p>
<h2 id="xception">Xception</h2>
<p>卷积需要同时考虑所有通道吗？</p>
<p>输入图片(feature map)是<span class="math inline">\(X \in \mathbb R^{M \times N \times D}\)</span>，深度是D</p>
<p><strong>1. 传统卷积核会同时考虑所有通道</strong></p>
<ul>
<li>传统<strong>1个卷积核</strong>会对所有channel的FM做同样的卷积</li>
<li>得到D个卷积结果</li>
<li>再<strong>对D个卷积结果进行相加求过激活函数</strong>得到<strong>一个FM</strong></li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv-stantard.jpg" style="display:block; margin:auto" width="70%"></p>
<p><strong>2. 深度可分离卷积核</strong></p>
<p><code>Depth Separable Convolution</code></p>
<p>输入数据有D个FM，输出P个FM。<code>深度可分离卷积(DepthWise Convolution)</code> 如下：</p>
<ul>
<li>对<span class="math inline">\(X\)</span>的每个channel，分别做一个单独的卷积，得到D个新的FM</li>
<li>对D个新的FM，做<span class="math inline">\(1\times 1\)</span>的传统卷积(<code>PointWise Convolution</code>)，<span class="math inline">\(P \times D \times (1 \times 1)\)</span></li>
<li>最终输出P个FM （通道数变换）</li>
</ul>
<p>卷积操作不一定需要同时考虑通道和区域。<code>可分离卷积</code>。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/xception-2.jpg" style="display:block; margin:auto" width="70%"></p>
<p><strong>3. 可分离卷积参数大大减小</strong></p>
<p>输入通道<span class="math inline">\(D=3\)</span>，输出通道<span class="math inline">\(P=256\)</span>，卷积核大小为<span class="math inline">\(3 \times 3\)</span></p>
<ul>
<li>传统卷积参数：<span class="math inline">\(256 \times 3 \times (3 \times 3) = 6912\)</span></li>
<li>DepthWise卷积参数：<span class="math inline">\(3 \times 3 \times 3 +256 \times 3 \times (1 \times 1) =795\)</span>， 降低九分之一</li>
</ul>
<p>同时，效果更好。</p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/depthwise-conv-compare.jpg" style="display:block; margin:auto" width="60%"></p>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/xception.jpg" style="display:block; margin:auto" width="70%"></p>
<h2 id="shuffle-net">Shuffle Net</h2>
<p><strong>1. AlexNet分组卷积</strong></p>
<ul>
<li>对所有通道进行分组，进行分组卷积，执行<strong>标准卷积操作</strong></li>
<li>在最后时刻才使用全连接融合通道的信息</li>
<li>降低了模型的泛化能力</li>
</ul>
<p><strong>2. ShuffleNet 分组卷积</strong></p>
<p><code>ShuffleNet</code> = <strong>分组卷积</strong>（通道分组）+ <strong>深度可分离卷积</strong>（Depthwise+PointWise）</p>
<p>对通道进行分组卷积时</p>
<ul>
<li>每一个组执行深度可分离卷积，而不是标准传统卷积</li>
<li>每一次层叠分组卷积时，都进行channel shuffle</li>
<li>实际上每个组各取一个也能实现shuffle</li>
</ul>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/Reading_Note_20170720_ShuffleNet_0.png" style="display:block; margin:auto" width="100%"></p>
<h2 id="senet">SENet</h2>
<p>Inception、ShuffleNet等网络中，<strong>对所有通道产生的特征</strong>都是<strong>不分权重直接相加求和</strong>的。</p>
<p>为什么所有通道的特征对模型的作用是相等的呢？</p>
<p><img src="" style="display:block; margin:auto" width="70%"></p>
<h2 id="总结">总结</h2>
<p>参考自<a href="https://zhuanlan.zhihu.com/p/28749411" target="_blank" rel="noopener">知乎卷积网络中十大拍案叫绝的操作</a></p>
<p><strong>1. 卷积核</strong></p>
<ol style="list-style-type: decimal">
<li>大卷积核用多个小卷积核代替</li>
<li>单一尺寸卷积核用多尺寸卷积核代替</li>
<li>固定形状卷积核趋于用可变形卷积核</li>
<li>使用<span class="math inline">\(1\times 1\)</span>卷积核</li>
</ol>
<p><strong>2. 卷积层通道</strong></p>
<ol style="list-style-type: decimal">
<li>标准卷积使用深度可分离卷积代替</li>
<li>使用分组卷积</li>
<li>分组卷积前使用channel shuffle</li>
<li>通道加权计算</li>
</ol>
<p><strong>3. 卷积层连接</strong></p>
<ol style="list-style-type: decimal">
<li>使用skip connection，让模型更深</li>
<li>densely connection，使每一层都融合其它层的特征输出</li>
</ol>
<p><img src="http://otafnwsmg.bkt.clouddn.com/img/dl/cnn/conv-compare.jpg" style="display:block; margin:auto" width="70%"></p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    最后更新时间：<time datetime="2018-04-12T03:35:38.783Z" itemprop="dateUpdated">2018-04-12 11:35:38</time>
</span><br>


        
        <br>原始链接：<a href="/2018/04/11/38-convolution/" target="_blank" rel="external">http://plmsmile.github.io/2018/04/11/38-convolution/</a>
        
    </div>
    <footer>
        <a href="http://plmsmile.github.io">
            <img src="/img/avatar.jpg" alt="PLM">
            PLM
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/卷积/">卷积</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/可分离卷积/">可分离卷积</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/04/11/38-convolution/&title=《卷积相关》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/04/11/38-convolution/&title=《卷积相关》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/04/11/38-convolution/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《卷积相关》 — PLM's Notes&url=http://plmsmile.github.io/2018/04/11/38-convolution/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/04/11/38-convolution/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2018/04/12/39-squard-models/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">阅读理解模型总结</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2018/04/01/37-reinforce-learning/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">强化学习</h4>
      </a>
    </div>
  
</nav>



    














</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        谢谢大爷~
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/img/wechat.png" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="/img/wechat.png" data-alipay="/img/alipay.png">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>



</div>

        <footer class="footer">
    <!-- <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>博客内容遵循 <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">知识共享 署名 - 非商业性 - 相同方式共享 4.0 国际协议</a></span>
        </p>
    </div> -->
    <div class="bottom">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
          <span>

          <!-- PLM  -->
          PLM's Notes &nbsp;
          &copy;
          &nbsp;
          2016 - 2018

          </span>
            <!-- <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span> -->
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://plmsmile.github.io/2018/04/11/38-convolution/&title=《卷积相关》 — PLM's Notes&pic=http://plmsmile.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://plmsmile.github.io/2018/04/11/38-convolution/&title=《卷积相关》 — PLM's Notes&source=NLP，DL，ML，Leetcode，Java/C++你学了吗？" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://plmsmile.github.io/2018/04/11/38-convolution/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《卷积相关》 — PLM's Notes&url=http://plmsmile.github.io/2018/04/11/38-convolution/&via=http://plmsmile.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://plmsmile.github.io/2018/04/11/38-convolution/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACMElEQVR42u3aQW7EIAwF0Ln/pafbStXQ/01aKeSxqiIm4bFwbczrFY/3t/Hzyffnn+a373z9xcDAwLgt470cM0a+6PXbkrVhYGA8h/Hpw5/m/M+vkvkYGBgY6+CYpHRF0MTAwMC4NOCuU8A8oUzmY2BgYORF7PpF+RHbLLhfVotjYGDckDELo//z95/0NzAwMG7FeJdjJxFsU8liVRgYGEcz8gCXJ4475e5sPRgYGE9gtEXptQ2ApFTO14CBgXE2I/9Anq7lyWIbyqNDNwwMjIMYbUsgT9Ra6k6wxsDAOJXRXvzKU8Ocl5fKH1eCgYFxNKNdxOyCxc5G1IdxGBgYj2G05egsBcwbmcOAi4GBcQRjFijblK5tatbHdhgYGEczkgDXXuRaJ5H5F9v0EQMD41RGcomhbTdetSnF1QoMDIyjGfmh2E6ql5emeeX9yzZhYGAcxJgd+icz90PnBZctMDAwDmLs3yzLt6MteqPEFAMD42hGvqy2JZmE19nSox4sBgbGoYz8SVvc5r9tU8mt9gAGBsZtGbP2ZHt5Ij+SK9gYGBgPY7TtyTbsDhuT629hYGAcyniXY+dwbVbKRoEeAwPjaEYb5hJAEenL/wzFnREMDIzjGLMW4+ziRdu8LLYAAwPjAYw2UM7K3aIlWa4WAwMDI0/R2osXlx3PYWBgYJQXI9aBOE8ri5kYGBgPYCSlZn7ov5MszrYJAwPjbMasdEwak2tku4gLmpoYGBj3Y3wBK/RLqV5zUc0AAAAASUVORK5CYII=" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script src="/js/main.min.js?v=1.7.0"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.0" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
